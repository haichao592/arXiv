{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2108.12084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.",
          "link": "http://arxiv.org/abs/2108.12084",
          "publishedOn": "2021-09-14T07:20:13.633Z",
          "wordCount": 611,
          "title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.",
          "link": "http://arxiv.org/abs/2106.06381",
          "publishedOn": "2021-09-14T07:20:13.577Z",
          "wordCount": 617,
          "title": "Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n\nBy combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.",
          "link": "http://arxiv.org/abs/2104.07155",
          "publishedOn": "2021-09-14T07:20:12.766Z",
          "wordCount": null,
          "title": "Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>",
          "description": "Recent neural text-to-SQL models can effectively translate natural language\nquestions to corresponding SQL queries on unseen databases. Working mostly on\nthe Spider dataset, researchers have proposed increasingly sophisticated\nsolutions to the problem. Contrary to this trend, in this paper we focus on\nsimplifications. We begin by building DuoRAT, a re-implementation of the\nstate-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware\nor vanilla transformers as the building blocks. We perform several ablation\nexperiments using DuoRAT as the baseline model. Our experiments confirm the\nusefulness of some techniques and point out the redundancy of others, including\nstructural SQL features and features that link the question with the schema.",
          "link": "http://arxiv.org/abs/2010.11119",
          "publishedOn": "2021-09-14T07:20:12.626Z",
          "wordCount": null,
          "title": "DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskar_N/0/1/0/all/0/1\">Nitish Shirish Keskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Paraphrase generation has benefited extensively from recent progress in the\ndesigning of training objectives and model architectures. However, previous\nexplorations have largely focused on supervised methods, which require a large\namount of labeled data that is costly to collect. To address this drawback, we\nadopt a transfer learning approach and propose a training pipeline that enables\npre-trained language models to generate high-quality paraphrases in an\nunsupervised setting. Our recipe consists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dynamic Blocking (DB). To enforce a\nsurface form dissimilar from the input, whenever the language model emits a\ntoken contained in the source sequence, DB prevents the model from outputting\nthe subsequent source token for the next generation step. We show with\nautomatic and human evaluations that our approach achieves state-of-the-art\nperformance on both the Quora Question Pair (QQP) and the ParaNMT datasets and\nis robust to domain shift between the two datasets of distinct distributions.\nWe also demonstrate that our model transfers to paraphrasing in other languages\nwithout any additional finetuning.",
          "link": "http://arxiv.org/abs/2010.12885",
          "publishedOn": "2021-09-14T07:20:12.548Z",
          "wordCount": null,
          "title": "Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaohan Huang Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on eight multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.",
          "link": "http://arxiv.org/abs/2104.08692",
          "publishedOn": "2021-09-14T07:20:12.502Z",
          "wordCount": null,
          "title": "MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>",
          "description": "To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.",
          "link": "http://arxiv.org/abs/2104.08731",
          "publishedOn": "2021-09-14T07:20:12.500Z",
          "wordCount": null,
          "title": "Can NLI Models Verify QA Systems' Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines,\nboth (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for\nunseen targets. Finally, we perform an exhaustive analysis of the cross-domain\nresults, and we highlight the important factors influencing the model\nperformance.",
          "link": "http://arxiv.org/abs/2104.07467",
          "publishedOn": "2021-09-14T07:20:12.492Z",
          "wordCount": null,
          "title": "Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>",
          "description": "The goal of question answering (QA) is to answer any question. However, major\nQA datasets have skewed distributions over gender, profession, and nationality.\nDespite that skew, model accuracy analysis reveals little evidence that\naccuracy is lower for people based on gender or nationality; instead, there is\nmore variation on professions (question topic). But QA's lack of representation\ncould itself hide evidence of bias, necessitating QA datasets that better\nrepresent global diversity.",
          "link": "http://arxiv.org/abs/2104.07571",
          "publishedOn": "2021-09-14T07:20:12.475Z",
          "wordCount": null,
          "title": "Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liqiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma2_J/0/1/0/all/0/1\">Jun Ma2</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Luna Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gomez_P/0/1/0/all/0/1\">Pascual Martinez-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1\">Nasser Zalmout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>",
          "description": "Successful conversational search systems can present natural, adaptive and\ninteractive shopping experience for online shopping customers. However,\nbuilding such systems from scratch faces real word challenges from both\nimperfect product schema/knowledge and lack of training dialog data.In this\nwork we first propose ConvSearch, an end-to-end conversational search system\nthat deeply combines the dialog system with search. It leverages the text\nprofile to retrieve products, which is more robust against imperfect product\nschema/knowledge compared with using product attributes alone. We then address\nthe lack of data challenges by proposing an utterance transfer approach that\ngenerates dialogue utterances by using existing dialog from other domains, and\nleveraging the search behavior data from e-commerce retailer. With utterance\ntransfer, we introduce a new conversational search dataset for online shopping.\nExperiments show that our utterance transfer method can significantly improve\nthe availability of training dialogue data without crowd-sourcing, and the\nconversational search system significantly outperformed the best tested\nbaseline.",
          "link": "http://arxiv.org/abs/2109.05460",
          "publishedOn": "2021-09-14T07:20:12.451Z",
          "wordCount": null,
          "title": "End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Shengyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1\">Michele Tufano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shao Kun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.",
          "link": "http://arxiv.org/abs/2009.08366",
          "publishedOn": "2021-09-14T07:20:12.416Z",
          "wordCount": null,
          "title": "GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1\">Tillmann Miltzow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1\">Reinier F. Schmiermann</a>",
          "description": "A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with a domain $U \\subset \\mathbb{R}$. We engage in a\nsystematic study to classify CCSPs that are complete of the Existential Theory\nof the Reals, i.e., ER-complete. To define this class, we first consider the\nproblem ETR, which also stands for Existential Theory of the Reals. In an\ninstance of this problem we are given some sentence of the form $\\exists x_1,\n\\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where $\\Phi$ is a\nwell-formed quantifier-free formula consisting of the symbols $\\{0, 1, +,\n\\cdot, \\geq, >, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n\nWe restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical condition. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical condition) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.\n\nWe apply our findings to geometric packing and answer an open question by\nAbrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing\nconvex pieces into a square container under rotations and translations.",
          "link": "http://arxiv.org/abs/2106.02397",
          "publishedOn": "2021-09-14T07:20:12.380Z",
          "wordCount": null,
          "title": "On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>",
          "description": "We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.",
          "link": "http://arxiv.org/abs/2109.05611",
          "publishedOn": "2021-09-14T07:20:12.321Z",
          "wordCount": null,
          "title": "Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.03760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhiming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Si Wei</a>",
          "description": "Disentanglement is a problem in which multiple conversations occur in the\nsame channel simultaneously, and the listener should decide which utterance is\npart of the conversation he will respond to. We propose a new model, named\nDialogue BERT (DialBERT), which integrates local and global semantics in a\nsingle stream of messages to disentangle the conversations that mixed together.\nWe employ BERT to capture the matching information in each utterance pair at\nthe utterance-level, and use a BiLSTM to aggregate and incorporate the\ncontext-level information. With only a 3% increase in parameters, a 12%\nimprovement has been attained in comparison to BERT, based on the F1-Score. The\nmodel achieves a state-of-the-art result on the a new dataset proposed by IBM\nand surpasses previous work by a substantial margin.",
          "link": "http://arxiv.org/abs/2004.03760",
          "publishedOn": "2021-09-14T07:20:12.253Z",
          "wordCount": null,
          "title": "DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>",
          "description": "Discontinuous constituent parsers have always lagged behind continuous\napproaches in terms of accuracy and speed, as the presence of constituents with\ndiscontinuous yield introduces extra complexity to the task. However, a\ndiscontinuous tree can be converted into a continuous variant by reordering\ntokens. Based on that, we propose to reduce discontinuous parsing to a\ncontinuous problem, which can then be directly solved by any off-the-shelf\ncontinuous parser. To that end, we develop a Pointer Network capable of\naccurately generating the continuous token arrangement for a given input\nsentence and define a bijective function to recover the original order.\nExperiments on the main benchmarks with two continuous parsers prove that our\napproach is on par in accuracy with purely discontinuous state-of-the-art\nalgorithms, but considerably faster.",
          "link": "http://arxiv.org/abs/2104.06239",
          "publishedOn": "2021-09-14T07:20:12.248Z",
          "wordCount": null,
          "title": "Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1\">Ryan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "To audit the robustness of named entity recognition (NER) models, we propose\nRockNER, a simple yet effective method to create natural adversarial examples.\nSpecifically, at the entity level, we replace target entities with other\nentities of the same semantic class in Wikidata; at the context level, we use\npre-trained language models (e.g., BERT) to generate word substitutions.\nTogether, the two levels of attack produce natural adversarial examples that\nresult in a shifted distribution from the training data on which our target\nmodels have been trained. We apply the proposed method to the OntoNotes dataset\nand create a new benchmark named OntoRock for evaluating the robustness of\nexisting NER models via a systematic evaluation protocol. Our experiments and\nanalysis reveal that even the best model has a significant performance drop,\nand these models seem to memorize in-domain entity patterns instead of\nreasoning from the context. Our work also studies the effects of a few simple\ndata augmentation methods to improve the robustness of NER models.",
          "link": "http://arxiv.org/abs/2109.05620",
          "publishedOn": "2021-09-14T07:20:12.237Z",
          "wordCount": null,
          "title": "RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>",
          "description": "Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.",
          "link": "http://arxiv.org/abs/2012.14541",
          "publishedOn": "2021-09-14T07:20:12.237Z",
          "wordCount": null,
          "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yiming Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Post-hoc interpretation aims to explain a trained model and reveal how the\nmodel arrives at a decision. Though research on post-hoc interpretations has\ndeveloped rapidly, one growing pain in this field is the difficulty in\nevaluating interpretations. There are some crucial logic traps behind existing\nevaluation methods, which are ignored by most works. In this opinion piece, we\nsummarize four kinds evaluation methods and point out the corresponding logic\ntraps behind them. We argue that we should be clear about these traps rather\nthan ignore them and draw conclusions assertively.",
          "link": "http://arxiv.org/abs/2109.05463",
          "publishedOn": "2021-09-14T07:20:12.232Z",
          "wordCount": null,
          "title": "The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matena_M/0/1/0/all/0/1\">Michael Matena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkan_K/0/1/0/all/0/1\">Karishma Malkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_J/0/1/0/all/0/1\">Jake Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>",
          "description": "The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.",
          "link": "http://arxiv.org/abs/2102.11972",
          "publishedOn": "2021-09-14T07:20:12.232Z",
          "wordCount": null,
          "title": "Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>",
          "description": "This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.",
          "link": "http://arxiv.org/abs/2109.05602",
          "publishedOn": "2021-09-14T07:20:12.231Z",
          "wordCount": null,
          "title": "Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.12289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corbara_S/0/1/0/all/0/1\">Silvia Corbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavoni_M/0/1/0/all/0/1\">Mirko Tavoni</a>",
          "description": "We present and make available MedLatinEpi and MedLatinLit, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,\nrespectively, labelled by author; MedLatinEpi texts are of epistolary nature,\nwhile MedLatinLit texts consist of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification. Along with the datasets\nwe provide experimental results, obtained on these datasets, for the authorship\nverification task, i.e., the task of predicting whether a text of unknown\nauthorship was written by a candidate author or not. We also make available the\nsource code of the authorship verification system we have used, thus allowing\nour experiments to be reproduced, and to be used as baselines, by other\nresearchers. We also describe the application of the above authorship\nverification system, using these datasets as training data, for investigating\nthe authorship of two medieval epistles whose authorship has been disputed by\nscholars.",
          "link": "http://arxiv.org/abs/2006.12289",
          "publishedOn": "2021-09-14T07:20:12.231Z",
          "wordCount": null,
          "title": "MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing the number of unique tokens) is a key factor for improved\nperformance.",
          "link": "http://arxiv.org/abs/2012.02339",
          "publishedOn": "2021-09-14T07:20:12.229Z",
          "wordCount": null,
          "title": "Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Developing a unified multilingual model has long been a pursuit for machine\ntranslation. However, existing approaches suffer from performance degradation\n-- a single multilingual model is inferior to separately trained bilingual ones\non rich-resource languages. We conjecture that such a phenomenon is due to\ninterference caused by joint training with multiple languages. To accommodate\nthe issue, we propose CIAT, an adapted Transformer model with a small parameter\noverhead for multilingual machine translation. We evaluate CIAT on multiple\nbenchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that\nCIAT consistently outperforms strong multilingual baselines on 64 of total 66\nlanguage directions, 42 of which see above 0.5 BLEU improvement. Our code is\navailable at \\url{https://github.com/Yaoming95/CIAT}~.",
          "link": "http://arxiv.org/abs/2104.08154",
          "publishedOn": "2021-09-14T07:20:12.225Z",
          "wordCount": null,
          "title": "Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>",
          "description": "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.",
          "link": "http://arxiv.org/abs/2104.03416",
          "publishedOn": "2021-09-14T07:20:12.224Z",
          "wordCount": null,
          "title": "Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>",
          "description": "With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.",
          "link": "http://arxiv.org/abs/2005.00033",
          "publishedOn": "2021-09-14T07:20:12.149Z",
          "wordCount": null,
          "title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>",
          "description": "Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide a more in-depth analysis on the representation space of\nBERT in search for distinct and meaningful subspaces that can explain the\nreasons behind these probing results. Based on a set of probing tasks and with\nthe help of attribution methods we show that BERT tends to encode meaningful\nknowledge in specific token representations (which are often ignored in\nstandard classification setups), allowing the model to detect syntactic and\nsemantic abnormalities, and to distinctively separate grammatical number and\ntense subspaces.",
          "link": "http://arxiv.org/abs/2104.01477",
          "publishedOn": "2021-09-14T07:20:12.120Z",
          "wordCount": null,
          "title": "Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junwen Bai</a>",
          "description": "Building ASR models across many languages is a challenging multi-task\nlearning problem due to large variations and heavily unbalanced data. Existing\nwork has shown positive transfer from high resource to low resource languages.\nHowever, degradations on high resource languages are commonly observed due to\ninterference from the heterogeneous multilingual data and reduction in\nper-language capacity. We conduct a capacity study on a 15-language task, with\nthe amount of data per language varying from 7.6K to 53.5K hours. We adopt\nGShard [1] to efficiently scale up to 10B parameters. Empirically, we find that\n(1) scaling the number of model parameters is an effective way to solve the\ncapacity bottleneck - our 500M-param model already outperforms monolingual\nbaselines and scaling it to 1B and 10B brought further quality gains; (2)\nlarger models are not only more data efficient, but also more efficient in\nterms of training cost as measured in TPU days - the 1B-param model reaches the\nsame accuracy at 34% of training time as the 500M-param model; (3) given a\nfixed capacity budget, adding depth works better than width and large encoders\ndo better than large decoders; (4) with continuous training, they can be\nadapted to new languages and domains.",
          "link": "http://arxiv.org/abs/2104.14830",
          "publishedOn": "2021-09-14T07:20:12.118Z",
          "wordCount": null,
          "title": "Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.",
          "link": "http://arxiv.org/abs/2009.06206",
          "publishedOn": "2021-09-14T07:20:12.109Z",
          "wordCount": null,
          "title": "On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Antonio Camara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>",
          "description": "Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.",
          "link": "http://arxiv.org/abs/2104.07814",
          "publishedOn": "2021-09-14T07:20:12.096Z",
          "wordCount": null,
          "title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qiu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English->Chinese, English->Japanese, Japanese->English and\nEnglish->German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English->Chinese, English->Japanese,\nJapanese->English and English->German, respectively. The BLEU scores of\nEnglish->Chinese, English->Japanese and Japanese->English are the highest among\nall submissions, and that of English->German is the highest among all\nconstrained submissions.",
          "link": "http://arxiv.org/abs/2108.02401",
          "publishedOn": "2021-09-14T07:20:12.095Z",
          "wordCount": null,
          "title": "WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Representation learning for text via pretraining a language model on a large\ncorpus has become a standard starting point for building NLP systems. This\napproach stands in contrast to autoencoders, also trained on raw text, but with\nthe objective of learning to encode each input as a vector that allows full\nreconstruction. Autoencoders are attractive because of their latent space\nstructure and generative properties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained, frozen transformer language\nmodel. We adapt the masked language modeling objective as a generative,\ndenoising one, while only training a sentence bottleneck and a single-layer\nmodified transformer decoder. We demonstrate that the sentence representations\ndiscovered by our model achieve better quality than previous methods that\nextract representations from pretrained transformers on text similarity tasks,\nstyle transfer (an example of controlled generation), and single-sentence\nclassification tasks in the GLUE benchmark, while using fewer parameters than\nlarge pretrained models.",
          "link": "http://arxiv.org/abs/2109.00055",
          "publishedOn": "2021-09-14T07:20:12.078Z",
          "wordCount": null,
          "title": "Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lekkas_A/0/1/0/all/0/1\">Andrea Lekkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.",
          "link": "http://arxiv.org/abs/2012.05776",
          "publishedOn": "2021-09-14T07:20:12.077Z",
          "wordCount": null,
          "title": "Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianran Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jianyun Nie</a>",
          "description": "The ability of a dialog system to express consistent language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. Although previous studies have demonstrated that style transfer\nis feasible with a large amount of parallel data, it is often impossible to\ncollect such data for different styles. In this paper, instead of manually\nconstructing conversation data with a certain style, we propose a flexible\nframework that adapts a generic retrieval-based dialogue system to mimic the\nlanguage style of a specified persona without any parallel data. Our approach\nis based on automatic generation of stylized data by learning the usage of\njargon, and then rewriting the generic conversations to a stylized one by\nincorporating the jargon. In experiments we implemented dialogue systems with\nfive distinct language styles, and the result shows our framework significantly\noutperforms baselines in terms of the average score of responses' relevance and\nstyle degree, and content diversity. A/B testing on a commercial chatbot shows\nthat users are more satisfied with our system. This study demonstrates the\nfeasibility of building stylistic dialogue systems by simple data augmentation.",
          "link": "http://arxiv.org/abs/2109.05477",
          "publishedOn": "2021-09-14T07:20:12.062Z",
          "wordCount": null,
          "title": "Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingchao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Heng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liaosa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Existing system dealing with online complaint provides a final decision\nwithout explanations. We propose to analyse the complaint text of internet\nfraud in a fine-grained manner. Considering the complaint text includes\nmultiple clauses with various functions, we propose to identify the role of\neach clause and classify them into different types of fraud element. We\nconstruct a large labeled dataset originated from a real finance service\nplatform. We build an element identification model on top of BERT and propose\nadditional two modules to utilize the context of complaint text for better\nelement label classification, namely, global context encoder and label refiner.\nExperimental results show the effectiveness of our model.",
          "link": "http://arxiv.org/abs/2108.08676",
          "publishedOn": "2021-09-14T07:20:12.057Z",
          "wordCount": null,
          "title": "Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1\">Feng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>",
          "description": "Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.",
          "link": "http://arxiv.org/abs/2010.03084",
          "publishedOn": "2021-09-14T07:20:11.951Z",
          "wordCount": null,
          "title": "Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Rohan Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>",
          "description": "Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic, which has demonstrated competitive performance in many\nNatural Language Processing (NLP) tasks, including sentiment analysis, text\nclassification, and Word Sense Disambiguation. To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the performance and interpretability of TM. We\nachieve this by extracting semantically related words from pre-trained word\nrepresentations as input features to the TM. Our experiments show that the\naccuracy of the proposed approach is significantly higher than the previous\nBOW-based TM, reaching the level of DNN-based models.",
          "link": "http://arxiv.org/abs/2104.06901",
          "publishedOn": "2021-09-14T07:20:11.951Z",
          "wordCount": null,
          "title": "Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>",
          "description": "Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model's robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models'\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .",
          "link": "http://arxiv.org/abs/2109.00544",
          "publishedOn": "2021-09-14T07:20:11.931Z",
          "wordCount": null,
          "title": "Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>",
          "description": "Recent advances in text autoencoders have significantly improved the quality\nof the latent space, which enables models to generate grammatical and\nconsistent text from aggregated latent vectors. As a successful application of\nthis property, unsupervised opinion summarization models generate a summary by\ndecoding the aggregated latent vectors of inputs. More specifically, they\nperform the aggregation via simple average. However, little is known about how\nthe vector aggregation step affects the generation quality. In this study, we\nrevisit the commonly used simple average approach by examining the latent space\nand generated summaries. We found that text autoencoders tend to generate\noverly generic summaries from simply averaged latent vectors due to an\nunexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we\nrefer to as summary vector degeneration. To overcome this issue, we develop a\nframework Coop, which searches input combinations for the latent vector\naggregation using input-output word overlap. Experimental results show that\nCoop successfully alleviates the summary vector degeneration issue and\nestablishes new state-of-the-art performance on two opinion summarization\nbenchmarks. Code is available at \\url{https://github.com/megagonlabs/coop}.",
          "link": "http://arxiv.org/abs/2104.01371",
          "publishedOn": "2021-09-14T07:20:11.911Z",
          "wordCount": null,
          "title": "Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baylon_J/0/1/0/all/0/1\">Julia Baylon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "Understanding how events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus mostly on event arguments or temporal relations. While these\ntasks partially evaluate machines' ability of narrative understanding,\nhuman-like reading comprehension requires the capability to process event-based\ninformation beyond arguments and temporal reasoning. For example, to understand\ncausality between events, we need to infer motivation or purpose; to establish\nevent hierarchy, we need to understand the composition of events. To facilitate\nthese tasks, we introduce ESTER, a comprehensive machine reading comprehension\n(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages\nnatural language queries to reason about the five most common event semantic\nrelations, provides more than 6K questions and captures 10.1K event relation\npairs. Experimental results show that the current SOTA systems achieve 22.1%,\n63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,\nwhich are all significantly below human performances (36.0%, 79.6%, 100%\nrespectively), highlighting our dataset as a challenging benchmark.",
          "link": "http://arxiv.org/abs/2104.08350",
          "publishedOn": "2021-09-14T07:20:11.907Z",
          "wordCount": null,
          "title": "ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingwei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>",
          "description": "Detecting events and their evolution through time is a crucial task in\nnatural language understanding. Recent neural approaches to event temporal\nrelation extraction typically map events to embeddings in the Euclidean space\nand train a classifier to detect temporal relations between event pairs.\nHowever, embeddings in the Euclidean space cannot capture richer asymmetric\nrelations such as event temporal relations. We thus propose to embed events\ninto hyperbolic spaces, which are intrinsically oriented at modeling\nhierarchical structures. We introduce two approaches to encode events and their\ntemporal relations in hyperbolic spaces. One approach leverages hyperbolic\nembeddings to directly infer event relations through simple geometrical\noperations. In the second one, we devise an end-to-end architecture composed of\nhyperbolic neural units tailored for the temporal relation extraction task.\nThorough experimental assessments on widely used datasets have shown the\nbenefits of revisiting the tasks on a different geometrical space, resulting in\nstate-of-the-art performance on several standard metrics. Finally, the ablation\nstudy and several qualitative analyses highlighted the rich event semantics\nimplicitly encoded into hyperbolic spaces.",
          "link": "http://arxiv.org/abs/2109.05527",
          "publishedOn": "2021-09-14T07:20:11.894Z",
          "wordCount": null,
          "title": "Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>",
          "description": "In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.",
          "link": "http://arxiv.org/abs/2102.00225",
          "publishedOn": "2021-09-14T07:20:11.888Z",
          "wordCount": null,
          "title": "Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>",
          "description": "End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.",
          "link": "http://arxiv.org/abs/2011.11715",
          "publishedOn": "2021-09-14T07:20:11.814Z",
          "wordCount": null,
          "title": "Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.",
          "link": "http://arxiv.org/abs/2012.15283",
          "publishedOn": "2021-09-14T07:20:11.653Z",
          "wordCount": null,
          "title": "ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xinnan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>",
          "description": "Single-table text-to-SQL aims to transform a natural language question into a\nSQL query according to one single table. Recent work has made promising\nprogress on this task by pre-trained language models and a multi-submodule\nframework. However, zero-shot table, that is, the invisible table in the\ntraining set, is currently the most critical bottleneck restricting the\napplication of existing approaches to real-world scenarios. Although some work\nhas utilized auxiliary tasks to help handle zero-shot tables, expensive extra\nmanual annotation limits their practicality. In this paper, we propose a new\napproach for the zero-shot text-to-SQL task which does not rely on any\nadditional manual annotations. Our approach consists of two parts. First, we\npropose a new model that leverages the abundant information of table content to\nhelp establish the mapping between questions and zero-shot tables. Further, we\npropose a simple but efficient meta-learning strategy to train our model. The\nstrategy utilizes the two-step gradient update to force the model to learn a\ngeneralization ability towards zero-shot tables. We conduct extensive\nexperiments on a public open-domain text-to-SQL dataset WikiSQL and a\ndomain-specific dataset ESQL. Compared to existing approaches using the same\npre-trained model, our approach achieves significant improvements on both\ndatasets. Compared to the larger pre-trained model and the tabular-specific\npre-trained model, our approach is still competitive. More importantly, on the\nzero-shot subsets of both the datasets, our approach further increases the\nimprovements.",
          "link": "http://arxiv.org/abs/2109.05395",
          "publishedOn": "2021-09-14T07:20:11.482Z",
          "wordCount": null,
          "title": "Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Amos Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.",
          "link": "http://arxiv.org/abs/2104.08727",
          "publishedOn": "2021-09-14T07:20:11.413Z",
          "wordCount": null,
          "title": "GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browne_M/0/1/0/all/0/1\">Mitchell Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_L/0/1/0/all/0/1\">Lily Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_F/0/1/0/all/0/1\">Fiona Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_J/0/1/0/all/0/1\">John Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_D/0/1/0/all/0/1\">David Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1\">Myfany Turpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_M/0/1/0/all/0/1\">Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilmoth_S/0/1/0/all/0/1\">Sasha Wilmoth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>",
          "description": "Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.",
          "link": "http://arxiv.org/abs/2103.14583",
          "publishedOn": "2021-09-14T07:20:11.380Z",
          "wordCount": null,
          "title": "Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.11830",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>",
          "description": "Dialogue models trained on human conversations inadvertently learn to\ngenerate toxic responses. In addition to producing explicitly offensive\nutterances, these models can also implicitly insult a group or individual by\naligning themselves with an offensive statement. To better understand the\ndynamics of contextually offensive language, we investigate the stance of\ndialogue model responses in offensive Reddit conversations. Specifically, we\ncreate ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model\nresponses labeled with offensive language and stance. Our analysis reveals that\n42% of human responses agree with toxic comments, whereas only 13% agree with\nsafe comments. This undesirable behavior is learned by neural dialogue models,\nsuch as DialoGPT, which we show are two times more likely to agree with\noffensive comments. To enable automatic detection of offensive language, we\nfine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for\noffensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the\neffectiveness of controllable text generation (CTG) methods to mitigate the\ntendency of neural dialogue models to agree with offensive comments. Compared\nto the baseline, our best CTG model achieves a 19% reduction in agreement with\noffensive comments and produces 29% fewer offensive replies. Our work\nhighlights the need for further efforts to characterize and analyze\ninappropriate behavior in dialogue models, in order to help make them safer.\nOur code and corpus are available at https://github.com/abaheti95/ToxiChat .",
          "link": "http://arxiv.org/abs/2108.11830",
          "publishedOn": "2021-09-14T07:20:11.371Z",
          "wordCount": null,
          "title": "Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haijun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>",
          "description": "Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.",
          "link": "http://arxiv.org/abs/2109.05523",
          "publishedOn": "2021-09-14T07:20:11.278Z",
          "wordCount": null,
          "title": "Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.",
          "link": "http://arxiv.org/abs/2007.00049",
          "publishedOn": "2021-09-14T07:20:11.264Z",
          "wordCount": null,
          "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05427",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>",
          "description": "Fine-grained classification involves dealing with datasets with larger number\nof classes with subtle differences between them. Guiding the model to focus on\ndifferentiating dimensions between these commonly confusable classes is key to\nimproving performance on fine-grained tasks. In this work, we analyse the\ncontrastive fine-tuning of pre-trained language models on two fine-grained text\nclassification tasks, emotion classification and sentiment analysis. We\nadaptively embed class relationships into a contrastive objective function to\nhelp differently weigh the positives and negatives, and in particular,\nweighting closely confusable negatives more than less similar negative\nexamples. We find that Label-aware Contrastive Loss outperforms previous\ncontrastive methods, in the presence of larger number and/or more confusable\nclasses, and helps models to produce output distributions that are more\ndifferentiated.",
          "link": "http://arxiv.org/abs/2109.05427",
          "publishedOn": "2021-09-14T07:20:11.182Z",
          "wordCount": null,
          "title": "Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arjmand_M/0/1/0/all/0/1\">Mehdi Arjmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1\">Mohammad Javad Dousti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>",
          "description": "Multimodal language analysis is a burgeoning field of NLP that aims to\nsimultaneously model a speaker's words, acoustical annotations, and facial\nexpressions. In this area, lexicon features usually outperform other modalities\nbecause they are pre-trained on large corpora via Transformer-based models.\nDespite their strong performance, training a new self-supervised learning (SSL)\nTransformer on any modality is not usually attainable due to insufficient data,\nwhich is the case in multimodal language learning. This work proposes a\nTransformer-Based Speech-Prefixed Language Model called TEASEL to approach the\nmentioned constraints without training a complete Transformer model. TEASEL\nmodel includes speech modality as a dynamic prefix besides the textual modality\ncompared to a conventional language model. This method exploits a conventional\npre-trained language model as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.\nExtensive experiments show that our model outperforms unimodal baseline\nlanguage models by 4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%\nsmaller than the SoTA model.",
          "link": "http://arxiv.org/abs/2109.05522",
          "publishedOn": "2021-09-14T07:20:10.926Z",
          "wordCount": 616,
          "title": "TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05473",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2109.05473",
          "publishedOn": "2021-09-14T07:20:10.850Z",
          "wordCount": 570,
          "title": "Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05494",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Anoop C S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_P/0/1/0/all/0/1\">Prathosh A P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">A G Ramakrishnan</a>",
          "description": "Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.",
          "link": "http://arxiv.org/abs/2109.05494",
          "publishedOn": "2021-09-14T07:20:10.697Z",
          "wordCount": 661,
          "title": "Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>",
          "description": "Cross-attention is an important component of neural machine translation\n(NMT), which is always realized by dot-product attention in previous methods.\nHowever, dot-product attention only considers the pair-wise correlation between\nwords, resulting in dispersion when dealing with long sentences and neglect of\nsource neighboring relationships. Inspired by linguistics, the above issues are\ncaused by ignoring a type of cross-attention, called concentrated attention,\nwhich focuses on several central words and then spreads around them. In this\nwork, we apply Gaussian Mixture Model (GMM) to model the concentrated attention\nin cross-attention. Experiments and analyses we conducted on three datasets\nshow that the proposed method outperforms the baseline and has significant\nimprovement on alignment quality, N-gram accuracy, and long sentence\ntranslation.",
          "link": "http://arxiv.org/abs/2109.05244",
          "publishedOn": "2021-09-14T07:20:10.625Z",
          "wordCount": 576,
          "title": "Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>",
          "description": "Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.",
          "link": "http://arxiv.org/abs/2109.05238",
          "publishedOn": "2021-09-14T07:20:10.543Z",
          "wordCount": 602,
          "title": "Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>",
          "description": "Extracting relations across large text spans has been relatively\nunderexplored in NLP, but it is particularly important for high-value domains\nsuch as biomedicine, where obtaining high recall of the latest findings is\ncrucial for practical applications. Compared to conventional information\nextraction confined to short text spans, document-level relation extraction\nfaces additional challenges in both inference and learning. Given longer text\nspans, state-of-the-art neural architectures are less effective and\ntask-specific self-supervision such as distant supervision becomes very noisy.\nIn this paper, we propose decomposing document-level relation extraction into\nrelation detection and argument resolution, taking inspiration from Davidsonian\nsemantics. This enables us to incorporate explicit discourse modeling and\nleverage modular self-supervision for each sub-problem, which is less\nnoise-prone and can be further refined end-to-end via variational EM. We\nconduct a thorough evaluation in biomedical machine reading for precision\noncology, where cross-paragraph relation mentions are prevalent. Our method\noutperforms prior state of the art, such as multi-scale learning and graph\nneural networks, by over 20 absolute F1 points. The gain is particularly\npronounced among the most challenging relation instances whose arguments never\nco-occur in a paragraph.",
          "link": "http://arxiv.org/abs/2109.05362",
          "publishedOn": "2021-09-14T07:20:10.531Z",
          "wordCount": 627,
          "title": "Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Testoni_A/0/1/0/all/0/1\">Alberto Testoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_R/0/1/0/all/0/1\">Raffaella Bernardi</a>",
          "description": "Generating goal-oriented questions in Visual Dialogue tasks is a challenging\nand long-standing problem. State-Of-The-Art systems are shown to generate\nquestions that, although grammatically correct, often lack an effective\nstrategy and sound unnatural to humans. Inspired by the cognitive literature on\ninformation search and cross-situational word learning, we design Confirm-it, a\nmodel based on a beam search re-ranking algorithm that guides an effective\ngoal-oriented strategy by asking questions that confirm the model's conjecture\nabout the referent. We take the GuessWhat?! game as a case-study. We show that\ndialogues generated by Confirm-it are more natural and effective than beam\nsearch decoding without re-ranking.",
          "link": "http://arxiv.org/abs/2109.05312",
          "publishedOn": "2021-09-14T07:20:10.524Z",
          "wordCount": 549,
          "title": "Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>",
          "description": "Framing has significant but subtle effects on public opinion and policy. We\npropose an NLP framework to measure entity-centric frames. We use it to\nunderstand media coverage on police violence in the United States in a new\nPolice Violence Frames Corpus of 82k news articles spanning 7k police killings.\nOur work uncovers more than a dozen framing devices and reveals significant\ndifferences in the way liberal and conservative news sources frame both the\nissue of police violence and the entities involved. Conservative sources\nemphasize when the victim is armed or attacking an officer and are more likely\nto mention the victim's criminal record. Liberal sources focus more on the\nunderlying systemic injustice, highlighting the victim's race and that they\nwere unarmed. We discover temporary spikes in these injustice frames near\nhigh-profile shooting events, and finally, we show protest volume correlates\nwith and precedes media framing decisions.",
          "link": "http://arxiv.org/abs/2109.05325",
          "publishedOn": "2021-09-14T07:20:10.502Z",
          "wordCount": 593,
          "title": "To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Aadit Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>",
          "description": "Enthymemes are defined as arguments where a premise or conclusion is left\nimplicit. We tackle the task of generating the implicit premise in an\nenthymeme, which requires not only an understanding of the stated conclusion\nand premise but also additional inferences that could depend on commonsense\nknowledge. The largest available dataset for enthymemes (Habernal et al., 2018)\nconsists of 1.7k samples, which is not large enough to train a neural text\ngeneration model. To address this issue, we take advantage of a similar task\nand dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).\nHowever, we show that simply using a state-of-the-art seq2seq model fine-tuned\non this data might not generate meaningful implicit premises associated with\nthe given enthymemes. We demonstrate that encoding discourse-aware commonsense\nduring fine-tuning improves the quality of the generated implicit premises and\noutperforms all other baselines both in automatic and human evaluations on\nthree different datasets.",
          "link": "http://arxiv.org/abs/2109.05358",
          "publishedOn": "2021-09-14T07:20:10.494Z",
          "wordCount": 595,
          "title": "Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>",
          "description": "Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.",
          "link": "http://arxiv.org/abs/2109.05433",
          "publishedOn": "2021-09-14T07:20:10.487Z",
          "wordCount": 602,
          "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.",
          "link": "http://arxiv.org/abs/2109.05281",
          "publishedOn": "2021-09-14T07:20:10.481Z",
          "wordCount": 630,
          "title": "COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain\nmanually annotated event-centric mentions of events and entities that form\ncoreference chains with identity relations. ECB+ is a state-of-the-art CDCR\ndataset that focuses on the resolution of events and their descriptive\nattributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that\nannotates coreference chains of both events and entities with a strong variance\nof word choice and more loosely-related coreference anaphora, e.g., bridging or\nnear-identity relations. In this paper, we qualitatively and quantitatively\ncompare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We\npropose a phrasing diversity metric (PD) that compares lexical diversity within\ncoreference chains on a more detailed level than previously proposed metric,\ne.g., a number of unique lemmas. We discuss the different tasks that both CDCR\ndatasets create, i.e., lexical disambiguation and lexical diversity challenges,\nand propose a direction for further CDCR evaluation.",
          "link": "http://arxiv.org/abs/2109.05250",
          "publishedOn": "2021-09-14T07:20:10.473Z",
          "wordCount": 591,
          "title": "Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Can pre-trained BERT for one language and GPT for another be glued together\nto translate texts? Self-supervised training using only monolingual data has\nled to the success of pre-trained (masked) language models in many NLP tasks.\nHowever, directly connecting BERT as an encoder and GPT as a decoder can be\nchallenging in machine translation, for GPT-like models lack a cross-attention\ncomponent that is needed in seq2seq decoders. In this paper, we propose\nGraformer to graft separately pre-trained (masked) language models for machine\ntranslation. With monolingual data for pre-training and parallel data for\ngrafting training, we maximally take advantage of the usage of both types of\ndata. Experiments on 60 directions show that our method achieves average\nimprovements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with\nthe multilingual Transformer of the same size.",
          "link": "http://arxiv.org/abs/2109.05256",
          "publishedOn": "2021-09-14T07:20:10.448Z",
          "wordCount": 580,
          "title": "Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1\">Pengda Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>",
          "description": "Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.",
          "link": "http://arxiv.org/abs/2109.05406",
          "publishedOn": "2021-09-14T07:20:10.433Z",
          "wordCount": 656,
          "title": "Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05339",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Chang_H/0/1/0/all/0/1\">Hsing-Yin Chang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Slaughter_E/0/1/0/all/0/1\">Elliott Slaughter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mirchandaney_S/0/1/0/all/0/1\">Seema Mirchandaney</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Donatelli_J/0/1/0/all/0/1\">Jeffrey Donatelli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoon_C/0/1/0/all/0/1\">Chun Hong Yoon</a>",
          "description": "The Linac Coherent Light Source (LCLS) is an X- ray free electron laser\n(XFEL) facility enabling the study of the structure and dynamics of single\nmacromolecules. A major upgrade will bring the repetition rate of the X-ray\nsource from 120 to 1 million pulses per second. Exascale high performance\ncomputing (HPC) capabilities will be required to process the corresponding data\nrates. We present SpiniFEL, an application used for structure determination of\nproteins from single-particle imaging (SPI) experiments. An emerging technique\nfor imaging individual proteins and other large molecular complexes by\noutrunning radiation damage, SPI breaks free from the need for crystallization\n(which is difficult for some proteins) and allows for imaging molecular\ndynamics at near ambient conditions. SpiniFEL is being developed to run on\nsupercomputers in near real-time while an experiment is taking place, so that\nthe feedback about the data can guide the data collection strategy. We describe\nhere how we reformulated the mathematical framework for parallelizable\nimplementation and accelerated the most compute intensive parts of the\napplication. We also describe the use of Pygion, a Python interface for the\nLegion task-based programming model and compare to our existing MPI+GPU\nimplementation.",
          "link": "http://arxiv.org/abs/2109.05339",
          "publishedOn": "2021-09-14T07:20:09.818Z",
          "wordCount": null,
          "title": "Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>",
          "description": "A flaw in QA evaluation is that annotations often only provide one gold\nanswer. Thus, model predictions semantically equivalent to the answer but\nsuperficially different are considered incorrect. This work explores mining\nalias entities from knowledge bases and using them as additional gold answers\n(i.e., equivalent answers). We incorporate answers for two settings: evaluation\nwith additional answers and model training with equivalent answers. We analyse\nthree QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion\nincreases the exact match score on all datasets for evaluation, while\nincorporating it helps model training over real-world datasets. We ensure the\nadditional answers are valid through a human post hoc evaluation.",
          "link": "http://arxiv.org/abs/2109.05289",
          "publishedOn": "2021-09-14T07:20:09.621Z",
          "wordCount": 554,
          "title": "What's in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>",
          "description": "When reading a literary piece, readers often make inferences about various\ncharacters' roles, personalities, relationships, intents, actions, etc. While\nhumans can readily draw upon their past experiences to build such a\ncharacter-centric view of the narrative, understanding characters in narratives\ncan be a challenging task for machines. To encourage research in this field of\ncharacter-centric narrative understanding, we present LiSCU -- a new dataset of\nliterary pieces and their summaries paired with descriptions of characters that\nappear in them. We also introduce two new tasks on LiSCU: Character\nIdentification and Character Description Generation. Our experiments with\nseveral pre-trained language models adapted for these tasks demonstrate that\nthere is a need for better models of narrative comprehension.",
          "link": "http://arxiv.org/abs/2109.05438",
          "publishedOn": "2021-09-14T07:20:09.602Z",
          "wordCount": 576,
          "title": "\"Let Your Characters Tell Their Story\": A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05317",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ahrens_M/0/1/0/all/0/1\">Maximilian Ahrens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashwin_J/0/1/0/all/0/1\">Julian Ashwin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calliess_J/0/1/0/all/0/1\">Jan-Peter Calliess</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>",
          "description": "Causal inference using observational text data is becoming increasingly\npopular in many research areas. This paper presents the Bayesian Topic\nRegression (BTR) model that uses both text and numerical information to model\nan outcome variable. It allows estimation of both discrete and continuous\ntreatment effects. Furthermore, it allows for the inclusion of additional\nnumerical confounding factors next to text data. To this end, we combine a\nsupervised Bayesian topic model with a Bayesian regression framework and\nperform supervised representation learning for the text features jointly with\nthe regression parameter training, respecting the Frisch-Waugh-Lovell theorem.\nOur paper makes two main contributions. First, we provide a regression\nframework that allows causal inference in settings when both text and numerical\nconfounders are of relevance. We show with synthetic and semi-synthetic\ndatasets that our joint approach recovers ground truth with lower bias than any\nbenchmark model, when text and numerical features are correlated. Second,\nexperiments on two real-world datasets demonstrate that a joint and supervised\nlearning strategy also yields superior prediction results compared to\nstrategies that estimate regression weights for text and non-text features\nseparately, being even competitive with more complex deep neural networks.",
          "link": "http://arxiv.org/abs/2109.05317",
          "publishedOn": "2021-09-14T07:20:09.595Z",
          "wordCount": 643,
          "title": "Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>",
          "description": "Although pre-training models have achieved great success in dialogue\ngeneration, their performance drops dramatically when the input contains an\nentity that does not appear in pre-training and fine-tuning datasets (unseen\nentity). To address this issue, existing methods leverage an external knowledge\nbase to generate appropriate responses. In real-world scenario, the entity may\nnot be included by the knowledge base or suffer from the precision of knowledge\nretrieval. To deal with this problem, instead of introducing knowledge base as\nthe input, we force the model to learn a better semantic representation by\npredicting the information in the knowledge base, only based on the input\ncontext. Specifically, with the help of a knowledge base, we introduce two\nauxiliary training objectives: 1) Interpret Masked Word, which conjectures the\nmeaning of the masked entity given the context; 2) Hypernym Generation, which\npredicts the hypernym of the entity based on the context. Experiment results on\ntwo dialogue corpus verify the effectiveness of our methods under both\nknowledge available and unavailable settings.",
          "link": "http://arxiv.org/abs/2109.05487",
          "publishedOn": "2021-09-14T07:20:09.588Z",
          "wordCount": 615,
          "title": "Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Arif Ul Alam</a>",
          "description": "We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task\nLearning (MSNF-MTCL) to predict 5 important students' retention risks: future\ndropout, next semester dropout, type of dropout, duration of dropout and cause\nof dropout. First, we develop a general purpose multi-modal neural fusion\nnetwork model MSNF for learning students' academic information representation\nby fusing spatial and temporal unstructured advising notes with spatiotemporal\nstructured data. MSNF combines a Bidirectional Encoder Representations from\nTransformers (BERT)-based document embedding framework to represent each\nadvising note, Long-Short Term Memory (LSTM) network to model temporal advising\nnote embeddings, LSTM network to model students' temporal performance variables\nand students' static demographics altogether. The final fused representation\nfrom MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model\ntowards building MSNF-MTCL for predicting 5 student retention risks. We\nevaluate MSNFMTCL on a large educational database consists of 36,445 college\nstudents over 18 years period of time that provides promising performances\ncomparing with the nearest state-of-art models. Additionally, we test the\nfairness of such model given the existence of biases.",
          "link": "http://arxiv.org/abs/2109.05178",
          "publishedOn": "2021-09-14T07:20:09.581Z",
          "wordCount": 631,
          "title": "College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05168",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaochen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>",
          "description": "Large pre-trained language models (PLMs) have led to great success on various\ncommonsense question answering (QA) tasks in an end-to-end fashion. However,\nlittle attention has been paid to what commonsense knowledge is needed to\ndeeply characterize these QA tasks. In this work, we proposed to categorize the\nsemantics needed for these tasks using the SocialIQA as an example. Building\nupon our labeled social knowledge categories dataset on top of SocialIQA, we\nfurther train neural QA models to incorporate such social knowledge categories\nand relation information from a knowledge base. Unlike previous work, we\nobserve our models with semantic categorizations of social knowledge can\nachieve comparable performance with a relatively simple model and smaller size\ncompared to other complex approaches.",
          "link": "http://arxiv.org/abs/2109.05168",
          "publishedOn": "2021-09-14T07:20:09.573Z",
          "wordCount": 572,
          "title": "Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_V/0/1/0/all/0/1\">Vinit Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>",
          "description": "In order to preserve word-order information in a non-autoregressive setting,\ntransformer architectures tend to include positional knowledge, by (for\ninstance) adding positional encodings to token embeddings. Several\nmodifications have been proposed over the sinusoidal positional encodings used\nin the original transformer architecture; these include, for instance,\nseparating position encodings and token embeddings, or directly modifying\nattention weights based on the distance between word pairs. We first show that\nsurprisingly, while these modifications tend to improve monolingual language\nmodels, none of them result in better multilingual language models. We then\nanswer why that is: Sinusoidal encodings were explicitly designed to facilitate\ncompositionality by allowing linear projections over arbitrary time steps.\nHigher variances in multilingual training distributions requires higher\ncompression, in which case, compositionality becomes indispensable. Learned\nabsolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal\nembeddings in multilingual settings, but more complex positional encoding\narchitectures lack the inductive bias to effectively learn compositionality and\ncross-lingual alignment. In other words, while sinusoidal positional encodings\nwere originally designed for monolingual applications, they are particularly\nuseful in multilingual language models.",
          "link": "http://arxiv.org/abs/2109.05388",
          "publishedOn": "2021-09-14T07:20:09.565Z",
          "wordCount": 614,
          "title": "The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "This paper investigates continual learning for semantic parsing. In this\nsetting, a neural semantic parser learns tasks sequentially without accessing\nfull training data from previous tasks. Direct application of the SOTA\ncontinual learning algorithms to this problem fails to achieve comparable\nperformance with re-training models with all seen tasks because they have not\nconsidered the special properties of structured outputs yielded by semantic\nparsers. Therefore, we propose TotalRecall, a continual learning method\ndesigned for neural semantic parsers from two aspects: i) a sampling method for\nmemory replay that diversifies logical form templates and balances\ndistributions of parse actions in a memory; ii) a two-stage training method\nthat significantly improves generalization capability of the parsers across\ntasks. We conduct extensive experiments to study the research problems involved\nin continual semantic parsing and demonstrate that a neural semantic parser\ntrained with TotalRecall achieves superior performance than the one trained\ndirectly with the SOTA continual learning algorithms and achieve a 3-6 times\nspeedup compared to re-training from scratch. Code and datasets are available\nat: https://github.com/zhuang-li/cl_nsp.",
          "link": "http://arxiv.org/abs/2109.05186",
          "publishedOn": "2021-09-14T07:20:09.558Z",
          "wordCount": 624,
          "title": "Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sovrano_F/0/1/0/all/0/1\">Francesco Sovrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitali_F/0/1/0/all/0/1\">Fabio Vitali</a>",
          "description": "Numerous government initiatives (e.g. the EU with GDPR) are coming to the\nconclusion that the increasing complexity of modern software systems must be\ncontrasted with some Rights to Explanation and metrics for the Impact\nAssessment of these tools, that allow humans to understand and oversee the\noutput of Automated Decision Making systems. Explainable AI was born as a\npathway to allow humans to explore and understand the inner working of complex\nsystems. But establishing what is an explanation and objectively evaluating\nexplainability, are not trivial tasks. With this paper, we present a new\nmodel-agnostic metric to measure the Degree of eXplainability of correct\ninformation in an objective way, exploiting a specific model from Ordinary\nLanguage Philosophy called the Achinstein's Theory of Explanations. In order to\nunderstand whether this metric is actually behaving as explainability is\nexpected to, we designed a few experiments and a user-study on two realistic\nAI-based systems for healthcare and finance, involving famous AI technology\nincluding Artificial Neural Networks and TreeSHAP. The results we obtained are\nvery encouraging, suggesting that our proposed metric for measuring the Degree\nof eXplainability is robust on several scenarios and it can be eventually\nexploited for a lawful Impact Assessment of an Automated Decision Making\nsystem.",
          "link": "http://arxiv.org/abs/2109.05327",
          "publishedOn": "2021-09-14T07:20:09.538Z",
          "wordCount": 665,
          "title": "An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "We present a grounded neural dialogue model that successfully collaborates\nwith people in a partially-observable reference game. We focus on a setting\nwhere two agents each observe an overlapping part of a world context and need\nto identify and agree on some object they share. Therefore, the agents should\npool their information and communicate pragmatically to solve the task. Our\ndialogue agent accurately grounds referents from the partner's utterances using\na structured reference resolver, conditions on these referents using a\nrecurrent memory, and uses a pragmatic generation procedure to ensure the\npartner can resolve the references the agent produces. We evaluate on the\nOneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving\na number of dots arranged on a board with continuously varying positions,\nsizes, and shades. Our agent substantially outperforms the previous state of\nthe art for the task, obtaining a 20% relative improvement in successful task\ncompletion in self-play evaluations and a 50% relative improvement in success\nin human evaluations.",
          "link": "http://arxiv.org/abs/2109.05042",
          "publishedOn": "2021-09-14T07:20:09.531Z",
          "wordCount": 602,
          "title": "Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.",
          "link": "http://arxiv.org/abs/2109.05125",
          "publishedOn": "2021-09-14T07:20:09.524Z",
          "wordCount": 621,
          "title": "MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abusing social entities. Detecting such harmful memes is\nchallenging as they can be highly satirical and cryptic. Moreover, while\nprevious work has focused on specific aspects of memes such as hate speech and\npropaganda, there has been little work on harm in general, and only one\nspecialized dataset for it. Here, we focus on bridging this gap. In particular,\nwe aim to solve two novel tasks: detecting harmful memes and identifying the\nsocial entities they target. We further extend the recently released HarMeme\ndataset to generalize on two prevalent topics - COVID-19 and US politics and\nname the two datasets as Harm-C and Harm-P, respectively. We then propose\nMOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a\nnovel multimodal (text + image) deep neural model, which uses global and local\nperspectives to detect harmful memes. MOMENTA identifies the object proposals\nand attributes and uses a multimodal model to perceive the comprehensive\ncontext in which the objects and the entities are portrayed in a given meme.\nMOMENTA is interpretable and generalizable, and it outperforms numerous\nbaselines.",
          "link": "http://arxiv.org/abs/2109.05184",
          "publishedOn": "2021-09-14T07:20:09.517Z",
          "wordCount": 724,
          "title": "MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiaqi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Rui Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "Information Extraction (IE) aims to extract structural information from\nunstructured texts. In practice, long-tailed distributions caused by the\nselection bias of a dataset, may lead to incorrect correlations, also known as\nspurious correlations, between entities and labels in the conventional\nlikelihood models. This motivates us to propose counterfactual IE (CFIE), a\nnovel framework that aims to uncover the main causalities behind data in the\nview of causal inference. Specifically, 1) we first introduce a unified\nstructural causal model (SCM) for various IE tasks, describing the\nrelationships among variables; 2) with our SCM, we then generate\ncounterfactuals based on an explicit language structure to better calculate the\ndirect causal effect during the inference stage; 3) we further propose a novel\ndebiasing approach to yield more robust predictions. Experiments on three IE\ntasks across five public datasets show the effectiveness of our CFIE model in\nmitigating the spurious correlation issues.",
          "link": "http://arxiv.org/abs/2109.05213",
          "publishedOn": "2021-09-14T07:20:09.498Z",
          "wordCount": 601,
          "title": "Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Haoda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>",
          "description": "In this work, we study the problem of named entity recognition (NER) in a low\nresource scenario, focusing on few-shot and zero-shot settings. Built upon\nlarge-scale pre-trained language models, we propose a novel NER framework,\nnamely SpanNER, which learns from natural language supervision and enables the\nidentification of never-seen entity classes without using in-domain labeled\ndata. We perform extensive experiments on 5 benchmark datasets and evaluate the\nproposed method in the few-shot learning, domain transfer and zero-shot\nlearning settings. The experimental results show that the proposed method can\nbring 10%, 23% and 26% improvements in average over the best baselines in\nfew-shot learning, domain transfer and zero-shot learning settings\nrespectively.",
          "link": "http://arxiv.org/abs/2109.05357",
          "publishedOn": "2021-09-14T07:20:09.491Z",
          "wordCount": 560,
          "title": "Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "Dialogue-based relation extraction (DiaRE) aims to detect the structural\ninformation from unstructured utterances in dialogues. Existing relation\nextraction models may be unsatisfactory under such a conversational setting,\ndue to the entangled logic and information sparsity issues in utterances\ninvolving multiple speakers. To this end, we introduce SOLS, a novel model\nwhich can explicitly induce speaker-oriented latent structures for better\nDiaRE. Specifically, we learn latent structures to capture the relationships\namong tokens beyond the utterance boundaries, alleviating the entangled logic\nissue. During the learning process, our speaker-specific regularization method\nprogressively highlights speaker-related key clues and erases the irrelevant\nones, alleviating the information sparsity issue. Experiments on three public\ndatasets demonstrate the effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2109.05182",
          "publishedOn": "2021-09-14T07:20:09.485Z",
          "wordCount": 569,
          "title": "Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1\">Shashank Bujimalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1\">Mahesh Subedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1\">Omesh Tickoo</a>",
          "description": "In this paper, we propose an approach to improve image captioning solutions\nfor images with novel objects that do not have caption labels in the training\ndataset. Our approach is agnostic to model architecture, and primarily focuses\non training technique that uses existing fully paired image-caption data and\nthe images with only the novel object detection labels (partially paired data).\nWe create synthetic paired captioning data for these novel objects by\nleveraging context from existing image-caption pairs. We further re-use these\npartially paired images with novel objects to create pseudo-label captions that\nare used to fine-tune the captioning model. Using a popular captioning model\n(Up-Down) as baseline, our approach achieves state-of-the-art results on\nheld-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for\nnovel object images by 75.8 and 26.6 points respectively, compared to baseline\nmodel that does not use partially paired images during training.",
          "link": "http://arxiv.org/abs/2109.05115",
          "publishedOn": "2021-09-14T07:20:09.478Z",
          "wordCount": 605,
          "title": "Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1\">Mai ElSherief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muchlinski_D/0/1/0/all/0/1\">David Muchlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anupindi_V/0/1/0/all/0/1\">Vaishnavi Anupindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybolt_J/0/1/0/all/0/1\">Jordyn Seybolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Munmun De Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>",
          "description": "Hate speech has grown significantly on social media, causing serious\nconsequences for victims of all demographics. Despite much attention being paid\nto characterize and detect discriminatory speech, most work has focused on\nexplicit or overt hate speech, failing to address a more pervasive form based\non coded or indirect language. To fill this gap, this work introduces a\ntheoretically-justified taxonomy of implicit hate speech and a benchmark corpus\nwith fine-grained labels for each message and its implication. We present\nsystematic analyses of our dataset using contemporary baselines to detect and\nexplain implicit hate speech, and we discuss key features that challenge\nexisting models. This dataset will continue to serve as a useful benchmark for\nunderstanding this multifaceted issue.",
          "link": "http://arxiv.org/abs/2109.05322",
          "publishedOn": "2021-09-14T07:20:09.469Z",
          "wordCount": 578,
          "title": "Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinxia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1\">John R. Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drake_J/0/1/0/all/0/1\">John Drake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiaofu Zhang</a>",
          "description": "Addressing the mismatch between natural language descriptions and the\ncorresponding SQL queries is a key challenge for text-to-SQL translation. To\nbridge this gap, we propose an SQL intermediate representation (IR) called\nNatural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities\nof SQL, while it simplifies the queries as follows: (1) dispensing with\noperators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are\nusually hard to find counterparts for in the text descriptions; (2) removing\nthe need for nested subqueries and set operators; and (3) making schema linking\neasier by reducing the required number of schema items. On Spider, a\nchallenging text-to-SQL benchmark that contains complex and nested SQL queries,\nwe demonstrate that NatSQL outperforms other IRs, and significantly improves\nthe performance of several previous SOTA models. Furthermore, for existing\nmodels that do not support executable SQL generation, NatSQL easily enables\nthem to generate executable SQL queries, and achieves the new state-of-the-art\nexecution accuracy.",
          "link": "http://arxiv.org/abs/2109.05153",
          "publishedOn": "2021-09-14T07:20:09.463Z",
          "wordCount": 617,
          "title": "Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.",
          "link": "http://arxiv.org/abs/2109.05252",
          "publishedOn": "2021-09-14T07:20:09.456Z",
          "wordCount": 684,
          "title": "XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tran-Binh Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Minh-Quan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>",
          "description": "Attention is all we need as long as we have enough data. Even so, it is\nsometimes not easy to determine how much data is enough while the models are\nbecoming larger and larger. In this paper, we propose HYDRA heads, lightweight\npretrained linguistic self-attention heads to inject knowledge into transformer\nmodels without pretraining them again. Our approach is a balanced paradigm\nbetween leaving the models to learn unsupervised and forcing them to conform to\nlinguistic knowledge rigidly as suggested in previous studies. Our experiment\nproves that the approach is not only the boost performance of the model but\nalso lightweight and architecture friendly. We empirically verify our framework\non benchmark datasets to show the contribution of linguistic knowledge to a\ntransformer model. This is a promising result for a new approach to\ntransferring knowledge from linguistic resources into transformer-based models.",
          "link": "http://arxiv.org/abs/2109.05349",
          "publishedOn": "2021-09-14T07:20:09.449Z",
          "wordCount": 583,
          "title": "HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_D/0/1/0/all/0/1\">Dylan Finkbeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Shilpa Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "For over thirty years, researchers have developed and analyzed methods for\nlatent tree induction as an approach for unsupervised syntactic parsing.\nNonetheless, modern systems still do not perform well enough compared to their\nsupervised counterparts to have any practical use as structural annotation of\ntext. In this work, we present a technique that uses distant supervision in the\nform of span constraints (i.e. phrase bracketing) to improve performance in\nunsupervised constituency parsing. Using a relatively small number of span\nconstraints we can substantially improve the output from DIORA, an already\ncompetitive unsupervised parsing system. Compared with full parse tree\nannotation, span constraints can be acquired with minimal effort, such as with\na lexicon derived from Wikipedia, to find exact text matches. Our experiments\nshow span constraints based on entities improves constituency parsing on\nEnglish WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to\nany domain where span constraints are easily attainable, and as a case study we\ndemonstrate its effectiveness by parsing biomedical text from the CRAFT\ndataset.",
          "link": "http://arxiv.org/abs/2109.05112",
          "publishedOn": "2021-09-14T07:20:09.421Z",
          "wordCount": 631,
          "title": "Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05149",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>",
          "description": "In this paper, we present a new verification style reading comprehension\ndataset named VGaokao from Chinese Language tests of Gaokao. Different from\nexisting efforts, the new dataset is originally designed for native speakers'\nevaluation, thus requiring more advanced language understanding skills. To\naddress the challenges in VGaokao, we propose a novel Extract-Integrate-Compete\napproach, which iteratively selects complementary evidence with a novel query\nupdating mechanism and adaptively distills supportive evidence, followed by a\npairwise competition to push models to learn the subtle difference among\nsimilar text pieces. Experiments show that our methods outperform various\nbaselines on VGaokao with retrieved complementary evidence, while having the\nmerits of efficiency and explainability. Our dataset and code are released for\nfurther research.",
          "link": "http://arxiv.org/abs/2109.05149",
          "publishedOn": "2021-09-14T07:20:09.393Z",
          "wordCount": 565,
          "title": "Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_H/0/1/0/all/0/1\">Hiroaki Sugiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizukami_M/0/1/0/all/0/1\">Masahiro Mizukami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arimoto_T/0/1/0/all/0/1\">Tsunehiro Arimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narimatsu_H/0/1/0/all/0/1\">Hiromi Narimatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_Y/0/1/0/all/0/1\">Yuya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_H/0/1/0/all/0/1\">Hideharu Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meguro_T/0/1/0/all/0/1\">Toyomi Meguro</a>",
          "description": "In recent years, several high-performance conversational systems have been\nproposed based on the Transformer encoder-decoder model. Although previous\nstudies analyzed the effects of the model parameters and the decoding method on\nsubjective dialogue evaluations with overall metrics, they did not analyze how\nthe differences of fine-tuning datasets affect on user's detailed impression.\nIn addition, the Transformer-based approach has only been verified for English,\nnot for such languages with large inter-language distances as Japanese. In this\nstudy, we develop large-scale Transformer-based Japanese dialogue models and\nJapanese chit-chat datasets to examine the effectiveness of the\nTransformer-based approach for building chit-chat dialogue systems. We\nevaluated and analyzed the impressions of human dialogues in different\nfine-tuning datasets, model parameters, and the use of additional information.",
          "link": "http://arxiv.org/abs/2109.05217",
          "publishedOn": "2021-09-14T07:20:09.371Z",
          "wordCount": 581,
          "title": "Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Mingyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zimo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>",
          "description": "A multi-turn dialogue always follows a specific topic thread, and topic shift\nat the discourse level occurs naturally as the conversation progresses,\nnecessitating the model's ability to capture different topics and generate\ntopic-aware responses. Previous research has either predicted the topic first\nand then generated the relevant response, or simply applied the attention\nmechanism to all topics, ignoring the joint distribution of the topic\nprediction and response generation models and resulting in uncontrollable and\nunrelated responses. In this paper, we propose a joint framework with a topic\nrefinement mechanism to learn these two tasks simultaneously. Specifically, we\ndesign a three-pass iteration mechanism to generate coarse response first, then\npredict corresponding topics, and finally generate refined response conditioned\non predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the\ntopic prediction task respectively, aiming to investigate the effects of joint\nlearning and the understanding ability of GPT model. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance at response generation task and the great potential understanding\ncapability of GPT model.",
          "link": "http://arxiv.org/abs/2109.05187",
          "publishedOn": "2021-09-14T07:20:09.354Z",
          "wordCount": 629,
          "title": "TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1\">Varun Embar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>",
          "description": "Existing research studies on cross-sentence relation extraction in long-form\nmulti-party conversations aim to improve relation extraction without\nconsidering the explainability of such methods. This work addresses that gap by\nfocusing on extracting explanations that indicate that a relation exists while\nusing only partially labeled data. We propose our model-agnostic framework,\nD-REX, a policy-guided semi-supervised algorithm that explains and ranks\nrelations. We frame relation extraction as a re-ranking task and include\nrelation- and entity-specific explanations as an intermediate step of the\ninference process. We find that about 90% of the time, human annotators prefer\nD-REX's explanations over a strong BERT-based joint relation extraction and\nexplanation model. Finally, our evaluations on a dialogue relation extraction\ndataset show that our method is simple yet effective and achieves a\nstate-of-the-art F1 score on relation extraction, improving upon existing\nmethods by 13.5%.",
          "link": "http://arxiv.org/abs/2109.05126",
          "publishedOn": "2021-09-14T07:20:09.335Z",
          "wordCount": 586,
          "title": "D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1\">Mateusz Klimaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Alina Wr&#xf3;blewska</a>",
          "description": "We introduce COMBO - a fully neural NLP system for accurate part-of-speech\ntagging, morphological analysis, lemmatisation, and (enhanced) dependency\nparsing. It predicts categorical morphosyntactic features whilst also exposes\ntheir vector representations, extracted from hidden layers. COMBO is an easy to\ninstall Python package with automatically downloadable pre-trained models for\nover 40 languages. It maintains a balance between efficiency and quality. As it\nis an end-to-end system and its modules are jointly trained, its training is\ncompetitively fast. As its models are optimised for accuracy, they achieve\noften better prediction quality than SOTA. The COMBO library is available at:\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.",
          "link": "http://arxiv.org/abs/2109.05361",
          "publishedOn": "2021-09-14T07:20:09.328Z",
          "wordCount": 537,
          "title": "COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>",
          "description": "Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.",
          "link": "http://arxiv.org/abs/2109.05424",
          "publishedOn": "2021-09-14T07:20:09.320Z",
          "wordCount": 609,
          "title": "Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jin Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>",
          "description": "Eliciting knowledge contained in language models via prompt-based learning\nhas shown great potential in many natural language processing tasks, such as\ntext classification and generation. Whereas, the applications for more complex\ntasks such as event extraction are less studied, since the design of prompt is\nnot straightforward due to the complicated types and arguments. In this paper,\nwe explore to elicit the knowledge from pre-trained language models for event\ntrigger detection and argument extraction. Specifically, we present various\njoint trigger/argument prompt methods, which can elicit more complementary\nknowledge by modeling the interactions between different triggers or arguments.\nThe experimental results on the benchmark dataset, namely ACE2005, show the\ngreat advantages of our proposed approach. In particular, our approach is\nsuperior to the recent advanced methods in the few-shot scenario where only a\nfew samples are used for training.",
          "link": "http://arxiv.org/abs/2109.05190",
          "publishedOn": "2021-09-14T07:20:09.301Z",
          "wordCount": 580,
          "title": "Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "A hyperbole is an intentional and creative exaggeration not to be taken\nliterally. Despite its ubiquity in daily life, the computational explorations\nof hyperboles are scarce. In this paper, we tackle the under-explored and\nchallenging task: sentence-level hyperbole generation. We start with a\nrepresentative syntactic pattern for intensification and systematically study\nthe semantic (commonsense and counterfactual) relationships between each\ncomponent in such hyperboles. Next, we leverage the COMeT and reverse COMeT\nmodels to do commonsense and counterfactual inference. We then generate\nmultiple hyperbole candidates based on our findings from the pattern, and train\nneural classifiers to rank and select high-quality hyperboles. Automatic and\nhuman evaluations show that our generation method is able to generate\nhyperboles creatively with high success rate and intensity scores.",
          "link": "http://arxiv.org/abs/2109.05097",
          "publishedOn": "2021-09-14T07:20:09.260Z",
          "wordCount": 576,
          "title": "HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1\">Hongtao Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peixian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>",
          "description": "State-of-the-art Named Entity Recognition(NER) models rely heavily on large\namountsof fully annotated training data. However, ac-cessible data are often\nincompletely annotatedsince the annotators usually lack comprehen-sive\nknowledge in the target domain. Normallythe unannotated tokens are regarded as\nnon-entities by default, while we underline thatthese tokens could either be\nnon-entities orpart of any entity. Here, we study NER mod-eling with incomplete\nannotated data whereonly a fraction of the named entities are la-beled, and the\nunlabeled tokens are equiva-lently multi-labeled by every possible label.Taking\nmulti-labeled tokens into account, thenumerous possible paths can distract the\ntrain-ing model from the gold path (ground truthlabel sequence), and thus\nhinders the learn-ing ability. In this paper, we propose AdaK-NER, named the\nadaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion\nwhere the gold path is more likely to belocated. We demonstrate the superiority\nofour approach through extensive experimentson both English and Chinese\ndatasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on\ntwo Chinese datasetscompared with the prior state-of-the-art works.",
          "link": "http://arxiv.org/abs/2109.05233",
          "publishedOn": "2021-09-14T07:20:09.238Z",
          "wordCount": 624,
          "title": "AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_K/0/1/0/all/0/1\">Kwan Wai Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>",
          "description": "Few-shot slot tagging is an emerging research topic in the field of Natural\nLanguage Understanding (NLU). With sufficient annotated data from source\ndomains, the key challenge is how to train and adapt the model to another\ntarget domain which only has few labels. Conventional few-shot approaches use\nall the data from the source domains without considering inter-domain relations\nand implicitly assume each sample in the domain contributes equally. However,\nour experiments show that the data distribution bias among different domains\nwill significantly affect the adaption performance. Moreover, transferring\nknowledge from dissimilar domains will even introduce some extra noises so that\naffect the performance of models. To tackle this problem, we propose an\neffective similarity-based method to select data from the source domains. In\naddition, we propose a Shared-Private Network (SP-Net) for the few-shot slot\ntagging task. The words from the same class would have some shared features. We\nextract those shared features from the limited annotated data on the target\ndomain and merge them together as the label embedding to help us predict other\nunlabelled data on the target domain. The experiment shows that our method\noutperforms the state-of-the-art approaches with fewer source data. The result\nalso proves that some training data from dissimilar sources are redundant and\neven negative for the adaption.",
          "link": "http://arxiv.org/abs/2109.05234",
          "publishedOn": "2021-09-14T07:20:09.045Z",
          "wordCount": 669,
          "title": "Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavabi_L/0/1/0/all/0/1\">Leili Tavabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1\">Mohammad Soleymani</a>",
          "description": "Dialogue Act (DA) classification is the task of classifying utterances with\nrespect to the function they serve in a dialogue. Existing approaches to DA\nclassification model utterances without incorporating the turn changes among\nspeakers throughout the dialogue, therefore treating it no different than\nnon-interactive written text. In this paper, we propose to integrate the turn\nchanges in conversations among speakers when modeling DAs. Specifically, we\nlearn conversation-invariant speaker turn embeddings to represent the speaker\nturns in a conversation; the learned speaker turn embeddings are then merged\nwith the utterance embeddings for the downstream task of DA classification.\nWith this simple yet effective mechanism, our model is able to capture the\nsemantics from the dialogue content while accounting for different speaker\nturns in a conversation. Validation on three benchmark public datasets\ndemonstrates superior performance of our model.",
          "link": "http://arxiv.org/abs/2109.05056",
          "publishedOn": "2021-09-14T07:20:09.036Z",
          "wordCount": 584,
          "title": "Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>",
          "description": "Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.",
          "link": "http://arxiv.org/abs/2109.05179",
          "publishedOn": "2021-09-14T07:20:08.980Z",
          "wordCount": 615,
          "title": "Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>",
          "description": "Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.",
          "link": "http://arxiv.org/abs/2109.05052",
          "publishedOn": "2021-09-14T07:20:08.961Z",
          "wordCount": 589,
          "title": "Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1\">Benjamin Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>",
          "description": "Neural language modelling has progressed the state-of-the-art in different\ndownstream Natural Language Processing (NLP) tasks. One such area is of\nopen-domain dialog modelling, neural dialog models based on GPT-2 such as\nDialoGPT have shown promising performance in single-turn conversation. However,\nsuch (neural) dialog models have been criticized for generating responses which\nalthough may have relevance to the previous human response, tend to quickly\ndissipate human interest and descend into trivial conversation. One reason for\nsuch performance is the lack of explicit conversation strategy being employed\nin human-machine conversation. Humans employ a range of conversation strategies\nwhile engaging in a conversation, one such key social strategies is\nSelf-disclosure(SD). A phenomenon of revealing information about one-self to\nothers. Social penetration theory (SPT) proposes that communication between two\npeople moves from shallow to deeper levels as the relationship progresses\nprimarily through self-disclosure. Disclosure helps in creating rapport among\nthe participants engaged in a conversation. In this paper, Self-disclosure\nenhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic\nModel (SDTM) during inference stage of a neural dialog model to re-rank\nresponse candidates to enhance self-disclosure in single-turn responses from\nfrom the model.",
          "link": "http://arxiv.org/abs/2109.05090",
          "publishedOn": "2021-09-14T07:20:08.953Z",
          "wordCount": 634,
          "title": "Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganter_T/0/1/0/all/0/1\">Tim Ganter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Walter Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1\">Jonathan Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>",
          "description": "With the explosive growth of livestream broadcasting, there is an urgent need\nfor new summarization technology that enables us to create a preview of\nstreamed content and tap into this wealth of knowledge. However, the problem is\nnontrivial due to the informal nature of spoken language. Further, there has\nbeen a shortage of annotated datasets that are necessary for transcript\nsummarization. In this paper, we present StreamHover, a framework for\nannotating and summarizing livestream transcripts. With a total of over 500\nhours of videos annotated with both extractive and abstractive summaries, our\nbenchmark dataset is significantly larger than currently existing annotated\ncorpora. We explore a neural extractive summarization model that leverages\nvector-quantized variational autoencoder to learn latent vector representations\nof spoken utterances and identify salient utterances from the transcripts to\nform summaries. We show that our model generalizes better and improves\nperformance over strong baselines. The results of this study provide an avenue\nfor future research to improve summarization solutions for efficient browsing\nof livestreams.",
          "link": "http://arxiv.org/abs/2109.05160",
          "publishedOn": "2021-09-14T07:20:08.946Z",
          "wordCount": 619,
          "title": "StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrick_S/0/1/0/all/0/1\">Steven Bedrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webber_B/0/1/0/all/0/1\">Bonnie Webber</a>",
          "description": "Many NLG tasks such as summarization, dialogue response, or open domain\nquestion answering focus primarily on a source text in order to generate a\ntarget response. This standard approach falls short, however, when a user's\nintent or context of work is not easily recoverable based solely on that source\ntext -- a scenario that we argue is more of the rule than the exception. In\nthis work, we argue that NLG systems in general should place a much higher\nlevel of emphasis on making use of additional context, and suggest that\nrelevance (as used in Information Retrieval) be thought of as a crucial tool\nfor designing user-oriented text-generating tasks. We further discuss possible\nharms and hazards around such personalization, and argue that value-sensitive\ndesign represents a crucial path forward through these challenges.",
          "link": "http://arxiv.org/abs/2109.05140",
          "publishedOn": "2021-09-14T07:20:08.939Z",
          "wordCount": 583,
          "title": "Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>",
          "description": "Can we get existing language models and refine them for zero-shot commonsense\nreasoning? This paper presents an initial study exploring the feasibility of\nzero-shot commonsense reasoning for the Winograd Schema Challenge by\nformulating the task as self-supervised refinement of a pre-trained language\nmodel. In contrast to previous studies that rely on fine-tuning annotated\ndatasets, we seek to boost conceptualization via loss landscape refinement. To\nthis end, we propose a novel self-supervised learning approach that refines the\nlanguage model utilizing a set of linguistic perturbations of similar concept\nrelationships. Empirical analysis of our conceptually simple framework\ndemonstrates the viability of zero-shot commonsense reasoning on multiple\nbenchmarks.",
          "link": "http://arxiv.org/abs/2109.05105",
          "publishedOn": "2021-09-14T07:20:08.904Z",
          "wordCount": 552,
          "title": "Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanuzzaman_M/0/1/0/all/0/1\">Mohammad Hasanuzzaman</a>",
          "description": "The new era of technology has brought us to the point where it is convenient\nfor people to share their opinions over an abundance of platforms. These\nplatforms have a provision for the users to express themselves in multiple\nforms of representations, including text, images, videos, and audio. This,\nhowever, makes it difficult for users to obtain all the key information about a\ntopic, making the task of automatic multi-modal summarization (MMS) essential.\nIn this paper, we present a comprehensive survey of the existing research in\nthe area of MMS.",
          "link": "http://arxiv.org/abs/2109.05199",
          "publishedOn": "2021-09-14T07:20:08.875Z",
          "wordCount": 533,
          "title": "A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>",
          "description": "Self-supervised learning has recently attracted considerable attention in the\nNLP community for its ability to learn discriminative features using a\ncontrastive objective. This paper investigates whether contrastive learning can\nbe extended to Transfomer attention to tackling the Winograd Schema Challenge.\nTo this end, we propose a novel self-supervised framework, leveraging a\ncontrastive loss directly at the level of self-attention. Experimental analysis\nof our attention-based models on multiple datasets demonstrates superior\ncommonsense reasoning capabilities. The proposed approach outperforms all\ncomparable unsupervised approaches while occasionally surpassing supervised\nones.",
          "link": "http://arxiv.org/abs/2109.05108",
          "publishedOn": "2021-09-14T07:20:08.797Z",
          "wordCount": 526,
          "title": "Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Diptanu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>",
          "description": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.",
          "link": "http://arxiv.org/abs/2109.05074",
          "publishedOn": "2021-09-14T07:20:08.710Z",
          "wordCount": 550,
          "title": "FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schucher_N/0/1/0/all/0/1\">Nathan Schucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>",
          "description": "Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.",
          "link": "http://arxiv.org/abs/2109.05093",
          "publishedOn": "2021-09-14T07:20:08.547Z",
          "wordCount": 558,
          "title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>",
          "description": "Recently, there has been significant progress in studying neural networks for\ntranslating text descriptions into SQL queries under the zero-shot cross-domain\nsetting. Despite achieving good performance on some public benchmarks, we\nobserve that existing text-to-SQL models do not generalize when facing domain\nknowledge that does not frequently appear in the training data, which may\nrender the worse prediction performance for unseen domains. In this work, we\ninvestigate the robustness of text-to-SQL models when the questions require\nrarely observed domain knowledge. In particular, we define five types of domain\nknowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),\na human-curated dataset based on the Spider benchmark for text-to-SQL\ntranslation. NL questions in Spider-DK are selected from Spider, and we modify\nsome samples by adding domain knowledge that reflects real-world question\nparaphrases. We demonstrate that the prediction accuracy dramatically drops on\nsamples that require such domain knowledge, even if the domain knowledge\nappears in the training set, and the model provides the correct predictions for\nrelated training samples.",
          "link": "http://arxiv.org/abs/2109.05157",
          "publishedOn": "2021-09-14T07:20:08.455Z",
          "wordCount": 611,
          "title": "Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yichao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Based on\nSeq2Seq models, the previous reinforcement learning framework on KG tasks\nutilizes the evaluation metrics to further improve the well-trained neural\nmodels. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are\nonly aware of the exact correctness of predictions on phrase-level and ignore\nthe semantic similarities between similar predictions and targets, which\ninhibits the model from learning deep linguistic patterns. In response to this\nproblem, we propose a new fine-grained evaluation metric to improve the RL\nframework, which considers different granularities: token-level $F_1$ score,\nedit distance, duplication, and prediction quantities. On the whole, the new\nframework includes two reward functions: the fine-grained evaluation score and\nthe vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on KG benchmarks show that our proposed training framework\noutperforms the previous RL training frameworks among all evaluation scores. In\naddition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction. The source code is available at\n\\url{https://github.com/xuyige/FGRL4KG}.",
          "link": "http://arxiv.org/abs/2104.08799",
          "publishedOn": "2021-09-13T07:20:36.173Z",
          "wordCount": 676,
          "title": "Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>",
          "description": "This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework, by using \"entailment\" pairs\nas positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nprevious best results. We also show -- both theoretically and empirically --\nthat contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.",
          "link": "http://arxiv.org/abs/2104.08821",
          "publishedOn": "2021-09-13T07:20:36.115Z",
          "wordCount": 675,
          "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoover_J/0/1/0/all/0/1\">Jacob Louis Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>",
          "description": "Are pairs of words that tend to occur together also likely to stand in a\nlinguistic dependency? This empirical question is motivated by a long history\nof literature in cognitive science, psycholinguistics, and NLP. In this work we\ncontribute an extensive analysis of the relationship between linguistic\ndependencies and statistical dependence between words. Improving on previous\nwork, we introduce the use of large pretrained language models to compute\ncontextualized estimates of the pointwise mutual information between words\n(CPMI). For multiple models and languages, we extract dependency trees which\nmaximize CPMI, and compare to gold standard linguistic dependencies. Overall,\nwe find that CPMI dependencies achieve an unlabelled undirected attachment\nscore of at most $\\approx 0.5$. While far above chance, and consistently above\na non-contextualized PMI baseline, this score is generally comparable to a\nsimple baseline formed by connecting adjacent words. We analyze which kinds of\nlinguistic dependencies are best captured in CPMI dependencies, and also find\nmarked differences between the estimates of the large pretrained language\nmodels, illustrating how their different training schemes affect the type of\ndependencies they capture.",
          "link": "http://arxiv.org/abs/2104.08685",
          "publishedOn": "2021-09-13T07:20:36.051Z",
          "wordCount": 662,
          "title": "Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">Michael Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">Paula Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">Susanna Dodd</a>",
          "description": "A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes, or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. To address this, we propose a method that\nuses both word-level and sentence-level information to simultaneously perform\noutcome span detection and outcome type classification. In addition to\ninjecting contextual information to hidden vectors, we use label attention to\nappropriately weight both word and sentence level information. Experimental\nresults on several benchmark datasets for health outcome detection show that\nour proposed method consistently outperforms decoupled methods, reporting\ncompetitive results.",
          "link": "http://arxiv.org/abs/2104.07789",
          "publishedOn": "2021-09-13T07:20:35.940Z",
          "wordCount": 676,
          "title": "Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>",
          "description": "A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2104.06644",
          "publishedOn": "2021-09-13T07:20:35.934Z",
          "wordCount": 653,
          "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bengoetxea_K/0/1/0/all/0/1\">Kepa Bengoetxea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>",
          "description": "Readability assessment is the task of determining how difficult or easy a\ntext is or which level/grade it has. Traditionally, language dependent\nreadability formula have been used, but these formulae take few text\ncharacteristics into account. However, Natural Language Processing (NLP) tools\nthat assess the complexity of texts are able to measure more different features\nand can be adapted to different languages. In this paper, we present the\nMultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over\n125 measures of cohesion,language, and readability for English, Spanish and\nBasque, but whose architecture is designed to easily adapt other languages;\n(ii) readability assessment classifiers that improve the performance of\nCoh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a\nweb tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into\nthree reading levels (elementary, intermediate, and advanced) in English and\n95.50 % in Basque and 90 % in Spanish when classifying into two reading levels\n(simple and complex) using a SMO classifier. Using cross-lingual features,\nMultiAzterTest also obtains competitive results above all in a complex vs\nsimple distinction.",
          "link": "http://arxiv.org/abs/2109.04870",
          "publishedOn": "2021-09-13T07:20:35.927Z",
          "wordCount": 640,
          "title": "MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_J/0/1/0/all/0/1\">Jax Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>",
          "description": "This paper presents a novel training method, Conditional Masked Language\nModeling (CMLM), to effectively learn sentence representations on large scale\nunlabeled corpora. CMLM integrates sentence representation learning into MLM\ntraining by conditioning on the encoded vectors of adjacent sentences. Our\nEnglish CMLM model achieves state-of-the-art performance on SentEval, even\noutperforming models learned using supervised signals. As a fully unsupervised\nlearning method, CMLM can be conveniently extended to a broad range of\nlanguages and domains. We find that a multilingual CMLM model co-trained with\nbitext retrieval (BR) and natural language inference (NLI) tasks outperforms\nthe previous state-of-the-art multilingual models by a large margin, e.g. 10%\nimprovement upon baseline models on cross-lingual semantic search. We explore\nthe same language bias of the learned representations, and propose a simple,\npost-training and model agnostic approach to remove the language identifying\ninformation from the representation while still retaining sentence semantics.",
          "link": "http://arxiv.org/abs/2012.14388",
          "publishedOn": "2021-09-13T07:20:35.912Z",
          "wordCount": 642,
          "title": "Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1\">Adithya Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>",
          "description": "Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.",
          "link": "http://arxiv.org/abs/2104.08597",
          "publishedOn": "2021-09-13T07:20:35.896Z",
          "wordCount": 586,
          "title": "XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Deep neural models for low-resource named entity recognition (NER) have shown\nimpressive results by leveraging distant super-vision or other meta-level\ninformation (e.g. explanation). However, the costs of acquiring such additional\ninformation are generally prohibitive, especially in domains where existing\nresources (e.g. databases to be used for distant supervision) may not exist. In\nthis paper, we present a novel two-stage framework (AutoTriggER) to improve NER\nperformance by automatically generating and leveraging \"entity triggers\" which\nare essentially human-readable clues in the text that can help guide the model\nto make better decisions. Thus, the framework is able to both create and\nleverage auxiliary supervision by itself. Through experiments on three\nwell-studied NER datasets, we show that our automatically extracted triggers\nare well-matched to human triggers, and AutoTriggER improves performance over a\nRoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a\nlow resource setting.",
          "link": "http://arxiv.org/abs/2109.04726",
          "publishedOn": "2021-09-13T07:20:35.858Z",
          "wordCount": 620,
          "title": "AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutuma_M/0/1/0/all/0/1\">Mutethia Mutuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Troy Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.",
          "link": "http://arxiv.org/abs/2108.12738",
          "publishedOn": "2021-09-13T07:20:35.811Z",
          "wordCount": 648,
          "title": "SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.",
          "link": "http://arxiv.org/abs/2104.05845",
          "publishedOn": "2021-09-13T07:20:35.795Z",
          "wordCount": 616,
          "title": "Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.",
          "link": "http://arxiv.org/abs/2103.01378",
          "publishedOn": "2021-09-13T07:20:35.295Z",
          "wordCount": 645,
          "title": "Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04620",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>",
          "description": "Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.",
          "link": "http://arxiv.org/abs/2109.04620",
          "publishedOn": "2021-09-13T07:20:35.213Z",
          "wordCount": 583,
          "title": "Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1\">Pieter Floris Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenniger_G/0/1/0/all/0/1\">Gideon Maillette de Buy Wenniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>",
          "description": "Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art NLP\nmodels. Here, we present an empirical study that compares different\nuncertainty-based algorithms with BERT$_{base}$ as the used classifier. We\nevaluate the algorithms on two NLP classification datasets: Stanford Sentiment\nTreebank and KvK-Frontpages. Additionally, we explore heuristics that aim to\nsolve presupposed problems of uncertainty-based AL; namely, that it is\nunscalable and that it is prone to selecting outliers. Furthermore, we explore\nthe influence of the query-pool size on the performance of AL. Whereas it was\nfound that the proposed heuristics for AL did not improve performance of AL;\nour results show that using uncertainty-based AL with BERT$_{base}$ outperforms\nrandom sampling of data. This difference in performance can decrease as the\nquery-pool size gets larger.",
          "link": "http://arxiv.org/abs/2109.04847",
          "publishedOn": "2021-09-13T07:20:35.207Z",
          "wordCount": 650,
          "title": "Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Isolated silos of scientific research and the growing challenge of\ninformation overload limit awareness across the literature and hinder\ninnovation. Algorithmic curation and recommendation, which often prioritize\nrelevance, can further reinforce these informational \"filter bubbles.\" In\nresponse, we describe Bridger, a system for facilitating discovery of scholars\nand their work, to explore design tradeoffs between relevant and novel\nrecommendations. We construct a faceted representation of authors with\ninformation gleaned from their papers and inferred author personas, and use it\nto develop an approach that locates commonalities (\"bridges\") and contrasts\nbetween scientists -- retrieving partially similar authors rather than aiming\nfor strict similarity. In studies with computer science researchers, this\napproach helps users discover authors considered useful for generating novel\nresearch directions, outperforming a state-of-art neural model. In addition to\nrecommending new content, we also demonstrate an approach for displaying it in\na manner that boosts researchers' ability to understand the work of authors\nwith whom they are unfamiliar. Finally, our analysis reveals that Bridger\nconnects authors who have different citation profiles, publish in different\nvenues, and are more distant in social co-authorship networks, raising the\nprospect of bridging diverse communities and facilitating discovery.",
          "link": "http://arxiv.org/abs/2108.05669",
          "publishedOn": "2021-09-13T07:20:34.830Z",
          "wordCount": 684,
          "title": "Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhenpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunjoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subba_R/0/1/0/all/0/1\">Rajen Subba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>",
          "description": "Zero-shot transfer learning for dialogue state tracking (DST) enables us to\nhandle a variety of task-oriented dialogue domains without the expense of\ncollecting in-domain data. In this work, we propose to transfer the\n\\textit{cross-task} knowledge from general question answering (QA) corpora for\nthe zero-shot DST task. Specifically, we propose TransferQA, a transferable\ngenerative QA model that seamlessly combines extractive QA and multi-choice QA\nvia a text-to-text transformer framework, and tracks both categorical slots and\nnon-categorical slots in DST. In addition, we introduce two effective ways to\nconstruct unanswerable questions, namely, negative question sampling and\ncontext truncation, which enable our model to handle \"none\" value slots in the\nzero-shot DST setting. The extensive experiments show that our approaches\nsubstantially improve the existing zero-shot and few-shot results on MultiWoz.\nMoreover, compared to the fully trained baseline on the Schema-Guided Dialogue\ndataset, our approach shows better generalization ability in unseen domains.",
          "link": "http://arxiv.org/abs/2109.04655",
          "publishedOn": "2021-09-13T07:20:34.821Z",
          "wordCount": 604,
          "title": "Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruijian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "The encoder-decoder framework achieves state-of-the-art results in keyphrase\ngeneration (KG) tasks by predicting both present keyphrases that appear in the\nsource document and absent keyphrases that do not. However, relying solely on\nthe source document can result in generating uncontrollable and inaccurate\nabsent keyphrases. To address these problems, we propose a novel graph-based\nmethod that can capture explicit knowledge from related references. Our model\nfirst retrieves some document-keyphrases pairs similar to the source document\nfrom a pre-defined index as references. Then a heterogeneous graph is\nconstructed to capture relationships of different granularities between the\nsource document and its references. To guide the decoding process, a\nhierarchical attention and copy mechanism is introduced, which directly copies\nappropriate words from both the source document and its references based on\ntheir relevance and significance. The experimental results on multiple KG\nbenchmarks show that the proposed model achieves significant improvements\nagainst other baseline models, especially with regard to the absent keyphrase\nprediction.",
          "link": "http://arxiv.org/abs/2109.04703",
          "publishedOn": "2021-09-13T07:20:34.799Z",
          "wordCount": 605,
          "title": "Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>",
          "description": "We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSimilar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,\nQuery Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from\n55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query\nBank and Relevance Set, where the former contains 1,236 human-paraphrased\nqueries while the latter contains ~32 human-annotated FAQ items for each query.\nWe analyze COUGH by testing different FAQ retrieval models built on top of BM25\nand BERT, among which the best model achieves 48.8 under P@5, indicating a\ngreat challenge presented by COUGH and encouraging future research for further\nimprovement. Our COUGH dataset is available at\nhttps://github.com/sunlab-osu/covid-faq.",
          "link": "http://arxiv.org/abs/2010.12800",
          "publishedOn": "2021-09-13T07:20:34.412Z",
          "wordCount": 652,
          "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.14876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Burdick_L/0/1/0/all/0/1\">Laura Burdick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures, both in English and in other\nlanguages. To gain further insight into word embeddings, we explore their\nstability (e.g., overlap between the nearest neighbors of a word in different\nembedding spaces) in diverse languages. We discuss linguistic properties that\nare related to stability, drawing out insights about correlations with\naffixing, language gender systems, and other features. This has implications\nfor embedding use, particularly in research that uses them to study language\ntrends.",
          "link": "http://arxiv.org/abs/2004.14876",
          "publishedOn": "2021-09-13T07:20:33.882Z",
          "wordCount": null,
          "title": "Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>",
          "description": "Recent work has demonstrated that pre-training in-domain language models can\nboost performance when adapting to a new domain. However, the costs associated\nwith pre-training raise an important question: given a fixed budget, what steps\nshould an NLP practitioner take to maximize performance? In this paper, we\nstudy domain adaptation under budget constraints, and approach it as a customer\nchoice problem between data annotation and pre-training. Specifically, we\nmeasure the annotation cost of three procedural text datasets and the\npre-training cost of three in-domain language models. Then we evaluate the\nutility of different combinations of pre-training and data annotation under\nvarying budget constraints to assess which combination strategy works best. We\nfind that, for small budgets, spending all funds on annotation leads to the\nbest performance; once the budget becomes large enough, a combination of data\nannotation and in-domain pre-training works more optimally. We therefore\nsuggest that task-specific data annotation should be part of an economical\nstrategy when adapting an NLP model to a new domain.",
          "link": "http://arxiv.org/abs/2109.04711",
          "publishedOn": "2021-09-13T07:20:33.658Z",
          "wordCount": 611,
          "title": "Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_T/0/1/0/all/0/1\">Thomas Hikaru Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>",
          "description": "Infusing factual knowledge into pre-trained models is fundamental for many\nknowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions\n(MoP), an infusion approach that can handle a very large knowledge graph (KG)\nby partitioning it into smaller sub-graphs and infusing their specific\nknowledge into various BERT models using lightweight adapters. To leverage the\noverall factual knowledge for a target task, these sub-graph adapters are\nfurther fine-tuned along with the underlying BERT through a mixture layer. We\nevaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on\nsix downstream tasks (inc. NLI, QA, Classification), and the results show that\nour MoP consistently enhances the underlying BERTs in task performance, and\nachieves new SOTA performances on five evaluated datasets.",
          "link": "http://arxiv.org/abs/2109.04810",
          "publishedOn": "2021-09-13T07:20:33.447Z",
          "wordCount": 566,
          "title": "Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.",
          "link": "http://arxiv.org/abs/2104.09574",
          "publishedOn": "2021-09-13T07:20:33.141Z",
          "wordCount": null,
          "title": "Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konno_R/0/1/0/all/0/1\">Ryuto Konno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubayashi_Y/0/1/0/all/0/1\">Yuichiroh Matsubayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>",
          "description": "Masked language models (MLMs) have contributed to drastic performance\nimprovements with regard to zero anaphora resolution (ZAR). To further improve\nthis approach, in this study, we made two proposals. The first is a new\npretraining task that trains MLMs on anaphoric relations with explicit\nsupervision, and the second proposal is a new finetuning method that remedies a\nnotorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese\nZAR demonstrated that our two proposals boost the state-of-the-art performance,\nand our detailed analysis provides new insights on the remaining challenges.",
          "link": "http://arxiv.org/abs/2104.07425",
          "publishedOn": "2021-09-13T07:20:33.109Z",
          "wordCount": null,
          "title": "Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanka_S/0/1/0/all/0/1\">Sameera Lanka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>",
          "description": "As neural language models approach human performance on NLP benchmark tasks,\ntheir advances are widely seen as evidence of an increasingly complex\nunderstanding of syntax. This view rests upon a hypothesis that has not yet\nbeen empirically tested: that word order encodes meaning essential to\nperforming these tasks. We refute this hypothesis in many cases: in the GLUE\nsuite and in various genres of English text, the words in a sentence or phrase\ncan rarely be permuted to form a phrase carrying substantially different\ninformation. Our surprising result relies on inference by iterative shuffling\n(IBIS), a novel, efficient procedure that finds the ordering of a bag of words\nhaving the highest likelihood under a fixed language model. IBIS can use any\nblack-box model without additional training and is superior to existing word\nordering algorithms. Coalescing our findings, we discuss how shuffling\ninference procedures such as IBIS can benefit language modeling and constrained\ngeneration.",
          "link": "http://arxiv.org/abs/2109.04867",
          "publishedOn": "2021-09-13T07:20:33.074Z",
          "wordCount": null,
          "title": "Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>",
          "description": "Language agnostic and semantic-language information isolation is an emerging\nresearch direction for multilingual representations models. We explore this\nproblem from a novel angle of geometric algebra and semantic space. A simple\nbut highly effective method \"Language Information Removal (LIR)\" factors out\nlanguage identity information from semantic related components in multilingual\nrepresentations pre-trained on multi-monolingual data. A post-training and\nmodel-agnostic method, LIR only uses simple linear operations, e.g. matrix\nfactorization and orthogonal projection. LIR reveals that for weak-alignment\nmultilingual systems, the principal components of semantic spaces primarily\nencodes language identity information. We first evaluate the LIR on a\ncross-lingual question answer retrieval task (LAReQA), which requires the\nstrong alignment for the multilingual embedding space. Experiment shows that\nLIR is highly effectively on this task, yielding almost 100% relative\nimprovement in MAP for weak-alignment models. We then evaluate the LIR on\nAmazon Reviews and XEVAL dataset, with the observation that removing language\ninformation is able to improve the cross-lingual transfer performance.",
          "link": "http://arxiv.org/abs/2109.04727",
          "publishedOn": "2021-09-13T07:20:30.059Z",
          "wordCount": 628,
          "title": "A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyoungSeok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gichang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_D/0/1/0/all/0/1\">Dong Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonhoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dongpil Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heungsub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minyoung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsub Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Suk Hyun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taeyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_N/0/1/0/all/0/1\">Na-Hyeon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1\">Soobin Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+In_S/0/1/0/all/0/1\">Sookyo In</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungduk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yong Goo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_D/0/1/0/all/0/1\">Donghoon Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongju Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1\">Inho Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>",
          "description": "GPT-3 shows remarkable in-context learning ability of large-scale language\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\nthe performances of different sized models, and the effect of recently\nintroduced prompt optimization on in-context learning. To achieve this, we\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\nwith our training configuration shows state-of-the-art in-context zero-shot and\nfew-shot learning performances on various downstream tasks in Korean. Also, we\nshow the performance benefits of prompt-based learning and demonstrate how it\ncan be integrated into the prompt engineering pipeline. Then we discuss the\npossibility of materializing the No Code AI paradigm by providing AI\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\nan interactive prompt engineering interface. Lastly, we demonstrate the\npotential of our methods with three successful in-house applications.",
          "link": "http://arxiv.org/abs/2109.04650",
          "publishedOn": "2021-09-13T07:20:29.831Z",
          "wordCount": 695,
          "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>",
          "description": "Zero-shot translation, directly translating between language pairs unseen in\ntraining, is a promising capability of multilingual neural machine translation\n(NMT). However, it usually suffers from capturing spurious correlations between\nthe output language and language invariant semantics due to the maximum\nlikelihood training objective, leading to poor transfer performance on\nzero-shot translation. In this paper, we introduce a denoising autoencoder\nobjective based on pivot language into traditional training objective to\nimprove the translation accuracy on zero-shot directions. The theoretical\nanalysis from the perspective of latent variables shows that our approach\nactually implicitly maximizes the probability distributions for zero-shot\ndirections. On two benchmark machine translation datasets, we demonstrate that\nthe proposed method is able to effectively eliminate the spurious correlations\nand significantly outperforms state-of-the-art methods with a remarkable\nperformance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.",
          "link": "http://arxiv.org/abs/2109.04705",
          "publishedOn": "2021-09-13T07:20:29.823Z",
          "wordCount": 590,
          "title": "Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>",
          "description": "Natural language relies on a finite lexicon to express an unbounded set of\nemerging ideas. One result of this tension is the formation of new\ncompositions, such that existing linguistic units can be combined with emerging\nitems into novel expressions. We develop a framework that exploits the\ncognitive mechanisms of chaining and multimodal knowledge to predict emergent\ncompositional expressions through time. We present the syntactic frame\nextension model (SFEM) that draws on the theory of chaining and knowledge from\n\"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to\nform new compositions with existing and novel nouns. We evaluate SFEM\nrigorously on the 1) modalities of knowledge and 2) categorization models of\nchaining, in a syntactically parsed English corpus over the past 150 years. We\nshow that multimodal SFEM predicts newly emerged verb syntax and arguments\nsubstantially better than competing models using purely linguistic or unimodal\nknowledge. We find support for an exemplar view of chaining as opposed to a\nprototype view and reveal how the joint approach of multimodal chaining may be\nfundamental to the creation of literal and figurative language uses including\nmetaphor and metonymy.",
          "link": "http://arxiv.org/abs/2109.04652",
          "publishedOn": "2021-09-13T07:20:29.762Z",
          "wordCount": 633,
          "title": "Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>",
          "description": "We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for\nsemantic parsing in task-oriented dialog. Our model consists of an encoder\nnetwork that incrementally builds the semantic parse tree by predicting the\nnon-terminal label and its positions in the linearized tree. At the generation\ntime, the model constructs the semantic parse tree by recursively inserting the\npredicted non-terminal labels at the predicted positions until termination.\nRINE achieves state-of-the-art exact match accuracy on low- and high-resource\nversions of the conversational semantic parsing benchmark TOP (Gupta et al.,\n2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and\ntransition-based parsers. We also show that our model design is applicable to\nnested named entity recognition task, where it performs on par with\nstate-of-the-art approach designed for that task. Finally, we demonstrate that\nour approach is 2-3.5 times faster than the sequence-to-sequence model at\ninference time.",
          "link": "http://arxiv.org/abs/2109.04500",
          "publishedOn": "2021-09-13T07:20:29.298Z",
          "wordCount": 581,
          "title": "Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangkai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to construct\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named Mixsum, in addition to the cross-entropy-loss.\nExperiments on real-world text classification datasets (Amazon-5, Yelp-5, AG\nNews, and IMDb) demonstrate the effectiveness of the proposed contrastive\nlearning framework with summarization-based data augmentation and Mixsum\nregularization.",
          "link": "http://arxiv.org/abs/2104.05094",
          "publishedOn": "2021-09-13T07:20:29.147Z",
          "wordCount": 629,
          "title": "Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.12251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ameen Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Sidharth Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_S/0/1/0/all/0/1\">Spyros Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1\">Ruhi Sarikaya</a>",
          "description": "Natural Language Understanding (NLU) is an established component within a\nconversational AI or digital assistant system, and it is responsible for\nproducing semantic understanding of a user request. We propose a scalable and\nautomatic approach for improving NLU in a large-scale conversational AI system\nby leveraging implicit user feedback, with an insight that user interaction\ndata and dialog context have rich information embedded from which user\nsatisfaction and intention can be inferred. In particular, we propose a general\ndomain-agnostic framework for curating new supervision data for improving NLU\nfrom live production traffic. With an extensive set of experiments, we show the\nresults of applying the framework and improving NLU for a large-scale\nproduction system and show its impact across 10 domains.",
          "link": "http://arxiv.org/abs/2010.12251",
          "publishedOn": "2021-09-13T07:20:29.140Z",
          "wordCount": 630,
          "title": "A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Falis_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Falis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>",
          "description": "Large-Scale Multi-Label Text Classification (LMTC) includes tasks with\nhierarchical label spaces, such as automatic assignment of ICD-9 codes to\ndischarge summaries. Performance of models in prior art is evaluated with\nstandard precision, recall, and F1 measures without regard for the rich\nhierarchical structure. In this work we argue for hierarchical evaluation of\nthe predictions of neural LMTC models. With the example of the ICD-9 ontology\nwe describe a structural issue in the representation of the structured label\nspace in prior art, and propose an alternative representation based on the\ndepth of the ontology. We propose a set of metrics for hierarchical evaluation\nusing the depth-based representation. We compare the evaluation scores from the\nproposed metrics with previously used metrics on prior art LMTC models for\nICD-9 coding in MIMIC-III. We also propose further avenues of research\ninvolving the proposed ontological representation.",
          "link": "http://arxiv.org/abs/2109.04853",
          "publishedOn": "2021-09-13T07:20:29.071Z",
          "wordCount": 594,
          "title": "CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Huiyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>",
          "description": "Human conversations naturally evolve around different topics and fluently\nmove between them. In research on dialog systems, the ability to actively and\nsmoothly transition to new topics is often ignored. In this paper we introduce\nTIAGE, a new topic-shift aware dialog benchmark constructed utilizing human\nannotations on topic shifts. Based on TIAGE, we introduce three tasks to\ninvestigate different scenarios of topic-shift modeling in dialog settings:\ntopic-shift detection, topic-shift triggered response generation and\ntopic-aware dialog generation. Experiments on these tasks show that the\ntopic-shift signals in TIAGE are useful for topic-shift response generation. On\nthe other hand, dialog systems still struggle to decide when to change topic.\nThis indicates further research is needed in topic-shift aware dialog modeling.",
          "link": "http://arxiv.org/abs/2109.04562",
          "publishedOn": "2021-09-13T07:20:28.332Z",
          "wordCount": 569,
          "title": "TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saladi_P/0/1/0/all/0/1\">Puneeth S.M. Saladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Transformers (Vaswani et al., 2017) have brought a remarkable improvement in\nthe performance of neural machine translation (NMT) systems but they could be\nsurprisingly vulnerable to noise. In this work, we try to investigate how noise\nbreaks Transformers and if there exist solutions to deal with such issues.\nThere is a large body of work in the NMT literature on analyzing the behavior\nof conventional models for the problem of noise but Transformers are relatively\nunderstudied in this context. Motivated by this, we introduce a novel\ndata-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate\nnoise during training. This idea is comparable to the well-known fine-tuning\nstrategy. Moreover, we propose two other novel extensions to the original\nTransformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that\nmodify the neural architecture as well as the training process to handle noise.\nOne important characteristic of our techniques is that they only impact the\ntraining phase and do not impose any overhead at inference time. We evaluated\nour techniques to translate the English--German pair in both directions and\nobserved that our models have a higher tolerance to noise. More specifically,\nthey perform with no deterioration where up to 10% of entire test words are\ninfected by noise.",
          "link": "http://arxiv.org/abs/2012.15710",
          "publishedOn": "2021-09-13T07:20:28.239Z",
          "wordCount": 682,
          "title": "Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-hun Lee</a>",
          "description": "Multi-turn response selection models have recently shown comparable\nperformance to humans in several benchmark datasets. However, in the real\nenvironment, these models often have weaknesses, such as making incorrect\npredictions based heavily on superficial patterns without a comprehensive\nunderstanding of the context. For example, these models often give a high score\nto the wrong response candidate containing several keywords related to the\ncontext but using the inconsistent tense. In this study, we analyze the\nweaknesses of the open-domain Korean Multi-turn response selection models and\npublish an adversarial dataset to evaluate these weaknesses. We also suggest a\nstrategy to build a robust model in this adversarial environment.",
          "link": "http://arxiv.org/abs/2109.04834",
          "publishedOn": "2021-09-13T07:20:28.222Z",
          "wordCount": 563,
          "title": "An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobnik_S/0/1/0/all/0/1\">Simon Dobnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_R/0/1/0/all/0/1\">Robin Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_B/0/1/0/all/0/1\">Bill Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_S/0/1/0/all/0/1\">Staffan Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilinykh_N/0/1/0/all/0/1\">Nikolai Ilinykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maraev_V/0/1/0/all/0/1\">Vladislav Maraev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somashekarappa_V/0/1/0/all/0/1\">Vidya Somashekarappa</a>",
          "description": "In this paper we examine different meaning representations that are commonly\nused in different natural language applications today and discuss their limits,\nboth in terms of the aspects of the natural language meaning they are modelling\nand in terms of the aspects of the application for which they are used.",
          "link": "http://arxiv.org/abs/2109.04949",
          "publishedOn": "2021-09-13T07:20:28.217Z",
          "wordCount": 528,
          "title": "We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>",
          "description": "Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space",
          "link": "http://arxiv.org/abs/2109.04993",
          "publishedOn": "2021-09-13T07:20:28.163Z",
          "wordCount": 581,
          "title": "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demotte_P/0/1/0/all/0/1\">Piyumal Demotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>",
          "description": "Text classification systems based on contextual embeddings are not viable\noptions for many of the low resource languages. On the other hand, recently\nintroduced capsule networks have shown performance in par with these text\nclassification models. Thus, they could be considered as a viable alternative\nfor text classification for languages that do not have pre-trained contextual\nembedding models. However, current capsule networks depend upon spatial\npatterns without considering the sequential features of the text. They are also\nsub-optimal in capturing the context-level information in longer sequences.\nThis paper presents a novel Dual-State Capsule (DS-Caps) network-based\ntechnique for text classification, which is optimized to mitigate these issues.\nTwo varieties of states, namely sentence-level and word-level, are integrated\nwith capsule layers to capture deeper context-level information for language\nmodeling. The dynamic routing process among capsules was also optimized using\nthe context-level information obtained through sentence-level states. The\nDS-Caps networks outperform the existing capsule network architectures for\nmultiple datasets, particularly for tasks with longer sequences of text. We\nalso demonstrate the superiority of DS-Caps in text classification for a low\nresource language.",
          "link": "http://arxiv.org/abs/2109.04762",
          "publishedOn": "2021-09-13T07:20:28.123Z",
          "wordCount": 622,
          "title": "Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yupei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>",
          "description": "Various measures have been proposed to quantify human-like social biases in\nword embeddings. However, bias scores based on these measures can suffer from\nmeasurement error. One indication of measurement quality is reliability,\nconcerning the extent to which a measure produces consistent results. In this\npaper, we assess three types of reliability of word embedding gender bias\nmeasures, namely test-retest reliability, inter-rater consistency and internal\nconsistency. Specifically, we investigate the consistency of bias scores across\ndifferent choices of random seeds, scoring rules and words. Furthermore, we\nanalyse the effects of various factors on these measures' reliability scores.\nOur findings inform better design of word embedding gender bias measures.\nMoreover, we urge researchers to be more critical about the application of such\nmeasures.",
          "link": "http://arxiv.org/abs/2109.04732",
          "publishedOn": "2021-09-13T07:20:27.855Z",
          "wordCount": 572,
          "title": "Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno Miguel Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>",
          "description": "Selective rationalization aims to produce decisions along with rationales\n(e.g., text highlights or word alignments between two sentences). Commonly,\nrationales are modeled as stochastic binary masks, requiring sampling-based\ngradient estimators, which complicates training and requires careful\nhyperparameter tuning. Sparse attention mechanisms are a deterministic\nalternative, but they lack a way to regularize the rationale extraction (e.g.,\nto control the sparsity of a text highlight or the number of alignments). In\nthis paper, we present a unified framework for deterministic extraction of\nstructured explanations via constrained inference on a factor graph, forming a\ndifferentiable layer. Our approach greatly eases training and rationale\nregularization, generally outperforming previous work on what comes to\nperformance and plausibility of the extracted rationales. We further provide a\ncomparative study of stochastic and deterministic methods for rationale\nextraction for classification and natural language inference tasks, jointly\nassessing their predictive power, quality of the explanations, and model\nvariability.",
          "link": "http://arxiv.org/abs/2109.04552",
          "publishedOn": "2021-09-13T07:20:27.838Z",
          "wordCount": 596,
          "title": "SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1\">Clifton Poth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.",
          "link": "http://arxiv.org/abs/2104.08247",
          "publishedOn": "2021-09-13T07:20:25.922Z",
          "wordCount": 620,
          "title": "What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.",
          "link": "http://arxiv.org/abs/2104.08645",
          "publishedOn": "2021-09-13T07:20:25.891Z",
          "wordCount": 664,
          "title": "Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yuchen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoef_T/0/1/0/all/0/1\">Tessa Verhoef</a>",
          "description": "Natural languages display a trade-off among different strategies to convey\nsyntactic structure, such as word order or inflection. This trade-off, however,\nhas not appeared in recent simulations of iterated language learning with\nneural network agents (Chaabouni et al., 2019b). We re-evaluate this result in\nlight of three factors that play an important role in comparable experiments\nfrom the Language Evolution field: (i) speaker bias towards efficient\nmessaging, (ii) non systematic input languages, and (iii) learning bottleneck.\nOur simulations show that neural agents mainly strive to maintain the utterance\ntype distribution observed during learning, instead of developing a more\nefficient or systematic language.",
          "link": "http://arxiv.org/abs/2104.07637",
          "publishedOn": "2021-09-13T07:20:25.885Z",
          "wordCount": 591,
          "title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>",
          "description": "Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.",
          "link": "http://arxiv.org/abs/2012.15562",
          "publishedOn": "2021-09-13T07:20:25.878Z",
          "wordCount": 669,
          "title": "UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n\nA crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.",
          "link": "http://arxiv.org/abs/2104.06979",
          "publishedOn": "2021-09-13T07:20:25.860Z",
          "wordCount": 640,
          "title": "TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>",
          "description": "The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.",
          "link": "http://arxiv.org/abs/2010.09697",
          "publishedOn": "2021-09-13T07:20:25.855Z",
          "wordCount": 712,
          "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>",
          "description": "Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.",
          "link": "http://arxiv.org/abs/2109.04611",
          "publishedOn": "2021-09-13T07:20:25.849Z",
          "wordCount": 626,
          "title": "Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.07352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "Neural Machine Translation models are sensitive to noise in the input texts,\nsuch as misspelled words and ungrammatical constructions. Existing robustness\ntechniques generally fail when faced with unseen types of noise and their\nperformance degrades on clean texts. In this paper, we focus on three types of\nrealistic noise that are commonly generated by humans and introduce the idea of\nvisual context to improve translation robustness for noisy texts. In addition,\nwe describe a novel error correction training regime that can be used as an\nauxiliary task to further improve translation robustness. Experiments on\nEnglish-French and English-German translation show that both multimodal and\nerror correction components improve model robustness to noisy texts, while\nstill retaining translation quality on clean texts.",
          "link": "http://arxiv.org/abs/2103.07352",
          "publishedOn": "2021-09-13T07:20:25.817Z",
          "wordCount": 593,
          "title": "Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>",
          "description": "Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.",
          "link": "http://arxiv.org/abs/2105.03432",
          "publishedOn": "2021-09-13T07:20:25.799Z",
          "wordCount": 620,
          "title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>",
          "description": "We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.",
          "link": "http://arxiv.org/abs/2104.06022",
          "publishedOn": "2021-09-13T07:20:25.793Z",
          "wordCount": 569,
          "title": "Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.11601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, REDCODER, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. REDCODER has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.",
          "link": "http://arxiv.org/abs/2108.11601",
          "publishedOn": "2021-09-13T07:20:25.776Z",
          "wordCount": 629,
          "title": "Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>",
          "description": "We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.",
          "link": "http://arxiv.org/abs/2104.08384",
          "publishedOn": "2021-09-13T07:20:25.770Z",
          "wordCount": 702,
          "title": "\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-09-13T07:20:25.729Z",
          "wordCount": 570,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>",
          "description": "A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n\nThis article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.",
          "link": "http://arxiv.org/abs/2108.12971",
          "publishedOn": "2021-09-13T07:20:25.722Z",
          "wordCount": 677,
          "title": "HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Codifying commonsense knowledge in machines is a longstanding goal of\nartificial intelligence. Recently, much progress toward this goal has been made\nwith automatic knowledge base (KB) construction techniques. However, such\ntechniques focus primarily on the acquisition of positive (true) KB statements,\neven though negative (false) statements are often also important for\ndiscriminative reasoning over commonsense KBs. As a first step toward the\nlatter, this paper proposes NegatER, a framework that ranks potential negatives\nin commonsense KBs using a contextual language model (LM). Importantly, as most\nKBs do not contain negatives, NegatER relies only on the positive knowledge in\nthe LM and does not require ground-truth negative examples. Experiments\ndemonstrate that, compared to multiple contrastive data augmentation\napproaches, NegatER yields negatives that are more grammatical, coherent, and\ninformative -- leading to statistically significant accuracy improvements in a\nchallenging KB completion task and confirming that the positive knowledge in\nLMs can be \"re-purposed\" to generate negative knowledge.",
          "link": "http://arxiv.org/abs/2011.07497",
          "publishedOn": "2021-09-13T07:20:25.714Z",
          "wordCount": 632,
          "title": "NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Keeping track of scientific challenges, advances and emerging directions is a\nfundamental part of research. However, researchers face a flood of papers that\nhinders discovery of important knowledge. In biomedicine, this directly impacts\nhuman lives. To address this problem, we present a novel task of extraction and\nsearch of scientific challenges and directions, to facilitate rapid knowledge\ndiscovery. We construct and release an expert-annotated corpus of texts sampled\nfrom full-length papers, labeled with novel semantic categories that generalize\nacross many types of challenges and directions. We focus on a large corpus of\ninterdisciplinary work relating to the COVID-19 pandemic, ranging from\nbiomedicine to areas such as AI and economics. We apply a model trained on our\ndata to identify challenges and directions across the corpus and build a\ndedicated search engine. In experiments with 19 researchers and clinicians\nusing our system, we outperform a popular scientific search engine in assisting\nknowledge discovery. Finally, we show that models trained on our resource\ngeneralize to the wider biomedical domain and to AI papers, highlighting its\nbroad utility. We make our data, model and search engine publicly available.\nhttps://challenges.apps.allenai.org/",
          "link": "http://arxiv.org/abs/2108.13751",
          "publishedOn": "2021-09-13T07:20:25.707Z",
          "wordCount": 720,
          "title": "A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>",
          "description": "The ability to recognise emotions lends a conversational artificial\nintelligence a human touch. While emotions in chit-chat dialogues have received\nsubstantial attention, emotions in task-oriented dialogues have been largely\noverlooked despite having an equally important role, such as to signal failure\nor success. Existing emotion-annotated task-oriented corpora are limited in\nsize, label richness, and public availability, creating a bottleneck for\ndownstream tasks. To lay a foundation for studies on emotions in task-oriented\ndialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus\nof task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain\ntask-oriented dialogue dataset. It contains more than 11K dialogues with more\nthan 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz\ndialogues from MultiWOZ, we collect human-machine dialogues within the same set\nof domains to sufficiently cover the space of various emotions that can happen\nduring the lifetime of a data-driven dialogue system. To the best of our\nknowledge, this is the first large-scale open-source corpus of its kind. We\npropose a novel emotion labelling scheme, which is tailored to task-oriented\ndialogues. We report a set of experimental results to show the usability of\nthis corpus for emotion recognition and state tracking in task-oriented\ndialogues.",
          "link": "http://arxiv.org/abs/2109.04919",
          "publishedOn": "2021-09-13T07:20:25.700Z",
          "wordCount": 655,
          "title": "EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Relational knowledge bases (KBs) are commonly used to represent world\nknowledge in machines. However, while advantageous for their high degree of\nprecision and interpretability, KBs are usually organized according to\nmanually-defined schemas, which limit their expressiveness and require\nsignificant human efforts to engineer and maintain. In this review, we take a\nnatural language processing perspective to these limitations, examining how\nthey may be addressed in part by training deep contextual language models (LMs)\nto internalize and express relational knowledge in more flexible forms. We\npropose to organize knowledge representation strategies in LMs by the level of\nKB supervision provided, from no KB supervision at all to entity- and\nrelation-level supervision. Our contributions are threefold: (1) We provide a\nhigh-level, extensible taxonomy for knowledge representation in LMs; (2) Within\nour taxonomy, we highlight notable models, evaluation tasks, and findings, in\norder to provide an up-to-date review of current knowledge representation\ncapabilities in LMs; and (3) We suggest future research directions that build\nupon the complementary aspects of LMs and KBs as knowledge representations.",
          "link": "http://arxiv.org/abs/2104.05837",
          "publishedOn": "2021-09-13T07:20:25.681Z",
          "wordCount": 650,
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Evgeny Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_B/0/1/0/all/0/1\">Benjamin Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Adelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>",
          "description": "We introduce Generative Spoken Language Modeling, the task of learning the\nacoustic and linguistic characteristics of a language from raw audio (no text,\nno labels), and a set of metrics to automatically evaluate the learned\nrepresentations at acoustic and linguistic levels for both encoding and\ngeneration. We set up baseline systems consisting of a discrete speech encoder\n(returning pseudo-text units), a generative language model (trained on\npseudo-text), and a speech decoder (generating a waveform from pseudo-text) all\ntrained without supervision and validate the proposed metrics with human\nevaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that\nthe number of discrete units (50, 100, or 200) matters in a task-dependent and\nencoder-dependent way, and that some combinations approach text-based systems.",
          "link": "http://arxiv.org/abs/2102.01192",
          "publishedOn": "2021-09-13T07:20:25.654Z",
          "wordCount": 606,
          "title": "Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chheda_T/0/1/0/all/0/1\">Tejas Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Purujit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "A major factor contributing to the success of modern representation learning\nis the ease of performing various vector operations. Recently, objects with\ngeometric structures (eg. distributions, complex or hyperbolic vectors, or\nregions such as cones, disks, or boxes) have been explored for their\nalternative inductive biases and additional representational capacities. In\nthis work, we introduce Box Embeddings, a Python library that enables\nresearchers to easily apply and extend probabilistic box embeddings.",
          "link": "http://arxiv.org/abs/2109.04997",
          "publishedOn": "2021-09-13T07:20:25.629Z",
          "wordCount": 552,
          "title": "Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>",
          "description": "Pretraining techniques leveraging enormous datasets have driven recent\nadvances in text summarization. While folk explanations suggest that knowledge\ntransfer accounts for pretraining's benefits, little is known about why it\nworks or what makes a pretraining task or dataset suitable. In this paper, we\nchallenge the knowledge transfer story, showing that pretraining on documents\nconsisting of character n-grams selected at random, we can nearly match the\nperformance of models pretrained on real corpora. This work holds the promise\nof eliminating upstream corpora, which may alleviate some concerns over\noffensive language, bias, and copyright issues. To see whether the small\nresidual benefit of using real data could be accounted for by the structure of\nthe pretraining task, we design several tasks motivated by a qualitative study\nof summarization corpora. However, these tasks confer no appreciable benefit,\nleaving open the possibility of a small role for knowledge transfer.",
          "link": "http://arxiv.org/abs/2109.04953",
          "publishedOn": "2021-09-13T07:20:25.592Z",
          "wordCount": 594,
          "title": "Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Influence functions approximate the \"influences\" of training data-points for\ntest predictions and have a wide variety of applications. Despite the\npopularity, their computational cost does not scale well with model and\ntraining data size. We present FastIF, a set of simple modifications to\ninfluence functions that significantly improves their run-time. We use\nk-Nearest Neighbors (kNN) to narrow the search space down to a subset of good\ncandidate data points, identify the configurations that best balance the\nspeed-quality trade-off in estimating the inverse Hessian-vector product, and\nintroduce a fast parallel variant. Our proposed method achieves about 80X\nspeedup while being highly correlated with the original influence values. With\nthe availability of the fast influence functions, we demonstrate their\nusefulness in four applications. First, we examine whether influential\ndata-points can \"explain\" test time behavior using the framework of\nsimulatability. Second, we visualize the influence interactions between\ntraining and test data-points. Third, we show that we can correct model errors\nby additional fine-tuning on certain influential data-points, improving the\naccuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we\nexperiment with a similar setup but fine-tuning on datapoints not seen during\ntraining, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI\ndatasets respectively. Overall, our fast influence functions can be efficiently\napplied to large models and datasets, and our experiments demonstrate the\npotential of influence functions in model interpretation and correcting model\nerrors. Code is available at\nhttps://github.com/salesforce/fast-influence-functions",
          "link": "http://arxiv.org/abs/2012.15781",
          "publishedOn": "2021-09-13T07:20:25.494Z",
          "wordCount": null,
          "title": "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiuhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>",
          "description": "The Transformer translation model is based on the multi-head attention\nmechanism, which can be parallelized easily. The multi-head attention network\nperforms the scaled dot-product attention function in parallel, empowering the\nmodel by jointly attending to information from different representation\nsubspaces at different positions. In this paper, we present an approach to\nlearning a hard retrieval attention where an attention head only attends to one\ntoken in the sentence rather than all tokens. The matrix multiplication between\nattention probabilities and the value sequence in the standard scaled\ndot-product attention can thus be replaced by a simple and efficient retrieval\noperation. We show that our hard retrieval attention mechanism is 1.43 times\nfaster in decoding, while preserving translation quality on a wide range of\nmachine translation tasks when used in the decoder self- and cross-attention\nnetworks.",
          "link": "http://arxiv.org/abs/2009.14658",
          "publishedOn": "2021-09-13T07:20:25.483Z",
          "wordCount": null,
          "title": "Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "Meta-learning has achieved great success in leveraging the historical learned\nknowledge to facilitate the learning process of the new task. However, merely\nlearning the knowledge from the historical tasks, adopted by current\nmeta-learning algorithms, may not generalize well to testing tasks when they\nare not well-supported by training tasks. This paper studies a low-resource\ntext classification problem and bridges the gap between meta-training and\nmeta-testing tasks by leveraging the external knowledge bases. Specifically, we\npropose KGML to introduce additional representation for each sentence learned\nfrom the extracted sentence-specific knowledge graph. The extensive experiments\non three datasets demonstrate the effectiveness of KGML under both supervised\nadaptation and unsupervised adaptation settings.",
          "link": "http://arxiv.org/abs/2109.04707",
          "publishedOn": "2021-09-13T07:20:25.482Z",
          "wordCount": null,
          "title": "Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>",
          "description": "Transformer-based models have gained increasing popularity achieving\nstate-of-the-art performance in many research fields including speech\ntranslation. However, Transformer's quadratic complexity with respect to the\ninput sequence length prevents its adoption as is with audio signals, which are\ntypically represented by long sequences. Current solutions resort to an initial\nsub-optimal compression based on a fixed sampling of raw audio features.\nTherefore, potentially useful linguistic information is not accessible to\nhigher-level layers in the architecture. To solve this issue, we propose\nSpeechformer, an architecture that, thanks to reduced memory usage in the\nattention layers, avoids the initial lossy compression and aggregates\ninformation only at a higher level according to more informed linguistic\ncriteria. Experiments on three language pairs (en->de/es/nl) show the efficacy\nof our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and\nof up to 4.0 BLEU in a low resource scenario.",
          "link": "http://arxiv.org/abs/2109.04574",
          "publishedOn": "2021-09-13T07:20:25.478Z",
          "wordCount": null,
          "title": "Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>",
          "description": "We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.",
          "link": "http://arxiv.org/abs/2105.03824",
          "publishedOn": "2021-09-13T07:20:25.470Z",
          "wordCount": null,
          "title": "FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>",
          "description": "Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.",
          "link": "http://arxiv.org/abs/2109.04947",
          "publishedOn": "2021-09-13T07:20:25.468Z",
          "wordCount": null,
          "title": "Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>",
          "description": "We present IndoBERTweet, the first large-scale pretrained model for\nIndonesian Twitter that is trained by extending a monolingually-trained\nIndonesian BERT model with additive domain-specific vocabulary. We focus in\nparticular on efficient model adaptation under vocabulary mismatch, and\nbenchmark different ways of initializing the BERT embedding layer for new word\ntypes. We find that initializing with the average BERT subword embedding makes\npretraining five times faster, and is more effective than proposed methods for\nvocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based\ndatasets.",
          "link": "http://arxiv.org/abs/2109.04607",
          "publishedOn": "2021-09-13T07:20:25.467Z",
          "wordCount": null,
          "title": "IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>",
          "description": "It is widely accepted that fine-tuning pre-trained language models usually\nbrings about performance improvements in downstream tasks. However, there are\nlimited studies on the reasons behind this effectiveness, particularly from the\nviewpoint of structural changes in the embedding space. Trying to fill this\ngap, in this paper, we analyze the extent to which the isotropy of the\nembedding space changes after fine-tuning. We demonstrate that, even though\nisotropy is a desirable geometrical property, fine-tuning does not necessarily\nresult in isotropy enhancements. Moreover, local structures in pre-trained\ncontextual word representations (CWRs), such as those encoding token types or\nfrequency, undergo a massive change during fine-tuning. Our experiments show\ndramatic growth in the number of elongated directions in the embedding space,\nwhich, in contrast to pre-trained CWRs, carry the essential linguistic\nknowledge in the fine-tuned embedding space, making existing isotropy\nenhancement methods ineffective.",
          "link": "http://arxiv.org/abs/2109.04740",
          "publishedOn": "2021-09-13T07:20:25.465Z",
          "wordCount": null,
          "title": "How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>",
          "description": "Dialogue summarization helps readers capture salient information from long\nconversations in meetings, interviews, and TV series. However, real-world\ndialogues pose a great challenge to current summarization models, as the\ndialogue length typically exceeds the input limits imposed by recent\ntransformer-based pre-trained models, and the interactive nature of dialogues\nmakes relevant information more context-dependent and sparsely distributed than\nnews articles. In this work, we perform a comprehensive study on long dialogue\nsummarization by investigating three strategies to deal with the lengthy input\nproblem and locate relevant information: (1) extended transformer models such\nas Longformer, (2) retrieve-then-summarize pipeline models with several\ndialogue utterance retrieval methods, and (3) hierarchical dialogue encoding\nmodels such as HMNet. Our experimental results on three long dialogue datasets\n(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline\nmodels yield the best performance. We also demonstrate that the summary quality\ncan be further improved with a stronger retrieval model and pretraining on\nproper external summarization datasets.",
          "link": "http://arxiv.org/abs/2109.04609",
          "publishedOn": "2021-09-13T07:20:25.462Z",
          "wordCount": null,
          "title": "An Exploratory Study on Long Dialogue Summarization: What Works and What's Next. (arXiv:2109.04609v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>",
          "description": "Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere \"too much information\" clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.",
          "link": "http://arxiv.org/abs/2101.00433",
          "publishedOn": "2021-09-13T07:20:25.462Z",
          "wordCount": null,
          "title": "Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "Reproducible benchmarks are crucial in driving progress of machine\ntranslation research. However, existing machine translation benchmarks have\nbeen mostly limited to high-resource or well-represented languages. Despite an\nincreasing interest in low-resource machine translation, there are no\nstandardized reproducible benchmarks for many African languages, many of which\nare used by millions of speakers but have less digitized textual data. To\ntackle these challenges, we propose AfroMT, a standardized, clean, and\nreproducible machine translation benchmark for eight widely spoken African\nlanguages. We also develop a suite of analysis tools for system diagnosis\ntaking into account the unique properties of these languages. Furthermore, we\nexplore the newly considered case of low-resource focused pretraining and\ndevelop two novel data augmentation-based strategies, leveraging word-level\nalignment information and pseudo-monolingual data for pretraining multilingual\nsequence-to-sequence models. We demonstrate significant improvements when\npretraining on 11 languages, with gains of up to 2 BLEU points over strong\nbaselines. We also show gains of up to 12 BLEU points over cross-lingual\ntransfer baselines in data-constrained scenarios. All code and pretrained\nmodels will be released as further steps towards larger reproducible benchmarks\nfor African languages.",
          "link": "http://arxiv.org/abs/2109.04715",
          "publishedOn": "2021-09-13T07:20:25.443Z",
          "wordCount": null,
          "title": "AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04602",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1\">Andr&#xe9;s Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>",
          "description": "Current language models are usually trained using a self-supervised scheme,\nwhere the main focus is learning representations at the word or sentence level.\nHowever, there has been limited progress in generating useful discourse-level\nrepresentations. In this work, we propose to use ideas from predictive coding\ntheory to augment BERT-style language models with a mechanism that allows them\nto learn suitable discourse-level representations. As a result, our proposed\napproach is able to predict future sentences using explicit top-down\nconnections that operate at the intermediate layers of the network. By\nexperimenting with benchmarks designed to evaluate discourse-related knowledge\nusing pre-trained sentence representations, we demonstrate that our approach\nimproves performance in 6 out of 11 tasks by excelling in discourse\nrelationship detection.",
          "link": "http://arxiv.org/abs/2109.04602",
          "publishedOn": "2021-09-13T07:20:25.440Z",
          "wordCount": null,
          "title": "Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "We study the problem of training named entity recognition (NER) models using\nonly distantly-labeled data, which can be automatically obtained by matching\nentity mentions in the raw text with entity types in a knowledge base. The\nbiggest challenge of distantly-supervised NER is that the distant supervision\nmay induce incomplete and noisy labels, rendering the straightforward\napplication of supervised learning ineffective. In this paper, we propose (1) a\nnoise-robust learning scheme comprised of a new loss function and a noisy label\nremoval step, for training NER models on distantly-labeled data, and (2) a\nself-training method that uses contextualized augmentations created by\npre-trained language models to improve the generalization ability of the NER\nmodel. On three benchmark datasets, our method achieves superior performance,\noutperforming existing distantly-supervised NER models by significant margins.",
          "link": "http://arxiv.org/abs/2109.05003",
          "publishedOn": "2021-09-13T07:20:25.439Z",
          "wordCount": null,
          "title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04832",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>",
          "description": "Asking questions about a situation is an inherent step towards understanding\nit. To this end, we introduce the task of role question generation, which,\ngiven a predicate mention and a passage, requires producing a set of questions\nasking about all possible semantic roles of the predicate. We develop a\ntwo-stage model for this task, which first produces a context-independent\nquestion prototype for each role and then revises it to be contextually\nappropriate for the passage. Unlike most existing approaches to question\ngeneration, our approach does not require conditioning on existing answers in\nthe text. Instead, we condition on the type of information to inquire about,\nregardless of whether the answer appears explicitly in the text, could be\ninferred from it, or should be sought elsewhere. Our evaluation demonstrates\nthat we generate diverse and well-formed questions for a large, broad-coverage\nontology of predicates and roles.",
          "link": "http://arxiv.org/abs/2109.04832",
          "publishedOn": "2021-09-13T07:20:25.438Z",
          "wordCount": null,
          "title": "Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04666",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanzheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>",
          "description": "It is a well-known approach for fringe groups and organizations to use\neuphemisms -- ordinary-sounding and innocent-looking words with a secret\nmeaning -- to conceal what they are discussing. For instance, drug dealers\noften use \"pot\" for marijuana and \"avocado\" for heroin. From a social media\ncontent moderation perspective, though recent advances in NLP have enabled the\nautomatic detection of such single-word euphemisms, no existing work is capable\nof automatically detecting multi-word euphemisms, such as \"blue dream\"\n(marijuana) and \"black tar\" (heroin). Our paper tackles the problem of\neuphemistic phrase detection without human effort for the first time, as far as\nwe are aware. We first perform phrase mining on a raw text corpus (e.g., social\nmedia posts) to extract quality phrases. Then, we utilize word embedding\nsimilarities to select a set of euphemistic phrase candidates. Finally, we rank\nthose candidates by a masked language model -- SpanBERT. Compared to strong\nbaselines, we report 20-50% higher detection accuracies using our algorithm for\ndetecting euphemistic phrases.",
          "link": "http://arxiv.org/abs/2109.04666",
          "publishedOn": "2021-09-13T07:20:25.392Z",
          "wordCount": 599,
          "title": "Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "In interpretable NLP, we require faithful rationales that reflect the model's\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to \"reasoning\"\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.",
          "link": "http://arxiv.org/abs/2010.12762",
          "publishedOn": "2021-09-13T07:20:25.386Z",
          "wordCount": 641,
          "title": "Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>",
          "description": "Framing a news article means to portray the reported event from a specific\nperspective, e.g., from an economic or a health perspective. Reframing means to\nchange this perspective. Depending on the audience or the submessage, reframing\ncan become necessary to achieve the desired effect on the readers. Reframing is\nrelated to adapting style and sentiment, which can be tackled with neural text\ngeneration techniques. However, it is more challenging since changing a frame\nrequires rewriting entire sentences rather than single phrases. In this paper,\nwe study how to computationally reframe sentences in news articles while\nmaintaining their coherence to the context. We treat reframing as a\nsentence-level fill-in-the-blank task for which we train neural models on an\nexisting media frame corpus. To guide the training, we propose three\nstrategies: framed-language pretraining, named-entity preservation, and\nadversarial learning. We evaluate respective models automatically and manually\nfor topic consistency, coherence, and successful reframing. Our results\nindicate that generating properly-framed text works well but with tradeoffs.",
          "link": "http://arxiv.org/abs/2109.04957",
          "publishedOn": "2021-09-13T07:20:25.378Z",
          "wordCount": 605,
          "title": "Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "Text generation systems are ubiquitous in natural language processing\napplications. However, evaluation of these systems remains a challenge,\nespecially in multilingual settings. In this paper, we propose L'AMBRE -- a\nmetric to evaluate the morphosyntactic well-formedness of text using its\ndependency parse and morphosyntactic rules of the language. We present a way to\nautomatically extract various rules governing morphosyntax directly from\ndependency treebanks. To tackle the noisy outputs from text generation systems,\nwe propose a simple methodology to train robust parsers. We show the\neffectiveness of our metric on the task of machine translation through a\ndiachronic study of systems translating into morphologically-rich languages.",
          "link": "http://arxiv.org/abs/2103.16590",
          "publishedOn": "2021-09-13T07:20:25.369Z",
          "wordCount": 586,
          "title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>",
          "description": "The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.",
          "link": "http://arxiv.org/abs/2109.04876",
          "publishedOn": "2021-09-13T07:20:25.364Z",
          "wordCount": 499,
          "title": "Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1\">Rahul Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Pre-trained language models (PTLMs) have achieved impressive performance on\ncommonsense inference benchmarks, but their ability to employ commonsense to\nmake robust inferences, which is crucial for effective communications with\nhumans, is debated. In the pursuit of advancing fluid human-AI communication,\nwe propose a new challenge, RICA: Robust Inference capability based on\nCommonsense Axioms, that evaluates robust commonsense inference despite textual\nperturbations. To generate data for this challenge, we develop a systematic and\nscalable procedure using commonsense knowledge bases and probe PTLMs across two\ndifferent evaluation settings. Extensive experiments on our generated probe\nsets with more than 10k statements show that PTLMs perform no better than\nrandom guessing on the zero-shot setting, are heavily impacted by statistical\nbiases, and are not robust to perturbation attacks. We also find that\nfine-tuning on similar statements offer limited gains, as PTLMs still fail to\ngeneralize to unseen inferences. Our new large-scale benchmark exposes a\nsignificant gap between PTLMs and human-level language understanding and offers\na new challenge for PTLMs to demonstrate commonsense.",
          "link": "http://arxiv.org/abs/2005.00782",
          "publishedOn": "2021-09-13T07:20:25.343Z",
          "wordCount": 689,
          "title": "RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>",
          "description": "State-of-the-art contextual embeddings are obtained from large language\nmodels available only for a few languages. For others, we need to learn\nrepresentations using a multilingual model. There is an ongoing debate on\nwhether multilingual embeddings can be aligned in a space shared across many\nlanguages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\\v{c}ek,\n2021) allows us to answer this question for specific linguistic features and\nlearn a projection based only on mono-lingual annotated datasets. We evaluate\nsyntactic (UD) and lexical (WordNet) structural information encoded inmBERT's\ncontextual representations for nine diverse languages. We observe that for\nlanguages closely related to English, no transformation is needed. The\nevaluated information is encoded in a shared cross-lingual embedding space. For\nother languages, it is beneficial to apply orthogonal transformation learned\nseparately for each language. We successfully apply our findings to zero-shot\nand few-shot cross-lingual parsing.",
          "link": "http://arxiv.org/abs/2109.04921",
          "publishedOn": "2021-09-13T07:20:25.337Z",
          "wordCount": 586,
          "title": "Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1\">Ronen Tamari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>",
          "description": "Large repositories of products, patents and scientific papers offer an\nopportunity for building systems that scour millions of ideas and help users\ndiscover inspirations. However, idea descriptions are typically in the form of\nunstructured text, lacking key structure that is required for supporting\ncreative innovation interactions. Prior work has explored idea representations\nthat were limited in expressivity, required significant manual effort from\nusers, or dependent on curated knowledge bases with poor coverage. We explore a\nnovel representation that automatically breaks up products into fine-grained\nfunctional facets capturing the purposes and mechanisms of ideas, and use it to\nsupport important creative innovation interactions: functional search for\nideas, and exploration of the design space around a focal problem by viewing\nrelated problem perspectives pooled from across many products. In user studies,\nour approach boosts the quality of creative search and inspirations,\noutperforming strong baselines by 50-60%.",
          "link": "http://arxiv.org/abs/2102.09761",
          "publishedOn": "2021-09-13T07:20:25.331Z",
          "wordCount": 631,
          "title": "Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarrahi_A/0/1/0/all/0/1\">Ali Jarrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safari_L/0/1/0/all/0/1\">Leila Safari</a>",
          "description": "In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.",
          "link": "http://arxiv.org/abs/2109.04835",
          "publishedOn": "2021-09-13T07:20:25.324Z",
          "wordCount": 637,
          "title": "FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1\">Reno Kriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "An important task in NLP applications such as sentence simplification is the\nability to take a long, complex sentence and split it into shorter sentences,\nrephrasing as necessary. We introduce a novel dataset and a new model for this\n`split and rephrase' task. Our BiSECT training data consists of 1 million long\nEnglish sentences paired with shorter, meaning-equivalent English sentences. We\nobtain these by extracting 1-2 sentence alignments in bilingual parallel\ncorpora and then using machine translation to convert both sides of the corpus\ninto the same language. BiSECT contains higher quality training examples than\nprevious Split and Rephrase corpora, with sentence splits that require more\nsignificant modifications. We categorize examples in our corpus, and use these\ncategories in a novel model that allows us to target specific regions of the\ninput sentence to be split and edited. Moreover, we show that models trained on\nBiSECT can perform a wider variety of split operations and improve upon\nprevious state-of-the-art approaches in automatic and human evaluations.",
          "link": "http://arxiv.org/abs/2109.05006",
          "publishedOn": "2021-09-13T07:20:25.316Z",
          "wordCount": 628,
          "title": "BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.16410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates quality assessment on pseudo labels by (meta) learning from the\nsuccessful and failed attempts on Relation Classification Network as an\nadditional meta-objective. To reduce the influence of noisy pseudo labels,\nMetaSRE adopts a pseudo label selection and exploitation scheme which assesses\npseudo label quality on unlabeled samples and only exploits high-quality pseudo\nlabels in a self-training fashion to incrementally augment labeled samples for\nboth robustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2010.16410",
          "publishedOn": "2021-09-13T07:20:25.296Z",
          "wordCount": 639,
          "title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>",
          "description": "Unlike well-structured text, such as news reports and encyclopedia articles,\ndialogue content often comes from two or more interlocutors, exchanging\ninformation with each other. In such a scenario, the topic of a conversation\ncan vary upon progression and the key information for a certain topic is often\nscattered across multiple utterances of different speakers, which poses\nchallenges to abstractly summarize dialogues. To capture the various topic\ninformation of a conversation and outline salient facts for the captured\ntopics, this work proposes two topic-aware contrastive learning objectives,\nnamely coherence detection and sub-summary generation objectives, which are\nexpected to implicitly model the topic change and handle information scattering\nchallenges for the dialogue summarization task. The proposed contrastive\nobjectives are framed as auxiliary tasks for the primary dialogue summarization\ntask, united via an alternative parameter updating strategy. Extensive\nexperiments on benchmark datasets demonstrate that the proposed simple method\nsignificantly outperforms strong baselines and achieves new state-of-the-art\nperformance. The code and trained models are publicly available via\n\\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.",
          "link": "http://arxiv.org/abs/2109.04994",
          "publishedOn": "2021-09-13T07:20:25.287Z",
          "wordCount": 624,
          "title": "Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>",
          "description": "While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.",
          "link": "http://arxiv.org/abs/2109.04699",
          "publishedOn": "2021-09-13T07:20:25.280Z",
          "wordCount": 594,
          "title": "EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>",
          "description": "As large-scale, pre-trained language models achieve human-level and\nsuperhuman accuracy on existing language understanding tasks, statistical bias\nin benchmark data and probing studies have recently called into question their\ntrue capabilities. For a more informative evaluation than accuracy on text\nclassification tasks can offer, we propose evaluating systems through a novel\nmeasure of prediction coherence. We apply our framework to two existing\nlanguage understanding benchmarks with different properties to demonstrate its\nversatility. Our experimental results show that this evaluation framework,\nalthough simple in ideas and implementation, is a quick, effective, and\nversatile measure to provide insight into the coherence of machines'\npredictions.",
          "link": "http://arxiv.org/abs/2109.04922",
          "publishedOn": "2021-09-13T07:20:25.264Z",
          "wordCount": 552,
          "title": "Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>",
          "description": "Warning: this paper contains content that may be offensive or upsetting.\n\nNumerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.",
          "link": "http://arxiv.org/abs/2103.11320",
          "publishedOn": "2021-09-13T07:20:25.246Z",
          "wordCount": 655,
          "title": "Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lulu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.",
          "link": "http://arxiv.org/abs/2109.04600",
          "publishedOn": "2021-09-13T07:20:25.239Z",
          "wordCount": 605,
          "title": "EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1\">Akiko Eriguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>",
          "description": "Multilingual Neural Machine Translation (NMT) enables one model to serve all\ntranslation directions, including ones that are unseen during training, i.e.\nzero-shot translation. Despite being theoretically attractive, current models\noften produce low quality translations -- commonly failing to even produce\noutputs in the right target language. In this work, we observe that off-target\ntranslation is dominant even in strong multilingual systems, trained on massive\nmultilingual corpora. To address this issue, we propose a joint approach to\nregularize NMT models at both representation-level and gradient-level. At the\nrepresentation level, we leverage an auxiliary target language prediction task\nto regularize decoder outputs to retain information about the target language.\nAt the gradient level, we leverage a small amount of direct data (in thousands\nof sentence pairs) to regularize model gradients. Our results demonstrate that\nour approach is highly effective in both reducing off-target translation\noccurrences and improving zero-shot translation performance by +5.59 and +10.38\nBLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our\nmethod also works well when the small amount of direct data is not available.",
          "link": "http://arxiv.org/abs/2109.04778",
          "publishedOn": "2021-09-13T07:20:25.225Z",
          "wordCount": 631,
          "title": "Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_R/0/1/0/all/0/1\">Rishabh Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_S/0/1/0/all/0/1\">Saket Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>",
          "description": "Existing black box search methods have achieved high success rate in\ngenerating adversarial attacks against NLP models. However, such search methods\nare inefficient as they do not consider the amount of queries required to\ngenerate adversarial attacks. Also, prior attacks do not maintain a consistent\nsearch space while comparing different search methods. In this paper, we\npropose a query efficient attack strategy to generate plausible adversarial\nexamples on text classification and entailment tasks. Our attack jointly\nleverages attention mechanism and locality sensitive hashing (LSH) to reduce\nthe query count. We demonstrate the efficacy of our approach by comparing our\nattack with four baselines across three different search spaces. Further, we\nbenchmark our results across the same search space used in prior attacks. In\ncomparison to attacks proposed, on an average, we are able to reduce the query\ncount by 75% across all datasets and target models. We also demonstrate that\nour attack achieves a higher success rate when compared to prior attacks in a\nlimited query setting.",
          "link": "http://arxiv.org/abs/2109.04775",
          "publishedOn": "2021-09-13T07:20:25.219Z",
          "wordCount": 620,
          "title": "A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04673",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>",
          "description": "Identifying relevant knowledge to be used in conversational systems that are\ngrounded in long documents is critical to effective response generation. We\nintroduce a knowledge identification model that leverages the document\nstructure to provide dialogue-contextualized passage encodings and better\nlocate knowledge relevant to the conversation. An auxiliary loss captures the\nhistory of dialogue-document connections. We demonstrate the effectiveness of\nour model on two document-grounded conversational datasets and provide analyses\nshowing generalization to unseen documents and long dialogue contexts.",
          "link": "http://arxiv.org/abs/2109.04673",
          "publishedOn": "2021-09-13T07:20:25.203Z",
          "wordCount": 525,
          "title": "DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>",
          "description": "Transformers that are pre-trained on multilingual corpora, such as, mBERT and\nXLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In\nthe zero-shot transfer setting, only English training data is used, and the\nfine-tuned model is evaluated on another target language. While this works\nsurprisingly well, substantial variance has been observed in target language\nperformance between different fine-tuning runs, and in the zero-shot setup, no\ntarget-language development data is available to select among multiple\nfine-tuned models. Prior work has relied on English dev data to select among\nmodels that are fine-tuned with different learning rates, number of steps and\nother hyperparameters, often resulting in suboptimal choices. In this paper, we\nshow that it is possible to select consistently better models when small\namounts of annotated data are available in auxiliary pivot languages. We\npropose a machine learning approach to model selection that uses the fine-tuned\nmodel's own internal representations to predict its cross-lingual capabilities.\nIn extensive experiments we find that this method consistently selects better\nmodels than English validation data across twenty five languages (including\neight low-resource languages), and often achieves results that are comparable\nto model selection using target language development data.",
          "link": "http://arxiv.org/abs/2010.06127",
          "publishedOn": "2021-09-13T07:20:25.192Z",
          "wordCount": 656,
          "title": "Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wegmann_A/0/1/0/all/0/1\">Anna Wegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>",
          "description": "Style is an integral part of natural language. However, evaluation methods\nfor style measures are rare, often task-specific and usually do not control for\ncontent. We propose the modular, fine-grained and content-controlled\nsimilarity-based STyle EvaLuation framework (STEL) to test the performance of\nany model that can compare two sentences on style. We illustrate STEL with two\ngeneral dimensions of style (formal/informal and simple/complex) as well as two\nspecific characteristics of style (contrac'tion and numb3r substitution). We\nfind that BERT-based methods outperform simple versions of commonly used style\nmeasures like 3-grams, punctuation frequency and LIWC-based approaches. We\ninvite the addition of further tasks and task instances to STEL and hope to\nfacilitate the improvement of style-sensitive measures.",
          "link": "http://arxiv.org/abs/2109.04817",
          "publishedOn": "2021-09-13T07:20:25.179Z",
          "wordCount": 562,
          "title": "Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamchyna_A/0/1/0/all/0/1\">Ale&#x161; Tamchyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>",
          "description": "We test the natural expectation that using MT in professional translation\nsaves human processing time. The last such study was carried out by\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\nwhich has become the state-of-the-art approach since then and also got adopted\nby most translation companies.\n\nThrough an experimental study involving over 30 professional translators for\nEnglish -> Czech translation, we examine the relationship between NMT\nperformance and post-editing time and quality. Across all models, we found that\nbetter MT systems indeed lead to fewer changes in the sentences in this\nindustry setting. The relation between system quality and post-editing time is\nhowever not straightforward and, contrary to the results on phrase-based MT,\nBLEU is definitely not a stable predictor of the time or final output quality.",
          "link": "http://arxiv.org/abs/2109.05016",
          "publishedOn": "2021-09-13T07:20:25.172Z",
          "wordCount": 596,
          "title": "Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>",
          "description": "Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.",
          "link": "http://arxiv.org/abs/2109.04838",
          "publishedOn": "2021-09-13T07:20:25.152Z",
          "wordCount": 595,
          "title": "Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>",
          "description": "The success of bidirectional encoders using masked language models, such as\nBERT, on numerous natural language processing tasks has prompted researchers to\nattempt to incorporate these pre-trained models into neural machine translation\n(NMT) systems. However, proposed methods for incorporating pre-trained models\nare non-trivial and mainly focus on BERT, which lacks a comparison of the\nimpact that other pre-trained models may have on translation performance. In\nthis paper, we demonstrate that simply using the output (contextualized\nembeddings) of a tailored and suitable bilingual pre-trained language model\n(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art\ntranslation performance. Moreover, we also propose a stochastic layer selection\napproach and a concept of dual-directional translation model to ensure the\nsufficient utilization of contextualized embeddings. In the case of without\nusing back translation, our best models achieve BLEU scores of 30.45 for En->De\nand 38.61 for De->En on the IWSLT'14 dataset, and 31.26 for En->De and 34.94\nfor De->En on the WMT'14 dataset, which exceeds all published numbers.",
          "link": "http://arxiv.org/abs/2109.04588",
          "publishedOn": "2021-09-13T07:20:25.128Z",
          "wordCount": 622,
          "title": "BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>",
          "description": "Recent work has shown that monolingual masked language models learn to\nrepresent data-driven notions of language variation which can be used for\ndomain-targeted training data selection. Dataset genre labels are already\nfrequently available, yet remain largely unexplored in cross-lingual setups. We\nharness this genre metadata as a weak supervision signal for targeted data\nselection in zero-shot dependency parsing. Specifically, we project\ntreebank-level genre information to the finer-grained sentence level, with the\ngoal to amplify information implicitly stored in unsupervised contextualized\nrepresentations. We demonstrate that genre is recoverable from multilingual\ncontextual embeddings and that it provides an effective signal for training\ndata selection in cross-lingual, zero-shot scenarios. For 12 low-resource\nlanguage treebanks, six of which are test-only, our genre-specific methods\nsignificantly outperform competitive baselines as well as recent\nembedding-based methods for data selection. Moreover, genre-based data\nselection provides new state-of-the-art results for three of these target\nlanguages.",
          "link": "http://arxiv.org/abs/2109.04733",
          "publishedOn": "2021-09-13T07:20:25.106Z",
          "wordCount": 594,
          "title": "Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giledereli_B/0/1/0/all/0/1\">Buse Giledereli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif Ozkirimli</a>",
          "description": "Multi-label text classification is a challenging task because it requires\ncapturing label dependencies. It becomes even more challenging when class\ndistribution is long-tailed. Resampling and re-weighting are common approaches\nused for addressing the class imbalance problem, however, they are not\neffective when there is label dependency besides class imbalance because they\nresult in oversampling of common labels. Here, we introduce the application of\nbalancing loss functions for multi-label text classification. We perform\nexperiments on a general domain dataset with 90 labels (Reuters-21578) and a\ndomain-specific dataset from PubMed with 18211 labels. We find that a\ndistribution-balanced loss function, which inherently addresses both the class\nimbalance and label linkage problems, outperforms commonly used loss functions.\nDistribution balancing methods have been successfully used in the image\nrecognition field. Here, we show their effectiveness in natural language\nprocessing. Source code is available at\nhttps://github.com/blessu/BalancedLossNLP.",
          "link": "http://arxiv.org/abs/2109.04712",
          "publishedOn": "2021-09-13T07:20:25.022Z",
          "wordCount": 592,
          "title": "Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergmanis_T/0/1/0/all/0/1\">Toms Bergmanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>",
          "description": "The majority of language domains require prudent use of terminology to ensure\nclarity and adequacy of information conveyed. While the correct use of\nterminology for some languages and domains can be achieved by adapting\ngeneral-purpose MT systems on large volumes of in-domain parallel data, such\nquantities of domain-specific data are seldom available for less-resourced\nlanguages and niche domains. Furthermore, as exemplified by COVID-19 recently,\nno domain-specific parallel data is readily available for emerging domains.\nHowever, the gravity of this recent calamity created a high demand for reliable\ntranslation of critical information regarding pandemic and infection\nprevention. This work is part of WMT2021 Shared Task: Machine Translation using\nTerminologies, where we describe Tilde MT systems that are capable of dynamic\nterminology integration at the time of translation. Our systems achieve up to\n94% COVID-19 term use accuracy on the test set of the EN-FR language pair\nwithout having access to any form of in-domain information during system\ntraining. We conclude our work with a broader discussion considering the Shared\nTask itself and terminology translation in MT.",
          "link": "http://arxiv.org/abs/2109.04708",
          "publishedOn": "2021-09-13T07:20:24.891Z",
          "wordCount": 668,
          "title": "Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>",
          "description": "Style transfer aims to rewrite a source text in a different target style\nwhile preserving its content. We propose a novel approach to this task that\nleverages generic resources, and without using any task-specific parallel\n(source-target) data outperforms existing unsupervised approaches on the two\nmost popular style transfer tasks: formality transfer and polarity swap. In\npractice, we adopt a multi-step procedure which builds on a generic pre-trained\nsequence-to-sequence model (BART). First, we strengthen the model's ability to\nrewrite by further pre-training BART on both an existing collection of generic\nparaphrases, as well as on synthetic pairs created using a general-purpose\nlexical resource. Second, through an iterative back-translation approach, we\ntrain two models, each in a transfer direction, so that they can provide each\nother with synthetically generated pairs, dynamically in the training process.\nLastly, we let our best reresulting model generate static synthetic pairs to be\nused in a supervised training regime. Besides methodology and state-of-the-art\nresults, a core contribution of this work is a reflection on the nature of the\ntwo tasks we address, and how their differences are highlighted by their\nresponse to our approach.",
          "link": "http://arxiv.org/abs/2109.04543",
          "publishedOn": "2021-09-13T07:20:24.857Z",
          "wordCount": null,
          "title": "Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Adapters are light-weight modules that allow parameter-efficient fine-tuning\nof pretrained models. Specialized language and task adapters have recently been\nproposed to facilitate cross-lingual transfer of multilingual pretrained models\n(Pfeiffer et al., 2020b). However, this approach requires training a separate\nlanguage adapter for every language one wishes to support, which can be\nimpractical for languages with limited data. An intuitive solution is to use a\nrelated language adapter for the new language variety, but we observe that this\nsolution can lead to sub-optimal performance. In this paper, we aim to improve\nthe robustness of language adapters to uncovered languages without training new\nadapters. We find that ensembling multiple existing language adapters makes the\nfine-tuned model significantly more robust to other language varieties not\nincluded in these adapters. Building upon this observation, we propose Entropy\nMinimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble\nweights of the pretrained language adapters for each test sentence by\nminimizing the entropy of its predictions. Experiments on three diverse groups\nof language varieties show that our method leads to significant improvements on\nboth named entity recognition and part-of-speech tagging across all languages.",
          "link": "http://arxiv.org/abs/2109.04877",
          "publishedOn": "2021-09-13T07:20:24.693Z",
          "wordCount": 638,
          "title": "Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>",
          "description": "The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.",
          "link": "http://arxiv.org/abs/2109.04604",
          "publishedOn": "2021-09-13T07:20:24.625Z",
          "wordCount": 600,
          "title": "How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "The transformer-based pre-trained language models have been tremendously\nsuccessful in most of the conventional NLP tasks. But they often struggle in\nthose tasks where numerical understanding is required. Some possible reasons\ncan be the tokenizers and pre-training objectives which are not specifically\ndesigned to learn and preserve numeracy. Here we investigate the ability of\ntext-to-text transfer learning model (T5), which has outperformed its\npredecessors in the conventional NLP tasks, to learn numeracy. We consider four\nnumeracy tasks: numeration, magnitude order prediction, finding minimum and\nmaximum in a series, and sorting. We find that, although T5 models perform\nreasonably well in the interpolation setting, they struggle considerably in the\nextrapolation setting across all four tasks.",
          "link": "http://arxiv.org/abs/2109.04672",
          "publishedOn": "2021-09-13T07:20:24.610Z",
          "wordCount": 576,
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew S. Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "We study the problem of generating arithmetic math word problems (MWPs) given\na math equation that specifies the mathematical computation and a context that\nspecifies the problem scenario. Existing approaches are prone to generating\nMWPs that are either mathematically invalid or have unsatisfactory language\nquality. They also either ignore the context or require manual specification of\na problem template, which compromises the diversity of the generated MWPs. In\nthis paper, we develop a novel MWP generation approach that leverages i)\npre-trained language models and a context keyword selection model to improve\nthe language quality of the generated MWPs and ii) an equation consistency\nconstraint for math equations to improve the mathematical validity of the\ngenerated MWPs. Extensive quantitative and qualitative experiments on three\nreal-world MWP datasets demonstrate the superior performance of our approach\ncompared to various baselines.",
          "link": "http://arxiv.org/abs/2109.04546",
          "publishedOn": "2021-09-13T07:20:24.604Z",
          "wordCount": 587,
          "title": "Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>",
          "description": "The dominant paradigm for semantic parsing in recent years is to formulate\nparsing as a sequence-to-sequence task, generating predictions with\nauto-regressive sequence decoders. In this work, we explore an alternative\nparadigm. We formulate semantic parsing as a dependency parsing task, applying\ngraph-based decoding techniques developed for syntactic parsing. We compare\nvarious decoding techniques given the same pre-trained Transformer encoder on\nthe TOP dataset, including settings where training data is limited or contains\nonly partially-annotated examples. We find that our graph-based approach is\ncompetitive with sequence decoders on the standard setting, and offers\nsignificant improvements in data efficiency and settings where\npartially-annotated data is available.",
          "link": "http://arxiv.org/abs/2109.04587",
          "publishedOn": "2021-09-13T07:20:24.579Z",
          "wordCount": 555,
          "title": "Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema(definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5)is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.",
          "link": "http://arxiv.org/abs/2109.04645",
          "publishedOn": "2021-09-13T07:20:24.574Z",
          "wordCount": 596,
          "title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1\">Andrei Popescu-Belis</a>",
          "description": "State-of-the-art multilingual systems rely on shared vocabularies that\nsufficiently cover all considered languages. To this end, a simple and\nfrequently used approach makes use of subword vocabularies constructed jointly\nover several languages. We hypothesize that such vocabularies are suboptimal\ndue to false positives (identical subwords with different meanings across\nlanguages) and false negatives (different subwords with similar meanings). To\naddress these issues, we propose Subword Mapping and Anchoring across Languages\n(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts\nsubword alignments using an unsupervised state-of-the-art mapping technique and\nuses them to create cross-lingual anchors based on subword similarities. We\ndemonstrate the benefits of SMALA for cross-lingual natural language inference\n(XNLI), where it improves zero-shot transfer to an unseen language without\ntask-specific data, but only by sharing subword embeddings. Moreover, in neural\nmachine translation, we show that joint subword vocabularies obtained with\nSMALA lead to higher BLEU scores on sentences that contain many false positives\nand false negatives.",
          "link": "http://arxiv.org/abs/2109.04556",
          "publishedOn": "2021-09-13T07:20:24.532Z",
          "wordCount": 597,
          "title": "Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lees_A/0/1/0/all/0/1\">Alyssa Lees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>",
          "description": "We present ReasonBert, a pre-training method that augments language models\nwith the ability to reason over long-range relations and multiple, possibly\nhybrid contexts. Unlike existing pre-training methods that only harvest\nlearning signals from local contexts of naturally occurring texts, we propose a\ngeneralized notion of distant supervision to automatically connect multiple\npieces of text and tables to create pre-training examples that require\nlong-range reasoning. Different types of reasoning are simulated, including\nintersecting multiple pieces of evidence, bridging from one piece of evidence\nto another, and detecting unanswerable cases. We conduct a comprehensive\nevaluation on a variety of extractive question answering datasets ranging from\nsingle-hop to multi-hop and from text-only to table-only to hybrid that require\nvarious reasoning capabilities and show that ReasonBert achieves remarkable\nimprovement over an array of strong baselines. Few-shot experiments further\ndemonstrate that our pre-training method substantially improves sample\nefficiency.",
          "link": "http://arxiv.org/abs/2109.04912",
          "publishedOn": "2021-09-13T07:20:24.526Z",
          "wordCount": 609,
          "title": "ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saret_B/0/1/0/all/0/1\">Benny Saret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_W/0/1/0/all/0/1\">Wayne Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_N/0/1/0/all/0/1\">Nathan Wasserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>",
          "description": "We present models which complete missing text given transliterations of\nancient Mesopotamian documents, originally written on cuneiform clay tablets\n(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on\ncontextual cues to manually fill in missing parts in the text in a subjective\nand time-consuming process. We identify that this challenge can be formulated\nas a masked language modelling task, used mostly as a pretraining objective for\ncontextualized language models. Following, we develop several architectures\nfocusing on the Akkadian language, the lingua franca of the time. We find that\ndespite data scarcity (1M tokens) we can achieve state of the art performance\non missing tokens prediction (89% hit@5) using a greedy decoding scheme and\npretraining on data from other languages and different time periods. Finally,\nwe conduct human evaluations showing the applicability of our models in\nassisting experts to transcribe texts in extinct languages.",
          "link": "http://arxiv.org/abs/2109.04513",
          "publishedOn": "2021-09-13T07:20:24.473Z",
          "wordCount": 608,
          "title": "Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sam Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "Document-level entity-based extraction (EE), aiming at extracting\nentity-centric information such as entity roles and entity relations, is key to\nautomatic knowledge acquisition from text corpora for various domains. Most\ndocument-level EE systems build extractive models, which struggle to model\nlong-term dependencies among entities at the document level. To address this\nissue, we propose a generative framework for two document-level EE tasks:\nrole-filler entity extraction (REE) and relation extraction (RE). We first\nformulate them as a template generation problem, allowing models to efficiently\ncapture cross-entity dependencies, exploit label semantics, and avoid the\nexponential computation complexity of identifying N-ary relations. A novel\ncross-attention guided copy mechanism, TopK Copy, is incorporated into a\npre-trained sequence-to-sequence model to enhance the capabilities of\nidentifying key information in the input document. Experiments done on the\nMUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),\nbinary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.",
          "link": "http://arxiv.org/abs/2109.04901",
          "publishedOn": "2021-09-13T07:20:24.466Z",
          "wordCount": 589,
          "title": "Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariev_S/0/1/0/all/0/1\">Sherzod Kariev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abduraufov_O/0/1/0/all/0/1\">Otabek Abduraufov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajili_M/0/1/0/all/0/1\">Mammad Hajili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaytbaev_A/0/1/0/all/0/1\">Abror Khaytbaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzodbek Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_E/0/1/0/all/0/1\">Esra Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>",
          "description": "Recent advances in neural machine translation (NMT) have pushed the quality\nof machine translation systems to the point where they are becoming widely\nadopted to build competitive systems. However, there is still a large number of\nlanguages that are yet to reap the benefits of NMT. In this paper, we provide\nthe first large-scale case study of the practical application of MT in the\nTurkic language family in order to realize the gains of NMT for Turkic\nlanguages under high-resource to extremely low-resource scenarios. In addition\nto presenting an extensive analysis that identifies the bottlenecks towards\nbuilding competitive systems to ameliorate data scarcity, our study has several\nkey contributions, including, i) a large parallel corpus covering 22 Turkic\nlanguages consisting of common public datasets in combination with new datasets\nof approximately 2 million parallel sentences, ii) bilingual baselines for 26\nlanguage pairs, iii) novel high-quality test sets in three different\ntranslation domains and iv) human evaluation scores. All models, scripts, and\ndata will be released to the public.",
          "link": "http://arxiv.org/abs/2109.04593",
          "publishedOn": "2021-09-13T07:20:24.460Z",
          "wordCount": 653,
          "title": "A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>",
          "description": "Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).",
          "link": "http://arxiv.org/abs/2109.04780",
          "publishedOn": "2021-09-13T07:20:24.390Z",
          "wordCount": 636,
          "title": "RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Humair Raj Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "Pre-trained language-vision models have shown remarkable performance on the\nvisual question answering (VQA) task. However, most pre-trained models are\ntrained by only considering monolingual learning, especially the resource-rich\nlanguage like English. Training such models for multilingual setups demand high\ncomputing resources and multilingual language-vision dataset which hinders\ntheir application in practice. To alleviate these challenges, we propose a\nknowledge distillation approach to extend an English language-vision model\n(teacher) into an equally effective multilingual and code-mixed model\n(student). Unlike the existing knowledge distillation methods, which only use\nthe output from the last layer of the teacher network for distillation, our\nstudent model learns and imitates the teacher from multiple intermediate layers\n(language and vision encoders) with appropriately designed distillation\nobjectives for incremental knowledge extraction. We also create the large-scale\nmultilingual and code-mixed VQA dataset in eleven different language setups\nconsidering the multiple Indian and European languages. Experimental results\nand in-depth analysis show the effectiveness of the proposed VQA model over the\npre-trained language-vision models on eleven diverse language setups.",
          "link": "http://arxiv.org/abs/2109.04653",
          "publishedOn": "2021-09-13T07:20:24.372Z",
          "wordCount": 622,
          "title": "Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>",
          "description": "Extracting moral sentiment from text is a vital component in understanding\npublic opinion, social movements, and policy decisions. The Moral Foundation\nTheory identifies five moral foundations, each associated with a positive and\nnegative polarity. However, moral sentiment is often motivated by its targets,\nwhich can correspond to individuals or collective entities. In this paper, we\nintroduce morality frames, a representation framework for organizing moral\nattitudes directed at different entities, and come up with a novel and\nhigh-quality annotated dataset of tweets written by US politicians. Then, we\npropose a relational learning model to predict moral attitudes towards entities\nand moral foundations jointly. We do qualitative and quantitative evaluations,\nshowing that moral sentiment towards entities differs highly across political\nideologies.",
          "link": "http://arxiv.org/abs/2109.04535",
          "publishedOn": "2021-09-13T07:20:24.279Z",
          "wordCount": 579,
          "title": "Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongkuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orme_Rogers_J/0/1/0/all/0/1\">James Orme-Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1\">Rajgopal Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1\">Viktor Prasanna</a>",
          "description": "Temporal Knowledge Graphs store events in the form of subjects, relations,\nobjects, and timestamps which are often represented by dynamic heterogeneous\ngraphs. Event forecasting is a critical and challenging task in Temporal\nKnowledge Graph reasoning that predicts the subject or object of an event in\nthe future. To obtain temporal embeddings multi-step away in the future,\nexisting methods learn generative models that capture the joint distribution of\nthe observed events. To reduce the high computation costs, these methods rely\non unrealistic assumptions of independence and approximations in training and\ninference. In this work, we propose SeDyT, a discriminative framework that\nperforms sequence modeling on the dynamic entity embeddings to solve the\nmulti-step event forecasting problem. SeDyT consists of two components: a\nTemporal Graph Neural Network that generates dynamic entity embeddings in the\npast and a sequence model that predicts the entity embeddings in the future.\nCompared with the generative models, SeDyT does not rely on any heuristic-based\nprobability model and has low computation complexity in both training and\ninference. SeDyT is compatible with most Temporal Graph Neural Networks and\nsequence models. We also design an efficient training method that trains the\ntwo components in one gradient descent propagation. We evaluate the performance\nof SeDyT on five popular datasets. By combining temporal Graph Neural Network\nmodels and sequence models, SeDyT achieves an average of 2.4% MRR improvement\nwhen not using the validation set and more than 10% MRR improvement when using\nthe validation set.",
          "link": "http://arxiv.org/abs/2109.04550",
          "publishedOn": "2021-09-13T07:20:24.206Z",
          "wordCount": 703,
          "title": "SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xintong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>",
          "description": "Resolving pronouns to their referents has long been studied as a fundamental\nnatural language understanding problem. Previous works on pronoun coreference\nresolution (PCR) mostly focus on resolving pronouns to mentions in text while\nignoring the exophoric scenario. Exophoric pronouns are common in daily\ncommunications, where speakers may directly use pronouns to refer to some\nobjects present in the environment without introducing the objects first.\nAlthough such objects are not mentioned in the dialogue text, they can often be\ndisambiguated by the general topics of the dialogue. Motivated by this, we\npropose to jointly leverage the local context and global topics of dialogues to\nsolve the out-of-text PCR problem. Extensive experiments demonstrate the\neffectiveness of adding topic regularization for resolving exophoric pronouns.",
          "link": "http://arxiv.org/abs/2109.04787",
          "publishedOn": "2021-09-13T07:20:24.139Z",
          "wordCount": 571,
          "title": "Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jaeyeol Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hee Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>",
          "description": "We present a model to predict fine-grained emotions along the continuous\ndimensions of valence, arousal, and dominance (VAD) with a corpus with\ncategorical emotion annotations. Our model is trained by minimizing the EMD\n(Earth Mover's Distance) loss between the predicted VAD score distribution and\nthe categorical emotion distributions sorted along VAD, and it can\nsimultaneously classify the emotion categories and predict the VAD scores for a\ngiven sentence. We use pre-trained RoBERTa-Large and fine-tune on three\ndifferent corpora with categorical labels and evaluate on EmoBank corpus with\nVAD scores. We show that our approach reaches comparable performance to that of\nthe state-of-the-art classifiers in categorical emotion classification and\nshows significant positive correlations with the ground truth VAD scores. Also,\nfurther training with supervision of VAD labels leads to improved performance\nespecially when dataset is small. We also present examples of predictions of\nappropriate emotion words that are not part of the original annotations.",
          "link": "http://arxiv.org/abs/1911.02499",
          "publishedOn": "2021-09-13T07:20:24.133Z",
          "wordCount": 636,
          "title": "Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>",
          "description": "In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.",
          "link": "http://arxiv.org/abs/2109.04939",
          "publishedOn": "2021-09-13T07:20:24.112Z",
          "wordCount": 592,
          "title": "Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1\">Mark Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Josh Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Pete Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>",
          "description": "We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.",
          "link": "http://arxiv.org/abs/2104.01454",
          "publishedOn": "2021-09-13T07:20:24.087Z",
          "wordCount": 655,
          "title": "Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quillen_C/0/1/0/all/0/1\">Carl Quillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dushyant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goderre_A/0/1/0/all/0/1\">Andrew Goderre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lainez_J/0/1/0/all/0/1\">Jos&#xe9; La&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanovic_L/0/1/0/all/0/1\">Ljubomir Milanovi&#x107;</a>",
          "description": "When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech\nRecognition (ASR) backend shows promising results. Recent literature has shown\ntraditional beamformer designs, such as MVDR (Minimum Variance Distortionless\nResponse) or fixed beamformers can be successfully integrated as the frontend\ninto an E2E ASR system with learnable parameters. In this work, we propose the\nself-attention channel combinator (SACC) ASR frontend, which leverages the\nself-attention mechanism to combine multichannel audio signals in the magnitude\nspectral domain. Experiments conducted on a multichannel playback test data\nshows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed\nbeamformer-based frontend, both jointly optimized with a ContextNet-based ASR\nbackend. We also demonstrate the connection between the SACC and the\ntraditional beamformers, and analyze the intermediate outputs of the SACC.",
          "link": "http://arxiv.org/abs/2109.04783",
          "publishedOn": "2021-09-13T07:20:24.077Z",
          "wordCount": 606,
          "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Alexander Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.",
          "link": "http://arxiv.org/abs/2109.04825",
          "publishedOn": "2021-09-13T07:20:24.070Z",
          "wordCount": 623,
          "title": "Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05547",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_N/0/1/0/all/0/1\">Nan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourian_P/0/1/0/all/0/1\">Pirouz Nourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roders_A/0/1/0/all/0/1\">Ana Pereira Roders</a>",
          "description": "The UNESCO World Heritage List (WHL) includes the exceptionally valuable\ncultural and natural heritage to be preserved for mankind. Evaluating and\njustifying the Outstanding Universal Value (OUV) is essential for each site\ninscribed in the WHL, and yet a complex task, even for experts, since the\nselection criteria of OUV are not mutually exclusive. Furthermore, manual\nannotation of heritage values and attributes from multi-source textual data,\nwhich is currently dominant in heritage studies, is knowledge-demanding and\ntime-consuming, impeding systematic analysis of such authoritative documents in\nterms of their implications on heritage management. This study applies\nstate-of-the-art NLP models to build a classifier on a new dataset containing\nStatements of OUV, seeking an explainable and scalable automation tool to\nfacilitate the nomination, evaluation, research, and monitoring processes of\nWorld Heritage sites. Label smoothing is innovatively adapted to improve the\nmodel performance by adding prior inter-class relationship knowledge to\ngenerate soft labels. The study shows that the best models fine-tuned from BERT\nand ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation\non the model prediction shows that the models are sufficiently generalizable.\nThe study is promising to be further developed and applied in heritage research\nand practice.",
          "link": "http://arxiv.org/abs/2104.05547",
          "publishedOn": "2021-09-13T07:20:24.052Z",
          "wordCount": 718,
          "title": "WHOSe Heritage: Classification of UNESCO World Heritage \"Outstanding Universal Value\" Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>",
          "description": "Motivated by suggested question generation in conversational news\nrecommendation systems, we propose a model for generating question-answer pairs\n(QA pairs) with self-contained, summary-centric questions and\nlength-constrained, article-summarizing answers. We begin by collecting a new\ndataset of news articles with questions as titles and pairing them with\nsummaries of varying length. This dataset is used to learn a QA pair generation\nmodel producing summaries as answers that balance brevity with sufficiency\njointly with their corresponding questions. We then reinforce the QA pair\ngeneration process with a differentiable reward function to mitigate exposure\nbias, a common problem in natural language generation. Both automatic metrics\nand human evaluation demonstrate these QA pairs successfully capture the\ncentral gists of the articles and achieve high answer accuracy.",
          "link": "http://arxiv.org/abs/2109.04689",
          "publishedOn": "2021-09-13T07:20:24.022Z",
          "wordCount": 586,
          "title": "Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.",
          "link": "http://arxiv.org/abs/2104.08836",
          "publishedOn": "2021-09-10T07:20:13.906Z",
          "wordCount": 619,
          "title": "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "Most existing NER methods rely on extensive labeled data for model training,\nwhich struggles in the low-resource scenarios with limited training data.\nRecently, prompt-tuning methods for pre-trained language models have achieved\nremarkable performance in few-shot learning by exploiting prompts as task\nguidance to reduce the gap between training progress and downstream tuning.\nInspired by prompt learning, we propose a novel lightweight generative\nframework with prompt-guided attention for low-resource NER (LightNER).\nSpecifically, we construct the semantic-aware answer space of entity categories\nfor prompt learning to generate the entity span sequence and entity categories\nwithout any label-specific classifiers. We further propose prompt-guided\nattention by incorporating continuous prompts into the self-attention layer to\nre-modulate the attention and adapt pre-trained weights. Note that we only tune\nthose continuous prompts with the whole parameter of the pre-trained language\nmodel fixed, thus, making our approach lightweight and flexible for\nlow-resource scenarios and can better transfer knowledge across domains.\nExperimental results show that LightNER can obtain comparable performance in\nthe standard supervised setting and outperform strong baselines in low-resource\nsettings by tuning only a small part of the parameters.",
          "link": "http://arxiv.org/abs/2109.00720",
          "publishedOn": "2021-09-10T07:20:13.769Z",
          "wordCount": 660,
          "title": "LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_D/0/1/0/all/0/1\">Devang Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Vlad Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>",
          "description": "In this work, we introduce back-training, an alternative to self-training for\nunsupervised domain adaptation (UDA) from source to target domain. While\nself-training generates synthetic training data where natural inputs are\naligned with noisy outputs, back-training results in natural outputs aligned\nwith noisy inputs. This significantly reduces the gap between the target domain\nand synthetic data distribution, and reduces model overfitting to the source\ndomain. We run UDA experiments on question generation and passage retrieval\nfrom the \\textit{Natural Questions} domain to machine learning and biomedical\ndomains. We find that back-training vastly outperforms self-training by a mean\nimprovement of 7.8 BLEU-4 points on generation, and 17.6\\% top-20 retrieval\naccuracy across both domains. We further propose consistency filters to remove\nlow-quality synthetic data before training. We also release a new\ndomain-adaptation dataset- \\textit{MLQuestions} containing 35K unaligned\nquestions, 50K unaligned passages, and 3K aligned question-passage pairs.",
          "link": "http://arxiv.org/abs/2104.08801",
          "publishedOn": "2021-09-10T07:20:13.734Z",
          "wordCount": 632,
          "title": "Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>",
          "description": "Recently, prompt-tuning has achieved promising results for certain few-shot\nclassification tasks. The core idea of prompt-tuning is to insert text pieces\n(i.e., templates) into the input and transform a classification task into a\nmasked language modeling problem. However, for relation extraction, determining\nan appropriate prompt template requires domain expertise, and it is cumbersome\nand time-consuming to obtain a suitable label word. Furthermore, there exist\nabundant semantic knowledge among the entities and relations that cannot be\nignored. To this end, we focus on incorporating knowledge into prompt-tuning\nfor relation extraction and propose a knowledge-aware prompt-tuning approach\nwith synergistic optimization (KnowPrompt). Specifically, we inject entity and\nrelation knowledge into prompt construction with learnable virtual template\nwords as well as answer words and synergistically optimize their representation\nwith knowledge constraints. Extensive experimental results on five datasets\nwith standard and low-resource settings demonstrate the effectiveness of our\napproach.",
          "link": "http://arxiv.org/abs/2104.07650",
          "publishedOn": "2021-09-10T07:20:13.536Z",
          "wordCount": 663,
          "title": "KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yucheng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhenru Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>",
          "description": "Recently, two approaches, fine-tuning large pre-trained language models and\nvariational training, have attracted significant interests, separately, for\nsemi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,\nwe propose Variational Latent-State GPT model (VLS-GPT), which is the first to\ncombine the strengths of the two approaches. Among many options of models, we\npropose the generative model and the inference model for variational learning\nof the end-to-end TOD system, both as auto-regressive language models based on\nGPT-2, which can be further trained over a mix of labeled and unlabeled dialog\ndata in a semi-supervised manner. We develop the strategy of\nsampling-then-forward-computation, which successfully overcomes the memory\nexplosion issue of using GPT in variational learning and speeds up training.\nSemi-supervised TOD experiments are conducted on two benchmark multi-domain\ndatasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to\nsignificantly outperform both supervised-only and semi-supervised baselines.",
          "link": "http://arxiv.org/abs/2109.04314",
          "publishedOn": "2021-09-10T07:20:13.457Z",
          "wordCount": 589,
          "title": "Variational Latent-State GPT for Semi-supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Timkey_W/0/1/0/all/0/1\">William Timkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1\">Marten van Schijndel</a>",
          "description": "Similarity measures are a vital tool for understanding how language models\nrepresent and process language. Standard representational similarity measures\nsuch as cosine similarity and Euclidean distance have been successfully used in\nstatic word embedding models to understand how words cluster in semantic space.\nRecently, these measures have been applied to embeddings from contextualized\nmodels such as BERT and GPT-2. In this work, we call into question the\ninformativity of such measures for contextualized language models. We find that\na small number of rogue dimensions, often just 1-3, dominate these measures.\nMoreover, we find a striking mismatch between the dimensions that dominate\nsimilarity measures and those which are important to the behavior of the model.\nWe show that simple postprocessing techniques such as standardization are able\nto correct for rogue dimensions and reveal underlying representational quality.\nWe argue that accounting for rogue dimensions is essential for any\nsimilarity-based analysis of contextual language models.",
          "link": "http://arxiv.org/abs/2109.04404",
          "publishedOn": "2021-09-10T07:20:13.408Z",
          "wordCount": 610,
          "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.",
          "link": "http://arxiv.org/abs/2109.04312",
          "publishedOn": "2021-09-10T07:20:13.400Z",
          "wordCount": 623,
          "title": "MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_Smith_E/0/1/0/all/0/1\">Edward Gow-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1\">Carolina Scarton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>",
          "description": "Despite their success in a variety of NLP tasks, pre-trained language models,\ndue to their heavy reliance on compositionality, fail in effectively capturing\nthe meanings of multiword expressions (MWEs), especially idioms. Therefore,\ndatasets and methods to improve the representation of MWEs are urgently needed.\nExisting datasets are limited to providing the degree of idiomaticity of\nexpressions along with the literal and, where applicable, (a single)\nnon-literal interpretation of MWEs. This work presents a novel dataset of\nnaturally occurring sentences containing MWEs manually classified into a\nfine-grained set of meanings, spanning both English and Portuguese. We use this\ndataset in two tasks designed to test i) a language model's ability to detect\nidiom usage, and ii) the effectiveness of a language model in generating\nrepresentations of sentences containing idioms. Our experiments demonstrate\nthat, on the task of detecting idiomatic usage, these models perform reasonably\nwell in the one-shot and few-shot scenarios, but that there is significant\nscope for improvement in the zero-shot scenario. On the task of representing\nidiomaticity, we find that pre-training is not always effective, while\nfine-tuning could provide a sample efficient method of learning representations\nof sentences containing MWEs.",
          "link": "http://arxiv.org/abs/2109.04413",
          "publishedOn": "2021-09-10T07:20:13.328Z",
          "wordCount": 655,
          "title": "AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models. (arXiv:2109.04413v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09363",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>",
          "description": "Despite significant recent progress in coreference resolution, the quality of\ncurrent state-of-the-art systems still considerably trails behind human-level\nperformance. Using the CoNLL-2012 and PreCo datasets, we dissect the best\ninstantiation of the mainstream end-to-end coreference resolution model that\nunderlies most current best-performing coreference systems, and empirically\nanalyze the behavior of its two components: mention detector and mention\nlinker. While the detector traditionally focuses heavily on recall as a design\ndecision, we demonstrate the importance of precision, calling for their\nbalance. However, we point out the difficulty in building a precise detector\ndue to its inability to make important anaphoricity decisions. We also\nhighlight the enormous room for improving the linker and show that the rest of\nits errors mainly involve pronoun resolution. We propose promising next steps\nand hope our findings will help future research in coreference resolution.",
          "link": "http://arxiv.org/abs/2009.09363",
          "publishedOn": "2021-09-10T07:20:13.104Z",
          "wordCount": 606,
          "title": "Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution. (arXiv:2009.09363v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.11818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>",
          "description": "Although neural sequence-to-sequence models have been successfully applied to\nsemantic parsing, they fail at compositional generalization, i.e., they are\nunable to systematically generalize to unseen compositions of seen components.\nMotivated by traditional semantic parsing where compositionality is explicitly\naccounted for by symbolic grammars, we propose a new decoding framework that\npreserves the expressivity and generality of sequence-to-sequence models while\nfeaturing lexicon-style alignments and disentangled information processing.\nSpecifically, we decompose decoding into two phases where an input utterance is\nfirst tagged with semantic symbols representing the meaning of individual\nwords, and then a sequence-to-sequence model is used to predict the final\nmeaning representation conditioning on the utterance and the predicted tag\nsequence. Experimental results on three semantic parsing datasets show that the\nproposed approach consistently improves compositional generalization across\nmodel architectures, domains, and semantic formalisms.",
          "link": "http://arxiv.org/abs/2010.11818",
          "publishedOn": "2021-09-10T07:20:13.047Z",
          "wordCount": 596,
          "title": "Compositional Generalization via Semantic Tagging. (arXiv:2010.11818v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Utama_P/0/1/0/all/0/1\">Prasetya Ajie Utama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_N/0/1/0/all/0/1\">Nafise Sadat Moosavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Recent prompt-based approaches allow pretrained language models to achieve\nstrong performances on few-shot finetuning by reformulating downstream tasks as\na language modeling problem. In this work, we demonstrate that, despite its\nadvantages on low data regimes, finetuned prompt-based models for sentence pair\nclassification tasks still suffer from a common pitfall of adopting inference\nheuristics based on lexical overlap, e.g., models incorrectly assuming a\nsentence pair is of the same meaning because they consist of the same set of\nwords. Interestingly, we find that this particular inference heuristic is\nsignificantly less present in the zero-shot evaluation of the prompt-based\nmodel, indicating how finetuning can be destructive to useful knowledge learned\nduring the pretraining. We then show that adding a regularization that\npreserves pretraining weights is effective in mitigating this destructive\ntendency of few-shot finetuning. Our evaluation on three datasets demonstrates\npromising improvements on the three corresponding challenge datasets used to\ndiagnose the inference heuristics.",
          "link": "http://arxiv.org/abs/2109.04144",
          "publishedOn": "2021-09-10T07:20:13.041Z",
          "wordCount": 603,
          "title": "Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning. (arXiv:2109.04144v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mar&#xed;a Dolores Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrera_D/0/1/0/all/0/1\">D&#xe9;bora Carrera</a>",
          "description": "Text classification tasks have improved substantially during the last years\nby the usage of transformers. However, the majority of researches focus on\nprose texts, with poetry receiving less attention, specially for Spanish\nlanguage. In this paper, we propose a semi-supervised learning approach for\ninferring 21 psychological categories evoked by a corpus of 4572 sonnets, along\nwith 10 affective and lexico-semantic multiclass ones. The subset of poems used\nfor training an evaluation includes 270 sonnets. With our approach, we achieve\nan AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65\nfor 60% on the multiclass ones. The sonnets are modelled using transformers,\nthrough sentence embeddings, along with lexico-semantic and affective features,\nobtained by using external lexicons. Consequently, we see that this approach\nprovides an AUC increase of up to 0.12, as opposed to using transformers alone.",
          "link": "http://arxiv.org/abs/2109.04152",
          "publishedOn": "2021-09-10T07:20:13.020Z",
          "wordCount": 602,
          "title": "Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dobrovolskii_V/0/1/0/all/0/1\">Vladimir Dobrovolskii</a>",
          "description": "Recent coreference resolution models rely heavily on span representations to\nfind coreference links between word spans. As the number of spans is $O(n^2)$\nin the length of text and the number of potential links is $O(n^4)$, various\npruning techniques are necessary to make this approach computationally\nfeasible. We propose instead to consider coreference links between individual\nwords rather than word spans and then reconstruct the word spans. This reduces\nthe complexity of the coreference model to $O(n^2)$ and allows it to consider\nall potential mentions without pruning any of them out. We also demonstrate\nthat, with these changes, SpanBERT for coreference resolution will be\nsignificantly outperformed by RoBERTa. While being highly efficient, our model\nperforms competitively with recent coreference resolution systems on the\nOntoNotes benchmark.",
          "link": "http://arxiv.org/abs/2109.04127",
          "publishedOn": "2021-09-10T07:20:12.914Z",
          "wordCount": 557,
          "title": "Word-Level Coreference Resolution. (arXiv:2109.04127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chairatanakul_N/0/1/0/all/0/1\">Nuttapong Chairatanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriwatanasakdi_N/0/1/0/all/0/1\">Noppayut Sriwatanasakdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>",
          "description": "In cross-lingual text classification, it is required that task-specific\ntraining data in high-resource source languages are available, where the task\nis identical to that of a low-resource target language. However, collecting\nsuch training data can be infeasible because of the labeling cost, task\ncharacteristics, and privacy concerns. This paper proposes an alternative\nsolution that uses only task-independent word embeddings of high-resource\nlanguages and bilingual dictionaries. First, we construct a dictionary-based\nheterogeneous graph (DHG) from bilingual dictionaries. This opens the\npossibility to use graph neural networks for cross-lingual transfer. The\nremaining challenge is the heterogeneity of DHG because multiple languages are\nconsidered. To address this challenge, we propose dictionary-based\nheterogeneous graph neural network (DHGNet) that effectively handles the\nheterogeneity of DHG by two-step aggregations, which are word-level and\nlanguage-level aggregations. Experimental results demonstrate that our method\noutperforms pretrained models even though it does not access to large corpora.\nFurthermore, it can perform well even though dictionaries contain many\nincorrect translations. Its robustness allows the usage of a wider range of\ndictionaries such as an automatically constructed dictionary and crowdsourced\ndictionary, which are convenient for real-world applications.",
          "link": "http://arxiv.org/abs/2109.04400",
          "publishedOn": "2021-09-10T07:20:12.643Z",
          "wordCount": 644,
          "title": "Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>",
          "description": "A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.",
          "link": "http://arxiv.org/abs/2010.03060",
          "publishedOn": "2021-09-10T07:20:12.609Z",
          "wordCount": 704,
          "title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hossu_P/0/1/0/all/0/1\">Philip Hossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parde_N/0/1/0/all/0/1\">Natalie Parde</a>",
          "description": "Grave human toll notwithstanding, the COVID-19 pandemic created uniquely\nunstable conditions in financial markets. In this work we uncover and discuss\nrelationships involving sentiment in financial publications during the 2020\npandemic-motivated U.S. financial crash. First, we introduce a set of expert\nannotations of financial sentiment for articles from major American financial\nnews publishers. After an exploratory data analysis, we then describe a\nCNN-based architecture to address the task of predicting financial sentiment in\nthis anomalous, tumultuous setting. Our best performing model achieves a\nmaximum weighted F1 score of 0.746, establishing a strong performance\nbenchmark. Using predictions from our top performing model, we close by\nconducting a statistical correlation study with real stock market data, finding\ninteresting and strong relationships between financial news and the S\\&P 500\nindex, trading volume, market volatility, and different single-factor ETFs.",
          "link": "http://arxiv.org/abs/2109.04369",
          "publishedOn": "2021-09-10T07:20:12.538Z",
          "wordCount": 614,
          "title": "Tracking Turbulence Through Financial News During COVID-19. (arXiv:2109.04369v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>",
          "description": "The ability to identify and resolve uncertainty is crucial for the robustness\nof a dialogue system. Indeed, this has been confirmed empirically on systems\nthat utilise Bayesian approaches to dialogue belief tracking. However, such\nsystems consider only confidence estimates and have difficulty scaling to more\ncomplex settings. Neural dialogue systems, on the other hand, rarely take\nuncertainties into account. They are therefore overconfident in their decisions\nand less robust. Moreover, the performance of the tracking task is often\nevaluated in isolation, without consideration of its effect on the downstream\npolicy optimisation. We propose the use of different uncertainty measures in\nneural belief tracking. The effects of these measures on the downstream task of\npolicy optimisation are evaluated by adding selected measures of uncertainty to\nthe feature space of the policy and training policies through interaction with\na user simulator. Both human and simulated user results show that incorporating\nthese measures leads to improvements both of the performance and of the\nrobustness of the downstream dialogue policy. This highlights the importance of\ndeveloping neural dialogue belief trackers that take uncertainty into account.",
          "link": "http://arxiv.org/abs/2109.04349",
          "publishedOn": "2021-09-10T07:20:12.503Z",
          "wordCount": 658,
          "title": "Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance. (arXiv:2109.04349v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steitz_J/0/1/0/all/0/1\">Jan-Martin O. Steitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>",
          "description": "Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),\nrequires an alignment of semantic concepts across domains. Despite the\nwidespread success of end-to-end learning, today's multimodal pipelines by and\nlarge leverage pre-extracted, fixed features from object detectors, typically\nFaster R-CNN, as representations of the visual world. The obvious downside is\nthat the visual representation is not specifically tuned to the multimodal task\nat hand. At the same time, while transformer-based object detectors have gained\npopularity, they have not been employed in today's multimodal pipelines. We\naddress both shortcomings with TxT, a transformer-based crossmodal pipeline\nthat enables fine-tuning both language and visual components on the downstream\ntask in a fully end-to-end manner. We overcome existing limitations of\ntransformer-based detectors for multimodal reasoning regarding the integration\nof global context and their scalability. Our transformer-based multimodal model\nachieves considerable gains from end-to-end learning for multimodal question\nanswering.",
          "link": "http://arxiv.org/abs/2109.04422",
          "publishedOn": "2021-09-10T07:20:12.490Z",
          "wordCount": 607,
          "title": "TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy-Trang Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "This paper considers the unsupervised domain adaptation problem for neural\nmachine translation (NMT), where we assume the access to only monolingual text\nin either the source or target language in the new domain. We propose a\ncross-lingual data selection method to extract in-domain sentences in the\nmissing language side from a large generic monolingual corpus. Our proposed\nmethod trains an adaptive layer on top of multilingual BERT by contrastive\nlearning to align the representation between the source and target language.\nThis then enables the transferability of the domain classifier between the\nlanguages in a zero-shot manner. Once the in-domain data is detected by the\nclassifier, the NMT model is then adapted to the new domain by jointly learning\ntranslation and domain discrimination tasks. We evaluate our cross-lingual data\nselection method on NMT across five diverse domains in three language pairs, as\nwell as a real-world scenario of translation for COVID-19. The results show\nthat our proposed method outperforms other selection baselines up to +1.5 BLEU\nscore.",
          "link": "http://arxiv.org/abs/2109.04292",
          "publishedOn": "2021-09-10T07:20:12.394Z",
          "wordCount": 662,
          "title": "Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection. (arXiv:2109.04292v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staerman_G/0/1/0/all/0/1\">Guillaume Staerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>",
          "description": "A new metric \\texttt{BaryScore} to evaluate text generation based on deep\ncontextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric\nis motivated by a new framework relying on optimal transport tools, i.e.,\nWasserstein distance and barycenter. By modelling the layer output of deep\ncontextualized embeddings as a probability distribution rather than by a vector\nembedding; this framework provides a natural way to aggregate the different\noutputs through the Wasserstein space topology. In addition, it provides\ntheoretical grounds to our metric and offers an alternative to available\nsolutions e.g., MoverScore and BertScore). Numerical evaluation is performed on\nfour different tasks: machine translation, summarization, data2text generation\nand image captioning. Our results show that \\texttt{BaryScore} outperforms\nother BERT based metrics and exhibits more consistent behaviour in particular\nfor text summarization.",
          "link": "http://arxiv.org/abs/2108.12463",
          "publishedOn": "2021-09-10T07:20:12.333Z",
          "wordCount": 596,
          "title": "Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altakrori_M/0/1/0/all/0/1\">Malik H. Altakrori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1\">Jackie Chi Kit Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_B/0/1/0/all/0/1\">Benjamin C. M. Fung</a>",
          "description": "Authorship attribution is the problem of identifying the most plausible\nauthor of an anonymous text from a set of candidate authors. Researchers have\ninvestigated same-topic and cross-topic scenarios of authorship attribution,\nwhich differ according to whether new, unseen topics are used in the testing\nphase. However, neither scenario allows us to explain whether errors are caused\nby a failure to capture authorship writing style or by a topic shift. Motivated\nby this, we propose the \\emph{topic confusion} task where we switch the\nauthor-topic configuration between the training and testing sets. This setup\nallows us to distinguish two types of errors: those caused by the topic shift\nand those caused by the features' inability to capture the writing styles. We\nshow that stylometric features with part-of-speech tags are the least\nsusceptible to topic variations. We further show that combining them with other\nfeatures leads to significantly lower topic confusion and higher attribution\naccuracy. Finally, we show that pretrained language models such as BERT and\nRoBERTa perform poorly on this task and are surpassed by simple features such\nas word-level $n$-grams.",
          "link": "http://arxiv.org/abs/2104.08530",
          "publishedOn": "2021-09-10T07:20:12.030Z",
          "wordCount": 671,
          "title": "The Topic Confusion Task: A Novel Scenario for Authorship Attribution. (arXiv:2104.08530v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1\">Peter Izsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berchansky_M/0/1/0/all/0/1\">Moshe Berchansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.",
          "link": "http://arxiv.org/abs/2104.07705",
          "publishedOn": "2021-09-10T07:20:11.999Z",
          "wordCount": 568,
          "title": "How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jisi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorila_C/0/1/0/all/0/1\">Catalin Zorila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jon Barker</a>",
          "description": "In this paper, we introduce a novel semi-supervised learning framework for\nend-to-end speech separation. The proposed method first uses mixtures of\nunseparated sources and the mixture invariant training (MixIT) criterion to\ntrain a teacher model. The teacher model then estimates separated sources that\nare used to train a student model with standard permutation invariant training\n(PIT). The student model can be fine-tuned with supervised data, i.e., paired\nartificial mixtures and clean speech sources, and further improved via model\ndistillation. Experiments with single and multi channel mixtures show that the\nteacher-student training resolves the over-separation problem observed in the\noriginal MixIT method. Further, the semisupervised performance is comparable to\na fully-supervised separation system trained using ten times the amount of\nsupervised data.",
          "link": "http://arxiv.org/abs/2106.07843",
          "publishedOn": "2021-09-10T07:20:11.991Z",
          "wordCount": 611,
          "title": "Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Honovich_O/0/1/0/all/0/1\">Or Honovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1\">Roee Aharoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeman_E/0/1/0/all/0/1\">Ella Neeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1\">Idan Szpektor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>",
          "description": "Neural knowledge-grounded generative models for dialogue often produce\ncontent that is factually inconsistent with the knowledge they rely on, making\nthem unreliable and limiting their applicability. Inspired by recent work on\nevaluating factual consistency in abstractive summarization, we propose an\nautomatic evaluation metric for factual consistency in knowledge-grounded\ndialogue using automatic question generation and question answering. Our\nmetric, denoted $Q^2$, compares answer spans using natural language inference\n(NLI), instead of token-based matching as done in previous work. To foster\nproper evaluation, we curate a novel dataset of dialogue system outputs for the\nWizard-of-Wikipedia dataset, manually annotated for factual consistency. We\nperform a thorough meta-evaluation of $Q^2$ against other metrics using this\ndataset and two others, where it consistently shows higher correlation with\nhuman judgements.",
          "link": "http://arxiv.org/abs/2104.08202",
          "publishedOn": "2021-09-10T07:20:11.871Z",
          "wordCount": 612,
          "title": "$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. (arXiv:2104.08202v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-09-10T07:20:11.789Z",
          "wordCount": 633,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>",
          "description": "Spoken dialog systems need to be able to handle both multiple languages and\nmultilinguality inside a conversation (\\textit{e.g} in case of code-switching).\nIn this work, we introduce new pretraining losses tailored to learn\nmultilingual spoken dialog representations. The goal of these losses is to\nexpose the model to code-switched language. To scale up training, we\nautomatically build a pretraining corpus composed of multilingual conversations\nin five different languages (French, Italian, English, German and Spanish) from\n\\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We\ntest the generic representations on \\texttt{MIAM}, a new benchmark composed of\nfive dialog act corpora on the same aforementioned languages as well as on two\nnovel multilingual downstream tasks (\\textit{i.e} multilingual mask utterance\nretrieval and multilingual inconsistency identification). Our experiments show\nthat our new code switched-inspired losses achieve a better performance in both\nmonolingual and multilingual settings.",
          "link": "http://arxiv.org/abs/2108.12465",
          "publishedOn": "2021-09-10T07:20:11.782Z",
          "wordCount": 608,
          "title": "Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>",
          "description": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.",
          "link": "http://arxiv.org/abs/2104.08027",
          "publishedOn": "2021-09-10T07:20:11.770Z",
          "wordCount": 694,
          "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>",
          "description": "Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.",
          "link": "http://arxiv.org/abs/2109.00922",
          "publishedOn": "2021-09-10T07:20:11.763Z",
          "wordCount": 642,
          "title": "Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>",
          "description": "Many implicit inferences exist in text depending on how it is structured that\ncan critically impact the text's interpretation and meaning. One such\nstructural aspect present in text with chronology is the order of its\npresentation. For narratives or stories, this is known as the narrative order.\nReordering a narrative can impact the temporal, causal, event-based, and other\ninferences readers draw from it, which in turn can have strong effects both on\nits interpretation and interestingness. In this paper, we propose and\ninvestigate the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat although our models can perform decently, NAREOR is a challenging task\nwith potential for further exploration. We also investigate two applications of\nNAREOR: generation of more interesting variations of stories and serving as\nadversarial sets for temporal/event-related tasks, besides discussing other\nprospective ones, such as for pedagogical setups related to language skills\nlike essay writing and applications to medicine involving clinical narratives.",
          "link": "http://arxiv.org/abs/2104.06669",
          "publishedOn": "2021-09-10T07:20:11.697Z",
          "wordCount": 703,
          "title": "NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zeyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Extractive text summarization aims at extracting the most representative\nsentences from a given document as its summary. To extract a good summary from\na long text document, sentence embedding plays an important role. Recent\nstudies have leveraged graph neural networks to capture the inter-sentential\nrelationship (e.g., the discourse graph) to learn contextual sentence\nembedding. However, those approaches neither consider multiple types of\ninter-sentential relationships (e.g., semantic similarity & natural\nconnection), nor model intra-sentential relationships (e.g, semantic &\nsyntactic relationship among words). To address these problems, we propose a\nnovel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model\ndifferent types of relationships among sentences and words. Based on Multi-GCN,\nwe propose a Multiplex Graph Summarization (Multi-GraS) model for extractive\ntext summarization. Finally, we evaluate the proposed models on the\nCNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.",
          "link": "http://arxiv.org/abs/2108.12870",
          "publishedOn": "2021-09-10T07:20:11.674Z",
          "wordCount": 601,
          "title": "Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>",
          "description": "Neural conversation models have shown great potentials towards generating\nfluent and informative responses by introducing external background knowledge.\nNevertheless, it is laborious to construct such knowledge-grounded dialogues,\nand existing models usually perform poorly when transfer to new domains with\nlimited training samples. Therefore, building a knowledge-grounded dialogue\nsystem under the low-resource setting is a still crucial issue. In this paper,\nwe propose a novel three-stage learning framework based on weakly supervised\nlearning which benefits from large scale ungrounded dialogues and unstructured\nknowledge base. To better cooperate with this framework, we devise a variant of\nTransformer with decoupled decoder which facilitates the disentangled learning\nof response generation and knowledge incorporation. Evaluation results on two\nbenchmarks indicate that our approach can outperform other state-of-the-art\nmethods with less training data, and even in zero-resource scenario, our\napproach still performs well.",
          "link": "http://arxiv.org/abs/2109.04096",
          "publishedOn": "2021-09-10T07:20:11.647Z",
          "wordCount": 594,
          "title": "A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation. (arXiv:2109.04096v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Srinagesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>",
          "description": "Albeit the universal representational power of pre-trained language models,\nadapting them onto a specific NLP task still requires a considerably large\namount of labeled data. Effective task fine-tuning meets challenges when only a\nfew labeled examples are present for the task. In this paper, we aim to the\naddress of the problem of few shot task learning by exploiting and transferring\nfrom a different task which admits a related but disparate label space.\nSpecifically, we devise a label transfer network (LTN) to transform the labels\nfrom source task to the target task of interest for training. Both the LTN and\nthe model for task prediction are learned via a bi-level optimization\nframework, which we term as MetaXT. MetaXT offers a principled solution to best\nadapt a pre-trained language model to the target task by transferring knowledge\nfrom the source task. Empirical evaluations on cross-task transfer settings for\nfour NLP tasks, from two different types of label space disparities,\ndemonstrate the effectiveness of MetaXT, especially when the labeled data in\nthe target task is limited.",
          "link": "http://arxiv.org/abs/2109.04240",
          "publishedOn": "2021-09-10T07:20:11.630Z",
          "wordCount": 617,
          "title": "MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Manqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chunguang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>",
          "description": "Neural relation extraction models have shown promising results in recent\nyears; however, the model performance drops dramatically given only a few\ntraining samples. Recent works try leveraging the advance in few-shot learning\nto solve the low resource problem, where they train label-agnostic models to\ndirectly compare the semantic similarities among context sentences in the\nembedding space. However, the label-aware information, i.e., the relation label\nthat contains the semantic knowledge of the relation itself, is often neglected\nfor prediction. In this work, we propose a framework considering both\nlabel-agnostic and label-aware semantic mapping information for low resource\nrelation extraction. We show that incorporating the above two types of mapping\ninformation in both pretraining and fine-tuning can significantly improve the\nmodel performance on low-resource relation extraction tasks.",
          "link": "http://arxiv.org/abs/2109.04108",
          "publishedOn": "2021-09-10T07:20:11.624Z",
          "wordCount": 584,
          "title": "MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction. (arXiv:2109.04108v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhirut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>",
          "description": "Back-translation (BT) of target monolingual corpora is a widely used data\naugmentation strategy for neural machine translation (NMT), especially for\nlow-resource language pairs. To improve effectiveness of the available BT data,\nwe introduce HintedBT -- a family of techniques which provides hints (through\ntags) to the encoder and decoder. First, we propose a novel method of using\nboth high and low quality BT data by providing hints (as source tags on the\nencoder) to the model about the quality of each source-target pair. We don't\nfilter out low quality data but instead show that these hints enable the model\nto learn effectively from noisy data. Second, we address the problem of\npredicting whether a source token needs to be translated or transliterated to\nthe target language, which is common in cross-script translation tasks (i.e.,\nwhere source and target do not share the written script). For such cases, we\npropose training the model with additional hints (as target tags on the\ndecoder) that provide information about the operation required on the source\n(translation or both translation and transliteration). We conduct experiments\nand detailed analyses on standard WMT benchmarks for three cross-script\nlow/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our\nmethods compare favorably with five strong and well established baselines. We\nshow that using these hints, both separately and together, significantly\nimproves translation quality and leads to state-of-the-art performance in all\nthree language pairs in corresponding bilingual settings.",
          "link": "http://arxiv.org/abs/2109.04443",
          "publishedOn": "2021-09-10T07:20:11.560Z",
          "wordCount": 692,
          "title": "HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haohai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jialun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Temporal knowledge graph (TKG) reasoning is a crucial task that has gained\nincreasing research interest in recent years. Most existing methods focus on\nreasoning at past timestamps to complete the missing facts, and there are only\na few works of reasoning on known TKGs to forecast future facts. Compared with\nthe completion task, the forecasting task is more difficult that faces two main\nchallenges: (1) how to effectively model the time information to handle future\ntimestamps? (2) how to make inductive inference to handle previously unseen\nentities that emerge over time? To address these challenges, we propose the\nfirst reinforcement learning method for forecasting. Specifically, the agent\ntravels on historical knowledge graph snapshots to search for the answer. Our\nmethod defines a relative time encoding function to capture the timespan\ninformation, and we design a novel time-shaped reward based on Dirichlet\ndistribution to guide the model learning. Furthermore, we propose a novel\nrepresentation method for unseen entities to improve the inductive inference\nability of the model. We evaluate our method for this link prediction task at\nfuture timestamps. Extensive experiments on four benchmark datasets demonstrate\nsubstantial performance improvement meanwhile with higher explainability, less\ncalculation, and fewer parameters when compared with existing state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2109.04101",
          "publishedOn": "2021-09-10T07:20:11.552Z",
          "wordCount": 656,
          "title": "TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendelson_M/0/1/0/all/0/1\">Michael Mendelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>",
          "description": "Model robustness to bias is often determined by the generalization on\ncarefully designed out-of-distribution datasets. Recent debiasing methods in\nnatural language understanding (NLU) improve performance on such datasets by\npressuring models into making unbiased predictions. An underlying assumption\nbehind such methods is that this also leads to the discovery of more robust\nfeatures in the model's inner representations. We propose a general\nprobing-based framework that allows for post-hoc interpretation of biases in\nlanguage models, and use an information-theoretic approach to measure the\nextractability of certain biases from the model's representations. We\nexperiment with several NLU datasets and known biases, and show that,\ncounter-intuitively, the more a language model is pushed towards a debiased\nregime, the more bias is actually encoded in its inner representations.",
          "link": "http://arxiv.org/abs/2109.04095",
          "publishedOn": "2021-09-10T07:20:11.544Z",
          "wordCount": 575,
          "title": "Debiasing Methods in Natural Language Understanding Make Bias More Accessible. (arXiv:2109.04095v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_L/0/1/0/all/0/1\">Liangjun Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>",
          "description": "Contrastive learning has been gradually applied to learn high-quality\nunsupervised sentence embedding. Among the previous un-supervised methods, the\nlatest state-of-the-art method, as far as we know, is unsupervised SimCSE\n(unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training\nstage by pulling semantically similar sentences together and pushing apart\ndis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE\nto get more adequate comparisons among samples and avoid overfitting. However,\nincreasing the batch size does not always lead to improvements, but instead\neven lead to performance degradation when the batch size exceeds a threshold.\nThrough statistical observation, we find that this is probably due to the\nintroduction of low-confidence negative pairs after in-creasing the batch size.\nTo alleviate this problem, we introduce a simple smoothing strategy upon the\nInfoNCE loss function, termedGaussian Smoothing InfoNCE\n(GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative\nsamples, which act asa smoothing of the negative sample space.Though being\nsimple, the proposed smooth-ing strategy brings substantial improvements to\nunsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity\n(STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an\naverage Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of\nBERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.",
          "link": "http://arxiv.org/abs/2109.04321",
          "publishedOn": "2021-09-10T07:20:11.536Z",
          "wordCount": 648,
          "title": "Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2109.04321v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>",
          "description": "Pretrained vision-and-language BERTs aim to learn representations that\ncombine information from both modalities. We propose a diagnostic method based\non cross-modal input ablation to assess the extent to which these models\nactually integrate cross-modal information. This method involves ablating\ninputs from one modality, either entirely or selectively based on cross-modal\ngrounding alignments, and evaluating the model prediction performance on the\nother modality. Model performance is measured by modality-specific tasks that\nmirror the model pretraining objectives (e.g. masked language modelling for\ntext). Models that have learned to construct cross-modal representations using\nboth modalities are expected to perform worse when inputs are missing from a\nmodality. We find that recently proposed models have much greater relative\ndifficulty predicting text when visual information is ablated, compared to\npredicting visual object categories when text is ablated, indicating that these\nmodels are not symmetrically cross-modal.",
          "link": "http://arxiv.org/abs/2109.04448",
          "publishedOn": "2021-09-10T07:20:11.492Z",
          "wordCount": 591,
          "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1\">Qiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1\">Guangzhi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qianlan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Songfang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>",
          "description": "Automatic Question Answering (QA) has been successfully applied in various\ndomains such as search engines and chatbots. Biomedical QA (BQA), as an\nemerging QA task, enables innovative applications to effectively perceive,\naccess and understand complex biomedical knowledge. There have been tremendous\ndevelopments of BQA in the past two decades, which we classify into 5\ndistinctive approaches: classic, information retrieval, machine reading\ncomprehension, knowledge base and question entailment approaches. In this\nsurvey, we introduce available datasets and representative methods of each BQA\napproach in detail. Despite the developments, BQA systems are still immature\nand rarely used in real-life settings. We identify and characterize several key\nchallenges in BQA that might lead to this issue, and discuss some potential\nfuture directions to explore.",
          "link": "http://arxiv.org/abs/2102.05281",
          "publishedOn": "2021-09-10T07:20:11.484Z",
          "wordCount": 613,
          "title": "Biomedical Question Answering: A Survey of Approaches and Challenges. (arXiv:2102.05281v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>",
          "description": "Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.",
          "link": "http://arxiv.org/abs/2109.04332",
          "publishedOn": "2021-09-10T07:20:11.465Z",
          "wordCount": 639,
          "title": "PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>",
          "description": "Recently, the textual adversarial attack models become increasingly popular\ndue to their successful in estimating the robustness of NLP models. However,\nexisting works have obvious deficiencies. (1) They usually consider only a\nsingle granularity of modification strategies (e.g. word-level or\nsentence-level), which is insufficient to explore the holistic textual space\nfor generation; (2) They need to query victim models hundreds of times to make\na successful attack, which is highly inefficient in practice. To address such\nproblems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to\neffectively generate high-quality adversarial samples with fewer queries to\nvictim models. Furthermore, we propose a reinforcement-learning based method to\ntrain a multi-granularity attack agent through behavior cloning with the expert\nknowledge from our MAYA algorithm to further reduce the query times.\nAdditionally, we also adapt the agent to attack black-box models that only\noutput labels without confidence scores. We conduct comprehensive experiments\nto evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two\ndifferent black-box attack settings and three benchmark datasets. Experimental\nresults show that our models achieve overall better attacking performance and\nproduce more fluent and grammatical adversarial samples compared to baseline\nmodels. Besides, our adversarial attack agent significantly reduces the query\ntimes in both attack settings. Our codes are released at\nhttps://github.com/Yangyi-Chen/MAYA.",
          "link": "http://arxiv.org/abs/2109.04367",
          "publishedOn": "2021-09-10T07:20:11.457Z",
          "wordCount": 670,
          "title": "Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhongqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>",
          "description": "There is an increasing interest in the use of mathematical word problem (MWP)\ngeneration in educational assessment. Different from standard natural question\ngeneration, MWP generation needs to maintain the underlying mathematical\noperations between quantities and variables, while at the same time ensuring\nthe relevance between the output and the given topic. To address above problem,\nwe develop an end-to-end neural model to generate diverse MWPs in real-world\nscenarios from commonsense knowledge graph and equations. The proposed model\n(1) learns both representations from edge-enhanced Levi graphs of symbolic\nequations and commonsense knowledge; (2) automatically fuses equation and\ncommonsense knowledge information via a self-planning module when generating\nthe MWPs. Experiments on an educational gold-standard set and a large-scale\ngenerated MWP set show that our approach is superior on the MWP generation\ntask, and it outperforms the SOTA models in terms of both automatic evaluation\nmetrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e.,\nequation relevance, topic relevance, and language coherence. To encourage\nreproducible results, we make our code and MWP dataset public available at\n\\url{https://github.com/tal-ai/MaKE_EMNLP2021}.",
          "link": "http://arxiv.org/abs/2010.06196",
          "publishedOn": "2021-09-10T07:20:11.429Z",
          "wordCount": 678,
          "title": "Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations. (arXiv:2010.06196v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.03437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks.",
          "link": "http://arxiv.org/abs/1911.03437",
          "publishedOn": "2021-09-10T07:20:11.394Z",
          "wordCount": 686,
          "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1\">Alireza Salemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kebriaei_E/0/1/0/all/0/1\">Emad Kebriaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_G/0/1/0/all/0/1\">Ghazal Neisi Minaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakery_A/0/1/0/all/0/1\">Azadeh Shakery</a>",
          "description": "Abstractive text summarization is one of the areas influenced by the\nemergence of pre-trained language models. Current pre-training works in\nabstractive summarization give more points to the summaries with more words in\ncommon with the main text and pay less attention to the semantic similarity\nbetween generated sentences and the original document. We propose ARMAN, a\nTransformer-based encoder-decoder model pre-trained with three novel objectives\nto address this issue. In ARMAN, salient sentences from a document are selected\naccording to a modified semantic score to be masked and form a pseudo summary.\nTo summarize more accurately and similar to human writing patterns, we applied\nmodified sentence reordering. We evaluated our proposed models on six\ndownstream Persian summarization tasks. Experimental results show that our\nproposed model achieves state-of-the-art performance on all six summarization\ntasks measured by ROUGE and BERTScore. Our models also outperform prior works\nin textual entailment, question paraphrasing, and multiple choice question\nanswering. Finally, we established a human evaluation and show that using the\nsemantic score significantly improves summarization results.",
          "link": "http://arxiv.org/abs/2109.04098",
          "publishedOn": "2021-09-10T07:20:11.311Z",
          "wordCount": 622,
          "title": "ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization. (arXiv:2109.04098v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "The factual knowledge acquired during pre-training and stored in the\nparameters of Language Models (LMs) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod which can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditordoes not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components need to be changed to manipulate\nfactual knowledge; our analysis shows that the updates tend to be concentrated\non a small subset of components. Source code available at\nhttps://github.com/nicola-decao/KnowledgeEditor",
          "link": "http://arxiv.org/abs/2104.08164",
          "publishedOn": "2021-09-10T07:20:11.254Z",
          "wordCount": 732,
          "title": "Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasigi_P/0/1/0/all/0/1\">Pradeep Dasigi</a>",
          "description": "Question Answering (QA) tasks requiring information from multiple documents\noften rely on a retrieval model to identify relevant information for reasoning.\nThe retrieval model is typically trained to maximize the likelihood of the\nlabeled supporting evidence. However, when retrieving from large text corpora\nsuch as Wikipedia, the correct answer can often be obtained from multiple\nevidence candidates. Moreover, not all such candidates are labeled as positive\nduring annotation, rendering the training signal weak and noisy. This problem\nis exacerbated when the questions are unanswerable or when the answers are\nBoolean, since the model cannot rely on lexical overlap to make a connection\nbetween the answer and supporting evidence. We develop a new parameterization\nof set-valued retrieval that handles unanswerable queries, and we show that\nmarginalizing over this set during training allows a model to mitigate false\nnegatives in supporting evidence annotations. We test our method on two\nmulti-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint\nmodeling with marginalization improves model performance by 5.5 F1 points and\nachieves a new state-of-the-art performance of 50.5 F1. We also show that\nretrieval marginalization results in 4.1 QA F1 improvement over a\nnon-marginalized baseline on HotpotQA in the fullwiki setting.",
          "link": "http://arxiv.org/abs/2103.12235",
          "publishedOn": "2021-09-10T07:20:11.246Z",
          "wordCount": 676,
          "title": "Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization. (arXiv:2103.12235v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinbei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-party multi-turn dialogue comprehension brings unprecedented challenges\non handling the complicated scenarios from multiple speakers and criss-crossed\ndiscourse relationship among speaker-aware utterances. Most existing methods\ndeal with dialogue contexts as plain texts and pay insufficient attention to\nthe crucial speaker-aware clues. In this work, we propose an enhanced\nspeaker-aware model with masking attention and heterogeneous graph networks to\ncomprehensively capture discourse clues from both sides of speaker property and\nspeaker-aware relationships. With such comprehensive speaker-aware modeling,\nexperimental results show that our speaker-aware model helps achieves\nstate-of-the-art performance on the benchmark dataset Molweni. Case analysis\nshows that our model enhances the connections between utterances and their own\nspeakers and captures the speaker-aware discourse relations, which are critical\nfor dialogue modeling.",
          "link": "http://arxiv.org/abs/2109.04066",
          "publishedOn": "2021-09-10T07:20:11.211Z",
          "wordCount": 554,
          "title": "Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension. (arXiv:2109.04066v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaochen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_L/0/1/0/all/0/1\">Liangjun Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jizhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Songlin Hu</a>",
          "description": "Contrastive learning has been attracting much attention for learning\nunsupervised sentence embeddings. The current state-of-the-art unsupervised\nmethod is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as\na minimal data augmentation method, and passes the same input sentence to a\npre-trained Transformer encoder (with dropout turned on) twice to obtain the\ntwo corresponding embeddings to build a positive pair. As the length\ninformation of a sentence will generally be encoded into the sentence\nembeddings due to the usage of position embedding in Transformer, each positive\npair in unsup-SimCSE actually contains the same length information. And thus\nunsup-SimCSE trained with these positive pairs is probably biased, which would\ntend to consider that sentences of the same or similar length are more similar\nin semantics. Through statistical observations, we find that unsup-SimCSE does\nhave such a problem. To alleviate it, we apply a simple repetition operation to\nmodify the input sentence, and then pass the input sentence and its modified\ncounterpart to the pre-trained Transformer encoder, respectively, to get the\npositive pair. Additionally, we draw inspiration from the community of computer\nvision and introduce a momentum contrast, enlarging the number of negative\npairs without additional calculations. The proposed two modifications are\napplied on positive and negative pairs separately, and build a new sentence\nembedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the\nproposed ESimCSE on several benchmark datasets w.r.t the semantic text\nsimilarity (STS) task. Experimental results show that ESimCSE outperforms the\nstate-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on\nBERT-base.",
          "link": "http://arxiv.org/abs/2109.04380",
          "publishedOn": "2021-09-10T07:20:11.192Z",
          "wordCount": 712,
          "title": "ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding. (arXiv:2109.04380v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>",
          "description": "We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.",
          "link": "http://arxiv.org/abs/2109.04114",
          "publishedOn": "2021-09-10T07:20:11.184Z",
          "wordCount": 489,
          "title": "Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04411",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higuchi_Y/0/1/0/all/0/1\">Yosuke Higuchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "This article describes an efficient end-to-end speech translation (E2E-ST)\nframework based on non-autoregressive (NAR) models. End-to-end speech\ntranslation models have several advantages over traditional cascade systems\nsuch as inference latency reduction. However, conventional AR decoding methods\nare not fast enough because each token is generated incrementally. NAR models,\nhowever, can accelerate the decoding speed by generating multiple tokens in\nparallel on the basis of the token-wise conditional independence assumption. We\npropose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder\nand an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary\nshallow AR decoder selects the best hypothesis by rescoring multiple candidates\ngenerated from the NAR decoder in parallel (parallel AR rescoring). We adopt\nconditional masked language model (CMLM) and a connectionist temporal\nclassification (CTC)-based model as NAR decoders for Orthros, referred to as\nOrthros-CMLM and Orthros-CTC, respectively. We also propose two training\nmethods to enhance the CMLM decoder. Experimental evaluations on three\nbenchmark datasets with six language directions demonstrated that Orthros\nachieved large improvements in translation quality with a very small overhead\ncompared with the baseline NAR model. Moreover, the Conformer encoder\narchitecture enabled large quality improvements, especially for CTC-based\nmodels. Orthros-CTC with the Conformer encoder increased decoding speed by\n3.63x on CPU with translation quality comparable to that of an AR model.",
          "link": "http://arxiv.org/abs/2109.04411",
          "publishedOn": "2021-09-10T07:20:11.172Z",
          "wordCount": 677,
          "title": "Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring. (arXiv:2109.04411v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>",
          "description": "We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.",
          "link": "http://arxiv.org/abs/2109.04282",
          "publishedOn": "2021-09-10T07:20:11.162Z",
          "wordCount": 575,
          "title": "Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "Opinion summarization has been traditionally approached with unsupervised,\nweakly-supervised and few-shot learning techniques. In this work, we collect a\nlarge dataset of summaries paired with user reviews for over 31,000 products,\nenabling supervised training. However, the number of reviews per product is\nlarge (320 on average), making summarization - and especially training a\nsummarizer - impractical. Moreover, the content of many reviews is not\nreflected in the human-written summaries, and, thus, the summarizer trained on\nrandom review subsets hallucinates. In order to deal with both of these\nchallenges, we formulate the task as jointly learning to select informative\nsubsets of reviews and summarizing the opinions expressed in these subsets. The\nchoice of the review subset is treated as a latent variable, predicted by a\nsmall and simple selector. The subset is then fed into a more powerful\nsummarizer. For joint training, we use amortized variational inference and\npolicy gradient methods. Our experiments demonstrate the importance of\nselecting informative reviews resulting in improved quality of summaries and\nreduced hallucinations.",
          "link": "http://arxiv.org/abs/2109.04325",
          "publishedOn": "2021-09-10T07:20:11.155Z",
          "wordCount": 615,
          "title": "Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolo_M/0/1/0/all/0/1\">Max Bartolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1\">Pontus Stenetorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinberg_B/0/1/0/all/0/1\">Bennett Kleinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffin_L/0/1/0/all/0/1\">Lewis D. Griffin</a>",
          "description": "Research shows that natural language processing models are generally\nconsidered to be vulnerable to adversarial attacks; but recent work has drawn\nattention to the issue of validating these adversarial inputs against certain\ncriteria (e.g., the preservation of semantics and grammaticality). Enforcing\nconstraints to uphold such criteria may render attacks unsuccessful, raising\nthe question of whether valid attacks are actually feasible. In this work, we\ninvestigate this through the lens of human language ability. We report on\ncrowdsourcing studies in which we task humans with iteratively modifying words\nin an input text, while receiving immediate model feedback, with the aim of\ncausing a sentiment classification model to misclassify the example. Our\nfindings suggest that humans are capable of generating a substantial amount of\nadversarial examples using semantics-preserving word substitutions. We analyze\nhow human-generated adversarial examples compare to the recently proposed\nTextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions\nnaturalness, preservation of sentiment, grammaticality and substitution rate.\nOur findings suggest that human-generated adversarial examples are not more\nable than the best algorithms to generate natural-reading, sentiment-preserving\nexamples, though they do so by being much more computationally efficient.",
          "link": "http://arxiv.org/abs/2109.04385",
          "publishedOn": "2021-09-10T07:20:11.148Z",
          "wordCount": 637,
          "title": "Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification. (arXiv:2109.04385v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1\">Jeff Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>",
          "description": "Providing natural language processing systems with commonsense knowledge is a\ncritical challenge for achieving language understanding. Recently, commonsense\nknowledge models have emerged as a suitable approach for hypothesizing\nsituation-relevant commonsense knowledge on-demand in natural language\napplications. However, these systems are limited by the fixed set of relations\ncaptured by schemas of the knowledge bases on which they're trained.\n\nTo address this limitation, we investigate training commonsense knowledge\nmodels in a few-shot setting with limited tuples per commonsense relation in\nthe graph. We perform five separate studies on different dimensions of few-shot\ncommonsense knowledge learning, providing a roadmap on best practices for\ntraining these systems efficiently. Importantly, we find that human quality\nratings for knowledge produced from a few-shot trained system can achieve\nperformance within 6% of knowledge produced from fully supervised systems. This\nfew-shot performance enables coverage of a wide breadth of relations in future\ncommonsense systems.",
          "link": "http://arxiv.org/abs/2101.00297",
          "publishedOn": "2021-09-10T07:20:11.141Z",
          "wordCount": 616,
          "title": "Understanding Few-Shot Commonsense Knowledge Models. (arXiv:2101.00297v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nicosia_M/0/1/0/all/0/1\">Massimo Nicosia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhongdi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>",
          "description": "While multilingual pretrained language models (LMs) fine-tuned on a single\nlanguage have shown substantial cross-lingual task transfer capabilities, there\nis still a wide performance gap in semantic parsing tasks when target language\nsupervision is available. In this paper, we propose a novel Translate-and-Fill\n(TaF) method to produce silver training data for a multilingual semantic\nparser. This method simplifies the popular Translate-Align-Project (TAP)\npipeline and consists of a sequence-to-sequence filler model that constructs a\nfull parse conditioned on an utterance and a view of the same parse. Our filler\nis trained on English data only but can accurately complete instances in other\nlanguages (i.e., translations of the English training utterances), in a\nzero-shot fashion. Experimental results on three multilingual semantic parsing\ndatasets show that data augmentation with TaF reaches accuracies competitive\nwith similar systems which rely on traditional alignment techniques.",
          "link": "http://arxiv.org/abs/2109.04319",
          "publishedOn": "2021-09-10T07:20:11.104Z",
          "wordCount": 598,
          "title": "Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>",
          "description": "We demonstrate that, hidden within one-layer randomly weighted neural\nnetworks, there exist subnetworks that can achieve impressive performance,\nwithout ever modifying the weight initializations, on machine translation\ntasks. To find subnetworks for one-layer randomly weighted neural networks, we\napply different binary masks to the same weight matrix to generate different\nlayers. Hidden within a one-layer randomly weighted Transformer, we find that\nsubnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed\npre-trained embedding layer, the previously found subnetworks are smaller than,\nbut can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained\nTransformer small/base on IWSLT14/WMT14. Furthermore, we demonstrate the\neffectiveness of larger and deeper transformers in this setting, as well as the\nimpact of different initialization methods. We released the source code at\nhttps://github.com/sIncerass/one_layer_lottery_ticket.",
          "link": "http://arxiv.org/abs/2109.03939",
          "publishedOn": "2021-09-10T07:20:11.025Z",
          "wordCount": 581,
          "title": "What's Hidden in a One-layer Randomly Weighted Transformer?. (arXiv:2109.03939v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Currently, multilingual machine translation is receiving more and more\nattention since it brings better performance for low resource languages (LRLs)\nand saves more space. However, existing multilingual machine translation models\nface a severe challenge: imbalance. As a result, the translation performance of\ndifferent languages in multilingual translation models are quite different. We\nargue that this imbalance problem stems from the different learning\ncompetencies of different languages. Therefore, we focus on balancing the\nlearning competencies of different languages and propose Competence-based\nCurriculum Learning for Multilingual Machine Translation, named CCL-M.\nSpecifically, we firstly define two competencies to help schedule the high\nresource languages (HRLs) and the low resource languages: 1) Self-evaluated\nCompetence, evaluating how well the language itself has been learned; and 2)\nHRLs-evaluated Competence, evaluating whether an LRL is ready to be learned\naccording to HRLs' Self-evaluated Competence. Based on the above competencies,\nwe utilize the proposed CCL-M algorithm to gradually add new languages into the\ntraining set in a curriculum learning manner. Furthermore, we propose a novel\ncompetenceaware dynamic balancing sampling strategy for better selecting\ntraining samples in multilingual training. Experimental results show that our\napproach has achieved a steady and significant performance gain compared to the\nprevious state-of-the-art approach on the TED talks dataset.",
          "link": "http://arxiv.org/abs/2109.04002",
          "publishedOn": "2021-09-10T07:20:11.005Z",
          "wordCount": 659,
          "title": "Competence-based Curriculum Learning for Multilingual Machine Translation. (arXiv:2109.04002v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>",
          "description": "Non-parametric neural language models (NLMs) learn predictive distributions\nof text utilizing an external datastore, which allows them to learn through\nexplicitly memorizing the training datapoints. While effective, these models\noften require retrieval from a large datastore at test time, significantly\nincreasing the inference overhead and thus limiting the deployment of\nnon-parametric NLMs in practical applications. In this paper, we take the\nrecently proposed $k$-nearest neighbors language model (Khandelwal et al.,\n2019) as an example, exploring methods to improve its efficiency along various\ndimensions. Experiments on the standard WikiText-103 benchmark and\ndomain-adaptation datasets show that our methods are able to achieve up to a 6x\nspeed-up in inference speed while retaining comparable performance. The\nempirical analysis we present may provide guidelines for future research\nseeking to develop or deploy more efficient non-parametric NLMs.",
          "link": "http://arxiv.org/abs/2109.04212",
          "publishedOn": "2021-09-10T07:20:10.998Z",
          "wordCount": 568,
          "title": "Efficient Nearest Neighbor Language Models. (arXiv:2109.04212v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bostrom_K/0/1/0/all/0/1\">Kaj Bostrom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Swarat Chaudhuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>",
          "description": "An interpretable system for open-domain reasoning needs to express its\nreasoning process in a transparent form. Natural language is an attractive\nrepresentation for this purpose -- it is both highly expressive and easy for\nhumans to understand. However, manipulating natural language statements in\nlogically consistent ways is hard: models must cope with variation in how\nmeaning is expressed while remaining precise. In this paper, we describe\nParaPattern, a method for building models to generate deductive inferences from\ndiverse natural language inputs without direct human supervision. We train\nBART-based models (Lewis et al., 2020) to generate the result of applying a\nparticular logical operation to one or more premise statements. Crucially, we\ndevelop a largely automated pipeline for constructing suitable training\nexamples from Wikipedia. We evaluate our models using out-of-domain sentence\ncompositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et\nal., 2021) datasets as well as targeted perturbation sets. Our results show\nthat our models are substantially more accurate and flexible than baseline\nsystems. ParaPattern achieves 85% validity on examples of the 'substitution'\noperation from EntailmentBank without the use of any in-domain training data,\nmatching the performance of a model fine-tuned for EntailmentBank. The full\nsource code for our method is publicly available.",
          "link": "http://arxiv.org/abs/2104.08825",
          "publishedOn": "2021-09-10T07:20:10.990Z",
          "wordCount": 693,
          "title": "Flexible Generation of Natural Language Deductions. (arXiv:2104.08825v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "Transformers are arguably the main workhorse in recent Natural Language\nProcessing research. By definition a Transformer is invariant with respect to\nreordering of the input. However, language is inherently sequential and word\norder is essential to the semantics and syntax of an utterance. In this\narticle, we provide an overview and theoretical comparison of existing methods\nto incorporate position information into Transformer models. The objectives of\nthis survey are to (1) showcase that position information in Transformer is a\nvibrant and extensive research area; (2) enable the reader to compare existing\nmethods by providing a unified notation and systematization of different\napproaches along important model dimensions; (3) indicate what characteristics\nof an application should be taken into account when selecting a position\nencoding; (4) provide stimuli for future research.",
          "link": "http://arxiv.org/abs/2102.11090",
          "publishedOn": "2021-09-10T07:20:10.971Z",
          "wordCount": 602,
          "title": "Position Information in Transformers: An Overview. (arXiv:2102.11090v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "With the rapid increase in the volume of dialogue data from daily life, there\nis a growing demand for dialogue summarization. Unfortunately, training a large\nsummarization model is generally infeasible due to the inadequacy of dialogue\ndata with annotated summaries. Most existing works for low-resource dialogue\nsummarization directly pretrain models in other domains, e.g., the news domain,\nbut they generally neglect the huge difference between dialogues and\nconventional articles. To bridge the gap between out-of-domain pretraining and\nin-domain fine-tuning, in this work, we propose a multi-source pretraining\nparadigm to better leverage the external summary data. Specifically, we exploit\nlarge-scale in-domain non-summary data to separately pretrain the dialogue\nencoder and the summary decoder. The combined encoder-decoder model is then\npretrained on the out-of-domain summary data using adversarial critics, aiming\nto facilitate domain-agnostic summarization. The experimental results on two\npublic datasets show that with only limited training data, our approach\nachieves competitive performance and generalizes well in different dialogue\nscenarios.",
          "link": "http://arxiv.org/abs/2109.04080",
          "publishedOn": "2021-09-10T07:20:10.940Z",
          "wordCount": 606,
          "title": "Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yinquan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haonan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1\">Guirong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Incorporating factual knowledge into pre-trained language models (PLM) such\nas BERT is an emerging trend in recent NLP studies. However, most of the\nexisting methods combine the external knowledge integration module with a\nmodified pre-training loss and re-implement the pre-training process on the\nlarge-scale corpus. Re-pretraining these models is usually resource-consuming,\nand difficult to adapt to another domain with a different knowledge graph (KG).\nBesides, those works either cannot embed knowledge context dynamically\naccording to textual context or struggle with the knowledge ambiguity issue. In\nthis paper, we propose a novel knowledge-aware language model framework based\non fine-tuning process, which equips PLM with a unified knowledge-enhanced text\ngraph that contains both text and multi-relational sub-graphs extracted from\nKG. We design a hierarchical relational-graph-based message passing mechanism,\nwhich can allow the representations of injected KG and text to mutually update\neach other and can dynamically select ambiguous mentioned entities that share\nthe same text. Our empirical results show that our model can efficiently\nincorporate world knowledge from KGs into existing language models such as\nBERT, and achieve significant improvement on the machine reading comprehension\n(MRC) task compared with other knowledge-enhanced models.",
          "link": "http://arxiv.org/abs/2109.04223",
          "publishedOn": "2021-09-10T07:20:10.926Z",
          "wordCount": 645,
          "title": "KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schick_T/0/1/0/all/0/1\">Timo Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udupa_S/0/1/0/all/0/1\">Sahana Udupa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "When trained on large, unfiltered crawls from the internet, language models\npick up and reproduce all kinds of undesirable biases that can be found in the\ndata: they often generate racist, sexist, violent or otherwise toxic language.\nAs large models require millions of training examples to achieve good\nperformance, it is difficult to completely prevent them from being exposed to\nsuch content. In this paper, we first demonstrate a surprising finding:\npretrained language models recognize, to a considerable degree, their\nundesirable biases and the toxicity of the content they produce. We refer to\nthis capability as self-diagnosis. Based on this finding, we then propose a\ndecoding algorithm that, given only a textual description of the undesired\nbehavior, reduces the probability of a language model producing problematic\ntext. We refer to this approach as self-debiasing. Self-debiasing does not rely\non manually curated word lists, nor does it require any training data or\nchanges to the model's parameters. While we by no means eliminate the issue of\nlanguage models generating biased text, we believe our approach to be an\nimportant step in this direction.",
          "link": "http://arxiv.org/abs/2103.00453",
          "publishedOn": "2021-09-10T07:20:10.896Z",
          "wordCount": 664,
          "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. (arXiv:2103.00453v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>",
          "description": "Counter machines have achieved a newfound relevance to the field of natural\nlanguage processing (NLP): recent work suggests some strong-performing\nrecurrent neural networks utilize their memory as counters. Thus, one potential\nway to understand the success of these networks is to revisit the theory of\ncounter computation. Therefore, we study the abilities of real-time counter\nmachines as formal grammars, focusing on formal properties that are relevant\nfor NLP models. We first show that several variants of the counter machine\nconverge to express the same class of formal languages. We also prove that\ncounter languages are closed under complement, union, intersection, and many\nother common set operations. Next, we show that counter machines cannot\nevaluate boolean expressions, even though they can weakly validate their\nsyntax. This has implications for the interpretability and evaluation of neural\nnetwork systems: successfully matching syntactic patterns does not guarantee\nthat counter memory accurately encodes compositional semantics. Finally, we\nconsider whether counter languages are semilinear. This work makes general\ncontributions to the theory of formal languages that are of potential interest\nfor understanding recurrent neural networks.",
          "link": "http://arxiv.org/abs/2004.06866",
          "publishedOn": "2021-09-10T07:20:10.888Z",
          "wordCount": 659,
          "title": "On the Linguistic Capacity of Real-Time Counter Automata. (arXiv:2004.06866v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.04556",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>",
          "description": "Code representation learning, which aims to encode the semantics of source\ncode into distributed vectors, plays an important role in recent\ndeep-learning-based models for code intelligence. Recently, many pre-trained\nlanguage models for source code (e.g., CuBERT and CodeBERT) have been proposed\nto model the context of code and serve as a basis for downstream code\nintelligence tasks such as code search, code clone detection, and program\ntranslation. Current approaches typically consider the source code as a plain\nsequence of tokens, or inject the structure information (e.g., AST and\ndata-flow) into the sequential model pre-training. To further explore the\nproperties of programming languages, this paper proposes SynCoBERT, a\nsyntax-guided multi-modal contrastive pre-training approach for better code\nrepresentations. Specially, we design two novel pre-training objectives\noriginating from the symbolic and syntactic properties of source code, i.e.,\nIdentifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to\npredict identifiers, and edges between two nodes of AST, respectively.\nMeanwhile, to exploit the complementary information in semantically equivalent\nmodalities (i.e., code, comment, AST) of the code, we propose a multi-modal\ncontrastive learning strategy to maximize the mutual information among\ndifferent modalities. Extensive experiments on four downstream tasks related to\ncode intelligence show that SynCoBERT advances the state-of-the-art with the\nsame pre-training corpus and model size.",
          "link": "http://arxiv.org/abs/2108.04556",
          "publishedOn": "2021-09-10T07:20:10.881Z",
          "wordCount": 720,
          "title": "SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation. (arXiv:2108.04556v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>",
          "description": "Training NLP systems typically assumes access to annotated data that has a\nsingle human label per example. Given imperfect labeling from annotators and\ninherent ambiguity of language, we hypothesize that single label is not\nsufficient to learn the spectrum of language interpretation. We explore new\nlabel annotation distribution schemes, assigning multiple labels per example\nfor a small subset of training examples. Introducing such multi label examples\nat the cost of annotating fewer examples brings clear gains on natural language\ninference task and entity typing task, even when we simply first train with a\nsingle label data and then fine tune with multi label examples. Extending a\nMixUp data augmentation framework, we propose a learning algorithm that can\nlearn from uneven training examples (with zero, one, or multiple labels). This\nalgorithm efficiently combines signals from uneven training data and brings\nadditional gains in low annotation budget and cross domain settings. Together,\nour method achieves consistent gains in both accuracy and label distribution\nmetrics in two tasks, suggesting training with uneven training data can be\nbeneficial for many NLP tasks.",
          "link": "http://arxiv.org/abs/2109.04408",
          "publishedOn": "2021-09-10T07:20:10.874Z",
          "wordCount": 642,
          "title": "Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Honglei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>",
          "description": "Existing conversational systems are mostly agent-centric, which assumes the\nuser utterances would closely follow the system ontology (for NLU or dialogue\nstate tracking). However, in real-world scenarios, it is highly desirable that\nthe users can speak freely in their own way. It is extremely hard, if not\nimpossible, for the users to adapt to the unknown system ontology. In this\nwork, we attempt to build a user-centric dialogue system. As there is no clean\nmapping for a user's free form utterance to an ontology, we first model the\nuser preferences as estimated distributions over the system ontology and map\nthe users' utterances to such distributions. Learning such a mapping poses new\nchallenges on reasoning over existing knowledge, ranging from factoid\nknowledge, commonsense knowledge to the users' own situations. To this end, we\nbuild a new dataset named NUANCED that focuses on such realistic settings for\nconversational recommendation. Collected via dialogue simulation and\nparaphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user\nresponses. We conduct experiments, showing both the usefulness and challenges\nof our problem setting. We believe NUANCED can serve as a valuable resource to\npush existing research from the agent-centric system to the user-centric\nsystem. The code and data is publicly available at\n\\url{https://github.com/facebookresearch/nuanced}.",
          "link": "http://arxiv.org/abs/2010.12758",
          "publishedOn": "2021-09-10T07:20:10.833Z",
          "wordCount": 693,
          "title": "NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions. (arXiv:2010.12758v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhihua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>",
          "description": "Human dialogue contains evolving concepts, and speakers naturally associate\nmultiple concepts to compose a response. However, current dialogue models with\nthe seq2seq framework lack the ability to effectively manage concept\ntransitions and can hardly introduce multiple concepts to responses in a\nsequential decoding manner. To facilitate a controllable and coherent dialogue,\nin this work, we devise a concept-guided non-autoregressive model (CG-nAR) for\nopen-domain dialogue generation. The proposed model comprises a multi-concept\nplanning module that learns to identify multiple associated concepts from a\nconcept graph and a customized Insertion Transformer that performs\nconcept-guided non-autoregressive generation to complete a response. The\nexperimental results on two public datasets show that CG-nAR can produce\ndiverse and coherent responses, outperforming state-of-the-art baselines in\nboth automatic and human evaluations with substantially faster inference speed.",
          "link": "http://arxiv.org/abs/2109.04084",
          "publishedOn": "2021-09-10T07:20:10.826Z",
          "wordCount": 582,
          "title": "Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems. (arXiv:2109.04084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1\">Tom Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Frank Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1\">Vlad Pandelea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jinjie Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>",
          "description": "The goal of building intelligent dialogue systems has largely been\n\\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)\nsystems, which perform goal-oriented functions, and open-domain dialogue (ODD)\nsystems, which focus on non-goal-oriented chitchat. The two dialogue modes can\npotentially be intertwined together seamlessly in the same conversation, as\neasily done by a friendly human assistant. Such ability is desirable in\nconversational agents, as the integration makes them more accessible and\nuseful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn\ndialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset\nFusedChat, by rewriting the existing TOD turns and adding new ODD turns. This\nprocedure constructs conversation sessions containing exchanges from both\ndialogue modes. It features inter-mode contextual dependency, i.e., the\ndialogue turns from the two modes depend on each other. Rich dependency\npatterns including co-reference and ellipsis are features. The new dataset,\nwith 60k new human-written ODD turns and 5k re-written TOD turns, offers a\nbenchmark to test a dialogue model's ability to perform inter-mode\nconversations. This is a more challenging task since the model has to determine\nthe appropriate dialogue mode and generate the response based on the inter-mode\ncontext. But such models would better mimic human-level conversation\ncapabilities. We evaluate baseline models on this task, including\n\\textit{classification-based} two-stage models and \\textit{two-in-one} fused\nmodels. We publicly release FusedChat and the baselines to propel future work\non inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.",
          "link": "http://arxiv.org/abs/2109.04137",
          "publishedOn": "2021-09-10T07:20:10.798Z",
          "wordCount": 685,
          "title": "Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.",
          "link": "http://arxiv.org/abs/2104.08803",
          "publishedOn": "2021-09-10T07:20:10.762Z",
          "wordCount": 607,
          "title": "Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04452",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Effenberger_A/0/1/0/all/0/1\">Anna Effenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_E/0/1/0/all/0/1\">Eva Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rhia Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1\">Alane Suhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>",
          "description": "We analyze language change over time in a collaborative, goal-oriented\ninstructional task, where utility-maximizing participants form conventions and\nincrease their expertise. Prior work studied such scenarios mostly in the\ncontext of reference games, and consistently found that language complexity is\nreduced along multiple dimensions, such as utterance length, as conventions are\nformed. In contrast, we find that, given the ability to increase instruction\nutility, instructors increase language complexity along these previously\nstudied dimensions to better collaborate with increasingly skilled instruction\nfollowers.",
          "link": "http://arxiv.org/abs/2109.04452",
          "publishedOn": "2021-09-10T07:20:10.685Z",
          "wordCount": 531,
          "title": "Analysis of Language Change in Collaborative Instruction Following. (arXiv:2109.04452v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-09-10T07:20:10.640Z",
          "wordCount": 627,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>",
          "description": "In natural language processing (NLP), the semantic similarity task requires\nlarge-scale, high-quality human-annotated labels for fine-tuning or evaluation.\nBy contrast, in cases of music similarity, such labels are expensive to collect\nand largely dependent on the annotator's artistic preferences. Recent research\nhas demonstrated that embedding calibration technique can greatly increase\nsemantic similarity performance of the pre-trained language model without\nfine-tuning. However, it is yet unknown which calibration method is the best\nand how much performance improvement can be achieved. To address these issues,\nwe propose using composer information to construct labels for automatically\nevaluating music similarity. Under this paradigm, we discover the optimal\ncombination of embedding calibration which achieves superior metrics than the\nbaseline methods.",
          "link": "http://arxiv.org/abs/2103.07656",
          "publishedOn": "2021-09-10T07:20:10.620Z",
          "wordCount": 590,
          "title": "Optimal Embedding Calibration for Symbolic Music Similarity. (arXiv:2103.07656v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>",
          "description": "Representation of linguistic phenomena in computational language models is\ntypically assessed against the predictions of existing linguistic theories of\nthese phenomena. Using the notion of polarity as a case study, we show that\nthis is not always the most adequate set-up. We probe polarity via so-called\n'negative polarity items' (in particular, English 'any') in two pre-trained\nTransformer-based models (BERT and GPT-2). We show that -- at least for\npolarity -- metrics derived from language models are more consistent with data\nfrom psycholinguistic experiments than linguistic theory predictions.\nEstablishing this allows us to more adequately evaluate the performance of\nlanguage models and also to use language models to discover new insights into\nnatural language grammar beyond existing linguistic theories. Overall, our\nresults encourage a closer tie between experiments with human subjects and with\nlanguage models. We propose methods to enable this closer tie, with language\nmodels as part of experimental pipeline, and show this pipeline at work.",
          "link": "http://arxiv.org/abs/2109.03926",
          "publishedOn": "2021-09-10T07:20:10.612Z",
          "wordCount": 596,
          "title": "Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>",
          "description": "Transformers have recently become very popular for sequence-to-sequence\napplications such as machine translation and speech recognition. In this work,\nwe propose a multi-task learning-based transformer model for low-resource\nmultilingual speech recognition for Indian languages. Our proposed model\nconsists of a conformer [1] encoder and two parallel transformer decoders. We\nuse a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme\ndecoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme\nrecognition task as an auxiliary task for our multi-task learning framework. We\njointly optimize the network for both phoneme and grapheme recognition tasks\nusing Joint CTC-Attention [2] training. We use a conditional decoding scheme to\ninject the language information into the model before predicting the grapheme\nsequence. Our experiments show that our proposed approach can obtain\nsignificant improvement over previous approaches [4]. We also show that our\nconformer-based dual-decoder approach outperforms both the transformer-based\ndual-decoder approach and single decoder approach. Finally, We compare\nmonolingual ASR models with our proposed multilingual ASR approach.",
          "link": "http://arxiv.org/abs/2109.03969",
          "publishedOn": "2021-09-10T07:20:10.606Z",
          "wordCount": 625,
          "title": "Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Shahar Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_a/0/1/0/all/0/1\">abriel Stanovsky</a>",
          "description": "Recent works have found evidence of gender bias in models of machine\ntranslation and coreference resolution using mostly synthetic diagnostic\ndatasets. While these quantify bias in a controlled experiment, they often do\nso on a small scale and consist mostly of artificial, out-of-distribution\nsentences. In this work, we find grammatical patterns indicating stereotypical\nand non-stereotypical gender-role assignments (e.g., female nurses versus male\ndancers) in corpora from three domains, resulting in a first large-scale gender\nbias dataset of 108K diverse real-world English sentences. We manually verify\nthe quality of our corpus and use it to evaluate gender bias in various\ncoreference resolution and machine translation models. We find that all tested\nmodels tend to over-rely on gender stereotypes when presented with natural\ninputs, which may be especially harmful when deployed in commercial systems.\nFinally, we show that our dataset lends itself to finetuning a coreference\nresolution model, finding it mitigates bias on a held out set. Our dataset and\nmodels are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will\nspur future research into gender bias evaluation mitigation techniques in\nrealistic settings.",
          "link": "http://arxiv.org/abs/2109.03858",
          "publishedOn": "2021-09-10T07:20:10.598Z",
          "wordCount": 636,
          "title": "Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yankai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Knowledge-based visual question answering (VQA) requires answering questions\nwith external knowledge in addition to the content of images. One dataset that\nis mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold\nstandard knowledge corpus for retrieval. Existing work leverage different\nknowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge.\nBecause of varying knowledge bases, it is hard to fairly compare models'\nperformance. To address this issue, we collect a natural language knowledge\nbase that can be used for any VQA system. Moreover, we propose a Visual\nRetriever-Reader pipeline to approach knowledge-based VQA. The visual retriever\naims to retrieve relevant knowledge, and the visual reader seeks to predict\nanswers based on given knowledge. We introduce various ways to retrieve\nknowledge using text and images and two reader styles: classification and\nextraction. Both the retriever and reader are trained with weak supervision.\nOur experimental results show that a good retriever can significantly improve\nthe reader's performance on the OK-VQA challenge. The code and corpus are\nprovided in https://github.com/luomancs/retriever\\_reader\\_for\\_okvqa.git",
          "link": "http://arxiv.org/abs/2109.04014",
          "publishedOn": "2021-09-10T07:20:10.592Z",
          "wordCount": 615,
          "title": "Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering. (arXiv:2109.04014v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuofu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We investigate the use of multimodal information contained in images as an\neffective method for enhancing the commonsense of Transformer models for text\ngeneration. We perform experiments using BART and T5 on concept-to-text\ngeneration, specifically the task of generative commonsense reasoning, or\nCommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text\nGeneration. VisCTG involves captioning images representing appropriate everyday\nscenarios, and using these captions to enrich and steer the generation process.\nComprehensive evaluation and analysis demonstrate that VisCTG noticeably\nimproves model performance while successfully addressing several issues of the\nbaseline generations, including poor commonsense, fluency, and specificity.",
          "link": "http://arxiv.org/abs/2109.03892",
          "publishedOn": "2021-09-10T07:20:10.571Z",
          "wordCount": 562,
          "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bongseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yong Suk Choi</a>",
          "description": "Dialogue-based relation extraction (RE) aims to extract relation(s) between\ntwo arguments that appear in a dialogue. Because dialogues have the\ncharacteristics of high personal pronoun occurrences and low information\ndensity, and since most relational facts in dialogues are not supported by any\nsingle sentence, dialogue-based relation extraction requires a comprehensive\nunderstanding of dialogue. In this paper, we propose the TUrn COntext awaRE\nGraph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way\npeople understand dialogues. In addition, we propose a novel approach which\ntreats the task of emotion recognition in conversations (ERC) as a\ndialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC\ndatasets demonstrate that our model is very effective in various dialogue-based\nnatural language understanding tasks. In these experiments, TUCORE-GCN\noutperforms the state-of-the-art models on most of the benchmark datasets. Our\ncode is available at https://github.com/BlackNoodle/TUCORE-GCN.",
          "link": "http://arxiv.org/abs/2109.04008",
          "publishedOn": "2021-09-10T07:20:10.564Z",
          "wordCount": 588,
          "title": "Graph Based Network with Contextualized Representations of Turns in Dialogue. (arXiv:2109.04008v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Han He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>",
          "description": "We introduce ELIT, the Emory Language and Information Toolkit, which is a\ncomprehensive NLP framework providing transformer-based end-to-end models for\ncore tasks with a special focus on memory efficiency while maintaining\nstate-of-the-art accuracy and speed. Compared to existing toolkits, ELIT\nfeatures an efficient Multi-Task Learning (MTL) model with many downstream\ntasks that include lemmatization, part-of-speech tagging, named entity\nrecognition, dependency parsing, constituency parsing, semantic role labeling,\nand AMR parsing. The backbone of ELIT's MTL framework is a pre-trained\ntransformer encoder that is shared across tasks to speed up their inference.\nELIT provides pre-trained models developed on a remix of eight datasets. To\nscale up its service, ELIT also integrates a RESTful Client/Server combination.\nOn the server side, ELIT extends its functionality to cover other tasks such as\ntokenization and coreference resolution, providing an end user with agile\nresearch experience. All resources including the source codes, documentation,\nand pre-trained models are publicly available at\nhttps://github.com/emorynlp/elit.",
          "link": "http://arxiv.org/abs/2109.03903",
          "publishedOn": "2021-09-10T07:20:10.556Z",
          "wordCount": 592,
          "title": "ELIT: Emory Language and Information Toolkit. (arXiv:2109.03903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kexuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>",
          "description": "Tables provide valuable knowledge that can be used to verify textual\nstatements. While a number of works have considered table-based fact\nverification, direct alignments of tabular data with tokens in textual\nstatements are rarely available. Moreover, training a generalized fact\nverification model requires abundant labeled training data. In this paper, we\npropose a novel system to address these problems. Inspired by counterfactual\ncausality, our system identifies token-level salience in the statement with\nprobing-based salience estimation. Salience estimation allows enhanced learning\nof fact verification from two perspectives. From one perspective, our system\nconducts masked salient token prediction to enhance the model for alignment and\nreasoning between the table and the statement. From the other perspective, our\nsystem applies salience-aware data augmentation to generate a more diverse set\nof training instances by replacing non-salient terms. Experimental results on\nTabFact show the effective improvement by the proposed salience-aware learning\ntechniques, leading to the new SOTA performance on the benchmark. Our code is\npublicly available at https://github.com/luka-group/Salience-aware-Learning .",
          "link": "http://arxiv.org/abs/2109.04053",
          "publishedOn": "2021-09-10T07:20:10.547Z",
          "wordCount": 616,
          "title": "Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>",
          "description": "Pimentel et al. (2020) recently analysed probing from an\ninformation-theoretic perspective. They argue that probing should be seen as\napproximating a mutual information. This led to the rather unintuitive\nconclusion that representations encode exactly the same information about a\ntarget task as the original sentences. The mutual information, however, assumes\nthe true probability distribution of a pair of random variables is known,\nleading to unintuitive results in settings where it is not. This paper proposes\na new framework to measure what we term Bayesian mutual information, which\nanalyses information from the perspective of Bayesian agents -- allowing for\nmore intuitive findings in scenarios with finite data. For instance, under\nBayesian MI we have that data can add information, processing can help, and\ninformation can hurt, which makes it more intuitive for machine learning\napplications. Finally, we apply our framework to probing where we believe\nBayesian mutual information naturally operationalises ease of extraction by\nexplicitly limiting the available background knowledge to solve a task.",
          "link": "http://arxiv.org/abs/2109.03853",
          "publishedOn": "2021-09-10T07:20:10.529Z",
          "wordCount": 615,
          "title": "A Bayesian Framework for Information-Theoretic Probing. (arXiv:2109.03853v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shaika Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baili_N/0/1/0/all/0/1\">Naouel Baili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vannah_B/0/1/0/all/0/1\">Brian Vannah</a>",
          "description": "Quality Estimation (QE) is an important component of the machine translation\nworkflow as it assesses the quality of the translated output without consulting\nreference translations. In this paper, we discuss our submission to the WMT\n2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that\nchallenge participants to predict the HTER score for sentence-level\npost-editing effort. Our proposed system is an ensemble of multilingual BERT\n(mBERT)-based regression models, which are generated by fine-tuning on\ndifferent input settings. It demonstrates comparable performance with respect\nto the Pearson's correlation and beats the baseline system in MAE/ RMSE for\nseveral language pairs. In addition, we adapt our system for the zero-shot\nsetting by exploiting target language-relevant language pairs and\npseudo-reference translations.",
          "link": "http://arxiv.org/abs/2109.03914",
          "publishedOn": "2021-09-10T07:20:10.379Z",
          "wordCount": 565,
          "title": "Ensemble Fine-tuned mBERT for Translation Quality Estimation. (arXiv:2109.03914v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>",
          "description": "While recent research on natural language inference has considerably\nbenefited from large annotated datasets, the amount of inference-related\nknowledge (including commonsense) provided in the annotated data is still\nrather limited. There have been two lines of approaches that can be used to\nfurther address the limitation: (1) unsupervised pretraining can leverage\nknowledge in much larger unstructured text data; (2) structured (often\nhuman-curated) knowledge has started to be considered in neural-network-based\nmodels for NLI. An immediate question is whether these two approaches\ncomplement each other, or how to develop models that can bring together their\nadvantages. In this paper, we propose models that leverage structured knowledge\nin different components of pre-trained models. Our results show that the\nproposed models perform better than previous BERT-based state-of-the-art\nmodels. Although our models are proposed for NLI, they can be easily extended\nto other sentence or sentence-pair classification problems.",
          "link": "http://arxiv.org/abs/2109.03941",
          "publishedOn": "2021-09-10T07:20:10.293Z",
          "wordCount": 596,
          "title": "Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yilong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "We study Comparative Preference Classification (CPC) which aims at predicting\nwhether a preference comparison exists between two entities in a given sentence\nand, if so, which entity is preferred over the other. High-quality CPC models\ncan significantly benefit applications such as comparative question answering\nand review-based recommendations. Among the existing approaches, non-deep\nlearning methods suffer from inferior performances. The state-of-the-art graph\nneural network-based ED-GAT (Ma et al., 2020) only considers syntactic\ninformation while ignoring the critical semantic relations and the sentiments\nto the compared entities. We proposed sentiment Analysis Enhanced COmparative\nNetwork (SAECON) which improves CPC ac-curacy with a sentiment analyzer that\nlearns sentiments to individual entities via domain adaptive knowledge\ntransfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset\npresent a significant improvement on the F1 scores over the best existing CPC\napproaches.",
          "link": "http://arxiv.org/abs/2109.03819",
          "publishedOn": "2021-09-10T07:20:10.262Z",
          "wordCount": 591,
          "title": "Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>",
          "description": "In this paper, we leverage large language models (LMs) to perform zero-shot\ntext style transfer. We present a prompting method that we call augmented\nzero-shot learning, which frames style transfer as a sentence rewriting task\nand requires only a natural language instruction, without model fine-tuning or\nexemplars in the target style. Augmented zero-shot learning is simple and\ndemonstrates promising results not just on standard style transfer tasks such\nas sentiment, but also on arbitrary transformations such as \"make this\nmelodramatic\" or \"insert a metaphor.\"",
          "link": "http://arxiv.org/abs/2109.03910",
          "publishedOn": "2021-09-10T07:20:10.240Z",
          "wordCount": 535,
          "title": "A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>",
          "description": "Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.",
          "link": "http://arxiv.org/abs/2109.03848",
          "publishedOn": "2021-09-10T07:20:10.230Z",
          "wordCount": 656,
          "title": "Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1\">Potsawee Manakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1\">Mark J. F. Gales</a>",
          "description": "Transformer models have achieved state-of-the-art results in a wide range of\nNLP tasks including summarization. Training and inference using large\ntransformer models can be computationally expensive. Previous work has focused\non one important bottleneck, the quadratic self-attention mechanism in the\nencoder. Modified encoder architectures such as LED or LoBART use local\nattention patterns to address this problem for summarization. In contrast, this\nwork focuses on the transformer's encoder-decoder attention mechanism. The cost\nof this attention becomes more significant in inference or training approaches\nthat require model-generated histories. First, we examine the complexity of the\nencoder-decoder attention. We demonstrate empirically that there is a sparse\nsentence structure in document summarization that can be exploited by\nconstraining the attention mechanism to a subset of input sentences, whilst\nmaintaining system performance. Second, we propose a modified architecture that\nselects the subset of sentences to constrain the encoder-decoder attention.\nExperiments are carried out on abstractive summarization tasks, including\nCNN/DailyMail, XSum, Spotify Podcast, and arXiv.",
          "link": "http://arxiv.org/abs/2109.03888",
          "publishedOn": "2021-09-10T07:20:10.213Z",
          "wordCount": 609,
          "title": "Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems. (arXiv:2109.03888v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zequn Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shukai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yiyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>",
          "description": "Precisely defining the terminology is the first step in scientific\ncommunication. Developing neural text generation models for definition\ngeneration can circumvent the labor-intensity curation, further accelerating\nscientific discovery. Unfortunately, the lack of large-scale terminology\ndefinition dataset hinders the process toward definition generation. In this\npaper, we present a large-scale terminology definition dataset Graphine\ncovering 2,010,648 terminology definition pairs, spanning 227 biomedical\nsubdisciplines. Terminologies in each subdiscipline further form a directed\nacyclic graph, opening up new avenues for developing graph-aware text\ngeneration models. We then proposed a novel graph-aware definition generation\nmodel Graphex that integrates transformer with graph neural network. Our model\noutperforms existing text generation models by exploiting the graph structure\nof terminologies. We further demonstrated how Graphine can be used to evaluate\npretrained language models, compare graph representation learning methods and\npredict sentence granularity. We envision Graphine to be a unique resource for\ndefinition generation and many other NLP tasks in biomedicine.",
          "link": "http://arxiv.org/abs/2109.04018",
          "publishedOn": "2021-09-10T07:20:10.143Z",
          "wordCount": 601,
          "title": "Graphine: A Dataset for Graph-aware Terminology Definition Generation. (arXiv:2109.04018v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">Daniel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Multilingual neural machine translation (MNMT) learns to translate multiple\nlanguage pairs with a single model, potentially improving both the accuracy and\nthe memory-efficiency of deployed models. However, the heavy data imbalance\nbetween languages hinders the model from performing uniformly across language\npairs. In this paper, we propose a new learning objective for MNMT based on\ndistributionally robust optimization, which minimizes the worst-case expected\nloss over the set of language pairs. We further show how to practically\noptimize this objective for large translation corpora using an iterated best\nresponse scheme, which is both effective and incurs negligible additional\ncomputational cost compared to standard empirical risk minimization. We perform\nextensive experiments on three sets of languages from two datasets and show\nthat our method consistently outperforms strong baseline methods in terms of\naverage and per-language performance under both many-to-one and one-to-many\ntranslation settings.",
          "link": "http://arxiv.org/abs/2109.04020",
          "publishedOn": "2021-09-10T07:20:10.114Z",
          "wordCount": 594,
          "title": "Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>",
          "description": "Sorani Kurdish, also known as Central Kurdish, has a complex morphology,\nparticularly due to the patterns in which morphemes appear. Although several\naspects of Kurdish morphology have been studied, such as pronominal endoclitics\nand Izafa constructions, Sorani Kurdish morphology has received trivial\nattention in computational linguistics. Moreover, some morphemes, such as the\nemphasis endoclitic =\\^i\\c{s}, and derivational morphemes have not been\npreviously studied. To tackle the complex morphology of Sorani, we provide a\nthorough description of Sorani Kurdish morphological and morphophonological\nconstructions in a formal way such that they can be used as finite-state\ntransducers for morphological analysis and synthesis.",
          "link": "http://arxiv.org/abs/2109.03942",
          "publishedOn": "2021-09-10T07:20:10.076Z",
          "wordCount": 555,
          "title": "A Formal Description of Sorani Kurdish Morphology. (arXiv:2109.03942v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "It has been shown for English that discrete and soft prompting perform\nstrongly in few-shot learning with pretrained language models (PLMs). In this\npaper, we show that discrete and soft prompting perform better than finetuning\nin multilingual cases: Crosslingual transfer and in-language training of\nmultilingual natural language inference. For example, with 48 English training\nexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely\nsurpassing the majority baseline (33.33%). In contrast, discrete and soft\nprompting outperform finetuning, achieving 36.43% and 38.79%. We also\ndemonstrate good performance of prompting with training data in multiple\nlanguages other than English.",
          "link": "http://arxiv.org/abs/2109.03630",
          "publishedOn": "2021-09-09T07:20:43.347Z",
          "wordCount": null,
          "title": "Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>",
          "description": "Sarcasm employs ambivalence, where one says something positive but actually\nmeans negative, and vice versa. Due to the sophisticated and obscure sentiment,\nsarcasm brings in great challenges to sentiment analysis. In this paper, we\nshow up the essence of sarcastic text is that the literal sentiment (expressed\nby the surface form of the text) is opposite to the deep sentiment (expressed\nby the actual meaning of the text). To this end, we propose a Dual-Channel\nFramework by modeling both literal and deep sentiments to recognize the\nsentiment conflict. Specifically, the proposed framework is capable of\ndetecting the sentiment conflict between the literal and deep meanings of the\ninput text. Experiments on the political debates and the Twitter datasets show\nthat our framework achieves the best performance on sarcasm recognition.",
          "link": "http://arxiv.org/abs/2109.03587",
          "publishedOn": "2021-09-09T07:20:43.294Z",
          "wordCount": null,
          "title": "A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mytkowicz_T/0/1/0/all/0/1\">Todd Mytkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Negar Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1\">Sarah Fakhoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinella_E/0/1/0/all/0/1\">Elizabeth Dinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu Lahiri</a>",
          "description": "Collaborative software development is an integral part of the modern software\ndevelopment life cycle, essential to the success of large-scale software\nprojects. When multiple developers make concurrent changes around the same\nlines of code, a merge conflict may occur. Such conflicts stall pull requests\nand continuous integration pipelines for hours to several days, seriously\nhurting developer productivity.\n\nIn this paper, we introduce MergeBERT, a novel neural program merge framework\nbased on the token-level three-way differencing and a transformer encoder\nmodel. Exploiting restricted nature of merge conflict resolutions, we\nreformulate the task of generating the resolution sequence as a classification\ntask over a set of primitive merge patterns extracted from real-world merge\ncommit data.\n\nOur model achieves 64--69% precision of merge resolution synthesis, yielding\nnearly a 2x performance improvement over existing structured and neural program\nmerge tools. Finally, we demonstrate versatility of our model, which is able to\nperform program merge in a multilingual setting with Java, JavaScript,\nTypeScript, and C# programming languages, generalizing zero-shot to unseen\nlanguages.",
          "link": "http://arxiv.org/abs/2109.00084",
          "publishedOn": "2021-09-09T07:20:43.286Z",
          "wordCount": null,
          "title": "MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>",
          "description": "Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.",
          "link": "http://arxiv.org/abs/2104.08817",
          "publishedOn": "2021-09-09T07:20:43.279Z",
          "wordCount": null,
          "title": "Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Recent work on multilingual AMR-to-text generation has exclusively focused on\ndata augmentation strategies that utilize silver AMR. However, this assumes a\nhigh quality of generated AMRs, potentially limiting the transferability to the\ntarget task. In this paper, we investigate different techniques for\nautomatically generating AMR annotations, where we aim to study which source of\ninformation yields better multilingual results. Our models trained on gold AMR\nwith silver (machine translated) sentences outperform approaches which leverage\ngenerated silver AMR. We find that combining both complementary sources of\ninformation further improves multilingual AMR-to-text generation. Our models\nsurpass the previous state of the art for German, Italian, Spanish, and Chinese\nby a large margin.",
          "link": "http://arxiv.org/abs/2109.03808",
          "publishedOn": "2021-09-09T07:20:43.221Z",
          "wordCount": null,
          "title": "Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.",
          "link": "http://arxiv.org/abs/2104.08663",
          "publishedOn": "2021-09-09T07:20:43.211Z",
          "wordCount": null,
          "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>",
          "description": "Policy compliance detection is the task of ensuring that a scenario conforms\nto a policy (e.g. a claim is valid according to government rules or a post in\nan online platform conforms to community guidelines). This task has been\npreviously instantiated as a form of textual entailment, which results in poor\naccuracy due to the complexity of the policies. In this paper we propose to\naddress policy compliance detection via decomposing it into question answering,\nwhere questions check whether the conditions stated in the policy apply to the\nscenario, and an expression tree combines the answers to obtain the label.\nDespite the initial upfront annotation cost, we demonstrate that this approach\nresults in better accuracy, especially in the cross-policy setup where the\npolicies during testing are unseen in training. In addition, it allows us to\nuse existing question answering models pre-trained on existing large datasets.\nFinally, it explicitly identifies the information missing from a scenario in\ncase policy compliance cannot be determined. We conduct our experiments using a\nrecent dataset consisting of government policies, which we augment with expert\nannotations and find that the cost of annotating question answering\ndecomposition is largely offset by improved inter-annotator agreement and\nspeed.",
          "link": "http://arxiv.org/abs/2109.03731",
          "publishedOn": "2021-09-09T07:20:43.202Z",
          "wordCount": null,
          "title": "Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2106.02902",
          "publishedOn": "2021-09-09T07:20:43.197Z",
          "wordCount": null,
          "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>",
          "description": "A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.",
          "link": "http://arxiv.org/abs/2109.02377",
          "publishedOn": "2021-09-09T07:20:43.150Z",
          "wordCount": null,
          "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Pre-training language models (LMs) on large-scale unlabeled text data makes\nthe model much easier to achieve exceptional downstream performance than their\ncounterparts directly trained on the downstream tasks. In this work, we study\nwhat specific traits in the pre-training data, other than the semantics, make a\npre-trained LM superior to their counterparts trained from scratch on\ndownstream tasks. We propose to use artificially constructed datasets as the\npre-training data to exclude the effect of semantics, and further control what\ncharacteristics the pre-training corpora have. By fine-tuning the pre-trained\nmodels on GLUE benchmark, we can learn how beneficial it is to transfer the\nknowledge from the model trained on the dataset possessing that specific trait.\nWe define and discuss three different characteristics in the artificial\ndataset: 1) matching the token's uni-gram or bi-gram distribution between\npre-training and downstream fine-tuning, 2) the presence of the explicit\ndependencies among the tokens in a sequence, 3) the length of the implicit\ndependencies among the tokens in a sequence. Our experiments show that the\nexplicit dependencies in the sequences of the pre-training data are critical to\nthe downstream performance. Our results also reveal that models achieve better\ndownstream performance when pre-trained on a dataset with a longer range of\nimplicit dependencies. Based on our analysis, we find that models pre-trained\nwith artificial datasets are prone to learn spurious correlation in downstream\ntasks. Our work reveals that even if the LMs are not pre-trained on natural\nlanguage, they still gain transferability on certain human language downstream\ntasks once the LMs learn to model the token dependencies in the sequences. This\nresult helps us understand the exceptional transferability of pre-trained LMs.",
          "link": "http://arxiv.org/abs/2109.03537",
          "publishedOn": "2021-09-09T07:20:43.141Z",
          "wordCount": null,
          "title": "On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seoh_R/0/1/0/all/0/1\">Ronald Seoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birle_I/0/1/0/all/0/1\">Ian Birle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tak_M/0/1/0/all/0/1\">Mrinal Tak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinette_B/0/1/0/all/0/1\">Brian Pinette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_A/0/1/0/all/0/1\">Alfred Hough</a>",
          "description": "For many business applications, we often seek to analyze sentiments\nassociated with any arbitrary aspects of commercial products, despite having a\nvery limited amount of labels or even without any labels at all. However,\nexisting aspect target sentiment classification (ATSC) models are not trainable\nif annotated datasets are not available. Even with labeled data, they fall\nshort of reaching satisfactory performance. To address this, we propose simple\napproaches that better solve ATSC with natural language prompts, enabling the\ntask under zero-shot cases and enhancing supervised settings, especially for\nfew-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop\ndomain, our method of reformulating ATSC as an NLI task outperforms supervised\nSOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.\nMoreover, we demonstrate that our prompts could handle implicitly stated\naspects as well: our models reach about 77% accuracy on detecting sentiments\nfor aspect categories (e.g., food), which do not necessarily appear within the\ntext, even though we trained the models only with explicitly mentioned aspect\nterms (e.g., fajitas) from just 16 reviews - while the accuracy of the\nno-prompt baseline is only around 65%.",
          "link": "http://arxiv.org/abs/2109.03685",
          "publishedOn": "2021-09-09T07:20:43.113Z",
          "wordCount": null,
          "title": "Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrena_A/0/1/0/all/0/1\">Ander Barrena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>",
          "description": "Relation extraction systems require large amounts of labeled examples which\nare costly to annotate. In this work we reformulate relation extraction as an\nentailment task, with simple, hand-made, verbalizations of relations produced\nin less than 15 min per relation. The system relies on a pretrained textual\nentailment engine which is run as-is (no training examples, zero-shot) or\nfurther fine-tuned on labeled examples (few-shot or fully trained). In our\nexperiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per\nrelation (17% points better than the best supervised system on the same\nconditions), and only 4 points short to the state-of-the-art (which uses 20\ntimes more training data). We also show that the performance can be improved\nsignificantly with larger entailment models, up to 12 points in zero-shot,\nallowing to report the best results to date on TACRED when fully trained. The\nanalysis shows that our few-shot systems are specially effective when\ndiscriminating between relations, and that the performance difference in low\ndata regimes comes mainly from identifying no-relation cases.",
          "link": "http://arxiv.org/abs/2109.03659",
          "publishedOn": "2021-09-09T07:20:43.110Z",
          "wordCount": null,
          "title": "Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Common acquisition functions for active learning use either uncertainty or\ndiversity sampling, aiming to select difficult and diverse data points from the\npool of unlabeled data, respectively. In this work, leveraging the best of both\nworlds, we propose an acquisition function that opts for selecting\n\\textit{contrastive examples}, i.e. data points that are similar in the model\nfeature space and yet the model outputs maximally different predictive\nlikelihoods. We compare our approach, CAL (Contrastive Active Learning), with a\ndiverse set of acquisition functions in four natural language understanding\ntasks and seven datasets. Our experiments show that CAL performs consistently\nbetter or equal than the best performing baseline across all tasks, on both\nin-domain and out-of-domain data. We also conduct an extensive ablation study\nof our method and we further analyze all actively acquired datasets showing\nthat CAL achieves a better trade-off between uncertainty and diversity compared\nto other strategies.",
          "link": "http://arxiv.org/abs/2109.03764",
          "publishedOn": "2021-09-09T07:20:43.103Z",
          "wordCount": null,
          "title": "Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>",
          "description": "All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.",
          "link": "http://arxiv.org/abs/2109.03300",
          "publishedOn": "2021-09-09T07:20:43.102Z",
          "wordCount": null,
          "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "Combining a pretrained language model (PLM) with textual patterns has been\nshown to help in both zero- and few-shot settings. For zero-shot performance,\nit makes sense to design patterns that closely resemble the text seen during\nself-supervised pretraining because the model has never seen anything else.\nSupervised training allows for more flexibility. If we allow for tokens outside\nthe PLM's vocabulary, patterns can be adapted more flexibly to a PLM's\nidiosyncrasies. Contrasting patterns where a \"token\" can be any continuous\nvector vs. those where a discrete choice between vocabulary elements has to be\nmade, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two\nestablished benchmarks for lexical inference in context (LIiC) a.k.a. predicate\nentailment, a challenging natural language understanding task with relatively\nsmall training sets. In a direct comparison with discrete patterns, CONAN\nconsistently leads to improved performance, setting a new state of the art. Our\nexperiments give valuable insights into the kind of pattern that enhances a\nPLM's performance on LIiC and raise important questions regarding our\nunderstanding of PLMs using text patterns.",
          "link": "http://arxiv.org/abs/2109.03695",
          "publishedOn": "2021-09-09T07:20:42.755Z",
          "wordCount": null,
          "title": "Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>",
          "description": "In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. In order to foster the research in the field of ODQA on such\nhistorical collections, we present ArchivalQA, a large question answering\ndataset consisting of 1,067,056 question-answer pairs which is designed for\ntemporal news QA. In addition, we create four subparts of our dataset based on\nthe question difficulty levels and the containment of temporal expressions,\nwhich we believe could be useful for training or testing ODQA systems\ncharacterized by different strengths and abilities. The novel QA\ndataset-constructing framework that we introduce can be also applied to create\ndatasets over other types of collections.",
          "link": "http://arxiv.org/abs/2109.03438",
          "publishedOn": "2021-09-09T07:20:42.753Z",
          "wordCount": null,
          "title": "ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.",
          "link": "http://arxiv.org/abs/2109.03481",
          "publishedOn": "2021-09-09T07:20:42.682Z",
          "wordCount": null,
          "title": "Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.",
          "link": "http://arxiv.org/abs/2108.12229",
          "publishedOn": "2021-09-09T07:20:42.631Z",
          "wordCount": null,
          "title": "ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "Traditional event extraction methods require predefined event types and their\ncorresponding annotations to learn event extractors. These prerequisites are\noften hard to be satisfied in real-world applications. This work presents a\ncorpus-based open-domain event type induction method that automatically\ndiscovers a set of event types from a given corpus. As events of the same type\ncould be expressed in multiple ways, we propose to represent each event type as\na cluster of <predicate sense, object head> pairs. Specifically, our method (1)\nselects salient predicates and object heads, (2) disambiguates predicate senses\nusing only a verb sense dictionary, and (3) obtains event types by jointly\nembedding and clustering <predicate sense, object head> pairs in a latent\nspherical space. Our experiments, on three datasets from different domains,\nshow our method can discover salient and high-quality event types, according to\nboth automatic and human evaluations.",
          "link": "http://arxiv.org/abs/2109.03322",
          "publishedOn": "2021-09-09T07:20:42.630Z",
          "wordCount": null,
          "title": "Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank\ntwice, reaD twice). The pipeline is composed of a retriever, passage reranker,\nextractive reader, generative reader and a mechanism that aggregates the final\nprediction from all system's components. We demonstrate its strength across\nthree open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,\nsurpassing state-of-the-art on the first two. Our analysis demonstrates that:\n(i) combining extractive and generative reader yields absolute improvements up\nto 5 exact match and it is at least twice as effective as the posterior\naveraging ensemble of the same models with different parameters, (ii) the\nextractive reader with fewer parameters can match the performance of the\ngenerative reader on extractive QA datasets.",
          "link": "http://arxiv.org/abs/2109.03502",
          "publishedOn": "2021-09-09T07:20:42.596Z",
          "wordCount": null,
          "title": "R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.",
          "link": "http://arxiv.org/abs/2104.01027",
          "publishedOn": "2021-09-09T07:20:41.642Z",
          "wordCount": null,
          "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.",
          "link": "http://arxiv.org/abs/2010.09313",
          "publishedOn": "2021-09-09T07:20:41.550Z",
          "wordCount": null,
          "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>",
          "description": "As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.",
          "link": "http://arxiv.org/abs/2004.13805",
          "publishedOn": "2021-09-09T07:20:41.312Z",
          "wordCount": 655,
          "title": "Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>",
          "description": "Measuring event salience is essential in the understanding of stories. This\npaper takes a recent unsupervised method for salience detection derived from\nBarthes Cardinal Functions and theories of surprise and applies it to longer\nnarrative forms. We improve the standard transformer language model by\nincorporating an external knowledgebase (derived from Retrieval Augmented\nGeneration) and adding a memory mechanism to enhance performance on longer\nworks. We use a novel approach to derive salience annotation using\nchapter-aligned summaries from the Shmoop corpus for classic literary works.\nOur evaluation against this data demonstrates that our salience detection model\nimproves performance over and above a non-knowledgebase and memory augmented\nlanguage model, both of which are crucial to this improvement.",
          "link": "http://arxiv.org/abs/2109.03754",
          "publishedOn": "2021-09-09T07:20:41.282Z",
          "wordCount": 590,
          "title": "Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>",
          "description": "Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.",
          "link": "http://arxiv.org/abs/2109.02401",
          "publishedOn": "2021-09-09T07:20:41.268Z",
          "wordCount": 658,
          "title": "Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>",
          "description": "The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run without a doubt\nin $O(n)$ time.",
          "link": "http://arxiv.org/abs/2102.03277",
          "publishedOn": "2021-09-09T07:20:41.260Z",
          "wordCount": 726,
          "title": "Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1\">Shardul Suryawanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcan_M/0/1/0/all/0/1\">Mihael Arcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Little_S/0/1/0/all/0/1\">Suzanne Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1\">Paul Buitelaar</a>",
          "description": "Research into the classification of Image with Text (IWT) troll memes has\nrecently become popular. Since the online community utilizes the refuge of\nmemes to express themselves, there is an abundance of data in the form of\nmemes. These memes have the potential to demean, harras, or bully targeted\nindividuals. Moreover, the targeted individual could fall prey to opinion\nmanipulation. To comprehend the use of memes in opinion manipulation, we define\nthree specific domains (product, political or others) which we classify into\ntroll or not-troll, with or without opinion manipulation. To enable this\nanalysis, we enhanced an existing dataset by annotating the data with our\ndefined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the\nEnglish language (TrollsWithOpinion dataset). We perform baseline experiments\non the annotated dataset, and our result shows that existing state-of-the-art\ntechniques could only reach a weighted-average F1-score of 0.37. This shows the\nneed for a development of a specific technique to deal with multimodal troll\nmemes.",
          "link": "http://arxiv.org/abs/2109.03571",
          "publishedOn": "2021-09-09T07:20:41.237Z",
          "wordCount": 628,
          "title": "TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.",
          "link": "http://arxiv.org/abs/2108.13990",
          "publishedOn": "2021-09-09T07:20:41.229Z",
          "wordCount": 626,
          "title": "Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>",
          "description": "Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.",
          "link": "http://arxiv.org/abs/2109.03772",
          "publishedOn": "2021-09-09T07:20:41.188Z",
          "wordCount": 565,
          "title": "Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>",
          "description": "We introduce SelfExplain, a novel self-explaining model that explains a text\nclassifier's predictions using phrase-based concepts. SelfExplain augments\nexisting neural classifiers by adding (1) a globally interpretable layer that\nidentifies the most influential concepts in the training set for a given sample\nand (2) a locally interpretable layer that quantifies the contribution of each\nlocal input concept by computing a relevance score relative to the predicted\nlabel. Experiments across five text-classification datasets show that\nSelfExplain facilitates interpretability without sacrificing performance. Most\nimportantly, explanations from SelfExplain show sufficiency for model\npredictions and are perceived as adequate, trustworthy and understandable by\nhuman judges compared to existing widely-used baselines.",
          "link": "http://arxiv.org/abs/2103.12279",
          "publishedOn": "2021-09-09T07:20:41.181Z",
          "wordCount": 581,
          "title": "SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "Generative approaches have been recently shown to be effective for both\nEntity Disambiguation and Entity Linking (i.e., joint mention detection and\ndisambiguation). However, the previously proposed autoregressive formulation\nfor EL suffers from i) high computational cost due to a complex (deep) decoder,\nii) non-parallelizable decoding that scales with the source sequence length,\nand iii) the need for training on a large amount of data. In this work, we\npropose a very efficient approach that parallelizes autoregressive linking\nacross all potential mentions and relies on a shallow and efficient decoder.\nMoreover, we augment the generative objective with an extra discriminative\ncomponent, i.e., a correction term which lets us directly optimize the\ngenerator's ranking. When taken together, these techniques tackle all the above\nissues: our model is >70 times faster and more accurate than the previous\ngenerative method, outperforming state-of-the-art approaches on the standard\nEnglish dataset AIDA-CoNLL. Source code available at\nhttps://github.com/nicola-decao/efficient-autoregressive-EL",
          "link": "http://arxiv.org/abs/2109.03792",
          "publishedOn": "2021-09-09T07:20:41.156Z",
          "wordCount": 635,
          "title": "Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>",
          "description": "Pretrained Transformers achieve remarkable performance when training and test\ndata are from the same distribution. However, in real-world scenarios, the\nmodel often faces out-of-distribution (OOD) instances that can cause severe\nsemantic shift problems at inference time. Therefore, in practice, a reliable\nmodel should identify such instances, and then either reject them during\ninference or pass them over to models that handle another distribution. In this\npaper, we develop an unsupervised OOD detection method, in which only the\nin-distribution (ID) data are used in training. We propose to fine-tune the\nTransformers with a contrastive loss, which improves the compactness of\nrepresentations, such that OOD instances can be better differentiated from ID\nones. These OOD instances can then be accurately detected using the Mahalanobis\ndistance in the model's penultimate layer. We experiment with comprehensive\nsettings and achieve near-perfect OOD detection performance, outperforming\nbaselines drastically. We further investigate the rationales behind the\nimprovement, finding that more compact representations through margin-based\ncontrastive learning bring the improvement. We release our code to the\ncommunity for future research.",
          "link": "http://arxiv.org/abs/2104.08812",
          "publishedOn": "2021-09-09T07:20:41.106Z",
          "wordCount": 650,
          "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">Saurabh Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>",
          "description": "The widespread presence of offensive language on social media motivated the\ndevelopment of systems capable of recognizing such content automatically. Apart\nfrom a few notable exceptions, most research on automatic offensive language\nidentification has dealt with English. To address this shortcoming, we\nintroduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first\ndataset of its kind compiled for Marathi, thus opening a new domain for\nresearch in low-resource Indo-Aryan languages. We present results from several\nmachine learning experiments on this dataset, including zero-short and other\ntransfer learning experiments on state-of-the-art cross-lingual transformers\nfrom existing data in Bengali, English, and Hindi.",
          "link": "http://arxiv.org/abs/2109.03552",
          "publishedOn": "2021-09-09T07:20:41.097Z",
          "wordCount": 581,
          "title": "Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>",
          "description": "Automatically translating images to texts involves image scene understanding\nand language modeling. In this paper, we propose a novel model, termed\nRefineCap, that refines the output vocabulary of the language decoder using\ndecoder-guided visual semantics, and implicitly learns the mapping between\nvisual tag words and images. The proposed Visual-Concept Refinement method can\nallow the generator to attend to semantic details in the image, thereby\ngenerating more semantically descriptive captions. Our model achieves superior\nperformance on the MS-COCO dataset in comparison with previous visual-concept\nbased models.",
          "link": "http://arxiv.org/abs/2109.03529",
          "publishedOn": "2021-09-09T07:20:41.075Z",
          "wordCount": 531,
          "title": "RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_M/0/1/0/all/0/1\">Ming-Chi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shu-Wei Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\nspeech enhancement, a task that aims to improve the quality of the artificial\nvoice from an electrolarynx device. In frame-based VC methods, time alignment\nneeds to be performed prior to model training, and the dynamic time warping\n(DTW) algorithm is widely adopted to compute the best time alignment between\neach utterance pair. The validity is based on the assumption that the same\nphonemes of the speakers have similar features and can be mapped by measuring a\npre-defined distance between speech frames of the source and the target.\nHowever, the special characteristics of the EL speech can break the assumption,\nresulting in a sub-optimal DTW alignment. In this work, we propose to use lip\nimages for time alignment, as we assume that the lip movements of laryngectomee\nremain normal compared to healthy people. We investigate two naive lip\nrepresentations and distance metrics, and experimental results demonstrate that\nthe proposed method can significantly outperform the audio-only alignment in\nterms of objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2109.03551",
          "publishedOn": "2021-09-09T07:20:41.054Z",
          "wordCount": 650,
          "title": "Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luken_T/0/1/0/all/0/1\">Tobias L&#xfc;ken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>",
          "description": "Unfair stereotypical biases (e.g., gender, racial, or religious biases)\nencoded in modern pretrained language models (PLMs) have negative ethical\nimplications for widespread adoption of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing techniques have recently been\nintroduced to remove such stereotypical biases from PLMs. Existing debiasing\nmethods, however, directly modify all of the PLMs parameters, which -- besides\nbeing computationally expensive -- comes with the inherent risk of\n(catastrophic) forgetting of useful language knowledge acquired in pretraining.\nIn this work, we propose a more sustainable modular debiasing approach based on\ndedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter\nmodules into the original PLM layers and (2) update only the adapters (i.e., we\nkeep the original PLM parameters frozen) via language modeling training on a\ncounterfactually augmented corpus. We showcase ADELE, in gender debiasing of\nBERT: our extensive evaluation, encompassing three intrinsic and two extrinsic\nbias measures, renders ADELE, very effective in bias mitigation. We further\nshow that -- due to its modular nature -- ADELE, coupled with task adapters,\nretains fairness even after large-scale downstream training. Finally, by means\nof multilingual BERT, we successfully transfer ADELE, to six target languages.",
          "link": "http://arxiv.org/abs/2109.03646",
          "publishedOn": "2021-09-09T07:20:41.040Z",
          "wordCount": 640,
          "title": "Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Pretrained language models (PLM) have recently advanced graph-to-text\ngeneration, where the input graph is linearized into a sequence and fed into\nthe PLM to obtain its representation. However, efficiently encoding the graph\nstructure in PLMs is challenging because such models were pretrained on natural\nlanguage, and modeling structured data may lead to catastrophic forgetting of\ndistributional knowledge. In this paper, we propose StructAdapt, an adapter\nmethod to encode graph structure into PLMs. Contrary to prior work, StructAdapt\neffectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we incorporate task-specific knowledge while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using StructAdapt, outperforming the state of the art\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.",
          "link": "http://arxiv.org/abs/2103.09120",
          "publishedOn": "2021-09-09T07:20:41.010Z",
          "wordCount": 627,
          "title": "Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>",
          "description": "Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms most graph-based\nmodels. We further fine-tune DistilBERT for comparison and find that it\noutperforms all state-of-the-art models. We suggest that future studies use at\nleast an MLP baseline to contextualize the results. We provide recommendations\nfor the design and training of such a baseline.",
          "link": "http://arxiv.org/abs/2109.03777",
          "publishedOn": "2021-09-09T07:20:40.998Z",
          "wordCount": 567,
          "title": "Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "Diverse machine translation aims at generating various target language\ntranslations for a given source language sentence. Leveraging the linear\nrelationship in the sentence latent space introduced by the mixup training, we\npropose a novel method, MixDiversity, to generate different translations for\nthe input sentence by linearly interpolating it with different sentence pairs\nsampled from the training corpus when decoding. To further improve the\nfaithfulness and diversity of the translations, we propose two simple but\neffective approaches to select diverse sentence pairs in the training corpus\nand adjust the interpolation weight for each pair correspondingly. Moreover, by\ncontrolling the interpolation weight, our method can achieve the trade-off\nbetween faithfulness and diversity without any additional training, which is\nrequired in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14\nen-de, and WMT'17 zh-en are conducted to show that our method substantially\noutperforms all previous diverse machine translation methods.",
          "link": "http://arxiv.org/abs/2109.03402",
          "publishedOn": "2021-09-09T07:20:40.976Z",
          "wordCount": 599,
          "title": "Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.06381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>",
          "description": "This paper presents a new technique for creating monolingual and\ncross-lingual meta-embeddings. Our method integrates multiple word embeddings\ncreated from complementary techniques, textual sources, knowledge bases and\nlanguages. Existing word vectors are projected to a common semantic space using\nlinear transformations and averaging. With our method the resulting\nmeta-embeddings maintain the dimensionality of the original embeddings without\nlosing information while dealing with the out-of-vocabulary problem. An\nextensive empirical evaluation demonstrates the effectiveness of our technique\nwith respect to previous work on various intrinsic and extrinsic multilingual\nevaluations, obtaining competitive results for Semantic Textual Similarity and\nstate-of-the-art performance for word similarity and POS tagging (English and\nSpanish). The resulting cross-lingual meta-embeddings also exhibit excellent\ncross-lingual transfer learning capabilities. In other words, we can leverage\npre-trained source embeddings from a resource-rich language in order to improve\nthe word representations for under-resourced languages.",
          "link": "http://arxiv.org/abs/2001.06381",
          "publishedOn": "2021-09-09T07:20:40.779Z",
          "wordCount": 612,
          "title": "A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>",
          "description": "Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.",
          "link": "http://arxiv.org/abs/2104.04670",
          "publishedOn": "2021-09-09T07:20:40.772Z",
          "wordCount": 715,
          "title": "Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03439",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>",
          "description": "Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at\ntransferring a speaking style to the synthesised speech in a target speaker's\nvoice. Most previous CSST approaches rely on expensive high-quality data\ncarrying desired speaking style during training and require a reference\nutterance to obtain speaking style descriptors as conditioning on the\ngeneration of a new sentence. This work presents Referee, a robust\nreference-free CSST approach for expressive TTS, which fully leverages\nlow-quality data to learn speaking styles from text. Referee is built by\ncascading a text-to-style (T2S) model with a style-to-wave (S2W) model.\nPhonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are\nadopted as fine-grained speaking style descriptors, which are predicted from\ntext using the T2S model. A novel pretrain-refinement method is adopted to\nlearn a robust T2S model by only using readily accessible low-quality data. The\nS2W model is trained with high-quality target data, which is adopted to\neffectively aggregate style descriptors and generate high-fidelity speech in\nthe target speaker's voice. Experimental results are presented, showing that\nReferee outperforms a global-style-token (GST)-based baseline approach in CSST.",
          "link": "http://arxiv.org/abs/2109.03439",
          "publishedOn": "2021-09-09T07:20:40.764Z",
          "wordCount": 651,
          "title": "Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>",
          "description": "Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in\npractical cases where order information of text blocks is not available.\nFurther experiments with a varying number of training examples demonstrate the\nhigh training efficiency of our approach. Our code will be open to the public.",
          "link": "http://arxiv.org/abs/2108.04539",
          "publishedOn": "2021-09-09T07:20:40.745Z",
          "wordCount": 745,
          "title": "BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>",
          "description": "Transformer-based models have recently become very popular for\nsequence-to-sequence applications such as machine translation and speech\nrecognition. This work proposes a dual-decoder transformer model for\nlow-resource multilingual speech recognition for Indian languages. Our proposed\nmodel consists of a Conformer [1] encoder, two parallel transformer decoders,\nand a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme\nrecognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence\nalong with language information. We consider phoneme recognition and language\nidentification as auxiliary tasks in the multi-task learning framework. We\njointly optimize the network for phoneme recognition, grapheme recognition, and\nlanguage identification tasks with Joint CTC-Attention [2] training. Our\nexperiments show that we can obtain a significant reduction in WER over the\nbaseline approaches. We also show that our dual-decoder approach obtains\nsignificant improvement over the single decoder approach.",
          "link": "http://arxiv.org/abs/2109.03277",
          "publishedOn": "2021-09-09T07:20:40.737Z",
          "wordCount": 590,
          "title": "A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuncheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>",
          "description": "Formal query building is an important part of complex question answering over\nknowledge bases. It aims to build correct executable queries for questions.\nRecent methods try to rank candidate queries generated by a state-transition\nstrategy. However, this candidate generation strategy ignores the structure of\nqueries, resulting in a considerable number of noisy queries. In this paper, we\npropose a new formal query building approach that consists of two stages. In\nthe first stage, we predict the query structure of the question and leverage\nthe structure to constrain the generation of the candidate queries. We propose\na novel graph generation framework to handle the structure prediction task and\ndesign an encoder-decoder model to predict the argument of the predetermined\noperation in each generative step. In the second stage, we follow the previous\nmethods to rank the candidate queries. The experimental results show that our\nformal query building approach outperforms existing methods on complex\nquestions while staying competitive on simple questions.",
          "link": "http://arxiv.org/abs/2109.03614",
          "publishedOn": "2021-09-09T07:20:40.730Z",
          "wordCount": 623,
          "title": "Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Cartagena_V/0/1/0/all/0/1\">V&#xed;ctor M. S&#xe1;nchez-Cartagena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espla_Gomis_M/0/1/0/all/0/1\">Miquel Espl&#xe0;-Gomis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_J/0/1/0/all/0/1\">Juan Antonio P&#xe9;rez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Martinez_F/0/1/0/all/0/1\">Felipe S&#xe1;nchez-Mart&#xed;nez</a>",
          "description": "In the context of neural machine translation, data augmentation (DA)\ntechniques may be used for generating additional training samples when the\navailable parallel data are scarce. Many DA approaches aim at expanding the\nsupport of the empirical data distribution by generating new sentence pairs\nthat contain infrequent words, thus making it closer to the true data\ndistribution of parallel sentences. In this paper, we propose to follow a\ncompletely different approach and present a multi-task DA approach in which we\ngenerate new sentence pairs with transformations, such as reversing the order\nof the target sentence, which produce unfluent target sentences. During\ntraining, these augmented sentences are used as auxiliary tasks in a multi-task\nframework with the aim of providing new contexts where the target prefix is not\ninformative enough to predict the next word. This strengthens the encoder and\nforces the decoder to pay more attention to the source representations of the\nencoder. Experiments carried out on six low-resource translation tasks show\nconsistent improvements over the baseline and over DA methods aiming at\nextending the support of the empirical data distribution. The systems trained\nwith our approach rely more on the source tokens, are more robust against\ndomain shift and suffer less hallucinations.",
          "link": "http://arxiv.org/abs/2109.03645",
          "publishedOn": "2021-09-09T07:20:40.722Z",
          "wordCount": 670,
          "title": "Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>",
          "description": "This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our models will be made\nfreely available after publication.",
          "link": "http://arxiv.org/abs/2109.03570",
          "publishedOn": "2021-09-09T07:20:40.713Z",
          "wordCount": 660,
          "title": "Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>",
          "description": "Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.",
          "link": "http://arxiv.org/abs/2108.13134",
          "publishedOn": "2021-09-09T07:20:40.691Z",
          "wordCount": 662,
          "title": "Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>",
          "description": "Language use differs between domains and even within a domain, language use\nchanges over time. For pre-trained language models like BERT, domain adaptation\nthrough continued pre-training has been shown to improve performance on\nin-domain downstream tasks. In this article, we investigate whether temporal\nadaptation can bring additional benefits. For this purpose, we introduce a\ncorpus of social media comments sampled over three years. It contains\nunlabelled data for adaptation and evaluation on an upstream masked language\nmodelling task as well as labelled data for fine-tuning and evaluation on a\ndownstream document classification task. We find that temporality matters for\nboth tasks: temporal adaptation improves upstream and temporal fine-tuning\ndownstream task performance. Time-specific models generally perform better on\npast than on future test sets, which matches evidence on the bursty usage of\ntopical words. However, adapting BERT to time and domain does not improve\nperformance on the downstream task over only adapting to domain. Token-level\nanalysis shows that temporal adaptation captures event-driven changes in\nlanguage use in the downstream task, but not those changes that are actually\nrelevant to task performance. Based on our findings, we discuss when temporal\nadaptation may be more effective.",
          "link": "http://arxiv.org/abs/2104.08116",
          "publishedOn": "2021-09-09T07:20:40.659Z",
          "wordCount": 680,
          "title": "Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>",
          "description": "Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.",
          "link": "http://arxiv.org/abs/2104.08656",
          "publishedOn": "2021-09-09T07:20:40.651Z",
          "wordCount": 628,
          "title": "Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03383",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landes_P/0/1/0/all/0/1\">Paul Landes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eugenio_B/0/1/0/all/0/1\">Barbara Di Eugenio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>",
          "description": "Reproducing results in publications by distributing publicly available source\ncode is becoming ever more popular. Given the difficulty of reproducing machine\nlearning (ML) experiments, there have been significant efforts in reducing the\nvariance of these results. As in any science, the ability to consistently\nreproduce results effectively strengthens the underlying hypothesis of the\nwork, and thus, should be regarded as important as the novel aspect of the\nresearch itself. The contribution of this work is a framework that is able to\nreproduce consistent results and provides a means of easily creating, training,\nand evaluating natural language processing (NLP) deep learning (DL) models.",
          "link": "http://arxiv.org/abs/2109.03383",
          "publishedOn": "2021-09-09T07:20:40.643Z",
          "wordCount": 547,
          "title": "DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01163",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>",
          "description": "The recently proposed Conformer architecture has shown state-of-the-art\nperformances in Automatic Speech Recognition by combining convolution with\nattention to model both local and global dependencies. In this paper, we study\nhow to reduce the Conformer architecture complexity with a limited computing\nbudget, leading to a more efficient architecture design that we call Efficient\nConformer. We introduce progressive downsampling to the Conformer encoder and\npropose a novel attention mechanism named grouped attention, allowing us to\nreduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence\nlength $n$, hidden dimension $d$ and group size parameter $g$. We also\nexperiment the use of strided multi-head self-attention as a global\ndownsampling operation. Our experiments are performed on the LibriSpeech\ndataset with CTC and RNN-Transducer losses. We show that within the same\ncomputing budget, the proposed architecture achieves better performances with\nfaster training and decoding compared to the Conformer. Our 13M parameters CTC\nmodel achieves competitive WERs of 3.6%/9.0% without using a language model and\n2.7%/6.7% with an external n-gram language model on the test-clean/test-other\nsets while being 29% faster than our CTC Conformer baseline at inference and\n36% faster to train.",
          "link": "http://arxiv.org/abs/2109.01163",
          "publishedOn": "2021-09-09T07:20:40.635Z",
          "wordCount": 682,
          "title": "Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>",
          "description": "Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work\naddressing the generative aspects of speech pre-training, which replaces text\nwith discovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.",
          "link": "http://arxiv.org/abs/2109.03264",
          "publishedOn": "2021-09-09T07:20:40.615Z",
          "wordCount": 669,
          "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>",
          "description": "Word embeddings are a powerful natural lan-guage processing technique, but\nthey are ex-tremely difficult to interpret. To enable inter-pretable NLP\nmodels, we create vectors whereeach dimension isinherently interpretable.\nByinherently interpretable, we mean a systemwhere each dimension is associated\nwith somehuman-understandablehintthat can describethe meaning of that\ndimension. In order tocreate more interpretable word embeddings,we transform\npretrained dense word embed-dings into sparse embeddings. These new em-beddings\nare inherently interpretable: each oftheir dimensions is created from and\nrepre-sents a natural language word or specific gram-matical concept. We\nconstruct these embed-dings through sparse coding, where each vec-tor in the\nbasis set is itself a word embedding.Therefore, each dimension of our sparse\nvec-tors corresponds to a natural language word.We also show that models\ntrained using thesesparse embeddings can achieve good perfor-mance and are more\ninterpretable in practice,including through human evaluations.",
          "link": "http://arxiv.org/abs/2004.13847",
          "publishedOn": "2021-09-09T07:20:40.607Z",
          "wordCount": 615,
          "title": "Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Welby_P/0/1/0/all/0/1\">Pauline Welby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinelli_E/0/1/0/all/0/1\">Elsa Spinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burki_A/0/1/0/all/0/1\">Audrey B&#xfc;rki</a>",
          "description": "L1 French participants learned novel L2 English words over two days of\nlearning sessions, with half of the words presented with their orthographic\nforms (Audio-Ortho) and half without (Audio only). One group heard the words\npronounced by a single talker, while another group heard them pronounced by\nmultiple talkers. On the third day, they completed a variety of tasks to\nevaluate their learning. Our results show a robust influence of orthography,\nwith faster response times in both production (picture naming) and recognition\n(picture mapping) tasks for words learned in the Audio-Ortho condition.\nMoreover, formant analyses of the picture naming responses show that\northographic input pulls pronunciations of English novel words towards a\nnon-native (French) phonological target. Words learned with their orthographic\nforms were pronounced more precisely (with smaller Dispersion Scores), but were\nmisplaced in the vowel space (as reflected by smaller Euclidian distances with\nrespect to French vowels). Finally, we found only limited evidence of an effect\nof talker-based acoustic variability: novel words learned with multiple talkers\nshowed faster responses times in the picture naming task, but only in the\nAudio-only condition, which suggests that orthographic information may have\noverwhelmed any advantage of talker-based acoustic variability.",
          "link": "http://arxiv.org/abs/2109.03490",
          "publishedOn": "2021-09-09T07:20:40.595Z",
          "wordCount": 663,
          "title": "Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>",
          "description": "Multimodal machine translation (MMT) systems have been shown to outperform\ntheir text-only neural machine translation (NMT) counterparts when visual\ncontext is available. However, recent studies have also shown that the\nperformance of MMT models is only marginally impacted when the associated image\nis replaced with an unrelated image or noise, which suggests that the visual\ncontext might not be exploited by the model at all. We hypothesize that this\nmight be caused by the nature of the commonly used evaluation benchmark, also\nknown as Multi30K, where the translations of image captions were prepared\nwithout actually showing the images to human translators. In this paper, we\npresent a qualitative study that examines the role of datasets in stimulating\nthe leverage of visual modality and we propose methods to highlight the\nimportance of visual signals in the datasets which demonstrate improvements in\nreliance of models on the source images. Our findings suggest the research on\neffective MMT architectures is currently impaired by the lack of suitable\ndatasets and careful consideration must be taken in creation of future MMT\ndatasets, for which we also provide useful insights.",
          "link": "http://arxiv.org/abs/2109.03415",
          "publishedOn": "2021-09-09T07:20:40.586Z",
          "wordCount": 637,
          "title": "Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hangping Qiu</a>",
          "description": "Using prompts to utilize language models to perform various downstream tasks,\nalso known as prompt-based learning or prompt-learning, has lately gained\nsignificant success in comparison to the pre-train and fine-tune paradigm.\nNonetheless, virtually all prompt-based methods are token-level, meaning they\nall utilize GPT's left-to-right language model or BERT's masked language model\nto perform cloze-style tasks. In this paper, we attempt to accomplish several\nNLP tasks in the zero-shot scenario using a BERT original pre-training task\nabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike\ntoken-level techniques, our sentence-level prompt-based method NSP-BERT does\nnot need to fix the length of the prompt or the position to be predicted,\nallowing it to handle tasks such as entity linking with ease. Based on the\ncharacteristics of NSP-BERT, we offer several quick building templates for\nvarious downstream tasks. We suggest a two-stage prompt method for word sense\ndisambiguation tasks in particular. Our strategies for mapping the labels\nsignificantly enhance the model's performance on sentence pair tasks. On the\nFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of\nthese tasks and comes close to the few-shot methods.",
          "link": "http://arxiv.org/abs/2109.03564",
          "publishedOn": "2021-09-09T07:20:40.566Z",
          "wordCount": 647,
          "title": "NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kelly Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_D/0/1/0/all/0/1\">Dan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_H/0/1/0/all/0/1\">Huitzilin Ortiz</a>",
          "description": "Building compositional explanations requires models to combine two or more\nfacts that, together, describe why the answer to a question is correct.\nTypically, these \"multi-hop\" explanations are evaluated relative to one (or a\nsmall number of) gold explanations. In this work, we show these evaluations\nsubstantially underestimate model performance, both in terms of the relevance\nof included facts, as well as the completeness of model-generated explanations,\nbecause models regularly discover and produce valid explanations that are\ndifferent than gold explanations. To address this, we construct a large corpus\nof 126k domain-expert (science teacher) relevance ratings that augment a corpus\nof explanations to standardized science exam questions, discovering 80k\nadditional relevant facts not rated as gold. We build three strong models based\non different methodologies (generation, ranking, and schemas), and empirically\nshow that while expert-augmented ratings provide better estimates of\nexplanation quality, both original (gold) and expert-augmented automatic\nevaluations still substantially underestimate performance by up to 36% when\ncompared with full manual expert judgements, with different models being\ndisproportionately affected. This poses a significant methodological challenge\nto accurately evaluating explanations produced by compositional reasoning\nmodels.",
          "link": "http://arxiv.org/abs/2109.03334",
          "publishedOn": "2021-09-09T07:20:40.554Z",
          "wordCount": 652,
          "title": "On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tran Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Branda Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>",
          "description": "Existing question answering (QA) datasets are created mainly for the\napplication of having AI to be able to answer questions asked by humans. But in\neducational applications, teachers and parents sometimes may not know what\nquestions they should ask a child that can maximize their language learning\nresults. With a newly released book QA dataset (FairytaleQA), which educational\nexperts labeled on 46 fairytale storybooks for early childhood readers, we\ndeveloped an automated QA generation model architecture for this novel\napplication. Our model (1) extracts candidate answers from a given storybook\npassage through carefully designed heuristics based on a pedagogical framework;\n(2) generates appropriate questions corresponding to each extracted answer\nusing a language model; and, (3) uses another QA model to rank top QA-pairs.\nAutomatic and human evaluations show that our model outperforms baselines. We\nalso demonstrate that our method can help with the scarcity issue of the\nchildren's book QA dataset via data augmentation on 200 unlabeled storybooks.",
          "link": "http://arxiv.org/abs/2109.03423",
          "publishedOn": "2021-09-09T07:20:40.542Z",
          "wordCount": 641,
          "title": "It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken question answering (SQA) requires fine-grained understanding of both\nspoken documents and questions for the optimal answer prediction. In this\npaper, we propose novel training schemes for spoken question answering with a\nself-supervised training stage and a contrastive representation learning stage.\nIn the self-supervised stage, we propose three auxiliary self-supervised tasks,\nincluding utterance restoration, utterance insertion, and question\ndiscrimination, and jointly train the model to capture consistency and\ncoherence among speech documents without any additional data or annotations. We\nthen propose to learn noise-invariant utterance representations in a\ncontrastive objective by adopting multiple augmentation strategies, including\nspan deletion and span substitution. Besides, we design a Temporal-Alignment\nattention to semantically align the speech-text clues in the learned common\nspace and benefit the SQA tasks. By this means, the training schemes can more\neffectively guide the generation model to predict more proper answers.\nExperimental results show that our model achieves state-of-the-art results on\nthree SQA benchmarks.",
          "link": "http://arxiv.org/abs/2109.03381",
          "publishedOn": "2021-09-09T07:20:40.531Z",
          "wordCount": 617,
          "title": "Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>",
          "description": "COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.",
          "link": "http://arxiv.org/abs/2106.13405",
          "publishedOn": "2021-09-09T07:20:40.520Z",
          "wordCount": 577,
          "title": "JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Landa_J/0/1/0/all/0/1\">J. Fernandez de Landa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">R. Agerri</a>",
          "description": "In this paper we take into account both social and linguistic aspects to\nperform demographic analysis by processing a large amount of tweets in Basque\nlanguage. The study of demographic characteristics and social relationships are\napproached by applying machine learning and modern deep-learning Natural\nLanguage Processing (NLP) techniques, combining social sciences with automatic\ntext processing. More specifically, our main objective is to combine\ndemographic inference and social analysis in order to detect young Basque\nTwitter users and to identify the communities that arise from their\nrelationships or shared content. This social and demographic analysis will be\nentirely based on the~automatically collected tweets using NLP to convert\nunstructured textual information into interpretable knowledge.",
          "link": "http://arxiv.org/abs/2109.03487",
          "publishedOn": "2021-09-09T07:20:40.508Z",
          "wordCount": 575,
          "title": "Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.",
          "link": "http://arxiv.org/abs/2004.12835",
          "publishedOn": "2021-09-08T07:20:13.097Z",
          "wordCount": 571,
          "title": "Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>",
          "description": "An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.",
          "link": "http://arxiv.org/abs/2109.03158",
          "publishedOn": "2021-09-08T07:20:12.978Z",
          "wordCount": 590,
          "title": "Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>",
          "description": "Recent work on opinion summarization produces general summaries based on a\nset of input reviews and the popularity of opinions expressed in them. In this\npaper, we propose an approach that allows the generation of customized\nsummaries based on aspect queries (e.g., describing the location and room of a\nhotel). Using a review corpus, we create a synthetic training dataset of\n(review, summary) pairs enriched with aspect controllers which are induced by a\nmulti-instance learning model that predicts the aspects of a document at\ndifferent levels of granularity. We fine-tune a pretrained model using our\nsynthetic dataset and generate aspect-specific summaries by modifying the\naspect controllers. Experiments on two benchmarks show that our model\noutperforms the previous state of the art and generates personalized summaries\nby controlling the number of aspects discussed in them.",
          "link": "http://arxiv.org/abs/2109.03171",
          "publishedOn": "2021-09-08T07:20:12.954Z",
          "wordCount": 567,
          "title": "Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bornheim_T/0/1/0/all/0/1\">Tobias Bornheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grieger_N/0/1/0/all/0/1\">Niklas Grieger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bialonski_S/0/1/0/all/0/1\">Stephan Bialonski</a>",
          "description": "The availability of language representations learned by large pretrained\nneural network models (such as BERT and ELECTRA) has led to improvements in\nmany downstream Natural Language Processing tasks in recent years. Pretrained\nmodels usually differ in pretraining objectives, architectures, and datasets\nthey are trained on which can affect downstream performance. In this\ncontribution, we fine-tuned German BERT and German ELECTRA models to identify\ntoxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)\nin Facebook data provided by the GermEval 2021 competition. We created\nensembles of these models and investigated whether and how classification\nperformance depends on the number of ensemble members and their composition. On\nout-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for\nall subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,\nrespectively.",
          "link": "http://arxiv.org/abs/2109.03094",
          "publishedOn": "2021-09-08T07:20:12.900Z",
          "wordCount": 616,
          "title": "FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rikters_M/0/1/0/all/0/1\">Mat&#x12b;ss Rikters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>",
          "description": "One of the most popular methods for context-aware machine translation (MT) is\nto use separate encoders for the source sentence and context as multiple\nsources for one target sentence. Recent work has cast doubt on whether these\nmodels actually learn useful signals from the context or are improvements in\nautomatic evaluation metrics just a side-effect. We show that multi-source\ntransformer models improve MT over standard transformer-base models even with\nempty lines provided as context, but the translation quality improves\nsignificantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is\nprovided. We also show that even though randomly shuffling in-domain context\ncan also improve over baselines, the correct context further improves\ntranslation quality and random out-of-domain context further degrades it.",
          "link": "http://arxiv.org/abs/2109.02995",
          "publishedOn": "2021-09-08T07:20:12.715Z",
          "wordCount": null,
          "title": "Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>",
          "description": "Identifying political perspective in news media has become an important task\ndue to the rapid growth of political commentary and the increasingly polarized\nideologies. Previous approaches only focus on leveraging the semantic\ninformation and leaves out the rich social and political context that helps\nindividuals understand political stances. In this paper, we propose a\nperspective detection method that incorporates external knowledge of real-world\npolitics. Specifically, we construct a contemporary political knowledge graph\nwith 1,071 entities and 10,703 triples. We then build a heterogeneous\ninformation network for each news document that jointly models article\nsemantics and external knowledge in knowledge graphs. Finally, we apply gated\nrelational graph convolutional networks and conduct political perspective\ndetection as graph-level classification. Extensive experiments show that our\nmethod achieves the best performance and outperforms state-of-the-art methods\nby 5.49%. Numerous ablation studies further bear out the necessity of external\nknowledge and the effectiveness of our graph-based approach.",
          "link": "http://arxiv.org/abs/2108.03861",
          "publishedOn": "2021-09-08T07:20:12.163Z",
          "wordCount": 631,
          "title": "Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagwanshi_P/0/1/0/all/0/1\">Prateek Nagwanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>",
          "description": "Over the years customers' expectation of getting information instantaneously\nhas given rise to the increased usage of channels like virtual assistants.\nTypically, customers try to get their questions answered by low-touch channels\nlike search and virtual assistant first, before getting in touch with a live\nchat agent or the phone representative. Higher usage of these low-touch systems\nis a win-win for both customers and the organization since it enables\norganizations to attain a low cost of service while customers get served\nwithout delay. In this paper, we propose a two-part framework where the first\npart describes methods to combine the information from different interaction\nchannels like call, search, and chat. We do this by summarizing (using a\nstacked Bi-LSTM network) the high-touch interaction channel data such as call\nand chat into short searchquery like customer intents and then creating an\norganically grown intent taxonomy from interaction data (using Hierarchical\nAgglomerative Clustering). The second part of the framework focuses on\nextracting customer questions by analyzing interaction data sources. It\ncalculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It\nalso maps these identified questions to the output of the first part of the\nframework using syntactic and semantic similarity.",
          "link": "http://arxiv.org/abs/2109.02935",
          "publishedOn": "2021-09-08T07:20:11.357Z",
          "wordCount": 668,
          "title": "Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borova_I/0/1/0/all/0/1\">Iana Borova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langdon_D/0/1/0/all/0/1\">Dylan Langdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_R/0/1/0/all/0/1\">Reema Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_M/0/1/0/all/0/1\">Matt Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Routledge_B/0/1/0/all/0/1\">Bryan Routledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>",
          "description": "The sheer volume of financial statements makes it difficult for humans to\naccess and analyze a business's financials. Robust numerical reasoning likewise\nfaces unique challenges in this domain. In this work, we focus on answering\ndeep questions over financial data, aiming to automate the analysis of a large\ncorpus of financial documents. In contrast to existing tasks on general domain,\nthe finance domain includes complex numerical reasoning and understanding of\nheterogeneous representations. To facilitate analytical progress, we propose a\nnew large-scale dataset, FinQA, with Question-Answering pairs over Financial\nreports, written by financial experts. We also annotate the gold reasoning\nprograms to ensure full explainability. We further introduce baselines and\nconduct comprehensive experiments in our dataset. The results demonstrate that\npopular, large, pre-trained models fall far short of expert humans in acquiring\nfinance knowledge and in complex multi-step numerical reasoning on that\nknowledge. Our dataset -- the first of its kind -- should therefore enable\nsignificant, new community research into complex application domains. The\ndataset and code are publicly available\\url{https://github.com/czyssrs/FinQA}.",
          "link": "http://arxiv.org/abs/2109.00122",
          "publishedOn": "2021-09-08T07:20:10.424Z",
          "wordCount": 649,
          "title": "FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shangbin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peisheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghua Zheng</a>",
          "description": "Political stance detection has become an important task due to the\nincreasingly polarized political ideologies. Most existing works focus on\nidentifying perspectives in news articles or social media posts, while social\nentities, such as individuals and organizations, produce these texts and\nactually take stances. In this paper, we propose the novel task of entity\nstance prediction, which aims to predict entities' stances given their social\nand political context. Specifically, we retrieve facts from Wikipedia about\nsocial entities regarding contemporary U.S. politics. We then annotate social\nentities' stances towards political ideologies with the help of domain experts.\nAfter defining the task of entity stance prediction, we propose a graph-based\nsolution, which constructs a heterogeneous information network from collected\nfacts and adopts gated relational graph convolutional networks for\nrepresentation learning. Our model is then trained with a combination of\nsupervised, self-supervised and unsupervised loss functions, which are\nmotivated by multiple social and political phenomenons. We conduct extensive\nexperiments to compare our method with existing text and graph analysis\nbaselines. Our model achieves highest stance detection accuracy and yields\ninspiring insights regarding social entity stances. We further conduct ablation\nstudy and parameter analysis to study the mechanism and effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/2108.03881",
          "publishedOn": "2021-09-08T07:20:10.396Z",
          "wordCount": 683,
          "title": "Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuyuan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>",
          "description": "While deep learning models have greatly improved the performance of most\nartificial intelligence tasks, they are often criticized to be untrustworthy\ndue to the black-box problem. Consequently, many works have been proposed to\nstudy the trustworthiness of deep learning. However, as most open datasets are\ndesigned for evaluating the accuracy of model outputs, there is still a lack of\nappropriate datasets for evaluating the inner workings of neural networks. The\nlack of datasets obviously hinders the development of trustworthiness research.\nTherefore, in order to systematically evaluate the factors for building\ntrustworthy systems, we propose a novel and well-annotated sentiment analysis\ndataset to evaluate robustness and interpretability. To evaluate these factors,\nour dataset contains diverse annotations about the challenging distribution of\ninstances, manual adversarial instances and sentiment explanations. Several\nevaluation metrics are further proposed for interpretability and robustness.\nBased on the dataset and metrics, we conduct comprehensive comparisons for the\ntrustworthiness of three typical models, and also study the relations between\naccuracy, robustness and interpretability. We release this trustworthiness\nevaluation dataset at \\url{https://github/xyz} and hope our work can facilitate\nthe progress on building more trustworthy systems for real-world applications.",
          "link": "http://arxiv.org/abs/2108.13140",
          "publishedOn": "2021-09-08T07:20:10.288Z",
          "wordCount": 657,
          "title": "DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>",
          "description": "We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.",
          "link": "http://arxiv.org/abs/2109.00904",
          "publishedOn": "2021-09-08T07:20:10.226Z",
          "wordCount": 629,
          "title": "MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.",
          "link": "http://arxiv.org/abs/2109.03137",
          "publishedOn": "2021-09-08T07:20:10.211Z",
          "wordCount": 626,
          "title": "NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.",
          "link": "http://arxiv.org/abs/2101.00234",
          "publishedOn": "2021-09-08T07:20:10.191Z",
          "wordCount": 587,
          "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1\">Mohd Abbas Zaidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Indurthi_S/0/1/0/all/0/1\">Sathish Indurthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Beomseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakumarapu_N/0/1/0/all/0/1\">Nikhil Kumar Lakumarapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangha Kim</a>",
          "description": "Simultaneous neural machine translation(SNMT) models start emitting the\ntarget sequence before they have processed the source sequence. The recent\nadaptive policies for SNMT use monotonic attention to perform read/write\ndecisions based on the partial source and target sequences. The lack of\nsufficient information might cause the monotonic attention to take poor\nread/write decisions, which in turn negatively affects the performance of the\nSNMT model. On the other hand, human translators make better read/write\ndecisions since they can anticipate the immediate future words using linguistic\ninformation and domain knowledge.Motivated by human translators, in this work,\nwe propose a framework to aid monotonic attention with an external language\nmodel to improve its decisions.We conduct experiments on the MuST-C\nEnglish-German and English-French speech-to-text translation tasks to show the\neffectiveness of the proposed framework.The proposed SNMT method improves the\nquality-latency trade-off over the state-of-the-art monotonic multihead\nattention.",
          "link": "http://arxiv.org/abs/2109.03121",
          "publishedOn": "2021-09-08T07:20:10.184Z",
          "wordCount": 589,
          "title": "Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02403",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yi Chang</a>",
          "description": "Aspect-level sentiment classification (ALSC) aims at identifying the\nsentiment polarity of a specified aspect in a sentence. ALSC is a practical\nsetting in aspect-based sentiment analysis due to no opinion term labeling\nneeded, but it fails to interpret why a sentiment polarity is derived for the\naspect. To address this problem, recent works fine-tune pre-trained Transformer\nencoders for ALSC to extract an aspect-centric dependency tree that can locate\nthe opinion words. However, the induced opinion words only provide an intuitive\ncue far below human-level interpretability. Besides, the pre-trained encoder\ntends to internalize an aspect's intrinsic sentiment, causing sentiment bias\nand thus affecting model performance. In this paper, we propose a span-based\nanti-bias aspect representation learning framework. It first eliminates the\nsentiment bias in the aspect embedding by adversarial learning against aspects'\nprior sentiment. Then, it aligns the distilled opinion candidates with the\naspect by span-based dependency modeling to highlight the interpretable opinion\nterms. Our method achieves new state-of-the-art performance on five benchmarks,\nwith the capability of unsupervised opinion extraction.",
          "link": "http://arxiv.org/abs/2109.02403",
          "publishedOn": "2021-09-08T07:20:10.177Z",
          "wordCount": 643,
          "title": "Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>",
          "description": "The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.",
          "link": "http://arxiv.org/abs/2109.03200",
          "publishedOn": "2021-09-08T07:20:10.159Z",
          "wordCount": 525,
          "title": "ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.",
          "link": "http://arxiv.org/abs/2109.03228",
          "publishedOn": "2021-09-08T07:20:10.136Z",
          "wordCount": 556,
          "title": "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perez_Mayos_L/0/1/0/all/0/1\">Laura P&#xe9;rez-Mayos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanner_L/0/1/0/all/0/1\">Leo Wanner</a>",
          "description": "Transformers-based pretrained language models achieve outstanding results in\nmany well-known NLU benchmarks. However, while pretraining methods are very\nconvenient, they are expensive in terms of time and resources. This calls for a\nstudy of the impact of pretraining data size on the knowledge of the models. We\nexplore this impact on the syntactic capabilities of RoBERTa, using models\ntrained on incremental sizes of raw text data. First, we use syntactic\nstructural probes to determine whether models pretrained on more data encode a\nhigher amount of syntactic information. Second, we perform a targeted syntactic\nevaluation to analyze the impact of pretraining data size on the syntactic\ngeneralization performance of the models. Third, we compare the performance of\nthe different models on three downstream applications: part-of-speech tagging,\ndependency parsing and paraphrase identification. We complement our study with\nan analysis of the cost-benefit trade-off of training such models. Our\nexperiments show that while models pretrained on more data encode more\nsyntactic knowledge and perform better on downstream applications, they do not\nalways offer a better performance across the different syntactic phenomena and\ncome at a higher financial and environmental cost.",
          "link": "http://arxiv.org/abs/2109.03160",
          "publishedOn": "2021-09-08T07:20:10.123Z",
          "wordCount": 651,
          "title": "How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Emil Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrenheim_V/0/1/0/all/0/1\">Vilhelm von Ehrenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_D/0/1/0/all/0/1\">Dhiana Deva Cavalcanti Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_S/0/1/0/all/0/1\">Sonja Horn</a>",
          "description": "Sentence embedding refers to a set of effective and versatile techniques for\nconverting raw text into numerical vector representations that can be used in a\nwide range of natural language processing (NLP) applications. The majority of\nthese techniques are either supervised or unsupervised. Compared to the\nunsupervised methods, the supervised ones make less assumptions about\noptimization objectives and usually achieve better results. However, the\ntraining requires a large amount of labeled sentence pairs, which is not\navailable in many industrial scenarios. To that end, we propose a generic and\nend-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence\nEmbedding), capable of learning high-quality sentence embeddings from a\npartially labeled dataset. We experimentally show that PAUSE achieves, and\nsometimes surpasses, state-of-the-art results using only a small fraction of\nlabeled sentence pairs on various benchmark tasks. When applied to a real\nindustrial use case where labeled samples are scarce, PAUSE encourages us to\nextend our dataset without the liability of extensive manual annotation work.",
          "link": "http://arxiv.org/abs/2109.03155",
          "publishedOn": "2021-09-08T07:20:10.115Z",
          "wordCount": 638,
          "title": "PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1\">Akshay Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanvarie_D/0/1/0/all/0/1\">Dittaya Wanvarie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phu T. V. Le</a>",
          "description": "Previous work in slogan generation focused on utilising slogan skeletons\nmined from existing slogans. While some generated slogans can be catchy, they\nare often not coherent with the company's focus or style across their marketing\ncommunications because the skeletons are mined from other companies' slogans.\nWe propose a sequence-to-sequence (seq2seq) transformer model to generate\nslogans from a brief company description. A naive seq2seq model fine-tuned for\nslogan generation is prone to introducing false information. We use company\nname delexicalisation and entity masking to alleviate this problem and improve\nthe generated slogans' quality and truthfulness. Furthermore, we apply\nconditional training based on the first words' POS tag to generate\nsyntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score\nof 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that\nour method generates significantly more factual, diverse and catchy slogans\nthan strong LSTM and transformer seq2seq baselines.",
          "link": "http://arxiv.org/abs/2102.05924",
          "publishedOn": "2021-09-08T07:20:10.108Z",
          "wordCount": 617,
          "title": "Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghai_B/0/1/0/all/0/1\">Bhavya Ghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_M/0/1/0/all/0/1\">Md Naimul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1\">Klaus Mueller</a>",
          "description": "Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.",
          "link": "http://arxiv.org/abs/2103.03598",
          "publishedOn": "2021-09-08T07:20:10.088Z",
          "wordCount": 666,
          "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhichun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.",
          "link": "http://arxiv.org/abs/2104.07228",
          "publishedOn": "2021-09-08T07:20:10.051Z",
          "wordCount": 574,
          "title": "Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07555",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>",
          "description": "QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.",
          "link": "http://arxiv.org/abs/2104.07555",
          "publishedOn": "2021-09-08T07:20:10.041Z",
          "wordCount": 600,
          "title": "Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.",
          "link": "http://arxiv.org/abs/2009.13401",
          "publishedOn": "2021-09-08T07:20:10.022Z",
          "wordCount": 600,
          "title": "Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>",
          "description": "The semantic understanding of natural dialogues composes of several parts.\nSome of them, like intent classification and entity detection, have a crucial\nrole in deciding the next steps in handling user input. Handling each task as\nan individual problem can be wasting of training resources, and also each\nproblem can benefit from each other. This paper tackles these problems as one.\nOur new model, which combine intent and entity recognition into one system, is\nachieving better metrics in both tasks with lower training requirements than\nsolving each task separately. We also optimize the model based on the inputs.",
          "link": "http://arxiv.org/abs/2109.03221",
          "publishedOn": "2021-09-08T07:20:10.003Z",
          "wordCount": 531,
          "title": "Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>",
          "description": "Conversation disentanglement aims to separate intermingled messages into\ndetached sessions, which is a fundamental task in understanding multi-party\nconversations. Existing work on conversation disentanglement relies heavily\nupon human-annotated datasets, which are expensive to obtain in practice. In\nthis work, we explore to train a conversation disentanglement model without\nreferencing any human annotations. Our method is built upon a deep co-training\nalgorithm, which consists of two neural networks: a message-pair classifier and\na session classifier. The former is responsible for retrieving local relations\nbetween two messages while the latter categorizes a message to a session by\ncapturing context-aware information. Both networks are initialized respectively\nwith pseudo data built from an unannotated corpus. During the deep co-training\nprocess, we use the session classifier as a reinforcement learning component to\nlearn a session assigning policy by maximizing the local rewards given by the\nmessage-pair classifier. For the message-pair classifier, we enrich its\ntraining data by retrieving message pairs with high confidence from the\ndisentangled sessions predicted by the session classifier. Experimental results\non the large Movie Dialogue Dataset demonstrate that our proposed approach\nachieves competitive performance compared to the previous supervised methods.\nFurther experiments show that the predicted disentangled conversations can\npromote the performance on the downstream task of multi-party response\nselection.",
          "link": "http://arxiv.org/abs/2109.03199",
          "publishedOn": "2021-09-08T07:20:09.977Z",
          "wordCount": 648,
          "title": "Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>",
          "description": "Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.",
          "link": "http://arxiv.org/abs/2109.03079",
          "publishedOn": "2021-09-08T07:20:09.881Z",
          "wordCount": 595,
          "title": "GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lianxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yang Mo</a>",
          "description": "Recently, large pre-trained neural language models have attained remarkable\nperformance on many downstream natural language processing (NLP) applications\nvia fine-tuning. In this paper, we target at how to further improve the token\nrepresentations on the language models. We, therefore, propose a simple yet\neffective plug-and-play module, Sequential Attention Module (SAM), on the token\nembeddings learned from a pre-trained language model. Our proposed SAM consists\nof two main attention modules deployed sequentially: Feature-wise Attention\nModule (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can\neffectively identify the importance of features at each dimension and promote\nthe effect via dot-product on the original token embeddings for downstream NLP\napplications. Meanwhile, TAM can further re-weight the features at the\ntoken-wise level. Moreover, we propose an adaptive filter on FAM to prevent\nnoise impact and increase information absorption. Finally, we conduct extensive\nexperiments to demonstrate the advantages and properties of our proposed SAM.\nWe first show how SAM plays a primary role in the champion solution of two\nsubtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis\nand three popular NLP tasks and demonstrate that SAM consistently outperforms\nthe state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2109.03009",
          "publishedOn": "2021-09-08T07:20:09.870Z",
          "wordCount": 642,
          "title": "Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Habernal_I/0/1/0/all/0/1\">Ivan Habernal</a>",
          "description": "Differential privacy provides a formal approach to privacy of individuals.\nApplications of differential privacy in various scenarios, such as protecting\nusers' original utterances, must satisfy certain mathematical properties. Our\ncontribution is a formal analysis of ADePT, a differentially private\nauto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising\nresults on downstream tasks while providing tight privacy guarantees. Our proof\nreveals that ADePT is not differentially private, thus rendering the\nexperimental results unsubstantiated. We also quantify the impact of the error\nin its private mechanism, showing that the true sensitivity is higher by at\nleast factor 6 in an optimistic case of a very small encoder's dimension and\nthat the amount of utterances that are not privatized could easily reach 100%\nof the entire dataset. Our intention is neither to criticize the authors, nor\nthe peer-reviewing process, but rather point out that if differential privacy\napplications in NLP rely on formal guarantees, these should be outlined in full\nand put under detailed scrutiny.",
          "link": "http://arxiv.org/abs/2109.03175",
          "publishedOn": "2021-09-08T07:20:09.810Z",
          "wordCount": 614,
          "title": "When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gawron_C/0/1/0/all/0/1\">Christian Gawron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_S/0/1/0/all/0/1\">Sebastian Schmidt</a>",
          "description": "In this paper we describe the methods we used for our submissions to the\nGermEval 2021 shared task on the identification of toxic, engaging, and\nfact-claiming comments. For all three subtasks we fine-tuned freely available\ntransformer-based models from the Huggingface model hub. We evaluated the\nperformance of various pre-trained models after fine-tuning on 80% of the\ntraining data with different hyperparameters and submitted predictions of the\ntwo best performing resulting models. We found that this approach worked best\nfor subtask 3, for which we achieved an F1-score of 0.736.",
          "link": "http://arxiv.org/abs/2109.02966",
          "publishedOn": "2021-09-08T07:20:09.802Z",
          "wordCount": 541,
          "title": "FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sangwon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-min Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_W/0/1/0/all/0/1\">Woo-Jong Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Despite advances in neural network language model, the representation\ndegeneration problem of embeddings is still challenging. Recent studies have\nfound that the learned output embeddings are degenerated into a narrow-cone\ndistribution which makes the similarity between each embeddings positive. They\nanalyzed the cause of the degeneration problem has been demonstrated as common\nto most embeddings. However, we found that the degeneration problem is\nespecially originated from the training of embeddings of rare words. In this\nstudy, we analyze the intrinsic mechanism of the degeneration of rare word\nembeddings with respect of their gradient about the negative log-likelihood\nloss function. Furthermore, we theoretically and empirically demonstrate that\nthe degeneration of rare word embeddings causes the degeneration of non-rare\nword embeddings, and that the overall degeneration problem can be alleviated by\npreventing the degeneration of rare word embeddings. Based on our analyses, we\npropose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the\ndegeneration problem. Experimental results demonstrate the effectiveness of the\nproposed method qualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2109.03127",
          "publishedOn": "2021-09-08T07:20:09.795Z",
          "wordCount": 605,
          "title": "Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Max L. Wilson</a>",
          "description": "Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.",
          "link": "http://arxiv.org/abs/2109.03039",
          "publishedOn": "2021-09-08T07:20:09.789Z",
          "wordCount": 609,
          "title": "POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianhao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Math word problem (MWP) is a challenging and critical task in natural\nlanguage processing. Many recent studies formalize MWP as a generation task and\nhave adopted sequence-to-sequence models to transform problem descriptions to\nmathematical expressions. However, mathematical expressions are prone to minor\nmistakes while the generation objective does not explicitly handle such\nmistakes. To address this limitation, we devise a new ranking task for MWP and\npropose Generate & Rank, a multi-task framework based on a generative\npre-trained language model. By joint training with generation and ranking, the\nmodel learns from its own mistakes and is able to distinguish between correct\nand incorrect expressions. Meanwhile, we perform tree-based disturbance\nspecially designed for MWP and an online update to boost the ranker. We\ndemonstrate the effectiveness of our proposed method on the benchmark and the\nresults show that our method consistently outperforms baselines in all\ndatasets. Particularly, in the classical Math23k, our method is 7% (78.4%\n$\\rightarrow$ 85.4%) higher than the state-of-the-art.",
          "link": "http://arxiv.org/abs/2109.03034",
          "publishedOn": "2021-09-08T07:20:09.780Z",
          "wordCount": 620,
          "title": "Generate & Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.06438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Siyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Cai Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Advertising is an important revenue source for many companies. However, it is\nexpensive to manually create advertisements that meet the needs of various\nqueries for massive items. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisements for different queries with various needs given the item\nkeywords. In this task, for many different queries there is only one general\npurposed advertisement with no predefined query-advertisement pair, which would\ndiscourage traditional End-to-End models from generating query-variant\nadvertisements for different queries with different needs. To deal with the\nproblem, we propose a query-variant advertisement text generation model that\ntakes keywords and associated external knowledge as input during training and\nadds different queries during inference. Adding external knowledge helps the\nmodel adapted to the information besides the item keywords during training,\nwhich makes the transition between training and inference more smoothing when\nthe query is added during inference. Both automatic and human evaluation show\nthat our model can generate more attractive and query-focused advertisements\nthan the strong baselines.",
          "link": "http://arxiv.org/abs/2004.06438",
          "publishedOn": "2021-09-08T07:20:09.772Z",
          "wordCount": 647,
          "title": "Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>",
          "description": "Multitask deep learning has been applied to patient outcome prediction from\ntext, taking clinical notes as input and training deep neural networks with a\njoint loss function of multiple tasks. However, the joint training scheme of\nmultitask learning suffers from inter-task interference, and diagnosis\nprediction among the multiple tasks has the generalizability issue due to rare\ndiseases or unseen diagnoses. To solve these challenges, we propose a\nhypernetwork-based approach that generates task-conditioned parameters and\ncoefficients of multitask prediction heads to learn task-specific prediction\nand balance the multitask learning. We also incorporate semantic task\ninformation to improves the generalizability of our task-conditioned multitask\nmodel. Experiments on early and discharge notes extracted from the real-world\nMIMIC database show our method can achieve better performance on multitask\npatient outcome prediction than strong baselines in most cases. Besides, our\nmethod can effectively handle the scenario with limited information and improve\nzero-shot prediction on unseen diagnosis categories.",
          "link": "http://arxiv.org/abs/2109.03062",
          "publishedOn": "2021-09-08T07:20:09.766Z",
          "wordCount": 592,
          "title": "Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>",
          "description": "This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.",
          "link": "http://arxiv.org/abs/2109.02938",
          "publishedOn": "2021-09-08T07:20:09.745Z",
          "wordCount": 544,
          "title": "Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15455",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1\">Alham Fikri Aji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>",
          "description": "This paper explores augmenting monolingual data for knowledge distillation in\nneural machine translation. Source language monolingual text can be\nincorporated as a forward translation. Interestingly, we find the best way to\nincorporate target language monolingual text is to translate it to the source\nlanguage and round-trip translate it back to the target language, resulting in\na fully synthetic corpus. We find that combining monolingual data from both\nsource and target languages yields better performance than a corpus twice as\nlarge only in one language. Moreover, experiments reveal that the improvement\ndepends upon the provenance of the test set. If the test set was originally in\nthe source language (with the target side written by translators), then forward\ntranslating source monolingual data matters. If the test set was originally in\nthe target language (with the source written by translators), then\nincorporating target monolingual data matters.",
          "link": "http://arxiv.org/abs/2012.15455",
          "publishedOn": "2021-09-08T07:20:09.732Z",
          "wordCount": 613,
          "title": "Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>",
          "description": "We propose a novel Chain Guided Retriever-reader ({\\tt CGR}) framework to\nmodel the reasoning chain for multi-hop Science Question Answering. Our\nframework is capable of performing explainable reasoning without the need of\nany corpus-specific annotations, such as the ground-truth reasoning chain, or\nhuman-annotated entity mentions. Specifically, we first generate reasoning\nchains from a semantic graph constructed by Abstract Meaning Representation of\nretrieved evidence facts. A \\textit{Chain-aware loss}, concerning both local\nand global chain information, is also designed to enable the generated chains\nto serve as distant supervision signals for training the retriever, where\nreinforcement learning is also adopted to maximize the utility of the reasoning\nchains. Our framework allows the retriever to capture step-by-step clues of the\nentire reasoning process, which is not only shown to be effective on two\nchallenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,\nbut also favors explainability.",
          "link": "http://arxiv.org/abs/2109.02905",
          "publishedOn": "2021-09-08T07:20:09.719Z",
          "wordCount": 592,
          "title": "Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>",
          "description": "In this paper we present IndicBART, a multilingual, sequence-to-sequence\npre-trained model focusing on 11 Indic languages and English. Different from\nexisting pre-trained models, IndicBART utilizes the orthographic similarity\nbetween Indic scripts to improve transfer learning between similar Indic\nlanguages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation\n(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs\nand extreme summarization for 7 languages using multilingual fine-tuning show\nthat IndicBART is competitive with or better than mBART50 despite containing\nsignificantly fewer parameters. Our analyses focus on identifying the impact of\nscript unification (to Devanagari), corpora size as well as multilingualism on\nthe final performance. The IndicBART model is available under the MIT license\nat https://indicnlp.ai4bharat.org/indic-bart .",
          "link": "http://arxiv.org/abs/2109.02903",
          "publishedOn": "2021-09-08T07:20:09.702Z",
          "wordCount": 606,
          "title": "IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.",
          "link": "http://arxiv.org/abs/2109.02941",
          "publishedOn": "2021-09-08T07:20:09.676Z",
          "wordCount": 641,
          "title": "Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1\">Wolfgang Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1\">Wolfgang Minker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1\">Stefan Ultes</a>",
          "description": "One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.",
          "link": "http://arxiv.org/abs/2109.03004",
          "publishedOn": "2021-09-08T07:20:09.650Z",
          "wordCount": 570,
          "title": "Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>",
          "description": "Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.",
          "link": "http://arxiv.org/abs/2109.02972",
          "publishedOn": "2021-09-08T07:20:09.607Z",
          "wordCount": 594,
          "title": "Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wendt_H/0/1/0/all/0/1\">Herwig Wendt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batlle_P/0/1/0/all/0/1\">Pau Batlle</a>",
          "description": "This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.",
          "link": "http://arxiv.org/abs/2109.03084",
          "publishedOn": "2021-09-08T07:20:09.502Z",
          "wordCount": 580,
          "title": "Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02882",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yichun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chengjie Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingquan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhenzhou Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lifeng Shang</a>",
          "description": "Human-designed rules are widely used to build industry applications. However,\nit is infeasible to maintain thousands of such hand-crafted rules. So it is\nvery important to integrate the rule knowledge into neural networks to build a\nhybrid model that achieves better performance. Specifically, the human-designed\nrules are formulated as Regular Expressions (REs), from which the equivalent\nMinimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to\nuse the MDFA as an intermediate model to capture the matched RE patterns as\nrule-based features for each input sentence and introduce these additional\nfeatures into neural networks. We evaluate the proposed method on the ATIS\nintent classification task. The experiment results show that the proposed\nmethod achieves the best performance compared to neural networks and four other\nmethods that combine REs and neural networks when the training dataset is\nrelatively small.",
          "link": "http://arxiv.org/abs/2109.02882",
          "publishedOn": "2021-09-08T07:20:09.493Z",
          "wordCount": 586,
          "title": "Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02950",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>",
          "description": "In this paper, we propose a new paradigm for paraphrase generation by\ntreating the task as unsupervised machine translation (UMT) based on the\nassumption that there must be pairs of sentences expressing the same meaning in\na large-scale unlabeled monolingual corpus. The proposed paradigm first splits\na large unlabeled corpus into multiple clusters, and trains multiple UMT models\nusing pairs of these clusters. Then based on the paraphrase pairs produced by\nthese UMT models, a unified surrogate model can be trained to serve as the\nfinal Seq2Seq model to generate paraphrases, which can be directly used for\ntest in the unsupervised setup, or be finetuned on labeled datasets in the\nsupervised setup. The proposed method offers merits over\nmachine-translation-based paraphrase generation methods, as it avoids reliance\non bilingual sentence pairs. It also allows human intervene with the model so\nthat more diverse paraphrases can be generated using different filtering\ncriteria. Extensive experiments on existing paraphrase dataset for both the\nsupervised and unsupervised setups demonstrate the effectiveness the proposed\nparadigm.",
          "link": "http://arxiv.org/abs/2109.02950",
          "publishedOn": "2021-09-08T07:20:09.465Z",
          "wordCount": 611,
          "title": "Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hayati_S/0/1/0/all/0/1\">Shirley Anugrah Hayati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dongyeop Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1\">Lyle Ungar</a>",
          "description": "People convey their intention and attitude through linguistic styles of the\ntext that they write. In this study, we investigate lexicon usages across\nstyles throughout two lenses: human perception and machine word importance,\nsince words differ in the strength of the stylistic cues that they provide. To\ncollect labels of human perception, we curate a new dataset, Hummingbird, on\ntop of benchmarking style datasets. We have crowd workers highlight the\nrepresentative words in the text that makes them think the text has the\nfollowing styles: politeness, sentiment, offensiveness, and five emotion types.\nWe then compare these human word labels with word importance derived from a\npopular fine-tuned style classifier like BERT. Our results show that the BERT\noften finds content words not relevant to the target style as important words\nused in style prediction, but humans do not perceive the same way even though\nfor some styles (e.g., positive sentiment and joy) human- and\nmachine-identified words share significant overlap for some styles.",
          "link": "http://arxiv.org/abs/2109.02738",
          "publishedOn": "2021-09-08T07:20:09.452Z",
          "wordCount": 615,
          "title": "Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lhoest_Q/0/1/0/all/0/1\">Quentin Lhoest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moral_A/0/1/0/all/0/1\">Albert Villanova del Moral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jernite_Y/0/1/0/all/0/1\">Yacine Jernite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1\">Abhishek Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Platen_P/0/1/0/all/0/1\">Patrick von Platen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Suraj Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaumond_J/0/1/0/all/0/1\">Julien Chaumond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drame_M/0/1/0/all/0/1\">Mariama Drame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plu_J/0/1/0/all/0/1\">Julien Plu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tunstall_L/0/1/0/all/0/1\">Lewis Tunstall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_J/0/1/0/all/0/1\">Joe Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasko_M/0/1/0/all/0/1\">Mario &#x160;a&#x161;ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_B/0/1/0/all/0/1\">Bhavitvya Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandeis_S/0/1/0/all/0/1\">Simon Brandeis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patry_N/0/1/0/all/0/1\">Nicolas Patry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_Major_A/0/1/0/all/0/1\">Angelina McMillan-Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_P/0/1/0/all/0/1\">Philipp Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gugger_S/0/1/0/all/0/1\">Sylvain Gugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delangue_C/0/1/0/all/0/1\">Cl&#xe9;ment Delangue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matussiere_T/0/1/0/all/0/1\">Th&#xe9;o Matussi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debut_L/0/1/0/all/0/1\">Lysandre Debut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekman_S/0/1/0/all/0/1\">Stas Bekman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cistac_P/0/1/0/all/0/1\">Pierric Cistac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goehringer_T/0/1/0/all/0/1\">Thibault Goehringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mustar_V/0/1/0/all/0/1\">Victor Mustar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>",
          "description": "The scale, variety, and quantity of publicly-available NLP datasets has grown\nrapidly as researchers propose new tasks, larger models, and novel benchmarks.\nDatasets is a community library for contemporary NLP designed to support this\necosystem. Datasets aims to standardize end-user interfaces, versioning, and\ndocumentation, while providing a lightweight front-end that behaves similarly\nfor small datasets as for internet-scale corpora. The design of the library\nincorporates a distributed, community-driven approach to adding datasets and\ndocumenting usage. After a year of development, the library now includes more\nthan 650 unique datasets, has more than 250 contributors, and has helped\nsupport a variety of novel cross-dataset research projects and shared tasks.\nThe library is available at https://github.com/huggingface/datasets.",
          "link": "http://arxiv.org/abs/2109.02846",
          "publishedOn": "2021-09-08T07:20:09.442Z",
          "wordCount": 622,
          "title": "Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>",
          "description": "Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.",
          "link": "http://arxiv.org/abs/2109.02789",
          "publishedOn": "2021-09-08T07:20:09.425Z",
          "wordCount": 723,
          "title": "Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaixin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1\">Filip Ilievski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozaki_S/0/1/0/all/0/1\">Satoru Ozaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1\">Alessandro Oltramari</a>",
          "description": "Commonsense reasoning benchmarks have been largely solved by fine-tuning\nlanguage models. The downside is that fine-tuning may cause models to overfit\nto task-specific data and thereby forget their knowledge gained during\npre-training. Recent works only propose lightweight model updates as models may\nalready possess useful knowledge from past experience, but a challenge remains\nin understanding what parts and to what extent models should be refined for a\ngiven task. In this paper, we investigate what models learn from commonsense\nreasoning datasets. We measure the impact of three different adaptation methods\non the generalization and accuracy of models. Our experiments with two models\nshow that fine-tuning performs best, by learning both the content and the\nstructure of the task, but suffers from overfitting and limited generalization\nto novel answers. We observe that alternative adaptation methods like\nprefix-tuning have comparable accuracy, but generalize better to unseen answers\nand are more robust to adversarial splits.",
          "link": "http://arxiv.org/abs/2109.02837",
          "publishedOn": "2021-09-08T07:20:09.417Z",
          "wordCount": 602,
          "title": "Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Cheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deore_U/0/1/0/all/0/1\">Uday Deore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Myah Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>",
          "description": "FDA has been promoting enrollment practices that could enhance the diversity\nof clinical trial populations, through broadening eligibility criteria.\nHowever, how to broaden eligibility remains a significant challenge. We propose\nan AI approach to Cohort Optimization (AICO) through transformer-based natural\nlanguage processing of the eligibility criteria and evaluation of the criteria\nusing real-world data. The method can extract common eligibility criteria\nvariables from a large set of relevant trials and measure the generalizability\nof trial designs to real-world patients. It overcomes the scalability limits of\nexisting manual methods and enables rapid simulation of eligibility criteria\ndesign for a disease of interest. A case study on breast cancer trial design\ndemonstrates the utility of the method in improving trial generalizability.",
          "link": "http://arxiv.org/abs/2109.02808",
          "publishedOn": "2021-09-08T07:20:09.374Z",
          "wordCount": 615,
          "title": "A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdick_R/0/1/0/all/0/1\">Ryerson Burdick</a>",
          "description": "The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($>10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.",
          "link": "http://arxiv.org/abs/2109.02797",
          "publishedOn": "2021-09-08T07:20:09.362Z",
          "wordCount": 537,
          "title": "Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.",
          "link": "http://arxiv.org/abs/2109.02747",
          "publishedOn": "2021-09-08T07:20:09.336Z",
          "wordCount": 528,
          "title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1\">Y-Lan Boureau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Halevy</a>",
          "description": "Inspiration moves a person to see new possibilities and transforms the way\nthey perceive their own potential. Inspiration has received little attention in\npsychology, and has not been researched before in the NLP community. To the\nbest of our knowledge, this work is the first to study inspiration through\nmachine learning methods. We aim to automatically detect inspiring content from\nsocial media data. To this end, we analyze social media posts to tease out what\nmakes a post inspiring and what topics are inspiring. We release a dataset of\n5,800 inspiring and 5,800 non-inspiring English-language public post unique ids\ncollected from a dump of Reddit public posts made available by a third party\nand use linguistic heuristics to automatically detect which social media\nEnglish-language posts are inspiring.",
          "link": "http://arxiv.org/abs/2109.02734",
          "publishedOn": "2021-09-08T07:20:09.326Z",
          "wordCount": 570,
          "title": "Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>",
          "description": "Most previous studies on information status (IS) classification and bridging\nanaphora recognition assume that the gold mention or syntactic tree information\nis given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,\n2020). In this paper, we propose an end-to-end neural approach for information\nstatus classification. Our approach consists of a mention extraction component\nand an information status assignment component. During the inference time, our\nsystem takes a raw text as the input and generates mentions together with their\ninformation status. On the ISNotes corpus (Markert et al., 2012), we show that\nour information status assignment component achieves new state-of-the-art\nresults on fine-grained IS classification based on gold mentions. Furthermore,\nour system performs significantly better than other baselines for both mention\nextraction and fine-grained IS classification in the end-to-end setting.\nFinally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,\n2016) to recognize referential bridging anaphora. We find that our end-to-end\nsystem trained on ISNotes achieves competitive results on bridging anaphora\nrecognition compared to the previous state-of-the-art system that relies on\nsyntactic information and is trained on the in-domain datasets (Yu and Poesio,\n2020).",
          "link": "http://arxiv.org/abs/2109.02753",
          "publishedOn": "2021-09-08T07:20:09.305Z",
          "wordCount": 623,
          "title": "End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1\">Frank Hopfgartner</a>",
          "description": "Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.",
          "link": "http://arxiv.org/abs/2109.02691",
          "publishedOn": "2021-09-08T07:20:09.238Z",
          "wordCount": 656,
          "title": "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\". (arXiv:2109.02691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiacheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>",
          "description": "We study a new problem setting of information extraction (IE), referred to as\ntext-to-table, which can be viewed as an inverse problem of the well-studied\ntable-to-text. In text-to-table, given a text, one creates a table or several\ntables expressing the main content of the text, while the model is learned from\ntext-table pair data. The problem setting differs from those of the existing\nmethods for IE. First, the extraction can be carried out from long texts to\nlarge tables with complex structures. Second, the extraction is entirely\ndata-driven, and there is no need to explicitly define the schemas. As far as\nwe know, there has been no previous work that studies the problem. In this\nwork, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.\nWe first employ a seq2seq model fine-tuned from a pre-trained language model to\nperform the task. We also develop a new method within the seq2seq approach,\nexploiting two additional techniques in table generation: table constraint and\ntable relation embeddings. We make use of four existing table-to-text datasets\nin our experiments on text-to-table. Experimental results show that the vanilla\nseq2seq model can outperform the baseline methods of using relation extraction\nand named entity extraction. The results also show that our method can further\nboost the performances of the vanilla seq2seq model. We further discuss the\nmain challenges of the proposed task. The code and data will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2109.02707",
          "publishedOn": "2021-09-08T07:20:09.149Z",
          "wordCount": 669,
          "title": "Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meichanetzidis_K/0/1/0/all/0/1\">Konstantinos Meichanetzidis</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Gogioso_S/0/1/0/all/0/1\">Stefano Gogioso</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Felice_G/0/1/0/all/0/1\">Giovanni de Felice</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Chiappori_N/0/1/0/all/0/1\">Nicol&#xf2; Chiappori</a> (Hashberg), <a href=\"http://arxiv.org/find/cs/1/au:+Toumi_A/0/1/0/all/0/1\">Alexis Toumi</a> (University of Oxford and Cambridge Quantum Computing Ltd.), <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a> (University of Oxford and Cambridge Quantum Computing Ltd.)",
          "description": "In this work, we describe a full-stack pipeline for natural language\nprocessing on near-term quantum computers, aka QNLP. The language-modelling\nframework we employ is that of compositional distributional semantics\n(DisCoCat), which extends and complements the compositional structure of\npregroup grammars. Within this model, the grammatical reduction of a sentence\nis interpreted as a diagram, encoding a specific interaction of words according\nto the grammar. It is this interaction which, together with a specific choice\nof word embedding, realises the meaning (or \"semantics\") of a sentence.\nBuilding on the formal quantum-like nature of such interactions, we present a\nmethod for mapping DisCoCat diagrams to quantum circuits. Our methodology is\ncompatible both with NISQ devices and with established Quantum Machine Learning\ntechniques, paving the way to near-term applications of quantum technology to\nnatural language processing.",
          "link": "http://arxiv.org/abs/2005.04147",
          "publishedOn": "2021-09-07T07:20:14.070Z",
          "wordCount": null,
          "title": "Quantum Natural Language Processing on Near-Term Quantum Computers. (arXiv:2005.04147v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gaurav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>",
          "description": "Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.",
          "link": "http://arxiv.org/abs/2108.10750",
          "publishedOn": "2021-09-07T07:20:14.068Z",
          "wordCount": null,
          "title": "Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhiheng Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>",
          "description": "Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(HKLM), a unified pre-trained language model (PLM) for all forms of text,\nincluding unstructured text, semi-structured text and well-structured text. To\ncapture the corresponding relations among these multi-format knowledge, our\napproach uses masked language model objective to learn word knowledge, uses\ntriple classification objective and title matching objective to learn entity\nknowledge and topic knowledge respectively. To obtain the aforementioned\nmulti-format text, we construct a corpus in the tourism domain and conduct\nexperiments on 5 tourism NLP datasets. The results show that our approach\noutperforms the pre-training of plain text using only 1/4 of the data. The\ncode, datasets, corpus and knowledge graph will be released.",
          "link": "http://arxiv.org/abs/2109.01048",
          "publishedOn": "2021-09-07T07:20:14.067Z",
          "wordCount": null,
          "title": "TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_D/0/1/0/all/0/1\">Daksh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>",
          "description": "Coherence is an important aspect of text quality and is crucial for ensuring\nits readability. It is essential desirable for outputs from text generation\nsystems like summarization, question answering, machine translation, question\ngeneration, table-to-text, etc. An automated coherence scoring model is also\nhelpful in essay scoring or providing writing feedback. A large body of\nprevious work has leveraged entity-based methods, syntactic patterns, discourse\nrelations, and more recently traditional deep learning architectures for text\ncoherence assessment. Previous work suffers from drawbacks like the inability\nto handle long-range dependencies, out-of-vocabulary words, or model sequence\ninformation. We hypothesize that coherence assessment is a cognitively complex\ntask that requires deeper models and can benefit from other related tasks.\nAccordingly, in this paper, we propose four different Transformer-based\narchitectures for the task: vanilla Transformer, hierarchical Transformer,\nmulti-task learning-based model, and a model with fact-based input\nrepresentation. Our experiments with popular benchmark datasets across multiple\ndomains on four different coherence assessment tasks demonstrate that our\nmodels achieve state-of-the-art results outperforming existing models by a good\nmargin.",
          "link": "http://arxiv.org/abs/2109.02176",
          "publishedOn": "2021-09-07T07:20:14.055Z",
          "wordCount": null,
          "title": "Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>",
          "description": "The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (< 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.",
          "link": "http://arxiv.org/abs/2109.01951",
          "publishedOn": "2021-09-07T07:20:13.892Z",
          "wordCount": null,
          "title": "FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_U/0/1/0/all/0/1\">Uri Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Arie_A/0/1/0/all/0/1\">Aviv Ben-Arie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>",
          "description": "The primary paradigm for multi-task training in natural language processing\nis to represent the input with a shared pre-trained language model, and add a\nsmall, thin network (head) per task. Given an input, a target head is the head\nthat is selected for outputting the final prediction. In this work, we examine\nthe behaviour of non-target heads, that is, the output of heads when given\ninput that belongs to a different task than the one they were trained for. We\nfind that non-target heads exhibit emergent behaviour, which may either explain\nthe target task, or generalize beyond their original task. For example, in a\nnumerical reasoning task, a span extraction head extracts from the input the\narguments to a computation that results in a number generated by a target\ngenerative head. In addition, a summarization head that is trained with a\ntarget question answering head, outputs query-based summaries when given a\nquestion and a context from which the answer is to be extracted. This emergent\nbehaviour suggests that multi-task training leads to non-trivial extrapolation\nof skills, which can be harnessed for interpretability and generalization.",
          "link": "http://arxiv.org/abs/2104.06129",
          "publishedOn": "2021-09-07T07:20:13.833Z",
          "wordCount": null,
          "title": "What's in your Head? Emergent Behaviour in Multi-Task Transformer Models. (arXiv:2104.06129v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>",
          "description": "As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.",
          "link": "http://arxiv.org/abs/2109.01850",
          "publishedOn": "2021-09-07T07:20:13.537Z",
          "wordCount": null,
          "title": "Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.09084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.",
          "link": "http://arxiv.org/abs/2108.09084",
          "publishedOn": "2021-09-07T07:20:13.527Z",
          "wordCount": null,
          "title": "Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v6 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02071",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>",
          "description": "Existing Natural Language Understanding (NLU) models have been shown to\nincorporate dataset biases leading to strong performance on in-distribution\n(ID) test sets but poor performance on out-of-distribution (OOD) ones. We\nintroduce a simple yet effective debiasing framework whereby the shallow\nrepresentations of the main model are used to derive a bias model and both\nmodels are trained simultaneously. We demonstrate on three well studied NLU\ntasks that despite its simplicity, our method leads to competitive OOD results.\nIt significantly outperforms other debiasing approaches on two tasks, while\nstill delivering high in-distribution performance.",
          "link": "http://arxiv.org/abs/2109.02071",
          "publishedOn": "2021-09-07T07:20:13.494Z",
          "wordCount": null,
          "title": "End-to-End Self-Debiasing Framework for Robust NLU Training. (arXiv:2109.02071v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Duo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.",
          "link": "http://arxiv.org/abs/2109.02297",
          "publishedOn": "2021-09-07T07:20:13.484Z",
          "wordCount": null,
          "title": "Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser. (arXiv:2109.02297v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.04128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Adriana D. Correia</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Stoof_H/0/1/0/all/0/1\">Henk T. C. Stoof</a> (Utrecht University), <a href=\"http://arxiv.org/find/cs/1/au:+Moortgat_M/0/1/0/all/0/1\">Michael Moortgat</a> (Utrecht University)",
          "description": "Extended versions of the Lambek Calculus currently used in computational\nlinguistics rely on unary modalities to allow for the controlled application of\nstructural rules affecting word order and phrase structure. These controlled\nstructural operations give rise to derivational ambiguities that are missed by\nthe original Lambek Calculus or its pregroup simplification. Proposals for\ncompositional interpretation of extended Lambek Calculus in the compact closed\ncategory of FVect and linear maps have been made, but in these proposals the\nsyntax-semantics mapping ignores the control modalities, effectively\nrestricting their role to the syntax. Our aim is to turn the modalities into\nfirst-class citizens of the vectorial interpretation. Building on the\ndirectional density matrix semantics, we extend the interpretation of the type\nsystem with an extra spin density matrix space. The interpretation of proofs\nthen results in ambiguous derivations being tensored with orthogonal spin\nstates. Our method introduces a way of simultaneously representing co-existing\ninterpretations of ambiguous utterances, and provides a uniform framework for\nthe integration of lexical and derivational ambiguity.",
          "link": "http://arxiv.org/abs/2004.04128",
          "publishedOn": "2021-09-07T07:20:13.476Z",
          "wordCount": null,
          "title": "Putting a Spin on Language: A Quantum Interpretation of Unary Connectives for Linguistic Applications. (arXiv:2004.04128v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>",
          "description": "In recent years, world business in online discussions and opinion sharing on\nsocial media is booming. Re-entry prediction task is thus proposed to help\npeople keep track of the discussions which they wish to continue. Nevertheless,\nexisting works only focus on exploiting chatting history and context\ninformation, and ignore the potential useful learning signals underlying\nconversation data, such as conversation thread patterns and repeated engagement\nof target users, which help better understand the behavior of target users in\nconversations. In this paper, we propose three interesting and well-founded\nauxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn\nAuthorship, as the self-supervised signals for re-entry prediction. These\nauxiliary tasks are trained together with the main task in a multi-task manner.\nExperimental results on two datasets newly collected from Twitter and Reddit\nshow that our method outperforms the previous state-of-the-arts with fewer\nparameters and faster convergence. Extensive experiments and analysis show the\neffectiveness of our proposed models and also point out some key ideas in\ndesigning self-supervised tasks.",
          "link": "http://arxiv.org/abs/2109.02020",
          "publishedOn": "2021-09-07T07:20:13.457Z",
          "wordCount": null,
          "title": "Re-entry Prediction for Online Conversations via Self-Supervised Learning. (arXiv:2109.02020v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_B/0/1/0/all/0/1\">Batool Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>",
          "description": "Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have\nled to significant performance gains on a wide range of cross-lingual NLP\ntasks, success on many downstream tasks still relies on the availability of\nsufficient annotated data. Traditional fine-tuning of pre-trained models using\nonly a few target samples can cause over-fitting. This can be quite limiting as\nmost languages in the world are under-resourced. In this work, we investigate\ncross-lingual adaptation using a simple nearest neighbor few-shot (<15 samples)\ninference technique for classification tasks. We experiment using a total of 16\ndistinct languages across two NLP tasks- XNLI and PAWS-X. Our approach\nconsistently improves traditional fine-tuning using only a handful of labeled\nsamples in target locales. We also demonstrate its generalization capability\nacross tasks.",
          "link": "http://arxiv.org/abs/2109.02221",
          "publishedOn": "2021-09-07T07:20:13.425Z",
          "wordCount": null,
          "title": "Nearest Neighbour Few-Shot Learning for Cross-lingual Classification. (arXiv:2109.02221v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "Learning multilingual and multi-domain translation model is challenging as\nthe heterogeneous and imbalanced data make the model converge inconsistently\nover different corpora in real world. One common practice is to adjust the\nshare of each corpus in the training, so that the learning process is balanced\nand low-resource cases can benefit from the high resource ones. However,\nautomatic balancing methods usually depend on the intra- and inter-dataset\ncharacteristics, which is usually agnostic or requires human priors. In this\nwork, we propose an approach, MultiUAT, that dynamically adjusts the training\ndata usage based on the model's uncertainty on a small set of trusted clean\ndata for multi-corpus machine translation. We experiments with two classes of\nuncertainty measures on multilingual (16 languages with 4 settings) and\nmulti-domain settings (4 for in-domain and 2 for out-of-domain on\nEnglish-German translation) and demonstrate our approach MultiUAT substantially\noutperforms its baselines, including both static and dynamic strategies. We\nanalyze the cross-domain transfer and show the deficiency of static and\nsimilarity based methods.",
          "link": "http://arxiv.org/abs/2109.02284",
          "publishedOn": "2021-09-07T07:20:13.418Z",
          "wordCount": null,
          "title": "Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training. (arXiv:2109.02284v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Liqun Ma</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Junnan Zhu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Lu Xiang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Zong_C/0/1/0/all/0/1\">Chengqing Zong</a> (1 and 2) ((1) National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China, (2) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, (3) Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China)",
          "description": "Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.",
          "link": "http://arxiv.org/abs/2108.13139",
          "publishedOn": "2021-09-07T07:20:13.345Z",
          "wordCount": null,
          "title": "CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DuSell_B/0/1/0/all/0/1\">Brian DuSell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>",
          "description": "Learning hierarchical structures in sequential data -- from simple\nalgorithmic patterns to natural language -- in a reliable, generalizable way\nremains a challenging problem for neural language models. Past work has shown\nthat recurrent neural networks (RNNs) struggle to generalize on held-out\nalgorithmic or syntactic patterns without supervision or some inductive bias.\nTo remedy this, many papers have explored augmenting RNNs with various\ndifferentiable stacks, by analogy with finite automata and pushdown automata.\nIn this paper, we present a stack RNN model based on the recently proposed\nNondeterministic Stack RNN (NS-RNN) that achieves lower cross-entropy than all\nprevious stack RNNs on five context-free language modeling tasks (within 0.05\nnats of the information-theoretic lower bound), including a task in which the\nNS-RNN previously failed to outperform a deterministic stack RNN baseline. Our\nmodel assigns arbitrary positive weights instead of probabilities to stack\nactions, and we provide an analysis of why this improves training. We also\npropose a restricted version of the NS-RNN that makes it practical to use for\nlanguage modeling on natural language and present results on the Penn Treebank\ncorpus.",
          "link": "http://arxiv.org/abs/2109.01982",
          "publishedOn": "2021-09-07T07:20:13.255Z",
          "wordCount": null,
          "title": "Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gonzalez_N/0/1/0/all/0/1\">Nurudin Alvarez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.",
          "link": "http://arxiv.org/abs/2109.01900",
          "publishedOn": "2021-09-07T07:20:13.187Z",
          "wordCount": null,
          "title": "Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.",
          "link": "http://arxiv.org/abs/2109.00840",
          "publishedOn": "2021-09-07T07:20:13.187Z",
          "wordCount": null,
          "title": "Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01958",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wanyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yangfeng Ji</a>",
          "description": "Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.",
          "link": "http://arxiv.org/abs/2109.01958",
          "publishedOn": "2021-09-07T07:20:13.172Z",
          "wordCount": null,
          "title": "SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks. (arXiv:2109.01958v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perry_T/0/1/0/all/0/1\">Tal Perry</a>",
          "description": "Text annotation tools assume that their user's goal is to create a labeled\ncorpus. However, users view annotation as a necessary evil on the way to\ndeliver business value through NLP. Thus an annotation tool should optimize for\nthe throughput of the global NLP process, not only the productivity of\nindividual annotators. LightTag is a text annotation tool designed and built on\nthat principle. This paper shares our design rationale, data modeling choices,\nand user interface decisions then illustrates how those choices serve the full\nNLP lifecycle.",
          "link": "http://arxiv.org/abs/2109.02320",
          "publishedOn": "2021-09-07T07:20:13.111Z",
          "wordCount": 520,
          "title": "LightTag: Text Annotation Platform. (arXiv:2109.02320v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_K/0/1/0/all/0/1\">Kailong Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Botao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "Distantly supervised relation extraction (RE) automatically aligns\nunstructured text with relation instances in a knowledge base (KB). Due to the\nincompleteness of current KBs, sentences implying certain relations may be\nannotated as N/A instances, which causes the so-called false negative (FN)\nproblem. Current RE methods usually overlook this problem, inducing improper\nbiases in both training and testing procedures. To address this issue, we\npropose a two-stage approach. First, it finds out possible FN samples by\nheuristically leveraging the memory mechanism of deep neural networks. Then, it\naligns those unlabeled data with the training data into a unified feature space\nby adversarial training to assign pseudo labels and further utilize the\ninformation contained in them. Experiments on two wildly-used benchmark\ndatasets demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2109.02099",
          "publishedOn": "2021-09-07T07:20:13.088Z",
          "wordCount": null,
          "title": "Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction. (arXiv:2109.02099v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ.",
          "link": "http://arxiv.org/abs/2104.07858",
          "publishedOn": "2021-09-07T07:20:12.831Z",
          "wordCount": 625,
          "title": "Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_G/0/1/0/all/0/1\">Go Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baimukan_N/0/1/0/all/0/1\">Nurpeiis Baimukan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouamor_H/0/1/0/all/0/1\">Houda Bouamor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habash_N/0/1/0/all/0/1\">Nizar Habash</a>",
          "description": "In this paper, we explore the effects of language variants, data sizes, and\nfine-tuning task types in Arabic pre-trained language models. To do so, we\nbuild three pre-trained language models across three variants of Arabic: Modern\nStandard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a\nfourth language model which is pre-trained on a mix of the three. We also\nexamine the importance of pre-training data size by building additional models\nthat are pre-trained on a scaled-down set of the MSA variant. We compare our\ndifferent models to each other, as well as to eight publicly available models\nby fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest\nthat the variant proximity of pre-training data to fine-tuning data is more\nimportant than the pre-training data size. We exploit this insight in defining\nan optimized system selection model for the studied tasks.",
          "link": "http://arxiv.org/abs/2103.06678",
          "publishedOn": "2021-09-07T07:20:12.541Z",
          "wordCount": 632,
          "title": "The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models. (arXiv:2103.06678v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.",
          "link": "http://arxiv.org/abs/2109.01924",
          "publishedOn": "2021-09-07T07:20:12.408Z",
          "wordCount": null,
          "title": "A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14638",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paula_A/0/1/0/all/0/1\">Amauri J Paula</a>",
          "description": "It is presented here a machine learning-based (ML) natural language\nprocessing (NLP) approach capable to automatically recognize and extract\ncategorical and numerical parameters from a corpus of articles. The approach\n(named a.RIX) operates with a concomitant/interchangeable use of ML models such\nas neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes\nclassifiers (NBC), and a pattern recognition model using regular expression\n(REGEX). A corpus of 7,873 scientific articles dealing with natural products\n(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine\nautomatically extracts categorical and numerical parameters such as (i) the\nplant species from which active molecules are extracted, (ii) the\nmicroorganisms species for which active molecules can act against, and (iii)\nthe values of minimum inhibitory concentration (MIC) against these\nmicroorganisms. The parameters are extracted without part-of-speech tagging\n(POS) and named entity recognition (NER) approaches (i.e. without the need of\ntext annotation), and the models training is performed with unsupervised\napproaches. In this way, a.RIX can be essentially used on articles from any\nscientific field. Finally, it can potentially make obsolete the current article\nreviewing process in some areas, especially those in which machine learning\nmodels capture texts structure, text semantics, and latent knowledge.",
          "link": "http://arxiv.org/abs/2107.14638",
          "publishedOn": "2021-09-07T07:20:12.404Z",
          "wordCount": null,
          "title": "An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1\">Will E. Hipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>",
          "description": "Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.",
          "link": "http://arxiv.org/abs/2103.01345",
          "publishedOn": "2021-09-07T07:20:12.393Z",
          "wordCount": null,
          "title": "Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1106.0107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jomy_J/0/1/0/all/0/1\">John Jomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_K/0/1/0/all/0/1\">K. V. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_B/0/1/0/all/0/1\">Balakrishnan Kannan</a>",
          "description": "Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.",
          "link": "http://arxiv.org/abs/1106.0107",
          "publishedOn": "2021-09-07T07:20:12.392Z",
          "wordCount": null,
          "title": "Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-09-07T07:20:12.391Z",
          "wordCount": null,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>",
          "description": "Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.",
          "link": "http://arxiv.org/abs/2109.00430",
          "publishedOn": "2021-09-07T07:20:12.385Z",
          "wordCount": null,
          "title": "M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>",
          "description": "Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.",
          "link": "http://arxiv.org/abs/2108.13875",
          "publishedOn": "2021-09-07T07:20:12.383Z",
          "wordCount": null,
          "title": "When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dr. Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dr. Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1\">Dr. Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1\">Dr. Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1\">Dr. Arjun Mukherjee</a>",
          "description": "Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.",
          "link": "http://arxiv.org/abs/2108.00270",
          "publishedOn": "2021-09-07T07:20:12.361Z",
          "wordCount": null,
          "title": "Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Luan Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "The rise of social media has led to the increasing of comments on online\nforums. However, there still exists invalid comments which are not informative\nfor users. Moreover, those comments are also quite toxic and harmful to people.\nIn this paper, we create a dataset for constructive and toxic speech detection,\nnamed UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)\nwith 10,000 human-annotated comments. For these tasks, we propose a system for\nconstructive and toxic speech detection with the state-of-the-art transfer\nlearning model in Vietnamese NLP as PhoBERT. With this system, we obtain\nF1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,\nrespectively. Besides, we implement various baseline models as traditional\nMachine Learning and Deep Neural Network-Based models to evaluate the dataset.\nWith the results, we can solve several tasks on the online discussions and\ndevelop the framework for identifying constructiveness and toxicity of\nVietnamese social media comments automatically.",
          "link": "http://arxiv.org/abs/2103.10069",
          "publishedOn": "2021-09-07T07:20:12.357Z",
          "wordCount": null,
          "title": "Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese. (arXiv:2103.10069v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1\">Ibrahim Faruk Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calik_N/0/1/0/all/0/1\">Necmettin Bera Calik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yapucuoglu_M/0/1/0/all/0/1\">Mert Yapucuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uluslu_A/0/1/0/all/0/1\">Ahmet Yavuz Uluslu</a>",
          "description": "We introduce Hocalarim (MyProfessors), the largest student review dataset\navailable for the Turkish language. It consists of over 5000 professor reviews\nleft online by students, with different aspects of education rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset and present its\nstatistics. We examine the impact of students' institution type on their\nratings and the correlation of students' bias to give positive or negative\nfeedback.",
          "link": "http://arxiv.org/abs/2109.02325",
          "publishedOn": "2021-09-07T07:20:12.355Z",
          "wordCount": null,
          "title": "Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-09-07T07:20:12.348Z",
          "wordCount": null,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1\">ChengXiang Zhai</a>",
          "description": "Biomedical entity linking is the task of linking entity mentions in a\nbiomedical document to referent entities in a knowledge base. Recently, many\nBERT-based models have been introduced for the task. While these models have\nachieved competitive results on many datasets, they are computationally\nexpensive and contain about 110M parameters. Little is known about the factors\ncontributing to their impressive performance and whether the\nover-parameterization is needed. In this work, we shed some light on the inner\nworking mechanisms of these large BERT-based models. Through a set of probing\nexperiments, we have found that the entity linking performance only changes\nslightly when the input word order is shuffled or when the attention scope is\nlimited to a fixed window size. From these observations, we propose an\nefficient convolutional neural network with residual connections for biomedical\nentity linking. Because of the sparse connectivity and weight sharing\nproperties, our model has a small number of parameters and is highly efficient.\nOn five public datasets, our model achieves comparable or even better linking\naccuracy than the state-of-the-art BERT-based models while having about 60\ntimes fewer parameters.",
          "link": "http://arxiv.org/abs/2109.02237",
          "publishedOn": "2021-09-07T07:20:12.249Z",
          "wordCount": null,
          "title": "BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks. (arXiv:2109.02237v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Sentence order prediction is the task of finding the correct order of\nsentences in a randomly ordered document. Correctly ordering the sentences\nrequires an understanding of coherence with respect to the chronological\nsequence of events described in the text. Document-level contextual\nunderstanding and commonsense knowledge centered around these events are often\nessential in uncovering this coherence and predicting the exact chronological\norder. In this paper, we introduce STaCK -- a framework based on graph neural\nnetworks and temporal commonsense knowledge to model global information and\npredict the relative order of sentences. Our graph network accumulates temporal\nevidence using knowledge of `past' and `future' and formulates sentence\nordering as a constrained edge classification problem. We report results on\nfive different datasets, and empirically show that the proposed method is\nnaturally suitable for order prediction. The implementation of this work is\npublicly available at: https://github.com/declare-lab/sentence-ordering.",
          "link": "http://arxiv.org/abs/2109.02247",
          "publishedOn": "2021-09-07T07:20:12.237Z",
          "wordCount": null,
          "title": "STaCK: Sentence Ordering with Temporal Commonsense Knowledge. (arXiv:2109.02247v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tambe_T/0/1/0/all/0/1\">Thierry Tambe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooper_C/0/1/0/all/0/1\">Coleman Hooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentecost_L/0/1/0/all/0/1\">Lillian Pentecost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1\">Tianyu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">En-Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_M/0/1/0/all/0/1\">Marco Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul N. Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>",
          "description": "Transformer-based language models such as BERT provide significant accuracy\nimprovement for a multitude of natural language processing (NLP) tasks.\nHowever, their hefty computational and memory demands make them challenging to\ndeploy to resource-constrained edge platforms with strict latency requirements.\nWe present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware\nenergy optimization for multi-task NLP. EdgeBERT employs entropy-based early\nexit predication in order to perform dynamic voltage-frequency scaling (DVFS),\nat a sentence granularity, for minimal energy consumption while adhering to a\nprescribed target latency. Computation and memory footprint overheads are\nfurther alleviated by employing a calibrated combination of adaptive attention\nspan, selective network pruning, and floating-point quantization. Furthermore,\nin order to maximize the synergistic benefits of these algorithms in always-on\nand intermediate edge computing settings, we specialize a 12nm scalable\nhardware accelerator system, integrating a fast-switching low-dropout voltage\nregulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,\nhigh-density embedded non-volatile memories (eNVMs) wherein the sparse\nfloating-point bit encodings of the shared multi-task parameters are carefully\nstored. Altogether, latency-aware multi-task NLP inference acceleration on the\nEdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy\ncompared to the conventional inference without early stopping, the\nlatency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson\nTegra X2 mobile GPU, respectively.",
          "link": "http://arxiv.org/abs/2011.14203",
          "publishedOn": "2021-09-07T07:20:11.760Z",
          "wordCount": null,
          "title": "EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. (arXiv:2011.14203v5 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sakhrani_H/0/1/0/all/0/1\">Harsh Sakhrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Saloni Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratadiya_P/0/1/0/all/0/1\">Pratik Ratadiya</a>",
          "description": "Question Paraphrase Identification (QPI) is a critical task for large-scale\nQuestion-Answering forums. The purpose of QPI is to determine whether a given\npair of questions are semantically identical or not. Previous approaches for\nthis task have yielded promising results, but have often relied on complex\nrecurrence mechanisms that are expensive and time-consuming in nature. In this\npaper, we propose a novel architecture combining a Bidirectional Transformer\nEncoder with Convolutional Neural Networks for the QPI task. We produce the\npredictions from the proposed architecture using two different inference\nsetups: Siamese and Matched Aggregation. Experimental results demonstrate that\nour model achieves state-of-the-art performance on the Quora Question Pairs\ndataset. We empirically prove that the addition of convolution layers to the\nmodel architecture improves the results in both inference setups. We also\ninvestigate the impact of partial and complete fine-tuning and analyze the\ntrade-off between computational power and accuracy in the process. Based on the\nobtained results, we conclude that the Matched-Aggregation setup consistently\noutperforms the Siamese setup. Our work provides insights into what\narchitecture combinations and setups are likely to produce better results for\nthe QPI task.",
          "link": "http://arxiv.org/abs/2109.01560",
          "publishedOn": "2021-09-07T07:20:11.755Z",
          "wordCount": null,
          "title": "Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourgeois_F/0/1/0/all/0/1\">Florence T. Bourgeois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_A/0/1/0/all/0/1\">Adam G. Dunn</a>",
          "description": "The rapid growth in published clinical trials makes it difficult to maintain\nup-to-date systematic reviews, which requires finding all relevant trials. This\nleads to policy and practice decisions based on out-of-date, incomplete, and\nbiased subsets of available clinical evidence. Extracting and then normalising\nPopulation, Intervention, Comparator, and Outcome (PICO) information from\nclinical trial articles may be an effective way to automatically assign trials\nto systematic reviews and avoid searching and screening - the two most\ntime-consuming systematic review processes. We propose and test a novel\napproach to PICO span detection. The major difference between our proposed\nmethod and previous approaches comes from detecting spans without needing\nannotated span data and using only crowdsourced sentence-level annotations.\nExperiments on two datasets show that PICO span detection results achieve much\nhigher results for recall when compared to fully supervised methods with PICO\nsentence detection at least as good as human annotations. By removing the\nreliance on expert annotations for span detection, this work could be used in\nhuman-machine pipeline for turning low-quality crowdsourced, and sentence-level\nPICO annotations into structured information that can be used to quickly assign\ntrials to relevant systematic reviews.",
          "link": "http://arxiv.org/abs/2109.02254",
          "publishedOn": "2021-09-07T07:20:11.748Z",
          "wordCount": null,
          "title": "Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations. (arXiv:2109.02254v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_G/0/1/0/all/0/1\">Gon&#xe7;alo Simoes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_R/0/1/0/all/0/1\">Ryan McDonald</a>",
          "description": "We introduce a simple but flexible mechanism to learn an intermediate plan to\nground the generation of abstractive summaries. Specifically, we prepend (or\nprompt) target summaries with entity chains -- ordered sequences of entities\nmentioned in the summary. Transformer-based sequence-to-sequence models are\nthen trained to generate the entity chain and then continue generating the\nsummary conditioned on the entity chain and the input. We experimented with\nboth pretraining and finetuning with this content planning objective. When\nevaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate\nempirically that the grounded generation with the planning objective improves\nentity specificity and planning in summaries for all datasets, and achieves\nstate-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we\ndemonstrate empirically that planning with entity chains provides a mechanism\nto control hallucinations in abstractive summaries. By prompting the decoder\nwith a modified content plan that drops hallucinated entities, we outperform\nstate-of-the-art approaches for faithfulness when evaluated automatically and\nby humans.",
          "link": "http://arxiv.org/abs/2104.07606",
          "publishedOn": "2021-09-07T07:20:11.670Z",
          "wordCount": null,
          "title": "Planning with Learned Entity Prompts for Abstractive Summarization. (arXiv:2104.07606v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>",
          "description": "In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.",
          "link": "http://arxiv.org/abs/2108.12202",
          "publishedOn": "2021-09-07T07:20:11.669Z",
          "wordCount": null,
          "title": "A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>",
          "description": "Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.",
          "link": "http://arxiv.org/abs/2104.08773",
          "publishedOn": "2021-09-07T07:20:11.661Z",
          "wordCount": null,
          "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shengcai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1\">Ning Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>",
          "description": "Over the past few years, various word-level textual attack approaches have\nbeen proposed to reveal the vulnerability of deep neural networks used in\nnatural language processing. Typically, these approaches involve an important\noptimization step to determine which substitute to be used for each word in the\noriginal input. However, current research on this step is still rather limited,\nfrom the perspectives of both problem-understanding and problem-solving. In\nthis paper, we address these issues by uncovering the theoretical properties of\nthe problem and proposing an efficient local search algorithm (LS) to solve it.\nWe establish the first provable approximation guarantee on solving the problem\nin general cases. Notably, for adversarial textual attack, it is even better\nthan the previous bound which only holds in special case. Extensive experiments\ninvolving five NLP tasks, six datasets and eleven NLP models show that LS can\nlargely reduce the number of queries usually by an order of magnitude to\nachieve high attack success rates. Further experiments show that the\nadversarial examples crafted by LS usually have higher quality, exhibit better\ntransferability, and can bring more robustness improvement to victim models by\nadversarial training.",
          "link": "http://arxiv.org/abs/2109.02229",
          "publishedOn": "2021-09-07T07:20:11.613Z",
          "wordCount": null,
          "title": "Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiao-Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>",
          "description": "Numerical reasoning skills are essential for complex question answering (CQA)\nover text. It requires opertaions including counting, comparison, addition and\nsubtraction. A successful approach to CQA on text, Neural Module Networks\n(NMNs), follows the programmer-interpreter paradigm and leverages specialised\nmodules to perform compositional reasoning. However, the NMNs framework does\nnot consider the relationship between numbers and entities in both questions\nand paragraphs. We propose effective techniques to improve NMNs' numerical\nreasoning capabilities by making the interpreter question-aware and capturing\nthe relationship between entities and numbers. On the same subset of the DROP\ndataset for CQA on text, experimental results show that our additions\noutperform the original NMNs by 3.0 points for the overall F1 score.",
          "link": "http://arxiv.org/abs/2109.02289",
          "publishedOn": "2021-09-07T07:20:11.607Z",
          "wordCount": null,
          "title": "Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text. (arXiv:2109.02289v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.16413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shankai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>",
          "description": "The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.",
          "link": "http://arxiv.org/abs/2010.16413",
          "publishedOn": "2021-09-07T07:20:11.597Z",
          "wordCount": null,
          "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuster_R/0/1/0/all/0/1\">Roei Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "Feed-forward layers constitute two-thirds of a transformer model's\nparameters, yet their role in the network remains under-explored. We show that\nfeed-forward layers in transformer-based language models operate as key-value\nmemories, where each key correlates with textual patterns in the training\nexamples, and each value induces a distribution over the output vocabulary. Our\nexperiments show that the learned patterns are human-interpretable, and that\nlower layers tend to capture shallow patterns, while upper layers learn more\nsemantic ones. The values complement the keys' input patterns by inducing\noutput distributions that concentrate probability mass on tokens likely to\nappear immediately after each pattern, particularly in the upper layers.\nFinally, we demonstrate that the output of a feed-forward layer is a\ncomposition of its memories, which is subsequently refined throughout the\nmodel's layers via residual connections to produce the final output\ndistribution.",
          "link": "http://arxiv.org/abs/2012.14913",
          "publishedOn": "2021-09-07T07:20:11.566Z",
          "wordCount": null,
          "title": "Transformer Feed-Forward Layers Are Key-Value Memories. (arXiv:2012.14913v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>",
          "description": "This paper demonstrates that by fine-tuning an autoregressive language model\n(GPT-Neo) on appropriately structured step-by-step demonstrations, it is\npossible to teach it to execute a mathematical task that has previously proved\ndifficult for Transformers - longhand modulo operations - with a relatively\nsmall number of examples. Specifically, we fine-tune GPT-Neo to solve the\nnumbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et\nal. (arXiv:1904.01557) reported below 40% accuracy on this task with 2 million\ntraining examples. We show that after fine-tuning on 200 appropriately\nstructured demonstrations of solving long division problems and reporting the\nremainders, the smallest available GPT-Neo model achieves over 80% accuracy.\nThis is achieved by constructing an appropriate dataset for fine-tuning, with\nno changes to the learning algorithm. These results suggest that fine-tuning\nautoregressive language models on small sets of well-crafted demonstrations may\nbe a useful paradigm for enabling individuals without training in machine\nlearning to coax such models to perform some kinds of complex multi-step tasks.",
          "link": "http://arxiv.org/abs/2109.02102",
          "publishedOn": "2021-09-07T07:20:11.526Z",
          "wordCount": null,
          "title": "Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1\">Joshua Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_Zahedi_K/0/1/0/all/0/1\">Keyan Ghazi-Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1\">Jean-Gabriel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_Dufresne_L/0/1/0/all/0/1\">Laurent H&#xe9;bert-Dufresne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galesic_M/0/1/0/all/0/1\">Mirta Galesic</a>",
          "description": "Citizen-generated counter speech is a promising way to fight hate speech and\npromote peaceful, non-polarized discourse. However, there is a lack of\nlarge-scale longitudinal studies of its effectiveness for reducing hate speech.\nTo this end, we perform an exploratory analysis of the effectiveness of counter\nspeech using several different macro- and micro-level measures to analyze\n180,000 political conversations that took place on German Twitter over four\nyears. We report on the dynamic interactions of hate and counter speech over\ntime and provide insights into whether, as in `classic' bullying situations,\norganized efforts are more effective than independent individuals in steering\nonline discourse. Taken together, our results build a multifaceted picture of\nthe dynamics of hate and counter speech online. While we make no causal claims\ndue to the complexity of discourse dynamics, our findings suggest that\norganized hate speech is associated with changes in public discourse and that\ncounter speech -- especially when organized -- may help curb hateful rhetoric\nin online discourse.",
          "link": "http://arxiv.org/abs/2009.08392",
          "publishedOn": "2021-09-07T07:20:11.485Z",
          "wordCount": null,
          "title": "Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07829",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">C.M. Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levow_G/0/1/0/all/0/1\">Gina-Anne Levow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinert_Threlkeld_S/0/1/0/all/0/1\">Shane Steinert-Threlkeld</a>",
          "description": "Segmentation remains an important preprocessing step both in languages where\n\"words\" or other important syntactic/semantic units (like morphemes) are not\nclearly delineated by white space, as well as when dealing with continuous\nspeech data, where there is often no meaningful pause between words.\nNear-perfect supervised methods have been developed for use in resource-rich\nlanguages such as Chinese, but many of the world's languages are both\nmorphologically complex, and have no large dataset of \"gold\" segmentations into\nmeaningful units. To solve this problem, we propose a new type of Segmental\nLanguage Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)\nfor use in both unsupervised and lightly supervised segmentation tasks. We\nintroduce a Masked Segmental Language Model (MSLM) built on a span-masking\ntransformer architecture, harnessing the power of a bi-directional masked\nmodeling context and attention. In a series of experiments, our model\nconsistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation\nquality, and performs similarly to the Recurrent model on English (PTB). We\nconclude by discussing the different challenges posed in segmenting\nphonemic-type writing systems.",
          "link": "http://arxiv.org/abs/2104.07829",
          "publishedOn": "2021-09-07T07:20:10.864Z",
          "wordCount": null,
          "title": "A Masked Segmental Language Model for Unsupervised Natural Language Segmentation. (arXiv:2104.07829v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_L/0/1/0/all/0/1\">Leonardo Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1\">Thamar Solorio</a>",
          "description": "Current work in named entity recognition (NER) shows that data augmentation\ntechniques can produce more robust models. However, most existing techniques\nfocus on augmenting in-domain data in low-resource scenarios where annotated\ndata is quite limited. In contrast, we study cross-domain data augmentation for\nthe NER task. We investigate the possibility of leveraging data from\nhigh-resource domains by projecting it into the low-resource domains.\nSpecifically, we propose a novel neural architecture to transform the data\nrepresentation from a high-resource to a low-resource domain by learning the\npatterns (e.g. style, noise, abbreviations, etc.) in the text that\ndifferentiate them and a shared feature space where both domains are aligned.\nWe experiment with diverse datasets and show that transforming the data to the\nlow-resource domain representation achieves significant improvements over only\nusing data from high-resource domains.",
          "link": "http://arxiv.org/abs/2109.01758",
          "publishedOn": "2021-09-07T07:20:10.017Z",
          "wordCount": 580,
          "title": "Data Augmentation for Cross-Domain Named Entity Recognition. (arXiv:2109.01758v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freire_G/0/1/0/all/0/1\">Guilherme Freire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ive_J/0/1/0/all/0/1\">Julia Ive</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>",
          "description": "Contextualised word embeddings is a powerful tool to detect contextual\nsynonyms. However, most of the current state-of-the-art (SOTA) deep learning\nconcept extraction methods remain supervised and underexploit the potential of\nthe context. In this paper, we propose a self-supervised pre-training approach\nwhich is able to detect contextual synonyms of concepts being training on the\ndata created by shallow matching. We apply our methodology in the sparse\nmulti-class setting (over 15,000 concepts) to extract phenotype information\nfrom electronic health records. We further investigate data augmentation\ntechniques to address the problem of the class sparsity. Our approach achieves\na new SOTA for the unsupervised phenotype concept annotation on clinical text\non F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and\n4.0 absolute points, respectively. After fine-tuning with as little as 20\\% of\nthe labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic\nevaluation on three ICU benchmarks also shows the benefit of using the\nphenotypes annotated by our model as features.",
          "link": "http://arxiv.org/abs/2109.01935",
          "publishedOn": "2021-09-07T07:20:10.008Z",
          "wordCount": 632,
          "title": "Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case. (arXiv:2109.01935v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>",
          "description": "In recent years, neural paraphrase generation based on Seq2Seq has achieved\nsuperior performance, however, the generated paraphrase still has the problem\nof lack of diversity. In this paper, we focus on improving the diversity\nbetween the generated paraphrase and the original sentence, i.e., making\ngenerated paraphrase different from the original sentence as much as possible.\nWe propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),\nwhich leverages multi-round paraphrase generation to improve diversity and\nemploys back-translation to preserve semantic information. We evaluate BTmPG on\ntwo benchmark datasets. Both automatic and human evaluation show BTmPG can\nimprove the diversity of paraphrase while preserving the semantics of the\noriginal sentence.",
          "link": "http://arxiv.org/abs/2109.01862",
          "publishedOn": "2021-09-07T07:20:09.988Z",
          "wordCount": 574,
          "title": "Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach. (arXiv:2109.01862v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_A/0/1/0/all/0/1\">Amir Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homayounpour_M/0/1/0/all/0/1\">Mohammad Mehdi Homayounpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>",
          "description": "Many endeavors have sought to develop countermeasure techniques as\nenhancements on Automatic Speaker Verification (ASV) systems, in order to make\nthem more robust against spoof attacks. As evidenced by the latest ASVspoof\n2019 countermeasure challenge, models currently deployed for the task of ASV\nare, at their best, devoid of suitable degrees of generalization to unseen\nattacks. Upon further investigation of the proposed methods, it appears that a\nbroader three-tiered view of the proposed systems. comprised of the classifier,\nfeature extraction phase, and model loss function, may to some extent lessen\nthe problem. Accordingly, the present study proposes the Efficient Attention\nBranch Network (EABN) modular architecture with a combined loss function to\naddress the generalization problem...",
          "link": "http://arxiv.org/abs/2109.02051",
          "publishedOn": "2021-09-07T07:20:09.980Z",
          "wordCount": 581,
          "title": "Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Masked language modeling (MLM), a self-supervised pretraining objective, is\nwidely used in natural language processing for learning text representations.\nMLM trains a model to predict a random sample of input tokens that have been\nreplaced by a [MASK] placeholder in a multi-class setting over the entire\nvocabulary. When pretraining, it is common to use alongside MLM other auxiliary\nobjectives on the token or sequence level to improve downstream performance\n(e.g. next sentence prediction). However, no previous work so far has attempted\nin examining whether other simpler linguistically intuitive or not objectives\ncan be used standalone as main pretraining objectives. In this paper, we\nexplore five simple pretraining objectives based on token-level classification\ntasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our\nproposed methods achieve comparable or better performance to MLM using a\nBERT-BASE architecture. We further validate our methods using smaller models,\nshowing that pretraining a model with 41% of the BERT-BASE's parameters,\nBERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.",
          "link": "http://arxiv.org/abs/2109.01819",
          "publishedOn": "2021-09-07T07:20:09.970Z",
          "wordCount": 626,
          "title": "Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.",
          "link": "http://arxiv.org/abs/2109.01934",
          "publishedOn": "2021-09-07T07:20:09.958Z",
          "wordCount": 643,
          "title": "Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>",
          "description": "Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR",
          "link": "http://arxiv.org/abs/2109.01949",
          "publishedOn": "2021-09-07T07:20:09.947Z",
          "wordCount": 618,
          "title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fofadiya_D/0/1/0/all/0/1\">Darshan Fofadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_P/0/1/0/all/0/1\">Prathap Ramachandra</a>",
          "description": "Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for <0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.",
          "link": "http://arxiv.org/abs/2109.01754",
          "publishedOn": "2021-09-07T07:20:09.935Z",
          "wordCount": 612,
          "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yingqiang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Juntao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>",
          "description": "While recent years have witnessed the emergence of various explainable\nmethods in machine learning, to what degree the explanations really represent\nthe reasoning process behind the model prediction -- namely, the faithfulness\nof explanation -- is still an open problem. One commonly used way to measure\nfaithfulness is \\textit{erasure-based} criteria. Though conceptually simple,\nerasure-based criterion could inevitably introduce biases and artifacts. We\npropose a new methodology to evaluate the faithfulness of explanations from the\n\\textit{counterfactual reasoning} perspective: the model should produce\nsubstantially different outputs for the original input and its corresponding\ncounterfactual edited on a faithful feature. Specially, we introduce two\nalgorithms to find the proper counterfactuals in both discrete and continuous\nscenarios and then use the acquired counterfactuals to measure faithfulness.\nEmpirical results on several datasets show that compared with existing metrics,\nour proposed counterfactual evaluation method can achieve top correlation with\nthe ground truth under diffe",
          "link": "http://arxiv.org/abs/2109.01962",
          "publishedOn": "2021-09-07T07:20:09.926Z",
          "wordCount": 593,
          "title": "Counterfactual Evaluation for Explainable AI. (arXiv:2109.01962v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Leandro Rodrigues de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>",
          "description": "Pretrained multilingual models have become a de facto default approach for\nzero-shot cross-lingual transfer. Previous work has shown that these models are\nable to achieve cross-lingual representations when pretrained on two or more\nlanguages with shared parameters. In this work, we provide evidence that a\nmodel can achieve language-agnostic representations even when pretrained on a\nsingle language. That is, we find that monolingual models pretrained and\nfinetuned on different languages achieve competitive performance compared to\nthe ones that use the same target language. Surprisingly, the models show a\nsimilar performance on a same task regardless of the pretraining language. For\nexample, models pretrained on distant languages such as German and Portuguese\nperform similarly on English tasks.",
          "link": "http://arxiv.org/abs/2109.01942",
          "publishedOn": "2021-09-07T07:20:09.918Z",
          "wordCount": 560,
          "title": "On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.",
          "link": "http://arxiv.org/abs/2109.01839",
          "publishedOn": "2021-09-07T07:20:09.899Z",
          "wordCount": 649,
          "title": "Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>",
          "description": "Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.",
          "link": "http://arxiv.org/abs/2109.02040",
          "publishedOn": "2021-09-07T07:20:09.882Z",
          "wordCount": 635,
          "title": "Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>",
          "description": "How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.",
          "link": "http://arxiv.org/abs/2109.01815",
          "publishedOn": "2021-09-07T07:20:09.860Z",
          "wordCount": 686,
          "title": "Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_M/0/1/0/all/0/1\">Manohar Kaul</a>",
          "description": "Active learning has emerged as a standard paradigm in areas with scarcity of\nlabeled training data, such as in the medical domain. Language models have\nemerged as the prevalent choice of several natural language tasks due to the\nperformance boost offered by these models. However, in several domains, such as\nmedicine, the scarcity of labeled training data is a common issue. Also, these\nmodels may not work well in cases where class imbalance is prevalent. Active\nlearning may prove helpful in these cases to boost the performance with a\nlimited label budget. To this end, we propose a novel method using sampling\ntechniques based on submodular optimization and optimal transport for active\nlearning in language models, dubbed ALLWAS. We construct a sampling strategy\nbased on submodular optimization of the designed objective in the gradient\ndomain. Furthermore, to enable learning from few samples, we propose a novel\nstrategy for sampling from the Wasserstein barycenters. Our empirical\nevaluations on standard benchmark datasets for text classification show that\nour methods perform significantly better (>20% relative increase in some cases)\nthan existing approaches for active learning on language models.",
          "link": "http://arxiv.org/abs/2109.01691",
          "publishedOn": "2021-09-07T07:20:09.599Z",
          "wordCount": 627,
          "title": "ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2106.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nianjin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>",
          "description": "Data association is important in the point cloud registration. In this work,\nwe propose to solve the partial-to-partial registration from a new perspective,\nby introducing multi-level feature interactions between the source and the\nreference clouds at the feature extraction stage, such that the registration\ncan be realized without the attentions or explicit mask estimation for the\noverlapping detection as adopted previously. Specifically, we present FINet, a\nfeature interaction-based structure with the capability to enable and\nstrengthen the information associating between the inputs at multiple stages.\nTo achieve this, we first split the features into two components, one for\nrotation and one for translation, based on the fact that they belong to\ndifferent solution spaces, yielding a dual branches structure. Second, we\ninsert several interaction modules at the feature extractor for the data\nassociation. Third, we propose a transformation sensitivity loss to obtain\nrotation-attentive and translation-attentive features. Experiments demonstrate\nthat our method performs higher precision and robustness compared to the\nstate-of-the-art traditional and learning-based methods. Code will be available\nat https://github.com/HaoXu-Work/FINet.",
          "link": "http://arxiv.org/abs/2106.03479",
          "publishedOn": "2021-09-14T07:20:14.270Z",
          "wordCount": 651,
          "title": "FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>",
          "description": "The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05205",
          "publishedOn": "2021-09-14T07:20:14.085Z",
          "wordCount": 578,
          "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stekovic_S/0/1/0/all/0/1\">Sinisa Stekovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mahdi Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>",
          "description": "We propose a novel method for reconstructing floor plans from noisy 3D point\nclouds. Our main contribution is a principled approach that relies on the Monte\nCarlo Tree Search (MCTS) algorithm to maximize a suitable objective function\nefficiently despite the complexity of the problem. Like previous work, we first\nproject the input point cloud to a top view to create a density map and extract\nroom proposals from it. Our method selects and optimizes the polygonal shapes\nof these room proposals jointly to fit the density map and outputs an accurate\nvectorized floor map even for large complex scenes. To do this, we adapted\nMCTS, an algorithm originally designed to learn to play games, to select the\nroom proposals by maximizing an objective function combining the fitness with\nthe density map as predicted by a deep network and regularizing terms on the\nroom shapes. We also introduce a refinement step to MCTS that adjusts the shape\nof the room proposals. For this step, we propose a novel differentiable method\nfor rendering the polygonal shapes of these proposals. We evaluate our method\non the recent and challenging Structured3D and Floor-SP datasets and show a\nsignificant improvement over the state-of-the-art, without imposing any hard\nconstraints nor assumptions on the floor plan configurations.",
          "link": "http://arxiv.org/abs/2103.11161",
          "publishedOn": "2021-09-14T07:20:14.055Z",
          "wordCount": 702,
          "title": "MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.01361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C. H. Hoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>",
          "description": "This paper investigates an open research task of text-to-image synthesis for\nautomatically generating or manipulating images from text descriptions.\nPrevailing methods mainly use the text as conditions for GAN generation, and\ntrain different models for the text-guided image generation and manipulation\ntasks. In this paper, we propose a novel unified framework of Cycle-consistent\nInverse GAN (CI-GAN) for both text-to-image generation and text-guided image\nmanipulation tasks. Specifically, we first train a GAN model without text\ninput, aiming to generate images with high diversity and quality. Then we learn\na GAN inversion model to convert the images back to the GAN latent space and\nobtain the inverted latent codes for each image, where we introduce the\ncycle-consistency training to learn more robust and consistent inverted latent\ncodes. We further uncover the latent space semantics of the trained GAN model,\nby learning a similarity model between text representations and the latent\ncodes. In the text-guided optimization module, we generate images with the\ndesired semantic attributes by optimizing the inverted latent codes. Extensive\nexperiments on the Recipe1M and CUB datasets validate the efficacy of our\nproposed framework.",
          "link": "http://arxiv.org/abs/2108.01361",
          "publishedOn": "2021-09-14T07:20:14.028Z",
          "wordCount": 663,
          "title": "Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pardo_A/0/1/0/all/0/1\">Alejandro Pardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1\">Fabian Caba Heilbron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Le&#xf3;n Alc&#xe1;zar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Understanding movies and their structural patterns is a crucial task to\ndecode the craft of video editing. While previous works have developed tools\nfor general analysis such as detecting characters or recognizing cinematography\nproperties at the shot level, less effort has been devoted to understanding the\nmost basic video edit, the Cut. This paper introduces the cut type recognition\ntask, which requires modeling of multi-modal information. To ignite research in\nthe new task, we construct a large-scale dataset called MovieCuts, which\ncontains more than 170K videoclips labeled among ten cut types. We benchmark a\nseries of audio-visual approaches, including some that deal with the problem's\nmulti-modal and multi-label nature. Our best model achieves 45.7% mAP, which\nsuggests that the task is challenging and that attaining highly accurate cut\ntype recognition is an open research problem.",
          "link": "http://arxiv.org/abs/2109.05569",
          "publishedOn": "2021-09-14T07:20:13.974Z",
          "wordCount": null,
          "title": "MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ze Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Songzhi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Henry Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kevin Sun</a>",
          "description": "We proposes a method of extracting intrest points and descriptors using\nself-supervised learning method on frame-based event data, which is called\nEventPoint. Different from other feature extraction methods on event data, we\ntrain our model on real event-form driving dataset--DSEC with the\nself-supervised learning method we proposed, the training progress fully\nconsider the characteristics of event data.To verify the effectiveness of our\nwork,we conducted several complete evaluations: we emulated DART and carried\nout feature matching experiments on N-caltech101 dataset, the results shows\nthat the effect of EventPoint is better than DART; We use Vid2e tool provided\nby UZH to convert Oxford robotcar data into event-based format, and combined\nwith INS information provided to carry out the global pose estimation\nexperiment which is important in SLAM. As far as we know, this is the first\nwork to carry out this challenging task.Sufficient experimental data show that\nEventPoint can get better results while achieve real time on CPU.",
          "link": "http://arxiv.org/abs/2109.00210",
          "publishedOn": "2021-09-14T07:20:13.951Z",
          "wordCount": null,
          "title": "EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.09662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lange_B/0/1/0/all/0/1\">Bernard Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Safe and proactive planning in robotic systems generally requires accurate\npredictions of the environment. Prior work on environment prediction applied\nvideo frame prediction techniques to bird's-eye view environment\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\npreviously often result in significant blurring and vanishing of moving\nobjects, thus hindering their applicability for use in safety-critical\napplications. In this work, we propose two extensions to the ConvLSTM to\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\nover baseline architectures on the real-world KITTI and Waymo datasets.",
          "link": "http://arxiv.org/abs/2010.09662",
          "publishedOn": "2021-09-14T07:20:13.940Z",
          "wordCount": null,
          "title": "Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_M/0/1/0/all/0/1\">Mina Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dorigatti_E/0/1/0/all/0/1\">Emilio Dorigatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruegamer_D/0/1/0/all/0/1\">David Ruegamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "One of the most promising approaches for unsupervised learning is combining\ndeep representation learning and deep clustering. Some recent works propose to\nsimultaneously learn representation using deep neural networks and perform\nclustering by defining a clustering loss on top of embedded features. However,\nthese approaches are sensitive to imbalanced data and out-of-distribution\nsamples. Hence, these methods optimize clustering by pushing data close to\nrandomly initialized cluster centers. This is problematic when the number of\ninstances varies largely in different classes or a cluster with few samples has\nless chance to be assigned a good centroid. To overcome these limitations, we\nintroduce StatDEC, a new unsupervised framework for joint statistical\nrepresentation learning and clustering. StatDEC simultaneously trains two deep\nlearning models, a deep statistics network that captures the data distribution,\nand a deep clustering network that learns embedded features and performs\nclustering by explicitly defining a clustering loss. Specifically, the\nclustering network and representation network both take advantage of our\nproposed statistics pooling layer that represents mean, variance, and\ncardinality to handle the out-of-distribution samples as well as a class\nimbalance. Our experiments show that using these representations, one can\nconsiderably improve results on imbalanced image clustering across a variety of\nimage datasets. Moreover, the learned representations generalize well when\ntransferred to the out-of-distribution dataset.",
          "link": "http://arxiv.org/abs/2109.05232",
          "publishedOn": "2021-09-14T07:20:13.727Z",
          "wordCount": null,
          "title": "Learning Statistical Representation with Joint Deep Embedded Clustering. (arXiv:2109.05232v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.07434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ye Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenjing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>",
          "description": "Spatial and channel attentions, modelling the semantic interdependencies in\nspatial and channel dimensions respectively, have recently been widely used for\nsemantic segmentation. However, computing spatial and channel attentions\nseparately sometimes causes errors, especially for those difficult cases. In\nthis paper, we propose Channelized Axial Attention (CAA) to seamlessly\nintegrate channel attention and spatial attention into a single operation with\nnegligible computation overhead. Specifically, we break down the dot-product\noperation of the spatial attention into two parts and insert channel relation\nin between, allowing for independently optimized channel attention on each\nspatial location. We further develop grouped vectorization, which allows our\nmodel to run with very little memory consumption without slowing down the\nrunning speed. Comparative experiments conducted on multiple benchmark\ndatasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate\nthat our CAA outperforms many state-of-the-art segmentation models (including\ndual attention) on all tested datasets.",
          "link": "http://arxiv.org/abs/2101.07434",
          "publishedOn": "2021-09-14T07:20:13.648Z",
          "wordCount": 669,
          "title": "Channelized Axial Attention for Semantic Segmentation -- Considering Channel Relation within Spatial Attention for Semantic Segmentation. (arXiv:2101.07434v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1\">Arantxa Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Careil_M/0/1/0/all/0/1\">Marl&#xe8;ne Careil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>",
          "description": "Generative Adversarial Networks (GANs) can generate near photo realistic\nimages in narrow domains such as human faces. Yet, modeling complex\ndistributions of datasets such as ImageNet and COCO-Stuff remains challenging\nin unconditional settings. In this paper, we take inspiration from kernel\ndensity estimation techniques and introduce a non-parametric approach to\nmodeling distributions of complex datasets. We partition the data manifold into\na mixture of overlapping neighborhoods described by a datapoint and its nearest\nneighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),\nwhich learns the distribution around each datapoint. Experimental results on\nImageNet and COCO-Stuff show that IC-GAN significantly improves over\nunconditional models and unsupervised data partitioning baselines. Moreover, we\nshow that IC-GAN can effortlessly transfer to datasets not seen during training\nby simply changing the conditioning instances, and still generate realistic\nimages. Finally, we extend IC-GAN to the class-conditional case and show\nsemantically controllable generation and competitive quantitative results on\nImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our\ncode and trained models to reproduce the reported results.",
          "link": "http://arxiv.org/abs/2109.05070",
          "publishedOn": "2021-09-14T07:20:13.568Z",
          "wordCount": null,
          "title": "Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Sen Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard Yi Da Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Shiming Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1\">Gaofeng Meng</a>",
          "description": "The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply,\nwhich usually manifests as that the images generated by generators tend to have\na high similarity amongst them, even though their corresponding latent vectors\nhave been very different. In this paper, we introduce a pluggable diversity\npenalty module (DPM) to alleviate mode collapse of GANs. It reduces the\nsimilarity of image pairs in feature space, i.e., if two latent vectors are\ndifferent, then we enforce the generator to generate two images with different\nfeatures. The normalized Gram matrix is used to measure the similarity. We\ncompare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN\n(Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et\nal. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results\nshow that the diversity penalty module can help GAN capture much more modes of\nthe data distribution. Further, in classification tasks, we apply this method\nas image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the\nclassification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared\nwith WGAN GP (Gulrajani et al. 2017), respectively. In domain translation,\ndiversity penalty module can help StarGAN (Choi et al. 2018) generate more\naccurate attention masks and accelarate the convergence process. Finally, we\nquantitatively evaluate the proposed method with IS and FID on CelebA,\nCIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity\npenalty module gets much higher IS and lower FID compared with some SOTA GAN\narchitectures.",
          "link": "http://arxiv.org/abs/2108.02353",
          "publishedOn": "2021-09-14T07:20:13.554Z",
          "wordCount": 751,
          "title": "Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>",
          "description": "Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing the number of unique tokens) is a key factor for improved\nperformance.",
          "link": "http://arxiv.org/abs/2012.02339",
          "publishedOn": "2021-09-14T07:20:13.534Z",
          "wordCount": 674,
          "title": "Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Chenyang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiebin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "We describe a deep high-dynamic-range (HDR) image tone mapping operator that\nis computationally efficient and perceptually optimized. We first decompose an\nHDR image into a normalized Laplacian pyramid, and use two deep neural networks\n(DNNs) to estimate the Laplacian pyramid of the desired tone-mapped image from\nthe normalized representation. We then end-to-end optimize the entire method\nover a database of HDR images by minimizing the normalized Laplacian pyramid\ndistance (NLPD), a recently proposed perceptual metric. Qualitative and\nquantitative experiments demonstrate that our method produces images with\nbetter visual quality, and runs the fastest among existing local tone mapping\nalgorithms.",
          "link": "http://arxiv.org/abs/2109.00180",
          "publishedOn": "2021-09-14T07:20:13.527Z",
          "wordCount": 575,
          "title": "Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Acien_A/0/1/0/all/0/1\">Alejandro Acien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monaco_J/0/1/0/all/0/1\">John V. Monaco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1\">Ruben Vera-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>",
          "description": "We study the performance of Long Short-Term Memory networks for keystroke\nbiometric authentication at large scale in free-text scenarios. For this we\nexplore the performance of Long Short-Term Memory (LSTMs) networks trained with\na moderate number of keystrokes per identity and evaluated under different\nscenarios including: i) three learning approaches depending on the loss\nfunction (softmax, contrastive, and triplet loss); ii) different number of\ntraining samples and lengths of keystroke sequences; iii) four databases based\non two device types (physical vs touchscreen keyboard); and iv) comparison with\nexisting approaches based on both traditional statistical methods and deep\nlearning architectures. Our approach called TypeNet achieves state-of-the-art\nkeystroke biometric authentication performance with an Equal Error Rate of 2.2%\nand 9.2% for physical and touchscreen keyboards, respectively, significantly\noutperforming previous approaches. Our experiments demonstrate a moderate\nincrease in error with up to 100,000 subjects, demonstrating the potential of\nTypeNet to operate at an Internet scale. To the best of our knowledge, the\ndatabases used in this work are the largest existing free-text keystroke\ndatabases available for research with more than 136 million keystrokes from\n168,000 subjects in physical keyboards, and 60,000 subjects with more than 63\nmillion keystrokes acquired on mobile touchscreens.",
          "link": "http://arxiv.org/abs/2101.05570",
          "publishedOn": "2021-09-14T07:20:13.520Z",
          "wordCount": 700,
          "title": "TypeNet: Deep Learning Keystroke Biometrics. (arXiv:2101.05570v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.11765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_A/0/1/0/all/0/1\">Andrew Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1\">Tat-Jun Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_Y/0/1/0/all/0/1\">Yee Wei Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasdelli_M/0/1/0/all/0/1\">Michele Sasdelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_R/0/1/0/all/0/1\">Ramesh Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dillon Campbell</a>",
          "description": "Deep neural networks (DNNs) have become essential for processing the vast\namounts of aerial imagery collected using earth-observing satellite platforms.\nHowever, DNNs are vulnerable towards adversarial examples, and it is expected\nthat this weakness also plagues DNNs for aerial imagery. In this work, we\ndemonstrate one of the first efforts on physical adversarial attacks on aerial\nimagery, whereby adversarial patches were optimised, fabricated and installed\non or near target objects (cars) to significantly reduce the efficacy of an\nobject detector applied on overhead images. Physical adversarial attacks on\naerial images, particularly those captured from satellite platforms, are\nchallenged by atmospheric factors (lighting, weather, seasons) and the distance\nbetween the observer and target. To investigate the effects of these\nchallenges, we devised novel experiments and metrics to evaluate the efficacy\nof physical adversarial attacks against object detectors in aerial scenes. Our\nresults indicate the palpable threat posed by physical adversarial attacks\ntowards DNNs for processing satellite imagery.",
          "link": "http://arxiv.org/abs/2108.11765",
          "publishedOn": "2021-09-14T07:20:13.467Z",
          "wordCount": 640,
          "title": "Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14336",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fox_G/0/1/0/all/0/1\">Geoffrey C. Fox</a>",
          "description": "Geoscience and seismology have utilized the most advanced technologies and\nequipment to monitor seismic events globally from the past few decades. With\nthe enormous amount of data, modern GPU-powered deep learning presents a\npromising approach to analyze data and discover patterns. In recent years,\nthere are plenty of successful deep learning models for picking seismic waves.\nHowever, forecasting extreme earthquakes, which can cause disasters, is still\nan underdeveloped topic in history. Relevant research in spatiotemporal\ndynamics mining and forecasting has revealed some successful predictions, a\ncrucial topic in many scientific research fields. Most studies of them have\nmany successful applications of using deep neural networks. In Geology and\nEarth science studies, earthquake prediction is one of the world's most\nchallenging problems, about which cutting-edge deep learning technologies may\nhelp discover some valuable patterns. In this project, we propose a deep\nlearning modeling approach, namely \\tseqpre, to mine spatiotemporal patterns\nfrom data to nowcast extreme earthquakes by discovering visual dynamics in\nregional coarse-grained spatial grids over time. In this modeling approach, we\nuse synthetic deep learning neural networks with domain knowledge in geoscience\nand seismology to exploit earthquake patterns for prediction using\nconvolutional long short-term memory neural networks. Our experiments show a\nstrong correlation between location prediction and magnitude prediction for\nearthquakes in Southern California. Ablation studies and visualization validate\nthe effectiveness of the proposed modeling method.",
          "link": "http://arxiv.org/abs/2012.14336",
          "publishedOn": "2021-09-14T07:20:13.297Z",
          "wordCount": null,
          "title": "Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "One-class classification (OCC) aims to learn an effective data description to\nenclose all normal training samples and detect anomalies based on the deviation\nfrom the data description. Current state-of-the-art OCC models learn a compact\nnormality description by hyper-sphere minimisation, but they often suffer from\noverfitting the training data, especially when the training set is small or\ncontaminated with anomalous samples. To address this issue, we introduce the\ninterpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a\none-class Gaussian anomaly classifier trained with adversarially interpolated\ntraining samples. The Gaussian anomaly classifier differentiates the training\nsamples based on their distance to the Gaussian centre and the standard\ndeviation of these distances, offering the model a discriminability w.r.t. the\ngiven samples during training. The adversarial interpolation is enforced to\nconsistently learn a smooth Gaussian descriptor, even when the training data is\nsmall or contaminated with anomalous samples. This enables our model to learn\nthe data description based on the representative normal samples rather than\nfringe or anomalous samples, resulting in significantly improved normality\ndescription. In extensive experiments on diverse popular benchmarks, including\nMNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves\nbetter detection accuracy than current state-of-the-art models. IGD also shows\nbetter robustness in problems with small or contaminated training sets. Code is\navailable at https://github.com/tianyu0207/IGD.",
          "link": "http://arxiv.org/abs/2101.10043",
          "publishedOn": "2021-09-14T07:20:13.287Z",
          "wordCount": null,
          "title": "Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+San_Roman_R/0/1/0/all/0/1\">Robin San-Roman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Generative diffusion models have emerged as leading models in speech and\nimage generation. However, in order to perform well with a small number of\ndenoising steps, a costly tuning of the set of noise parameters is needed. In\nthis work, we present a simple and versatile learning scheme that can\nstep-by-step adjust those noise parameters, for any given number of steps,\nwhile the previous work needs to retune for each number separately.\nFurthermore, without modifying the weights of the diffusion model, we are able\nto significantly improve the synthesis results, for a small number of steps.\nOur approach comes at a negligible computation cost.",
          "link": "http://arxiv.org/abs/2104.02600",
          "publishedOn": "2021-09-14T07:20:13.285Z",
          "wordCount": null,
          "title": "Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Hongtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shancheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yadong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongdong Zhang</a>",
          "description": "Scene text removal (STR) contains two processes: text localization and\nbackground reconstruction. Through integrating both processes into a single\nnetwork, previous methods provide an implicit erasure guidance by modifying all\npixels in the entire image. However, there exists two problems: 1) the implicit\nerasure guidance causes the excessive erasure to non-text areas; 2) the\none-stage erasure lacks the exhaustive removal of text region. In this paper,\nwe propose a ProgrEssively Region-based scene Text eraser (PERT), introducing\nan explicit erasure guidance and performing balanced multi-stage erasure for\naccurate and exhaustive text removal. Firstly, we introduce a new region-based\nmodification strategy (RegionMS) to explicitly guide the erasure process.\nDifferent from previous implicitly guided methods, RegionMS performs targeted\nand regional erasure on only text region, and adaptively perceives stroke-level\ninformation to improve the integrity of non-text areas with only bounding box\nlevel annotations. Secondly, PERT performs balanced multi-stage erasure with\nseveral progressive erasing stages. Each erasing stage takes an equal step\ntoward the text-erased image to ensure the exhaustive erasure of text regions.\nCompared with previous methods, PERT outperforms them by a large margin without\nthe need of adversarial loss, obtaining SOTA results with high speed (71 FPS)\nand at least 25% lower parameter complexity. Code is available at\nhttps://github.com/wangyuxin87/PERT.",
          "link": "http://arxiv.org/abs/2106.13029",
          "publishedOn": "2021-09-14T07:20:13.283Z",
          "wordCount": null,
          "title": "PERT: A Progressively Region-based Network for Scene Text Removal. (arXiv:2106.13029v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jingjun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Gaojuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almpanidis_G/0/1/0/all/0/1\">George Almpanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garcia</a>",
          "description": "In predictive tasks, real-world datasets often present different degrees of\nimbalanced (i.e., long-tailed or skewed) distributions. While the majority (the\nhead) classes have sufficient samples, the minority (the tail) classes can be\nunder-represented by a rather limited number of samples. Data pre-processing\nhas been shown to be very effective in dealing with such problems. On one hand,\ndata re-sampling is a common approach to tackling class imbalance. On the other\nhand, dimension reduction, which reduces the feature space, is a conventional\ntechnique for reducing noise and inconsistencies in a dataset. However, the\npossible synergy between feature selection and data re-sampling for\nhigh-performance imbalance classification has rarely been investigated before.\nTo address this issue, we carry out a comprehensive empirical study on the\njoint influence of feature selection and re-sampling on two-class imbalance\nclassification. Specifically, we study the performance of two opposite\npipelines for imbalance classification by applying feature selection before or\nafter data re-sampling. We conduct a large number of experiments, with a total\nof 9225 tests, on 52 publicly available datasets, using 9 feature selection\nmethods, 6 re-sampling approaches for class imbalance learning, and 3\nwell-known classification algorithms. Experimental results show that there is\nno constant winner between the two pipelines; thus both of them should be\nconsidered to derive the best performing model for imbalance classification. We\nfind that the performance of an imbalance classification model not only depends\non the classifier adopted and the ratio between the number of majority and\nminority samples, but also depends on the ratio between the number of samples\nand features. Overall, this study should provide new reference value for\nresearchers and practitioners in imbalance learning.",
          "link": "http://arxiv.org/abs/2109.00201",
          "publishedOn": "2021-09-14T07:20:13.279Z",
          "wordCount": null,
          "title": "An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05627",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Da_M/0/1/0/all/0/1\">Ma Da</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Donghuan_L/0/1/0/all/0/1\">Lu Donghuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karteek_P/0/1/0/all/0/1\">Popuri Karteek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Faisal_B/0/1/0/all/0/1\">Beg Mirza Faisal</a>",
          "description": "Frontotemporal dementia and Alzheimer's disease are two common forms of\ndementia and are easily misdiagnosed as each other due to their similar pattern\nof clinical symptoms. Differentiating between the two dementia types is crucial\nfor determining disease-specific intervention and treatment. Recent development\nof Deep-learning-based approaches in the field of medical image computing are\ndelivering some of the best performance for many binary classification tasks,\nalthough its application in differential diagnosis, such as neuroimage-based\ndifferentiation for multiple types of dementia, has not been explored. In this\nstudy, a novel framework was proposed by using the Generative Adversarial\nNetwork technique to distinguish FTD, AD and normal control subjects, using\nvolumetric features extracted at coarse-to-fine structural scales from Magnetic\nResonance Imaging scans. Experiments of 10-folds cross-validation on 1,954\nimages achieved high accuracy. With the proposed framework, we have\ndemonstrated that the combination of multi-scale structural features and\nsynthetic data augmentation based on generative adversarial network can improve\nthe performance of challenging tasks such as differentiating Dementia\nsub-types.",
          "link": "http://arxiv.org/abs/2109.05627",
          "publishedOn": "2021-09-14T07:20:13.086Z",
          "wordCount": 626,
          "title": "Differential Diagnosis of Frontotemporal Dementia and Alzheimer's Disease using Generative Adversarial Network. (arXiv:2109.05627v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya-Qin Zhang</a>",
          "description": "3D scene understanding from point clouds plays a vital role for various\nrobotic applications. Unfortunately, current state-of-the-art methods use\nseparate neural networks for different tasks like object detection or room\nlayout estimation. Such a scheme has two limitations: 1) Storing and running\nseveral networks for different tasks are expensive for typical robotic\nplatforms. 2) The intrinsic structure of separate outputs are ignored and\npotentially violated. To this end, we propose the first transformer\narchitecture that predicts 3D objects and layouts simultaneously, using point\ncloud inputs. Unlike existing methods that either estimate layout keypoints or\nedges, we directly parameterize room layout as a set of quads. As such, the\nproposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the\nnovel quad representation, we propose a tailored physical constraint loss\nfunction that discourages object-layout interference. The quantitative and\nqualitative evaluations on the public benchmark ScanNet show that the proposed\nPQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a\nquasi-real-time (8.91 FPS) rate without efficiency-oriented optimization.\nMoreover, the new physical constraint loss can improve strong baselines, and\nthe F1-score of the room layout is significantly promoted from 37.9% to 57.9%.",
          "link": "http://arxiv.org/abs/2109.05566",
          "publishedOn": "2021-09-14T07:20:12.933Z",
          "wordCount": null,
          "title": "PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.00460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1\">Vladislav Gennadievich Malyshkin</a>",
          "description": "Problems of interpolation, classification, and clustering are considered. In\nthe tenets of Radon--Nikodym approach $\\langle f(\\mathbf{x})\\psi^2 \\rangle /\n\\langle\\psi^2\\rangle$, where the $\\psi(\\mathbf{x})$ is a linear function on\ninput attributes, all the answers are obtained from a generalized eigenproblem\n$|f|\\psi^{[i]}\\rangle = \\lambda^{[i]} |\\psi^{[i]}\\rangle$. The solution to the\ninterpolation problem is a regular Radon-Nikodym derivative. The solution to\nthe classification problem requires prior and posterior probabilities that are\nobtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian\napproach new observations change only outcome probabilities, in the\nRadon-Nikodym approach not only outcome probabilities but also the probability\nspace $|\\psi^{[i]}\\rangle$ change with new observations. This is a remarkable\nfeature of the approach: both the probabilities and the probability space are\nconstructed from the data. The Lebesgue quadrature technique can be also\napplied to the optimal clustering problem. The problem is solved by\nconstructing a Gaussian quadrature on the Lebesgue measure. A distinguishing\nfeature of the Radon-Nikodym approach is the knowledge of the invariant group:\nall the answers are invariant relatively any non-degenerated linear transform\nof input vector $\\mathbf{x}$ components. A software product implementing the\nalgorithms of interpolation, classification, and optimal clustering is\navailable from the authors.",
          "link": "http://arxiv.org/abs/1906.00460",
          "publishedOn": "2021-09-14T07:20:12.916Z",
          "wordCount": null,
          "title": "On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>",
          "description": "Deep neural networks have been a prevailing technique in the field of medical\nimage processing. However, the most popular convolutional neural networks\n(CNNs) based methods for medical image segmentation are imperfect because they\nmodel long-range dependencies by stacking layers or enlarging filters.\nTransformers and the self-attention mechanism are recently proposed to\neffectively learn long-range dependencies by modeling all pairs of word-to-word\nattention regardless of their positions. The idea has also been extended to the\ncomputer vision field by creating and treating image patches as embeddings.\nConsidering the computation complexity for whole image self-attention, current\ntransformer-based models settle for a rigid partitioning scheme that\npotentially loses informative relations. Besides, current medical transformers\nmodel global context on full resolution images, leading to unnecessary\ncomputation costs. To address these issues, we developed a novel method to\nintegrate multi-scale attention and CNN feature extraction using a pyramidal\nnetwork architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans\ncaptured multi-range relations by working on multi-resolution images. An\nadaptive partitioning scheme was implemented to retain informative relations\nand to access different receptive fields efficiently. Experimental results on\nthree medical image datasets (gland segmentation, MoNuSeg, and HECKTOR\ndatasets) showed that PMTrans outperformed the latest CNN-based and\ntransformer-based models for medical image segmentation.",
          "link": "http://arxiv.org/abs/2104.14702",
          "publishedOn": "2021-09-14T07:20:12.915Z",
          "wordCount": null,
          "title": "Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shvetsova_N/0/1/0/all/0/1\">Nina Shvetsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakker_B/0/1/0/all/0/1\">Bart Bakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedulova_I/0/1/0/all/0/1\">Irina Fedulova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_H/0/1/0/all/0/1\">Heinrich Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>",
          "description": "Anomaly detection is the problem of recognizing abnormal inputs based on the\nseen examples of normal data. Despite recent advances of deep learning in\nrecognizing image anomalies, these methods still prove incapable of handling\ncomplex medical images, such as barely visible abnormalities in chest X-rays\nand metastases in lymph nodes. To address this problem, we introduce a new\npowerful method of image anomaly detection. It relies on the classical\nautoencoder approach with a re-designed training pipeline to handle\nhigh-resolution, complex images and a robust way of computing an image\nabnormality score. We revisit the very problem statement of fully unsupervised\nanomaly detection, where no abnormal examples at all are provided during the\nmodel setup. We propose to relax this unrealistic assumption by using a very\nsmall number of anomalies of confined variability merely to initiate the search\nof hyperparameters of the model. We evaluate our solution on natural image\ndatasets with a known benchmark, as well as on two medical datasets containing\nradiology and digital pathology images. The proposed approach suggests a new\nstrong baseline for image anomaly detection and outperforms state-of-the-art\napproaches in complex medical image analysis tasks.",
          "link": "http://arxiv.org/abs/2006.13265",
          "publishedOn": "2021-09-14T07:20:12.895Z",
          "wordCount": null,
          "title": "Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders. (arXiv:2006.13265v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1\">Shashank Bujimalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1\">Mahesh Subedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1\">Omesh Tickoo</a>",
          "description": "In this paper, we propose an approach to improve image captioning solutions\nfor images with novel objects that do not have caption labels in the training\ndataset. Our approach is agnostic to model architecture, and primarily focuses\non training technique that uses existing fully paired image-caption data and\nthe images with only the novel object detection labels (partially paired data).\nWe create synthetic paired captioning data for these novel objects by\nleveraging context from existing image-caption pairs. We further re-use these\npartially paired images with novel objects to create pseudo-label captions that\nare used to fine-tune the captioning model. Using a popular captioning model\n(Up-Down) as baseline, our approach achieves state-of-the-art results on\nheld-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for\nnovel object images by 75.8 and 26.6 points respectively, compared to baseline\nmodel that does not use partially paired images during training.",
          "link": "http://arxiv.org/abs/2109.05115",
          "publishedOn": "2021-09-14T07:20:12.808Z",
          "wordCount": null,
          "title": "Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>",
          "description": "Different from visible cameras which record intensity images frame by frame,\nthe biologically inspired event camera produces a stream of asynchronous and\nsparse events with much lower latency. In practice, the visible cameras can\nbetter perceive texture details and slow motion, while event cameras can be\nfree from motion blurs and have a larger dynamic range which enables them to\nwork well under fast motion and low illumination. Therefore, the two sensors\ncan cooperate with each other to achieve more reliable object tracking. In this\nwork, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to\nthe lack of a realistic and scaled dataset for this task. Our dataset consists\nof 820 video pairs captured under low illumination, high speed, and background\nclutter scenarios, and it is divided into a training and a testing subset, each\nof which contains 500 and 320 videos, respectively. Based on VisEvent, we\ntransform the event flows into event images and construct more than 30 baseline\nmethods by extending current single-modality trackers into dual-modality\nversions. More importantly, we further build a simple but effective tracking\nalgorithm by proposing a cross-modality transformer, to achieve more effective\nfeature fusion between visible and event data. Extensive experiments on the\nproposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and\nVOT-DVS), validated the effectiveness of our model. The dataset and source code\nhave been released at our project page:\n\\url{https://sites.google.com/view/viseventtrack/}.",
          "link": "http://arxiv.org/abs/2108.05015",
          "publishedOn": "2021-09-14T07:20:12.803Z",
          "wordCount": null,
          "title": "VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Anomaly detection in video is a challenging computer vision problem. Due to\nthe lack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without full supervision. In this paper, we approach\nanomalous event detection in video through self-supervised and multi-task\nlearning at the object level. We first utilize a pre-trained detector to detect\nobjects. Then, we train a 3D convolutional neural network to produce\ndiscriminative anomaly-specific information by jointly learning multiple proxy\ntasks: three self-supervised and one based on knowledge distillation. The\nself-supervised tasks are: (i) discrimination of forward/backward moving\nobjects (arrow of time), (ii) discrimination of objects in\nconsecutive/intermittent frames (motion irregularity) and (iii) reconstruction\nof object-specific appearance information. The knowledge distillation task\ntakes into account both classification and detection information, generating\nlarge prediction discrepancies between teacher and student models when\nanomalies occur. To the best of our knowledge, we are the first to approach\nanomalous event detection in video as a multi-task learning problem,\nintegrating multiple self-supervised and knowledge distillation proxy tasks in\na single architecture. Our lightweight architecture outperforms the\nstate-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD\nPed2. Additionally, we perform an ablation study demonstrating the importance\nof integrating self-supervised learning and normality-specific distillation in\na multi-task learning setting.",
          "link": "http://arxiv.org/abs/2011.07491",
          "publishedOn": "2021-09-14T07:20:12.776Z",
          "wordCount": null,
          "title": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1\">Aichun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xili Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fangqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1\">Gang Hua</a>",
          "description": "Many previous methods on text-based person retrieval tasks are devoted to\nlearning a latent common space mapping, with the purpose of extracting\nmodality-invariant features from both visual and textual modality.\nNevertheless, due to the complexity of high-dimensional data, the unconstrained\nmapping paradigms are not able to properly catch discriminative clues about the\ncorresponding person while drop the misaligned information. Intuitively, the\ninformation contained in visual data can be divided into person information\n(PI) and surroundings information (SI), which are mutually exclusive from each\nother. To this end, we propose a novel Deep Surroundings-person Separation\nLearning (DSSL) model in this paper to effectively extract and match person\ninformation, and hence achieve a superior retrieval accuracy. A\nsurroundings-person separation and fusion mechanism plays the key role to\nrealize an accurate and effective surroundings-person separation under a\nmutually exclusion constraint. In order to adequately utilize multi-modal and\nmulti-granular information for a higher retrieval accuracy, five diverse\nalignment paradigms are adopted. Extensive experiments are carried out to\nevaluate the proposed DSSL on CUHK-PEDES, which is currently the only\naccessible dataset for text-base person retrieval task. DSSL achieves the\nstate-of-the-art performance on CUHK-PEDES. To properly evaluate our proposed\nDSSL in the real scenarios, a Real Scenarios Text-based Person Reidentification\n(RSTPReid) dataset is constructed to benefit future research on text-based\nperson retrieval, which will be publicly available.",
          "link": "http://arxiv.org/abs/2109.05534",
          "publishedOn": "2021-09-14T07:20:12.775Z",
          "wordCount": null,
          "title": "DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval. (arXiv:2109.05534v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "This paper addresses the deep face recognition problem under an open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. To this end, hyperspherical face recognition, as a promising line\nof research, has attracted increasing attention and gradually become a major\nfocus in face recognition research. As one of the earliest works in\nhyperspherical face recognition, SphereFace explicitly proposed to learn face\nembeddings with large inter-class angular margin. However, SphereFace still\nsuffers from severe training instability which limits its application in\npractice. In order to address this problem, we introduce a unified framework to\nunderstand large angular margin in hyperspherical face recognition. Under this\nframework, we extend the study of SphereFace and propose an improved variant\nwith substantially better training stability -- SphereFace-R. Specifically, we\npropose two novel ways to implement the multiplicative margin, and study\nSphereFace-R under three different feature normalization schemes (no feature\nnormalization, hard feature normalization and soft feature normalization). We\nalso propose an implementation strategy -- \"characteristic gradient detachment\"\n-- to stabilize training. Extensive experiments on SphereFace-R show that it is\nconsistently better than or competitive with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05565",
          "publishedOn": "2021-09-14T07:20:12.732Z",
          "wordCount": null,
          "title": "SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.13029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kiyoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>",
          "description": "Zero-shot action recognition is the task of classifying action categories\nthat are not available in the training set. In this setting, the standard\nevaluation protocol is to use existing action recognition datasets(e.g. UCF101)\nand randomly split the classes into seen and unseen. However, most recent work\nbuilds on representations pre-trained on the Kinetics dataset, where classes\nlargely overlap with classes in the zero-shot evaluation datasets. As a result,\nclasses which are supposed to be unseen, are present during supervised\npre-training, invalidating the condition of the zero-shot setting. A similar\nconcern was previously noted several years ago for image based zero-shot\nrecognition but has not been considered by the zero-shot action recognition\ncommunity. In this paper, we propose a new split for true zero-shot action\nrecognition with no overlap between unseen test classes and training or\npre-training classes. We benchmark several recent approaches on the proposed\nTrue Zero-Shot(TruZe) Split for UCF101 and HMDB51, with zero-shot and\ngeneralized zero-shot evaluation. In our extensive analysis, we find that our\nTruZesplits are significantly harder than comparable random splits as nothing\nis leaking from pre-training, i.e. unseen performance is consistently lower,up\nto 8.9% for zero-shot action recognition. In an additional evaluation we also\nfind that similar issues exist in the splits used in few-shot action\nrecognition, here we see differences of up to 17.1%. We publish oursplits1and\nhope that our benchmark analysis will change how the field is evaluating zero-\nand few-shot action recognition moving forward.",
          "link": "http://arxiv.org/abs/2107.13029",
          "publishedOn": "2021-09-14T07:20:12.704Z",
          "wordCount": null,
          "title": "A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_e/0/1/0/all/0/1\">enda Chen</a>",
          "description": "Bridge inspection is an important step in preserving and rehabilitating\ntransportation infrastructure for extending their service lives. The\nadvancement of mobile robotic technology allows the rapid collection of a large\namount of inspection video data. However, the data are mainly images of complex\nscenes, wherein a bridge of various structural elements mix with a cluttered\nbackground. Assisting bridge inspectors in extracting structural elements of\nbridges from the big complex video data, and sorting them out by classes, will\nprepare inspectors for the element-wise inspection to determine the condition\nof bridges. This paper is motivated to develop an assistive intelligence model\nfor segmenting multiclass bridge elements from inspection videos captured by an\naerial inspection platform. With a small initial training dataset labeled by\ninspectors, a Mask Region-based Convolutional Neural Network (Mask R-CNN)\npre-trained on a large public dataset was transferred to the new task of\nmulticlass bridge element segmentation. Besides, the temporal coherence\nanalysis attempts to recover false negatives and identify the weakness that the\nneural network can learn to improve. Furthermore, a semi-supervised\nself-training (S$^3$T) method was developed to engage experienced inspectors in\nrefining the network iteratively. Quantitative and qualitative results from\nevaluating the developed deep neural network demonstrate that the proposed\nmethod can utilize a small amount of time and guidance from experienced\ninspectors (3.58 hours for labeling 66 images) to build the network of\nexcellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score).\nImportantly, the paper illustrates an approach to leveraging the domain\nknowledge and experiences of bridge professionals into computational\nintelligence models to efficiently adapt the models to varied bridges in the\nNational Bridge Inventory.",
          "link": "http://arxiv.org/abs/2109.05078",
          "publishedOn": "2021-09-14T07:20:12.679Z",
          "wordCount": null,
          "title": "A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. (arXiv:2109.05078v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.11096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;rome Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>",
          "description": "Class imbalance and noisy labels are the norm rather than the exception in\nmany large-scale classification datasets. Nevertheless, most works in machine\nlearning typically assume balanced and clean data. There have been some recent\nattempts to tackle, on one side, the problem of learning from noisy labels and,\non the other side, learning from long-tailed data. Each group of methods make\nsimplifying assumptions about the other. Due to this separation, the proposed\nsolutions often underperform when both assumptions are violated. In this work,\nwe present a simple two-stage approach based on recent advances in\nself-supervised learning to treat both challenges simultaneously. It consists\nof, first, task-agnostic self-supervised pre-training, followed by\ntask-specific fine-tuning using an appropriate loss. Most significantly, we\nfind that self-supervised learning approaches are effectively able to cope with\nsevere class imbalance. In addition, the resulting learned representations are\nalso remarkably robust to label noise, when fine-tuned with an imbalance- and\nnoise-resistant loss function. We validate our claims with experiments on\nCIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as\nthe large-scale inherently noisy Clothing-1M dataset.",
          "link": "http://arxiv.org/abs/2108.11096",
          "publishedOn": "2021-09-14T07:20:12.656Z",
          "wordCount": null,
          "title": "Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yujing Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>",
          "description": "We present Voxel Transformer (VoTr), a novel and effective voxel-based\nTransformer backbone for 3D object detection from point clouds. Conventional 3D\nconvolutional backbones in voxel-based 3D detectors cannot efficiently capture\nlarge context information, which is crucial for object recognition and\nlocalization, owing to the limited receptive fields. In this paper, we resolve\nthe problem by introducing a Transformer-based architecture that enables\nlong-range relationships between voxels by self-attention. Given the fact that\nnon-empty voxels are naturally sparse but numerous, directly applying standard\nTransformer on voxels is non-trivial. To this end, we propose the sparse voxel\nmodule and the submanifold voxel module, which can operate on the empty and\nnon-empty voxel positions effectively. To further enlarge the attention range\nwhile maintaining comparable computational overhead to the convolutional\ncounterparts, we propose two attention mechanisms for multi-head attention in\nthose two modules: Local Attention and Dilated Attention, and we further\npropose Fast Voxel Query to accelerate the querying process in multi-head\nattention. VoTr contains a series of sparse and submanifold voxel modules and\ncan be applied in most voxel-based detectors. Our proposed VoTr shows\nconsistent improvement over the convolutional baselines while maintaining\ncomputational efficiency on the KITTI dataset and the Waymo Open dataset.",
          "link": "http://arxiv.org/abs/2109.02497",
          "publishedOn": "2021-09-14T07:20:12.585Z",
          "wordCount": null,
          "title": "Voxel Transformer for 3D Object Detection. (arXiv:2109.02497v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kwanyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Sanghyun Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "Unsupervised Domain Adaptation for semantic segmentation has gained immense\npopularity since it can transfer knowledge from simulation to real (Sim2Real)\nby largely cutting out the laborious per pixel labeling efforts at real. In\nthis work, we present a new video extension of this task, namely Unsupervised\nDomain Adaptation for Video Semantic Segmentation. As it became easy to obtain\nlarge-scale video labels through simulation, we believe attempting to maximize\nSim2Real knowledge transferability is one of the promising directions for\nresolving the fundamental data-hungry issue in the video. To tackle this new\nproblem, we present a novel two-phase adaptation scheme. In the first step, we\nexhaustively distill source domain knowledge using supervised loss functions.\nSimultaneously, video adversarial training (VAT) is employed to align the\nfeatures from source to target utilizing video context. In the second step, we\napply video self-training (VST), focusing only on the target data. To construct\nrobust pseudo labels, we exploit the temporal information in the video, which\nhas been rarely explored in the previous image-based self-training approaches.\nWe set strong baseline scores on 'VIPER to CityscapeVPS' adaptation scenario.\nWe show that our proposals significantly outperform previous image-based UDA\nmethods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.",
          "link": "http://arxiv.org/abs/2107.11052",
          "publishedOn": "2021-09-14T07:20:12.549Z",
          "wordCount": null,
          "title": "Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>",
          "description": "Learning to model how the world changes as time elapses has proven a\nchallenging problem for the computer vision community. We propose a\nself-supervised solution to this problem using temporal cycle consistency\njointly in vision and language, training on narrated video. Our model learns\nmodality-agnostic functions to predict forward and backward in time, which must\nundo each other when composed. This constraint leads to the discovery of\nhigh-level transitions between moments in time, since such transitions are\neasily inverted and shared across modalities. We justify the design of our\nmodel with an ablation study on different configurations of the cycle\nconsistency problem. We then show qualitatively and quantitatively that our\napproach yields a meaningful, high-level model of the future and past. We apply\nthe learned dynamics model without further training to various tasks, such as\npredicting future action and temporally ordering sets of images. Project page:\nhttps://dave.ml/mmcc",
          "link": "http://arxiv.org/abs/2101.02337",
          "publishedOn": "2021-09-14T07:20:12.504Z",
          "wordCount": null,
          "title": "Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1\">Carlos Guestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>",
          "description": "Recent observations have advanced our understanding of the neural network\noptimization landscape, revealing the existence of (1) paths of high accuracy\ncontaining diverse solutions and (2) wider minima offering improved\nperformance. Previous methods observing diverse paths require multiple training\nruns. In contrast we aim to leverage both property (1) and (2) with a single\nmethod and in a single training run. With a similar computational cost as\ntraining one model, we learn lines, curves, and simplexes of high-accuracy\nneural networks. These neural network subspaces contain diverse solutions that\ncan be ensembled, approaching the ensemble performance of independently trained\nnetworks without the training cost. Moreover, using the subspace midpoint\nboosts accuracy, calibration, and robustness to label noise, outperforming\nStochastic Weight Averaging.",
          "link": "http://arxiv.org/abs/2102.10472",
          "publishedOn": "2021-09-14T07:20:12.490Z",
          "wordCount": null,
          "title": "Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ivashechkin_M/0/1/0/all/0/1\">Maksym Ivashechkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1\">Daniel Barath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>",
          "description": "We present VSAC, a RANSAC-type robust estimator with a number of novelties.\nIt benefits from the introduction of the concept of independent inliers that\nimproves significantly the efficacy of the dominant plane handling and, also,\nallows near error-free rejection of incorrect models, without false positives.\nThe local optimization process and its application is improved so that it is\nrun on average only once. Further technical improvements include adaptive\nsequential hypothesis verification and efficient model estimation via Gaussian\nelimination. Experiments on four standard datasets show that VSAC is\nsignificantly faster than all its predecessors and runs on average in 1-2 ms,\non a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,\nthe currently most accurate estimator of two-view geometry. In the repeated\nruns on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.",
          "link": "http://arxiv.org/abs/2106.10240",
          "publishedOn": "2021-09-14T07:20:12.486Z",
          "wordCount": null,
          "title": "VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-09-14T07:20:12.474Z",
          "wordCount": null,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shijie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dapeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haobin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shixiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinguo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Recent years have witnessed significant progress in person re-identification\n(ReID). However, current ReID approaches still suffer from considerable\nperformance degradation when unseen testing domains exhibit different\ncharacteristics from the source training ones, known as the domain\ngeneralization problem. Given multiple source training domains, previous Domain\nGeneralizable ReID (DG-ReID) methods usually learn all domains together using a\nshared network, which can't learn sufficient knowledge from each domain. In\nthis paper, we propose a novel Multiple Domain Experts Collaborative Learning\n(MECL) framework for better exploiting all training domains, which benefits\nfrom the proposed Domain-Domain Collaborative Learning (DDCL) and\nUniversal-Domain Collaborative Learning (UDCL). DDCL utilizes domain-specific\nexperts for fully exploiting each domain, and prevents experts from\nover-fitting the corresponding domain using a meta-learning strategy. In UDCL,\na universal expert supervises the learning of domain experts and continuously\ngathers knowledge from all domain experts. Note, only the universal expert will\nbe used for inference. Extensive experiments on DG-ReID benchmarks demonstrate\nthe effectiveness of DDCL and UDCL, and show that the whole MECL framework\nsignificantly outperforms state-of-the-arts. Experimental results on\nDG-classification benchmarks also reveal the great potential of applying MECL\nto other DG tasks.",
          "link": "http://arxiv.org/abs/2105.12355",
          "publishedOn": "2021-09-14T07:20:12.462Z",
          "wordCount": null,
          "title": "Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1\">Shailesh Arya</a>",
          "description": "The vision of the internet of things (IoT) is a reality now. IoT devices are\ngetting cheaper, smaller. They are becoming more and more computationally and\nenergy-efficient. The global market of IoT-based video analytics has seen\nsignificant growth in recent years and it is expected to be a growing market\nsegment. For any IoT-based video analytics application, few key points\nrequired, such as cost-effectiveness, widespread use, flexible design, accurate\nscene detection, reusability of the framework. Video-based smart doorbell\nsystem is one such application domain for video analytics where many commercial\nofferings are available in the consumer market. However, such existing\nofferings are costly, monolithic, and proprietary. Also, there will be a\ntrade-off between accuracy and portability. To address the foreseen problems,\nI'm proposing a distributed framework for video analytics with a use case of a\nsmart doorbell system. The proposed framework uses AWS cloud services as a base\nplatform and to meet the price affordability constraint, the system was\nimplemented on affordable Raspberry Pi. The smart doorbell will be able to\nrecognize the known/unknown person with at most accuracy. The smart doorbell\nsystem is also having additional detection functionalities such as harmful\nweapon detection, noteworthy vehicle detection, animal/pet detection. An iOS\napplication is specifically developed for this implementation which can receive\nthe notification from the smart doorbell in real-time. Finally, the paper also\nmentions the classical approaches for video analytics, their feasibility in\nimplementing with this use-case, and comparative analysis in terms of accuracy\nand time required to detect an object in the frame is carried out. Results\nconclude that AWS cloud-based approach is worthy for this smart doorbell use\ncase.",
          "link": "http://arxiv.org/abs/2105.06508",
          "publishedOn": "2021-09-14T07:20:12.455Z",
          "wordCount": null,
          "title": "Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodasara_S/0/1/0/all/0/1\">Satyam Ghodasara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Suyash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilello_M/0/1/0/all/0/1\">Michel Bilello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1\">Evan Calabrese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colak_E/0/1/0/all/0/1\">Errol Colak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farahani_K/0/1/0/all/0/1\">Keyvan Farahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_F/0/1/0/all/0/1\">Felipe C. Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevedello_L/0/1/0/all/0/1\">Luciano M. Prevedello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudie_J/0/1/0/all/0/1\">Jeffrey D. Rudie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sako_C/0/1/0/all/0/1\">Chiharu Sako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinohara_R/0/1/0/all/0/1\">Russell T. Shinohara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergquist_T/0/1/0/all/0/1\">Timothy Bergquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_R/0/1/0/all/0/1\">Rong Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eddy_J/0/1/0/all/0/1\">James Eddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_J/0/1/0/all/0/1\">Julia Elliott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reade_W/0/1/0/all/0/1\">Walter Reade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaffter_T/0/1/0/all/0/1\">Thomas Schaffter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Thomas Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moawad_A/0/1/0/all/0/1\">Ahmed W. Moawad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coelho_L/0/1/0/all/0/1\">Luiz Otavio Coelho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonnell_O/0/1/0/all/0/1\">Olivia McDonnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_E/0/1/0/all/0/1\">Elka Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moron_F/0/1/0/all/0/1\">Fanny E. Moron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswood_M/0/1/0/all/0/1\">Mark C. Oswood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_R/0/1/0/all/0/1\">Robert Y. Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siakallis_L/0/1/0/all/0/1\">Loizos Siakallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronstein_Y/0/1/0/all/0/1\">Yulia Bronstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mason_J/0/1/0/all/0/1\">James R. Mason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1\">Anthony F. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_G/0/1/0/all/0/1\">Gagandeep Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aanchal Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besada_C/0/1/0/all/0/1\">Cristina H. Besada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derakhshan_J/0/1/0/all/0/1\">Jamal J. Derakhshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diogo_M/0/1/0/all/0/1\">Mariana C. Diogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_Dai_D/0/1/0/all/0/1\">Daniel D. Do-Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farage_L/0/1/0/all/0/1\">Luciano Farage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_J/0/1/0/all/0/1\">John L. Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadi_M/0/1/0/all/0/1\">Mohiuddin Hadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_V/0/1/0/all/0/1\">Virginia B. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iv_M/0/1/0/all/0/1\">Michael Iv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joyner_D/0/1/0/all/0/1\">David Joyner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lincoln_C/0/1/0/all/0/1\">Christie Lincoln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotan_E/0/1/0/all/0/1\">Eyal Lotan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyakoshi_A/0/1/0/all/0/1\">Asako Miyakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Montano_M/0/1/0/all/0/1\">Mariana Sanchez-Montano</a>, et al. (54 additional authors not shown)",
          "description": "The BraTS 2021 challenge celebrates its 10th anniversary and is jointly\norganized by the Radiological Society of North America (RSNA), the American\nSociety of Neuroradiology (ASNR), and the Medical Image Computing and Computer\nAssisted Interventions (MICCAI) society. Since its inception, BraTS has been\nfocusing on being a common benchmarking venue for brain glioma segmentation\nalgorithms, with well-curated multi-institutional multi-parametric magnetic\nresonance imaging (mpMRI) data. Gliomas are the most common primary\nmalignancies of the central nervous system, with varying degrees of\naggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets\nthe evaluation of computational algorithms assessing the same tumor\ncompartmentalization, as well as the underlying tumor's molecular\ncharacterization, in pre-operative baseline mpMRI data from 2,040 patients.\nSpecifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation\nof the histologically distinct brain tumor sub-regions, and b) the\nclassification of the tumor's O[6]-methylguanine-DNA methyltransferase (MGMT)\npromoter methylation status. The performance evaluation of all participating\nalgorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse\nplatform (Task 1) and Kaggle (Task 2), concluding in distributing to the top\nranked participants monetary awards of $60,000 collectively.",
          "link": "http://arxiv.org/abs/2107.02314",
          "publishedOn": "2021-09-14T07:20:12.450Z",
          "wordCount": null,
          "title": "The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification. (arXiv:2107.02314v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chogovadze_G/0/1/0/all/0/1\">George Chogovadze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_A/0/1/0/all/0/1\">Andreas Kunz</a>",
          "description": "We present Border-SegGCN, a novel architecture to improve semantic\nsegmentation by refining the border outline using graph convolutional networks\n(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as\na base network to have pre-segmented output. This output is converted into a\ngraphical structure and fed into the GCN to improve the border pixel prediction\nof the pre-segmented output. We explored and studied the factors such as border\nthickness, number of edges for a node, and the number of features to be fed\ninto the GCN by performing experiments. We demonstrate the effectiveness of the\nBorder-SegGCN on the CamVid and Carla dataset, achieving a test set performance\nof 81.96% without any post-processing on CamVid dataset. It is higher than the\nreported state of the art mIoU achieved on CamVid dataset by 0.404%",
          "link": "http://arxiv.org/abs/2109.05353",
          "publishedOn": "2021-09-14T07:20:12.431Z",
          "wordCount": null,
          "title": "Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network. (arXiv:2109.05353v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05159",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiarun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>",
          "description": "With the development of deep learning, medical image classification has been\nsignificantly improved. However, deep learning requires massive data with\nlabels. While labeling the samples by human experts is expensive and\ntime-consuming, collecting labels from crowd-sourcing suffers from the noises\nwhich may degenerate the accuracy of classifiers. Therefore, approaches that\ncan effectively handle label noises are highly desired. Unfortunately, recent\nprogress on handling label noise in deep learning has gone largely unnoticed by\nthe medical image. To fill the gap, this paper proposes a noise-tolerant\nmedical image classification framework named Co-Correcting, which significantly\nimproves classification accuracy and obtains more accurate labels through\ndual-network mutual learning, label probability estimation, and curriculum\nlabel correcting. On two representative medical image datasets and the MNIST\ndataset, we test six latest Learning-with-Noisy-Labels methods and conduct\ncomparative studies. The experiments show that Co-Correcting achieves the best\naccuracy and generalization under different noise ratios in various tasks. Our\nproject can be found at: https://github.com/JiarunLiu/Co-Correcting.",
          "link": "http://arxiv.org/abs/2109.05159",
          "publishedOn": "2021-09-14T07:20:12.425Z",
          "wordCount": null,
          "title": "Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cuong_N/0/1/0/all/0/1\">Nguyen-Tien Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Ngoc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nhat Minh Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Hoai Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Synh Viet-Uyen Ha</a>",
          "description": "Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.",
          "link": "http://arxiv.org/abs/2106.03776",
          "publishedOn": "2021-09-14T07:20:12.422Z",
          "wordCount": null,
          "title": "CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Juhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>",
          "description": "Despite advances in feature representation, leveraging geometric relations is\ncrucial for establishing reliable visual correspondences under large variations\nof images. In this work we introduce a Hough transform perspective on\nconvolutional matching and propose an effective geometric matching algorithm,\ndubbed Convolutional Hough Matching (CHM). The method distributes similarities\nof candidate matches over a geometric transformation space and evaluates them\nin a convolutional manner. We cast it into a trainable neural layer with a\nsemi-isotropic high-dimensional kernel, which learns non-rigid matching with a\nsmall number of interpretable parameters. To further improve the efficiency of\nhigh-dimensional voting, we also propose to use an efficient kernel\ndecomposition with center-pivot neighbors, which significantly sparsifies the\nproposed semi-isotropic kernels without performance degradation. To validate\nthe proposed techniques, we develop the neural network with CHM layers that\nperform convolutional matching in the space of translation and scaling. Our\nmethod sets a new state of the art on standard benchmarks for semantic visual\ncorrespondence, proving its strong robustness to challenging intra-class\nvariations.",
          "link": "http://arxiv.org/abs/2109.05221",
          "publishedOn": "2021-09-14T07:20:12.420Z",
          "wordCount": null,
          "title": "Convolutional Hough Matching Networks for Robust and Efficient Visual Correspondence. (arXiv:2109.05221v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05524",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_P/0/1/0/all/0/1\">Pedro Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_G/0/1/0/all/0/1\">Gladston Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_V/0/1/0/all/0/1\">Vander Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1\">Rodrigo Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luz_E/0/1/0/all/0/1\">Eduardo Luz</a>",
          "description": "Nowadays, deep learning is the standard approach for a wide range of\nproblems, including biometrics, such as face recognition and speech\nrecognition, etc. Biometric problems often use deep learning models to extract\nfeatures from images, also known as embeddings. Moreover, the loss function\nused during training strongly influences the quality of the generated\nembeddings. In this work, a loss function based on the decidability index is\nproposed to improve the quality of embeddings for the verification routine. Our\nproposal, the D-loss, avoids some Triplet-based loss disadvantages such as the\nuse of hard samples and tricky parameter tuning, which can lead to slow\nconvergence. The proposed approach is compared against the Softmax\n(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four\ndifferent benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The\nachieved results show the efficacy of the proposal when compared to other\npopular metrics in the literature. The D-loss computation, besides being\nsimple, non-parametric and easy to implement, favors both the inter-class and\nintra-class scenarios.",
          "link": "http://arxiv.org/abs/2109.05524",
          "publishedOn": "2021-09-14T07:20:12.411Z",
          "wordCount": null,
          "title": "A Decidability-Based Loss Function. (arXiv:2109.05524v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05023",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M C Baum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>",
          "description": "We present Free Point Transformer (FPT) - a deep neural network architecture\nfor non-rigid point-set registration. Consisting of two modules, a global\nfeature extraction module and a point transformation module, FPT does not\nassume explicit constraints based on point vicinity, thereby overcoming a\ncommon requirement of previous learning-based point-set registration methods.\nFPT is designed to accept unordered and unstructured point-sets with a variable\nnumber of points and uses a \"model-free\" approach without heuristic\nconstraints. Training FPT is flexible and involves minimizing an intuitive\nunsupervised loss function, but supervised, semi-supervised, and partially- or\nweakly-supervised training are also supported. This flexibility makes FPT\namenable to multimodal image registration problems where the ground-truth\ndeformations are difficult or impossible to measure. In this paper, we\ndemonstrate the application of FPT to non-rigid registration of prostate\nmagnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound\n(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete\nTRUS imaging and sparsely-sampled TRUS imaging, respectively. The results\nindicate superior accuracy to the alternative rigid and non-rigid registration\nalgorithms tested and substantially lower computation time. The rapid inference\npossible with FPT makes it particularly suitable for applications where\nreal-time registration is beneficial.",
          "link": "http://arxiv.org/abs/2109.05023",
          "publishedOn": "2021-09-14T07:20:12.410Z",
          "wordCount": null,
          "title": "Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1\">Zhihao Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_D/0/1/0/all/0/1\">Di Bo</a>",
          "description": "Convolutional networks have been widely applied for computer vision system.\nEncouraged by these results, a U-Net convolutional network was applied to\nrecognition of vessels and materials in chemistry lab using the recent\nVector-LabPics dataset, which contains 2187 images of materials within mostly\ntransparent vessels in a chemistry lab and other general settings, labeled with\n13 classes. By optimizing hyperparameters including learning rates and learning\nrate decays, 87% accuracy in vessel recognition was achieved. In the case of\nrelatively small training and test sets (relatively rare materials states, the\nnumber of training set samples less than 500 and the number of test set samples\nless than 100), a comprehensive improvement over 18% in IoU and 19% in accuracy\nfor the best model were achieved. Further improvements may be achievable by\nincorporating improved convolutional network structure into our models.",
          "link": "http://arxiv.org/abs/2109.05585",
          "publishedOn": "2021-09-14T07:20:12.409Z",
          "wordCount": null,
          "title": "U-Net Convolutional Network for Recognition of Vessels and Materials in Chemistry Lab. (arXiv:2109.05585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuerong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "Training robust supervised deep learning models for many geospatial\napplications of computer vision is difficult due to dearth of class-balanced\nand diverse training data. Conversely, obtaining enough training data for many\napplications is financially prohibitive or may be infeasible, especially when\nthe application involves modeling rare or extreme events. Synthetically\ngenerating data (and labels) using a generative model that can sample from a\ntarget distribution and exploit the multi-scale nature of images can be an\ninexpensive solution to address scarcity of labeled data. Towards this goal, we\npresent a deep conditional generative model, called VAE-Info-cGAN, that\ncombines a Variational Autoencoder (VAE) with a conditional Information\nMaximizing Generative Adversarial Network (InfoGAN), for synthesizing\nsemantically rich images simultaneously conditioned on a pixel-level condition\n(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC\ncan only vary in the channel dimension from the synthesized image and is meant\nto be a task-specific input. The FLC is modeled as an attribute vector in the\nlatent space of the generated image which controls the contributions of various\ncharacteristic attributes germane to the target distribution. Experiments on a\nGPS trajectories dataset show that the proposed model can accurately generate\nvarious forms of spatiotemporal aggregates across different geographic\nlocations while conditioned only on a raster representation of the road\nnetwork. The primary intended application of the VAE-Info-cGAN is synthetic\ndata (and label) generation for targeted data augmentation for computer\nvision-based modeling of problems relevant to geospatial analysis and remote\nsensing.",
          "link": "http://arxiv.org/abs/2109.05201",
          "publishedOn": "2021-09-14T07:20:12.403Z",
          "wordCount": null,
          "title": "Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dianlin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1\">Wenxiang Cong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shaoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Tomographic image reconstruction with deep learning is an emerging field, but\na recent landmark study reveals that several deep reconstruction networks are\nunstable for computed tomography (CT) and magnetic resonance imaging (MRI).\nSpecifically, three kinds of instabilities were reported: (1) strong image\nartefacts from tiny perturbations, (2) small features missing in a deeply\nreconstructed image, and (3) decreased imaging performance with increased input\ndata. On the other hand, compressed sensing (CS) inspired reconstruction\nmethods do not suffer from these instabilities because of their built-in kernel\nawareness. For deep reconstruction to realize its full potential and become a\nmainstream approach for tomographic imaging, it is thus critically important to\nmeet this challenge by stabilizing deep reconstruction networks. Here we\npropose an Analytic Compressed Iterative Deep (ACID) framework to address this\nchallenge. ACID synergizes a deep reconstruction network trained on big data,\nkernel awareness from CS-inspired processing, and iterative refinement to\nminimize the data residual relative to real measurement. Our study demonstrates\nthat the deep reconstruction using ACID is accurate and stable, and sheds light\non the converging mechanism of the ACID iteration under a Bounded Relative\nError Norm (BREN) condition. In particular, the study shows that ACID-based\nreconstruction is resilient against adversarial attacks, superior to classic\nsparsity-regularized reconstruction alone, and eliminates the three kinds of\ninstabilities. We anticipate that this integrative data-driven approach will\nhelp promote development and translation of deep tomographic image\nreconstruction networks into clinical applications.",
          "link": "http://arxiv.org/abs/2008.01846",
          "publishedOn": "2021-09-14T07:20:12.398Z",
          "wordCount": null,
          "title": "Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.03918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "Dynamical models estimate and predict the temporal evolution of physical\nsystems. State Space Models (SSMs) in particular represent the system dynamics\nwith many desirable properties, such as being able to model uncertainty in both\nthe model and measurements, and optimal (in the Bayesian sense) recursive\nformulations e.g. the Kalman Filter. However, they require significant domain\nknowledge to derive the parametric form and considerable hand-tuning to\ncorrectly set all the parameters. Data driven techniques e.g. Recurrent Neural\nNetworks have emerged as compelling alternatives to SSMs with wide success\nacross a number of challenging tasks, in part due to their ability to extract\nrelevant features from rich inputs. They however lack interpretability and\nrobustness to unseen conditions. In this work, we present DynaNet, a hybrid\ndeep learning and time-varying state-space model which can be trained\nend-to-end. Our neural Kalman dynamical model allows us to exploit the relative\nmerits of each approach. We demonstrate state-of-the-art estimation and\nprediction on a number of physically challenging tasks, including visual\nodometry, sensor fusion for visual-inertial navigation and pendulum control. In\naddition we show how DynaNet can indicate failures through investigation of\nproperties such as the rate of innovation (Kalman Gain).",
          "link": "http://arxiv.org/abs/1908.03918",
          "publishedOn": "2021-09-14T07:20:12.371Z",
          "wordCount": null,
          "title": "DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Samarth Shah</a>",
          "description": "Wildfires are uncontrolled fires in the environment that can be caused by\nhumans or nature. In 2020 alone, wildfires in California have burned 4.2\nmillion acres, damaged 10,500 buildings or structures, and killed more than 31\npeople, exacerbated by climate change and a rise in average global\ntemperatures. This also means there has been an increase in the costs of\nextinguishing these treacherous wildfires. The objective of the research is to\ndetect forest fires in their earlier stages to prevent them from spreading,\nprevent them from causing damage to a variety of things, and most importantly,\nreduce or eliminate the chances of someone dying from a wildfire. A fire\ndetection system should be efficient and accurate with respect to extinguishing\nwildfires in their earlier stages to prevent the spread of them along with\ntheir consequences. Computer Vision is potentially a more reliable, fast, and\nwidespread method we need. The current research in the field of preliminary\nfire detection has several problems related to unrepresentative data being used\nto train models and their existing varied amounts of label imbalance in the\nclasses of their dataset. We propose a more representative and evenly\ndistributed data through better settings, lighting, atmospheres, etc., and\nclass distribution in the entire dataset. After thoroughly examining the\nresults of this research, it can be inferred that they supported the datasets\nstrengths by being a viable resource when tested in the real world on\nunfamiliar data. This is evident since as the model trains on the dataset, it\nis able to generalize on it, hence confirming this is a viable Machine Learning\nsetting that has practical impact.",
          "link": "http://arxiv.org/abs/2109.05083",
          "publishedOn": "2021-09-14T07:20:12.339Z",
          "wordCount": null,
          "title": "Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks. (arXiv:2109.05083v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yandong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_D/0/1/0/all/0/1\">Dong Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1\">Boqing Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Many objects do not appear frequently enough in complex scenes (e.g., certain\nhandbags in living rooms) for training an accurate object detector, but are\noften found frequently by themselves (e.g., in product images). Yet, these\nobject-centric images are not effectively leveraged for improving object\ndetection in scene-centric images. In this paper, we propose Mosaic of\nObject-centric images as Scene-centric images (MosaicOS), a simple and novel\nframework that is surprisingly effective at tackling the challenges of\nlong-tailed object detection. Keys to our approach are three-fold: (i) pseudo\nscene-centric image construction from object-centric images for mitigating\ndomain differences, (ii) high-quality bounding box imputation using the\nobject-centric images' class labels, and (iii) a multi-stage training\nprocedure. On LVIS object detection (and instance segmentation), MosaicOS leads\nto a massive 60% (and 23%) relative improvement in average precision for rare\nobject categories. We also show that our framework can be compatibly used with\nother existing approaches to achieve even further gains. Our pre-trained models\nare publicly available at https://github.com/czhang0528/MosaicOS/.",
          "link": "http://arxiv.org/abs/2102.08884",
          "publishedOn": "2021-09-14T07:20:12.305Z",
          "wordCount": null,
          "title": "MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection. (arXiv:2102.08884v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Ximeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_B/0/1/0/all/0/1\">Bowen Pan Kailash Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>",
          "description": "Quantizing deep networks with adaptive bit-widths is a promising technique\nfor efficient inference across many devices and resource constraints. In\ncontrast to static methods that repeat the quantization process and train\ndifferent models for different constraints, adaptive quantization enables us to\nflexibly adjust the bit-widths of a single deep network during inference for\ninstant adaptation in different scenarios. While existing research shows\nencouraging results on common image classification benchmarks, this paper\ninvestigates how to train such adaptive networks more effectively.\nSpecifically, we present two novel techniques for quantizing deep neural\nnetworks with adaptive bit-widths of weights and activations. First, we propose\na collaborative strategy to choose a high-precision teacher for transferring\nknowledge to the low-precision student while jointly optimizing the model with\nall bit-widths. Second, to effectively transfer knowledge, we develop a dynamic\nblock swapping method by randomly replacing the blocks in the lower-precision\nstudent network with the corresponding blocks in the higher-precision teacher\nnetwork. Extensive experiments on multiple image classification datasets\nincluding video classification benchmarks for the first time, well demonstrate\nthe efficacy of our approach over state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2103.01435",
          "publishedOn": "2021-09-14T07:20:12.300Z",
          "wordCount": null,
          "title": "Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.13675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>",
          "description": "With the rapid development of image restoration techniques, high-definition\nreconstruction of early videos has achieved impressive results. However, there\nare few studies about the interlacing artifacts that often appear in early\nvideos and significantly affect visual perception. Traditional deinterlacing\napproaches are mainly focused on early interlacing scanning systems and thus\ncannot handle the complex and complicated artifacts in real-world early\ninterlaced videos. Hence, this paper proposes a specific deinterlacing network\n(DIN), which is motivated by the traditional deinterlacing strategy. The\nproposed DIN consists of two stages, i.e., a cooperative vertical interpolation\nstage for split fields, and a merging stage that is applied to perceive\nmovements and remove ghost artifacts. Experimental results demonstrate that the\nproposed method can effectively remove complex artifacts in early interlaced\nvideos.",
          "link": "http://arxiv.org/abs/2011.13675",
          "publishedOn": "2021-09-14T07:20:12.272Z",
          "wordCount": null,
          "title": "Rethinking deinterlacing for early interlaced videos. (arXiv:2011.13675v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.10857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stabinger_S/0/1/0/all/0/1\">Sebastian Stabinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+David_P/0/1/0/all/0/1\">Peer David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piater_J/0/1/0/all/0/1\">Justus Piater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Sanchez_A/0/1/0/all/0/1\">Antonio Rodr&#xed;guez-S&#xe1;nchez</a>",
          "description": "Convolutional Neural Networks (CNNs) have become the state of the art method\nfor image classification in the last ten years. Despite the fact that they\nachieve superhuman classification accuracy on many popular datasets, they often\nperform much worse on more abstract image classification tasks. We will show\nthat these difficult tasks are linked to relational concepts from cognitive\npsychology and that despite progress over the last few years, such relational\nreasoning tasks still remain difficult for current neural network\narchitectures.\n\nWe will review deep learning research that is linked to relational concept\nlearning, even if it was not originally presented from this angle. Reviewing\nthe current literature, we will argue that some form of attention will be an\nimportant component of future systems to solve relational tasks.\n\nIn addition, we will point out the shortcomings of currently used datasets,\nand we will recommend steps to make future datasets more relevant for testing\nsystems on relational reasoning.",
          "link": "http://arxiv.org/abs/2001.10857",
          "publishedOn": "2021-09-14T07:20:12.261Z",
          "wordCount": null,
          "title": "Evaluating the Progress of Deep Learning for Visual Relational Concepts. (arXiv:2001.10857v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naderi_M/0/1/0/all/0/1\">Mohammadreza Naderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabizadeh_Z/0/1/0/all/0/1\">Zahra Nabizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1\">Nader Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirani_S/0/1/0/all/0/1\">Shahram Shirani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samavi_S/0/1/0/all/0/1\">Shadrokh Samavi</a>",
          "description": "Conditional Generative Adversarial Networks (cGANs) have been used in many\nimage processing tasks. However, they still have serious problems maintaining\nthe balance between conditioning the output on the input and creating the\noutput with the desired distribution based on the corresponding ground truth.\nThe traditional cGANs, similar to most conventional GANs, suffer from vanishing\ngradients, which backpropagate from the discriminator to the generator.\nMoreover, the traditional cGANs are sensitive to architectural changes due to\npreviously mentioned gradient problems. Therefore, balancing the architecture\nof the cGANs is almost impossible. Recently MSG-GAN has been proposed to\nstabilize the performance of the GANs by applying multiple connections between\nthe generator and discriminator. In this work, we propose a method called\nMSGDD-cGAN, which first stabilizes the performance of the cGANs using\nmulti-connections gradients flow. Secondly, the proposed network architecture\nbalances the correlation of the output to input and the fitness of the output\non the target distribution. This balance is generated by using the proposed\ndual discrimination procedure. We tested our model by segmentation of fetal\nultrasound images. Our model shows a 3.18% increase in the F1 score comparing\nto the pix2pix version of cGANs.",
          "link": "http://arxiv.org/abs/2109.05614",
          "publishedOn": "2021-09-14T07:20:12.255Z",
          "wordCount": null,
          "title": "MSGDD-cGAN: Multi-Scale Gradients Dual Discriminator Conditional Generative Adversarial Network. (arXiv:2109.05614v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Deep Neural Networks (DNNs) have been utilized in various applications\nranging from image classification and facial recognition to medical imagery\nanalysis and real-time object detection. As our models become more\nsophisticated and complex, the computational cost of training such models\nbecomes a burden for small companies and individuals; for this reason,\noutsourcing the training process has been the go-to option for such users.\nUnfortunately, outsourcing the training process comes at the cost of\nvulnerability to backdoor attacks. These attacks aim at establishing hidden\nbackdoors in the DNN such that the model performs well on benign samples but\noutputs a particular target label when a trigger is applied to the input.\nCurrent backdoor attacks rely on generating triggers in the image/pixel domain;\nhowever, as we show in this paper, it is not the only domain to exploit and one\nshould always \"check the other doors\". In this work, we propose a complete\npipeline for generating a dynamic, efficient, and invisible backdoor attack in\nthe frequency domain. We show the advantages of utilizing the frequency domain\nfor establishing undetectable and powerful backdoor attacks through extensive\nexperiments on various datasets and network architectures. The backdoored\nmodels are shown to break various state-of-the-art defences. We also show two\npossible defences that succeed against frequency-based backdoor attacks and\npossible ways for the attacker to bypass them. We conclude the work with some\nremarks regarding a network's learning capacity and the capability of embedding\na backdoor attack in the model.",
          "link": "http://arxiv.org/abs/2109.05507",
          "publishedOn": "2021-09-14T07:20:12.252Z",
          "wordCount": null,
          "title": "Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_D/0/1/0/all/0/1\">David Martinez Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>",
          "description": "Universal Adversarial Perturbations (UAPs) are input perturbations that can\nfool a neural network on large sets of data. They are a class of attacks that\nrepresents a significant threat as they facilitate realistic, practical, and\nlow-cost attacks on neural networks. In this work, we derive upper bounds for\nthe effectiveness of UAPs based on norms of data-dependent Jacobians. We\nempirically verify that Jacobian regularization greatly increases model\nrobustness to UAPs by up to four times whilst maintaining clean performance.\nOur theoretical analysis also allows us to formulate a metric for the strength\nof shared adversarial perturbations between pairs of inputs. We apply this\nmetric to benchmark datasets and show that it is highly correlated with the\nactual observed robustness. This suggests that realistic and practical\nuniversal attacks can be reliably mitigated without sacrificing clean accuracy,\nwhich shows promise for the robustness of machine learning systems.",
          "link": "http://arxiv.org/abs/2104.10459",
          "publishedOn": "2021-09-14T07:20:12.142Z",
          "wordCount": null,
          "title": "Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Rui Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>",
          "description": "Cross-modal attention mechanisms have been widely applied to the image-text\nmatching task and have achieved remarkable improvements thanks to its\ncapability of learning fine-grained relevance across different modalities.\nHowever, the cross-modal attention models of existing methods could be\nsub-optimal and inaccurate because there is no direct supervision provided\nduring the training process. In this work, we propose two novel training\nstrategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive\nContent Swapping (CCS) constraints, to address such limitations. These\nconstraints supervise the training of cross-modal attention models in a\ncontrastive learning manner without requiring explicit attention annotations.\nThey are plug-in training strategies and can be easily integrated into existing\ncross-modal attention models. Additionally, we introduce three metrics\nincluding Attention Precision, Recall, and F1-Score to quantitatively measure\nthe quality of learned attention models. We evaluate the proposed constraints\nby incorporating them into four state-of-the-art cross-modal attention-based\nimage-text matching models. Experimental results on both Flickr30k and MS-COCO\ndatasets demonstrate that integrating these constraints improves the model\nperformance in terms of both retrieval performance and attention metrics.",
          "link": "http://arxiv.org/abs/2105.09597",
          "publishedOn": "2021-09-14T07:20:12.140Z",
          "wordCount": null,
          "title": "More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching. (arXiv:2105.09597v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoeffmann_k/0/1/0/all/0/1\">klaus Schoeffmann</a>",
          "description": "Semantic segmentation in cataract surgery has a wide range of applications\ncontributing to surgical outcome enhancement and clinical risk reduction.\nHowever, the varying issues in segmenting the different relevant instances make\nthe designation of a unique network quite challenging. This paper proposes a\nsemantic segmentation network termed as DeepPyram that can achieve superior\nperformance in segmenting relevant objects in cataract surgery videos with\nvarying issues. This superiority mainly originates from three modules: (i)\nPyramid View Fusion, which provides a varying-angle global view of the\nsurrounding region centering at each pixel position in the input convolutional\nfeature map; (ii) Deformable Pyramid Reception, which enables a wide deformable\nreceptive field that can adapt to geometric transformations in the object of\ninterest; and (iii) Pyramid Loss that adaptively supervises multi-scale\nsemantic feature maps. These modules can effectively boost semantic\nsegmentation performance, especially in the case of transparency,\ndeformability, scalability, and blunt edges in objects. The proposed approach\nis evaluated using four datasets of cataract surgery for objects with different\ncontextual features and compared with thirteen state-of-the-art segmentation\nnetworks. The experimental results confirm that DeepPyram outperforms the rival\napproaches without imposing additional trainable parameters. Our comprehensive\nablation study further proves the effectiveness of the proposed modules.",
          "link": "http://arxiv.org/abs/2109.05352",
          "publishedOn": "2021-09-14T07:20:12.133Z",
          "wordCount": null,
          "title": "DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.00937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangfu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>",
          "description": "Point cloud registration is a key task in many computational fields. Previous\ncorrespondence matching based methods require the inputs to have distinctive\ngeometric structures to fit a 3D rigid transformation according to point-wise\nsparse feature matches. However, the accuracy of transformation heavily relies\non the quality of extracted features, which are prone to errors with respect to\npartiality and noise. In addition, they can not utilize the geometric knowledge\nof all the overlapping regions. On the other hand, previous global feature\nbased approaches can utilize the entire point cloud for the registration,\nhowever they ignore the negative effect of non-overlapping points when\naggregating global features. In this paper, we present OMNet, a global feature\nbased iterative network for partial-to-partial point cloud registration. We\nlearn overlapping masks to reject non-overlapping regions, which converts the\npartial-to-partial registration to the registration of the same shape.\nMoreover, the previously used data is sampled only once from the CAD models for\neach object, resulting in the same point clouds for the source and reference.\nWe propose a more practical manner of data generation where a CAD model is\nsampled twice for the source and reference, avoiding the previously prevalent\nover-fitting issue. Experimental results show that our method achieves\nstate-of-the-art performance compared to traditional and deep learning based\nmethods. Code is available at https://github.com/megvii-research/OMNet.",
          "link": "http://arxiv.org/abs/2103.00937",
          "publishedOn": "2021-09-14T07:20:12.133Z",
          "wordCount": null,
          "title": "OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. Without natural split of features, single-view\nco-training works at the cost of training extra classifiers, where the\nalgorithm should be delicately designed to prevent individual classifiers from\ncollapsing into each other. To remove these obstacles which deter the adoption\nof single-view co-training, we present a simple and efficient algorithm\nMulti-Head Co-Training. By integrating base learners into a multi-head\nstructure, the model is in a minimal amount of extra parameters. Every\nclassification head in the unified model interacts with its peers through a\n\"Weak and Strong Augmentation\" strategy, in which the diversity is naturally\nbrought by the strong data augmentation. Therefore, the proposed method\nfacilitates single-view co-training by 1). promoting diversity implicitly and\n2). only requiring a small extra computational overhead. The effectiveness of\nMulti-Head Co-Training is demonstrated in an empirical study on standard\nsemi-supervised learning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-09-14T07:20:12.131Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05479",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1\">ErKang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">XinRui Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>",
          "description": "This paper proposes an end-to-end Efficient Re-parameterizationResidual\nAttention Network(ERRA-Net) to directly restore the nonhomogeneous hazy image.\nThe contribution of this paper mainly has the following three aspects: 1) A\nnovel Multi-branch Attention (MA) block. The spatial attention mechanism better\nreconstructs high-frequency features, and the channel attention mechanism\ntreats the features of different channels differently. Multi-branch structure\ndramatically improves the representation ability of the model and can be\nchanged into a single path structure after re-parameterization to speed up the\nprocess of inference. Local Residual Connection allows the low-frequency\ninformation in the nonhomogeneous area to pass through the block without\nprocessing so that the block can focus on detailed features. 2) A lightweight\nnetwork structure. We use cascaded MA blocks to extract high-frequency features\nstep by step, and the Multi-layer attention fusion tail combines the shallow\nand deep features of the model to get the residual of the clean image finally.\n3)We propose two novel loss functions to help reconstruct the hazy image\nColorAttenuation loss and Laplace Pyramid loss. ERRA-Net has an impressive\nspeed, processing 1200x1600 HD quality images with an average runtime of 166.11\nfps. Extensive evaluations demonstrate that ERSANet performs favorably against\nthe SOTA approaches on the real-world hazy images.",
          "link": "http://arxiv.org/abs/2109.05479",
          "publishedOn": "2021-09-14T07:20:12.127Z",
          "wordCount": null,
          "title": "Efficient Re-parameterization Residual Attention Network For Nonhomogeneous Image Dehazing. (arXiv:2109.05479v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.01293",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breckon_T/0/1/0/all/0/1\">Toby Breckon</a>",
          "description": "X-ray security screening is widely used to maintain aviation/transport\nsecurity, and its significance poses a particular interest in automated\nscreening systems. This paper aims to review computerised X-ray security\nimaging algorithms by taxonomising the field into conventional machine learning\nand contemporary deep learning applications. The first part briefly discusses\nthe classical machine learning approaches utilised within X-ray security\nimaging, while the latter part thoroughly investigates the use of modern deep\nlearning algorithms. The proposed taxonomy sub-categorises the use of deep\nlearning approaches into supervised, semi-supervised and unsupervised learning,\nwith a particular focus on object classification, detection, segmentation and\nanomaly detection tasks. The paper further explores well-established X-ray\ndatasets and provides a performance benchmark. Based on the current and future\ntrends in deep learning, the paper finally presents a discussion and future\ndirections for X-ray security imagery.",
          "link": "http://arxiv.org/abs/2001.01293",
          "publishedOn": "2021-09-14T07:20:12.122Z",
          "wordCount": null,
          "title": "Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging. (arXiv:2001.01293v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korosteleva_M/0/1/0/all/0/1\">Maria Korosteleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>",
          "description": "Garments are ubiquitous in both real and many of the virtual worlds. They are\nhighly deformable objects, exhibit an immense variety of designs and shapes,\nand yet, most garments are created from a set of regularly shaped flat pieces.\nExploration of garment structure presents a peculiar case for an object\nstructure estimation task and might prove useful for downstream tasks of neural\n3D garment modeling and reconstruction by providing strong prior on garment\nshapes. To facilitate research in these directions, we propose a method for\ngenerating large synthetic datasets of 3D garment designs and their sewing\npatterns. Our method consists of a flexible description structure for\nspecifying parametric sewing pattern templates and the automatic generation\npipeline to produce garment 3D models with little-to-none manual intervention.\nTo add realism, the pipeline additionally creates corrupted versions of the\nfinal meshes that imitate artifacts of 3D scanning.\n\nWith this pipeline, we created the first large-scale synthetic dataset of 3D\ngarment models with their sewing patterns. The dataset contains more than 20000\ngarment design variations produced from 19 different base types. Seven of these\ngarment types are specifically designed to target evaluation of the\ngeneralization across garment sewing pattern topologies.",
          "link": "http://arxiv.org/abs/2109.05633",
          "publishedOn": "2021-09-14T07:20:12.094Z",
          "wordCount": null,
          "title": "Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05025",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bengs_M/0/1/0/all/0/1\">Marcel Bengs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bockmayr_M/0/1/0/all/0/1\">Michael Bockmayr</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuller_U/0/1/0/all/0/1\">Ulrich Sch&#xfc;ller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schlaefer_A/0/1/0/all/0/1\">Alexander Schlaefer</a>",
          "description": "Medulloblastoma (MB) is the most common malignant brain tumor in childhood.\nThe diagnosis is generally based on the microscopic evaluation of\nhistopathological tissue slides. However, visual-only assessment of\nhistopathological patterns is a tedious and time-consuming task and is also\naffected by observer variability. Hence, automated MB tumor classification\ncould assist pathologists by promoting consistency and robust quantification.\nRecently, convolutional neural networks (CNNs) have been proposed for this\ntask, while transfer learning has shown promising results. In this work, we\npropose an end-to-end MB tumor classification and explore transfer learning\nwith various input sizes and matching network dimensions. We focus on\ndifferentiating between the histological subtypes classic and\ndesmoplastic/nodular. For this purpose, we systematically evaluate recently\nproposed EfficientNets, which uniformly scale all dimensions of a CNN. Using a\ndata set with 161 cases, we demonstrate that pre-trained EfficientNets with\nlarger input resolutions lead to significant performance improvements compared\nto commonly used pre-trained CNN architectures. Also, we highlight the\nimportance of transfer learning, when using such large architectures. Overall,\nour best performing method achieves an F1-Score of 80.1%.",
          "link": "http://arxiv.org/abs/2109.05025",
          "publishedOn": "2021-09-14T07:20:12.091Z",
          "wordCount": null,
          "title": "Medulloblastoma Tumor Classification using Deep Transfer Learning with Multi-Scale EfficientNets. (arXiv:2109.05025v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Deqiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hannah Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_T/0/1/0/all/0/1\">Tianshu Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_Y/0/1/0/all/0/1\">Yankun Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeseung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gateno_J/0/1/0/all/0/1\">Jaime Gateno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Steve Guofang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">James J. Xia</a>",
          "description": "Virtual orthognathic surgical planning involves simulating surgical\ncorrections of jaw deformities on 3D facial bony shape models. Due to the lack\nof necessary guidance, the planning procedure is highly experience-dependent\nand the planning results are often suboptimal. A reference facial bony shape\nmodel representing normal anatomies can provide an objective guidance to\nimprove planning accuracy. Therefore, we propose a self-supervised deep\nframework to automatically estimate reference facial bony shape models. Our\nframework is an end-to-end trainable network, consisting of a simulator and a\ncorrector. In the training stage, the simulator maps jaw deformities of a\npatient bone to a normal bone to generate a simulated deformed bone. The\ncorrector then restores the simulated deformed bone back to normal. In the\ninference stage, the trained corrector is applied to generate a\npatient-specific normal-looking reference bone from a real deformed bone. The\nproposed framework was evaluated using a clinical dataset and compared with a\nstate-of-the-art method that is based on a supervised point-cloud network.\nExperimental results show that the estimated shape models given by our approach\nare clinically acceptable and significantly more accurate than that of the\ncompeting method.",
          "link": "http://arxiv.org/abs/2109.05191",
          "publishedOn": "2021-09-14T07:20:12.090Z",
          "wordCount": null,
          "title": "A Self-Supervised Deep Framework for Reference Bony Shape Estimation in Orthognathic Surgical Planning. (arXiv:2109.05191v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05021",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Asiri_N/0/1/0/all/0/1\">Norah Asiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim Aboalsamh</a>",
          "description": "Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early\nsigns of diabetic retinopathy (DR). The automatic detection of MAs and HMs on\nretinal fundus images is a challenging task. Most of the existing methods\ndetect either only MAs or only HMs because of the difference in their texture,\nsizes, and morphology. Though some methods detect both MAs and HMs, they suffer\nfrom the curse of dimensionality of shape and colors features and fail to\ndetect all shape variations of HMs such as flame-shaped HM. Leveraging the\nprogress in deep learning, we proposed a two-stream red lesions detection\nsystem dealing simultaneously with small and large red lesions. For this\nsystem, we introduced a new ROIs candidates generation method for large red\nlesions fundus images; it is based on blood vessel segmentation and\nmorphological operations, and reduces the computational complexity, and\nenhances the detection accuracy by generating a small number of potential\ncandidates. For detection, we adapted the Faster RCNN framework with two\nstreams. We used pre-trained VGGNet as a bone model and carried out several\nextensive experiments to tune it for vessels segmentation and candidates\ngeneration, and finally learning the appropriate mapping, which yields better\ndetection of the red lesions comparing with the state-of-the-art methods. The\nexperimental results validated the effectiveness of the system in the detection\nof both MAs and HMs; the method yields higher performance for per lesion\ndetection according to sensitivity under 4 FPIs on DiaretDB1-MA and\nDiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state\nof the art methods w.r.t. various evaluation metrics. For DR screening, the\nsystem outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha\ndatasets.",
          "link": "http://arxiv.org/abs/2109.05021",
          "publishedOn": "2021-09-14T07:20:12.089Z",
          "wordCount": null,
          "title": "A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Sikai Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>",
          "description": "Person re-identification (re-ID) has gained more and more attention due to\nits widespread applications in intelligent video surveillance. Unfortunately,\nthe mainstream deep learning methods still need a large quantity of labeled\ndata to train models, and annotating data is an expensive work in real-world\nscenarios. In addition, due to domain gaps between different datasets, the\nperformance is dramatically decreased when re-ID models pre-trained on\nlabel-rich datasets (source domain) are directly applied to other unlabeled\ndatasets (target domain). In this paper, we attempt to remedy these problems\nfrom two aspects, namely data and methodology. Firstly, we develop a data\ncollector to automatically generate synthetic re-ID samples in a computer game,\nand construct a data labeler to simultaneously annotate them, which free humans\nfrom heavy data collections and annotations. Based on them, we build two\nsynthetic person re-ID datasets with different scales, \"GSPR\" and \"mini-GSPR\"\ndatasets. Secondly, we propose a synthesis-based multi-domain collaborative\nrefinement (SMCR) network, which contains a synthetic pretraining module and\ntwo collaborative-refinement modules to implement sufficient learning for the\nvaluable knowledge from multiple domains. Extensive experiments show that our\nproposed framework obtains significant performance improvements over the\nstate-of-the-art methods on multiple unsupervised domain adaptation tasks of\nperson re-ID.",
          "link": "http://arxiv.org/abs/2109.05542",
          "publishedOn": "2021-09-14T07:20:12.078Z",
          "wordCount": null,
          "title": "Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>",
          "description": "Automated segmentation can assist radiotherapy treatment planning by saving\nmanual contouring efforts and reducing intra-observer and inter-observer\nvariations. The recent development of deep learning approaches has revoluted\nmedical data processing, including semantic segmentation, by dramatically\nimproving performance. However, training effective deep learning models usually\nrequire a large amount of high-quality labeled data, which are often costly to\ncollect. We developed a novel semi-supervised adversarial deep learning\napproach for 3D pelvic CT image semantic segmentation. Unlike supervised deep\nlearning methods, the new approach can utilize both annotated and un-annotated\ndata for training. It generates un-annotated synthetic data by a data\naugmentation scheme using generative adversarial networks (GANs). We applied\nthe new approach to segmenting multiple organs in male pelvic CT images, where\nCT images without annotations and GAN-synthesized un-annotated images were used\nin semi-supervised learning. Experimental results, evaluated by three metrics\n(Dice similarity coefficient, average Hausdorff distance, and average surface\nHausdorff distance), showed that the new method achieved either comparable\nperformance with substantially fewer annotated images or better performance\nwith the same amount of annotated data, outperforming the existing\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2009.09571",
          "publishedOn": "2021-09-14T07:20:12.046Z",
          "wordCount": null,
          "title": "Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05372",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sadafi_A/0/1/0/all/0/1\">Ario Sadafi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makhro_A/0/1/0/all/0/1\">Asya Makhro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Livshits_L/0/1/0/all/0/1\">Leonid Livshits</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogdanova_A/0/1/0/all/0/1\">Anna Bogdanova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>",
          "description": "Sickle cell disease (SCD) is a severe genetic hemoglobin disorder that\nresults in premature destruction of red blood cells. Assessment of the severity\nof the disease is a challenging task in clinical routine since the causes of\nbroad variance in SCD manifestation despite the common genetic cause remain\nunclear. Identification of the biomarkers that would predict the severity grade\nis of importance for prognosis and assessment of responsiveness of patients to\ntherapy. Detection of the changes in red blood cell (RBC) density through\nseparation of Percoll density gradient could be such marker as it allows to\nresolve intercellular differences and follow the most damaged dense cells prone\nto destruction and vaso-occlusion. Quantification of the images obtained from\nthe distribution of RBCs in Percoll gradient and interpretation of the obtained\nis an important prerequisite for establishment of this approach. Here, we\npropose a novel approach combining a graph convolutional network, a\nconvolutional neural network, fast Fourier transform, and recursive feature\nelimination to predict the severity of SCD directly from a Percoll image. Two\nimportant but expensive laboratory blood test parameters measurements are used\nfor training the graph convolutional network. To make the model independent\nfrom such tests during prediction, the two parameters are estimated by a neural\nnetwork from the Percoll image directly. On a cohort of 216 subjects, we\nachieve a prediction performance that is only slightly below an approach where\nthe groundtruth laboratory measurements are used. Our proposed method is the\nfirst computational approach for the difficult task of SCD severity prediction.\nThe two-step approach relies solely on inexpensive and simple blood analysis\ntools and can have a significant impact on the patients' survival in\nunderdeveloped countries where access to medical instruments and doctors is\nlimited",
          "link": "http://arxiv.org/abs/2109.05372",
          "publishedOn": "2021-09-14T07:20:12.044Z",
          "wordCount": null,
          "title": "Sickle Cell Disease Severity Prediction from Percoll Gradient Images using Graph Convolutional Networks. (arXiv:2109.05372v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yue-Jie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zai-Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian-Hu/0/1/0/all/0/1\">Jian-Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao-Shen/0/1/0/all/0/1\">Yao-Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>",
          "description": "The fingerprint classification is an important and effective method to\nquicken the process and improve the accuracy in the fingerprint matching\nprocess. Conventional supervised methods need a large amount of pre-labeled\ndata and thus consume immense human resources. In this paper, we propose a new\nand efficient unsupervised deep learning method that can extract fingerprint\nfeatures and classify fingerprint patterns automatically. In this approach, a\nnew model named constraint convolutional auto-encoder (CCAE) is used to extract\nfingerprint features and a hybrid clustering strategy is applied to obtain the\nfinal clusters. A set of experiments in the NIST-DB4 dataset shows that the\nproposed unsupervised method exhibits the efficient performance on fingerprint\nclassification. For example, the CCAE achieves an accuracy of 97.3% on only\n1000 unlabeled fingerprints in the NIST-DB4.",
          "link": "http://arxiv.org/abs/2109.05526",
          "publishedOn": "2021-09-14T07:20:11.986Z",
          "wordCount": null,
          "title": "An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>",
          "description": "Video semantic segmentation requires to utilize the complex temporal\nrelations between frames of the video sequence. Previous works usually exploit\naccurate optical flow to leverage the temporal relations, which suffer much\nfrom heavy computational cost. In this paper, we propose a Temporal Memory\nAttention Network (TMANet) to adaptively integrate the long-range temporal\nrelations over the video sequence based on the self-attention mechanism without\nexhaustive optical flow prediction. Specially, we construct a memory using\nseveral past frames to store the temporal information of the current frame. We\nthen propose a temporal memory attention module to capture the relation between\nthe current frame and the memory to enhance the representation of the current\nframe. Our method achieves new state-of-the-art performances on two challenging\nvideo semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and\n76.5% mIoU on CamVid with ResNet-50.",
          "link": "http://arxiv.org/abs/2102.08643",
          "publishedOn": "2021-09-14T07:20:11.973Z",
          "wordCount": null,
          "title": "Temporal Memory Attention for Video Semantic Segmentation. (arXiv:2102.08643v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matveev_A/0/1/0/all/0/1\">Albert Matveev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rakhimov_R/0/1/0/all/0/1\">Ruslan Rakhimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bobrovskikh_G/0/1/0/all/0/1\">Gleb Bobrovskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1\">Emil Bogomolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panozzo_D/0/1/0/all/0/1\">Daniele Panozzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zorin_D/0/1/0/all/0/1\">Denis Zorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "We propose Deep Estimators of Features (DEFs), a learning-based framework for\npredicting sharp geometric features in sampled 3D shapes. Differently from\nexisting data-driven methods, which reduce this problem to feature\nclassification, we propose to regress a scalar field representing the distance\nfrom point samples to the closest feature line on local patches. Our approach\nis the first that scales to massive point clouds by fusing distance-to-feature\nestimates obtained on individual patches. We extensively evaluate our approach\nagainst five baselines on newly proposed synthetic and real-world 3D CAD model\nbenchmarks. Our approach not only outperforms the baselines (with improvements\nin Recall and False Positives Rates), but generalizes to real-world scans after\ntraining our model on synthetic data and fine-tuning it on a small dataset of\nscanned data. We demonstrate a downstream application, where we reconstruct an\nexplicit representation of straight and curved sharp feature lines from range\nscan data.",
          "link": "http://arxiv.org/abs/2011.15081",
          "publishedOn": "2021-09-14T07:20:11.971Z",
          "wordCount": null,
          "title": "DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Modeling the hand-object (HO) interaction not only requires estimation of the\nHO pose, but also pays attention to the contact due to their interaction.\nSignificant progress has been made in estimating hand and object separately\nwith deep learning methods, simultaneous HO pose estimation and contact\nmodeling has not yet been fully explored. In this paper, we present an explicit\ncontact representation namely Contact Potential Field (CPF), and a\nlearning-fitting hybrid framework namely MIHO to Modeling the Interaction of\nHand and Object. In CPF, we treat each contacting HO vertex pair as a\nspring-mass system. Hence the whole system forms a potential field with minimal\nelastic energy at the grasp position. Extensive experiments on the two commonly\nused benchmarks have demonstrated that our method can achieve state-of-the-art\nin several reconstruction metrics, and allow us to produce more physically\nplausible HO pose even when the ground-truth exhibits severe interpenetration\nor disjointedness. Our code is available at https://github.com/lixiny/CPF.",
          "link": "http://arxiv.org/abs/2012.00924",
          "publishedOn": "2021-09-14T07:20:11.946Z",
          "wordCount": null,
          "title": "CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction. (arXiv:2012.00924v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05196",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Victorova_M/0/1/0/all/0/1\">Maria Victorova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1\">Michael Ka-Shing Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Navarro_Alarcon_D/0/1/0/all/0/1\">David Navarro-Alarcon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yongping Zheng</a>",
          "description": "The scoliosis progression in adolescents requires close monitoring to timely\ntake treatment measures. Ultrasound imaging is a radiation-free and low-cost\nalternative in scoliosis assessment to X-rays, which are typically used in\nclinical practice. However, ultrasound images are prone to speckle noises,\nmaking it challenging for sonographers to detect bony features and follow the\nspine's curvature. This paper introduces a robotic-ultrasound approach for\nspinal curvature tracking and automatic navigation. A fully connected network\nwith deconvolutional heads is developed to locate the spinous process\nefficiently with real-time ultrasound images. We use this machine\nlearning-based method to guide the motion of the robot-held ultrasound probe\nand follow the spinal curvature while capturing ultrasound images and\ncorrespondent position. We developed a new force-driven controller that\nautomatically adjusts the probe's pose relative to the skin surface to ensure a\ngood acoustic coupling between the probe and skin. After the scanning, the\nacquired data is used to reconstruct the coronal spinal image, where the\ndeformity of the scoliosis spine can be assessed and measured. To evaluate the\nperformance of our methodology, we conducted an experimental study with human\nsubjects where the deviations from the image center during the robotized\nprocedure are compared to that obtained from manual scanning. The angles of\nspinal deformity measured on spinal reconstruction images were similar for both\nmethods, implying that they equally reflect human anatomy.",
          "link": "http://arxiv.org/abs/2109.05196",
          "publishedOn": "2021-09-14T07:20:11.905Z",
          "wordCount": null,
          "title": "Follow the Curve: Robotic-Ultrasound Navigation with Learning Based Localization of Spinous Processes for Scoliosis Assessment. (arXiv:2109.05196v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05580",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saueressig_C/0/1/0/all/0/1\">Camillo Saueressig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berkley_A/0/1/0/all/0/1\">Adam Berkley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munbodh_R/0/1/0/all/0/1\">Reshma Munbodh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1\">Ritambhara Singh</a>",
          "description": "We present a joint graph convolution-image convolution neural network as our\nsubmission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model\neach brain as a graph composed of distinct image regions, which is initially\nsegmented by a graph neural network (GNN). Subsequently, the tumorous volume\nidentified by the GNN is further refined by a simple (voxel) convolutional\nneural network (CNN), which produces the final segmentation. This approach\ncaptures both global brain feature interactions via the graphical\nrepresentation and local image details through the use of convolutional\nfilters. We find that the GNN component by itself can effectively identify and\nsegment the brain tumors. The addition of the CNN further improves the median\nperformance of the model by 2 percent across all metrics evaluated. On the\nvalidation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,\n0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm\non the whole tumor, core tumor, and enhancing tumor, respectively.",
          "link": "http://arxiv.org/abs/2109.05580",
          "publishedOn": "2021-09-14T07:20:11.655Z",
          "wordCount": null,
          "title": "A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genova_K/0/1/0/all/0/1\">Kyle Genova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1\">Sean Fanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haene_C/0/1/0/all/0/1\">Christian Haene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskin_C/0/1/0/all/0/1\">Cem Keskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>",
          "description": "We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical\nrepresentation that can recover fine geometry detail, while being able to\nperform global operations such as shape completion. Our model represents a\ncomplex 3D shape with a hierarchy of latent grids, which can be decoded into\ndifferent levels of detail and also achieve better accuracy. For shape\ncompletion, we propose latent grid dropout to simulate partial data in the\nlatent space and therefore defer the completing functionality to the decoder\nside. This along with our multires design significantly improves the shape\ncompletion quality under decoder-only latent optimization. To the best of our\nknowledge, MDIF is the first deep implicit function model that can at the same\ntime (1) represent different levels of detail and allow progressive decoding;\n(2) support both encoder-decoder inference and decoder-only latent\noptimization, and fulfill multiple applications; (3) perform detailed\ndecoder-only shape completion. Experiments demonstrate its superior performance\nagainst prior art in various 3D reconstruction tasks.",
          "link": "http://arxiv.org/abs/2109.05591",
          "publishedOn": "2021-09-14T07:20:11.580Z",
          "wordCount": null,
          "title": "Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>",
          "description": "Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.",
          "link": "http://arxiv.org/abs/2109.05433",
          "publishedOn": "2021-09-14T07:20:11.559Z",
          "wordCount": null,
          "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.17020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhongze Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "Natural image matting aims to precisely separate foreground objects from\nbackground using alpha matte. Fully automatic natural image matting without\nexternal annotation is challenging. Well-performed matting methods usually\nrequire accurate labor-intensive handcrafted trimap as extra input, while the\nperformance of automatic trimap generation method of dilating foreground\nsegmentation fluctuates with segmentation quality. Therefore, we argue that how\nto handle trade-off of additional information input is a major issue in\nautomatic matting. This paper presents a semantic-guided automatic natural\nimage matting pipeline with Trimap Generation Network and light-weight\nnon-local attention, which does not need trimap and background as input.\nSpecifically, guided by foreground segmentation, Trimap Generation Network\nestimates accurate trimap. Then, with estimated trimap as guidance, our\nlight-weight Non-local Matting Network with Refinement produces final alpha\nmatte, whose trimap-guided global aggregation attention block is equipped with\nstride downsampling convolution, reducing computation complexity and promoting\nperformance. Experimental results show that our matting algorithm has\ncompetitive performance with state-of-the-art methods in both trimap-free and\ntrimap-needed aspects.",
          "link": "http://arxiv.org/abs/2103.17020",
          "publishedOn": "2021-09-14T07:20:11.486Z",
          "wordCount": null,
          "title": "Semantic-guided Automatic Natural Image Matting with Trimap Generation Network and Light-weight Non-local Attention. (arXiv:2103.17020v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.03771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Recently, heatmap regression models have become popular due to their superior\nperformance in locating facial landmarks. However, three major problems still\nexist among these models: (1) they are computationally expensive; (2) they\nusually lack explicit constraints on global shapes; (3) domain gaps are\ncommonly present. To address these problems, we propose Pixel-in-Pixel Net\n(PIPNet) for facial landmark detection. The proposed model is equipped with a\nnovel detection head based on heatmap regression, which conducts score and\noffset predictions simultaneously on low-resolution feature maps. By doing so,\nrepeated upsampling layers are no longer necessary, enabling the inference time\nto be largely reduced without sacrificing model accuracy. Besides, a simple but\neffective neighbor regression module is proposed to enforce local constraints\nby fusing predictions from neighboring landmarks, which enhances the robustness\nof the new detection head. To further improve the cross-domain generalization\ncapability of PIPNet, we propose self-training with curriculum. This training\nstrategy is able to mine more reliable pseudo-labels from unlabeled data across\ndomains by starting with an easier task, then gradually increasing the\ndifficulty to provide more precise labels. Extensive experiments demonstrate\nthe superiority of PIPNet, which obtains state-of-the-art results on three out\nof six popular benchmarks under the supervised setting. The results on two\ncross-domain test sets are also consistently improved compared to the\nbaselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200\nFPS on CPU and GPU, respectively, while still maintaining a competitive\naccuracy to state-of-the-art methods. The code of PIPNet is available at\nhttps://github.com/jhb86253817/PIPNet.",
          "link": "http://arxiv.org/abs/2003.03771",
          "publishedOn": "2021-09-14T07:20:11.294Z",
          "wordCount": null,
          "title": "Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild. (arXiv:2003.03771v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wudenka_M/0/1/0/all/0/1\">Martin Wudenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_M/0/1/0/all/0/1\">Marcus G. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedler_A/0/1/0/all/0/1\">Armin Wedler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturzl_W/0/1/0/all/0/1\">Wolfgang St&#xfc;rzl</a>",
          "description": "In the future, extraterrestrial expeditions will not only be conducted by\nrovers but also by flying robots. The technical demonstration drone Ingenuity,\nthat just landed on Mars, will mark the beginning of a new era of exploration\nunhindered by terrain traversability. Robust self-localization is crucial for\nthat. Cameras that are lightweight, cheap and information-rich sensors are\nalready used to estimate the ego-motion of vehicles. However, methods proven to\nwork in man-made environments cannot simply be deployed on other planets. The\nhighly repetitive textures present in the wastelands of Mars pose a huge\nchallenge to descriptor matching based approaches.\n\nIn this paper, we present an advanced robust monocular odometry algorithm\nthat uses efficient optical flow tracking to obtain feature correspondences\nbetween images and a refined keyframe selection criterion. In contrast to most\nother approaches, our framework can also handle rotation-only motions that are\nparticularly challenging for monocular odometry systems. Furthermore, we\npresent a novel approach to estimate the current risk of scale drift based on a\nprincipal component analysis of the relative translation information matrix.\nThis way we obtain an implicit measure of uncertainty. We evaluate the validity\nof our approach on all sequences of a challenging real-world dataset captured\nin a Mars-like environment and show that it outperforms state-of-the-art\napproaches.",
          "link": "http://arxiv.org/abs/2109.05509",
          "publishedOn": "2021-09-14T07:20:11.153Z",
          "wordCount": null,
          "title": "Towards Robust Monocular Visual Odometry for Flying Robots on Planetary Missions. (arXiv:2109.05509v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bohong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "A resource-adaptive supernet adjusts its subnets for inference to fit the\ndynamically available resources. In this paper, we propose Prioritized Subnet\nSampling to train a resource-adaptive supernet, termed PSS-Net. We maintain\nmultiple subnet pools, each of which stores the information of substantial\nsubnets with similar resource consumption. Considering a resource constraint,\nsubnets conditioned on this resource constraint are sampled from a pre-defined\nsubnet structure space and high-quality ones will be inserted into the\ncorresponding subnet pool. Then, the sampling will gradually be prone to\nsampling subnets from the subnet pools. Moreover, the one with a better\nperformance metric is assigned with higher priority to train our PSS-Net, if\nsampling is from a subnet pool. At the end of training, our PSS-Net retains the\nbest subnet in each pool to entitle a fast switch of high-quality subnets for\ninference when the available resources vary. Experiments on ImageNet using\nMobileNetV1/V2 show that our PSS-Net can well outperform state-of-the-art\nresource-adaptive supernets. Our project is at\nhttps://github.com/chenbong/PSS-Net.",
          "link": "http://arxiv.org/abs/2109.05432",
          "publishedOn": "2021-09-14T07:20:11.081Z",
          "wordCount": 618,
          "title": "Prioritized Subnet Sampling for Resource-Adaptive Supernet Training. (arXiv:2109.05432v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zeyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbo Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suttie_M/0/1/0/all/0/1\">Michael Suttie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>",
          "description": "Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result\nin a series of cranio-facial anomalies, and behavioral and neurocognitive\nproblems. Current diagnosis of FAS is typically done by identifying a set of\nfacial characteristics, which are often obtained by manual examination.\nAnatomical landmark detection, which provides rich geometric information, is\nimportant to detect the presence of FAS associated facial anomalies. This\nimaging application is characterized by large variations in data appearance and\nlimited availability of labeled data. Current deep learning-based heatmap\nregression methods designed for facial landmark detection in natural images\nassume availability of large datasets and are therefore not wellsuited for this\napplication. To address this restriction, we develop a new regularized transfer\nlearning approach that exploits the knowledge of a network learned on large\nfacial recognition datasets. In contrast to standard transfer learning which\nfocuses on adjusting the pre-trained weights, the proposed learning approach\nregularizes the model behavior. It explicitly reuses the rich visual semantics\nof a domain-similar source model on the target task data as an additional\nsupervisory signal for regularizing landmark detection optimization.\nSpecifically, we develop four regularization constraints for the proposed\ntransfer learning, including constraining the feature outputs from\nclassification and intermediate layers, as well as matching activation\nattention maps in both spatial and channel levels. Experimental evaluation on a\ncollected clinical imaging dataset demonstrate that the proposed approach can\neffectively improve model generalizability under limited training samples, and\nis advantageous to other approaches in the literature.",
          "link": "http://arxiv.org/abs/2109.05485",
          "publishedOn": "2021-09-14T07:20:11.074Z",
          "wordCount": 726,
          "title": "Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1\">Fangzhou Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruigang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>",
          "description": "State-of-the-art methods for driving-scene LiDAR-based perception (including\npoint cloud semantic segmentation, panoptic segmentation and 3D detection,\n\\etc) often project the point clouds to 2D space and then process them via 2D\nconvolution. Although this cooperation shows the competitiveness in the point\ncloud, it inevitably alters and abandons the 3D topology and geometric\nrelations. A natural remedy is to utilize the 3D voxelization and 3D\nconvolution network. However, we found that in the outdoor point cloud, the\nimprovement obtained in this way is quite limited. An important reason is the\nproperty of the outdoor point cloud, namely sparsity and varying density.\nMotivated by this investigation, we propose a new framework for the outdoor\nLiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution\nnetworks are designed to explore the 3D geometric pattern while maintaining\nthese inherent properties. The proposed model acts as a backbone and the\nlearned features from this model can be used for downstream tasks such as point\ncloud semantic and panoptic segmentation or 3D detection. In this paper, we\nbenchmark our model on these three tasks. For semantic segmentation, we\nevaluate the proposed model on several large-scale datasets, \\ie,\nSemanticKITTI, nuScenes and A2D2. Our method achieves the state-of-the-art on\nthe leaderboard of SemanticKITTI (both single-scan and multi-scan challenge),\nand significantly outperforms existing methods on nuScenes and A2D2 dataset.\nFurthermore, the proposed 3D framework also shows strong performance and good\ngeneralization on LiDAR panoptic segmentation and LiDAR 3D detection.",
          "link": "http://arxiv.org/abs/2109.05441",
          "publishedOn": "2021-09-14T07:20:11.039Z",
          "wordCount": 714,
          "title": "Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-based Perception. (arXiv:2109.05441v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_N/0/1/0/all/0/1\">Naina Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_F/0/1/0/all/0/1\">Florian Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunz_A/0/1/0/all/0/1\">Andreas Kunz</a>",
          "description": "Scene graphs are nodes and edges consisting of objects and object-object\nrelationships, respectively. Scene graph generation (SGG) aims to identify the\nobjects and their relationships. We propose a bidirectional GRU (BiGRU)\ntransformer network (BGT-Net) for the scene graph generation for images. This\nmodel implements novel object-object communication to enhance the object\ninformation using a BiGRU layer. Thus, the information of all objects in the\nimage is available for the other objects, which can be leveraged later in the\nobject prediction step. This object information is used in a transformer\nencoder to predict the object class as well as to create object-specific edge\ninformation via the use of another transformer encoder. To handle the dataset\nbias induced by the long-tailed relationship distribution, softening with a\nlog-softmax function and adding a bias adaptation term to regulate the bias for\nevery relation prediction individually showed to be an effective approach. We\nconducted an elaborate study on experiments and ablations using open-source\ndatasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection\ndatasets, demonstrating the effectiveness of the proposed model over state of\nthe art.",
          "link": "http://arxiv.org/abs/2109.05346",
          "publishedOn": "2021-09-14T07:20:11.016Z",
          "wordCount": 652,
          "title": "BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation. (arXiv:2109.05346v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>",
          "description": "The utilization of prior knowledge about anomalies is an essential issue for\nanomaly detections. Recently, the visual attention mechanism has become a\npromising way to improve the performance of CNNs for some computer vision\ntasks. In this paper, we propose a novel model called Layer-wise External\nAttention Network (LEA-Net) for efficient image anomaly detection. The core\nidea relies on the integration of unsupervised and supervised anomaly detectors\nvia the visual attention mechanism. Our strategy is as follows: (i) Prior\nknowledge about anomalies is represented as the anomaly map generated by\nunsupervised learning of normal instances, (ii) The anomaly map is translated\nto an attention map by the external network, (iii) The attention map is then\nincorporated into intermediate layers of the anomaly detection network.\nNotably, this layer-wise external attention can be applied to any CNN model in\nan end-to-end training manner. For a pilot study, we validate LEA-Net on color\nanomaly detection tasks. Through extensive experiments on PlantVillage, MVTec\nAD, and Cloud datasets, we demonstrate that the proposed layer-wise visual\nattention mechanism consistently boosts anomaly detection performances of an\nexisting CNN model, even on imbalanced datasets. Moreover, we show that our\nattention mechanism successfully boosts the performance of several CNN models.",
          "link": "http://arxiv.org/abs/2109.05493",
          "publishedOn": "2021-09-14T07:20:11.009Z",
          "wordCount": 650,
          "title": "LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05457",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roshanzamir_M/0/1/0/all/0/1\">Mohamad Roshanzamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roshanzamir_M/0/1/0/all/0/1\">Mahdi Roshanzamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosrave_A/0/1/0/all/0/1\">Abbas Khosrave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>",
          "description": "One of the most common problems encountered in human-computer interaction is\nautomatic facial expression recognition. Although it is easy for human observer\nto recognize facial expressions, automatic recognition remains difficult for\nmachines. One of the methods that machines can recognize facial expression is\nanalyzing the changes in face during facial expression presentation. In this\npaper, optical flow algorithm was used to extract deformation or motion vectors\ncreated in the face because of facial expressions. Then, these extracted motion\nvectors are used to be analyzed. Their positions and directions were exploited\nfor automatic facial expression recognition using different data mining\ntechniques. It means that by employing motion vector features used as our data,\nfacial expressions were recognized. Some of the most state-of-the-art\nclassification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL),\nSVM and Discriminant algorithms were used to classify the extracted motion\nvectors. Using 10-fold cross validation, their performances were calculated. To\ncompare their performance more precisely, the test was repeated 50 times.\nMeanwhile, the deformation of face was also analyzed in this research. For\nexample, what exactly happened in each part of face when a person showed fear?\nExperimental results on Extended Cohen-Kanade (CK+) facial expression dataset\ndemonstrated that the best methods were DL, SVM and C5.0, with the accuracy of\n95.3%, 92.8% and 90.2% respectively.",
          "link": "http://arxiv.org/abs/2109.05457",
          "publishedOn": "2021-09-14T07:20:10.986Z",
          "wordCount": 690,
          "title": "What happens in Face during a facial expression? Using data mining techniques to analyze facial expression motion vectors. (arXiv:2109.05457v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_E/0/1/0/all/0/1\">Erfan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>",
          "description": "Recent studies have shown that convolutional neural networks (CNNs) are not\nthe only feasible solution for image classification. Furthermore, weight\nsharing and backpropagation used in CNNs do not correspond to the mechanisms\npresent in the primate visual system. To propose a more biologically plausible\nsolution, we designed a locally connected spiking neural network (SNN) trained\nusing spike-timing-dependent plasticity (STDP) and its reward-modulated variant\n(R-STDP) learning rules. The use of spiking neurons and local connections along\nwith reinforcement learning (RL) led us to the nomenclature BioLCNet for our\nproposed architecture. Our network consists of a rate-coded input layer\nfollowed by a locally connected hidden layer and a decoding output layer. A\nspike population-based voting scheme is adopted for decoding in the output\nlayer. We used the MNIST dataset to obtain image classification accuracy and to\nassess the robustness of our rewarding system to varying target responses.",
          "link": "http://arxiv.org/abs/2109.05539",
          "publishedOn": "2021-09-14T07:20:10.972Z",
          "wordCount": 612,
          "title": "BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2009.09780",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lucas O. Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1\">Rodolfo M. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1\">Diego Bertolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1\">George D. C. Cavalcanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1\">Yandre M. G. Costa</a>",
          "description": "COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.",
          "link": "http://arxiv.org/abs/2009.09780",
          "publishedOn": "2021-09-14T07:20:10.965Z",
          "wordCount": 763,
          "title": "Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1\">Jatin Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Sahil Sharma</a>",
          "description": "Deep learning has been successfully appertained to solve various complex\nproblems in the area of big data analytics to computer vision. A deep\nlearning-powered application recently emerged is Deep Fake. It helps to create\nfake images and videos that human cannot distinguish them from the real ones\nand are recent off-shelf manipulation technique that allows swapping two\nidentities in a single video. Technology is a controversial technology with\nmany wide-reaching issues impacting society. So, to counter this emerging\nproblem, we introduce a dataset of 140k real and fake faces which contain 70k\nreal faces from the Flickr dataset collected by Nvidia, as well as 70k fake\nfaces sampled from 1 million fake faces generated by style GAN. We will train\nour model in the dataset so that our model can identify real or fake faces.",
          "link": "http://arxiv.org/abs/2109.05397",
          "publishedOn": "2021-09-14T07:20:10.955Z",
          "wordCount": 576,
          "title": "Challenges and Solutions in DeepFakes. (arXiv:2109.05397v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chuanxin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yucheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Wenxuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>",
          "description": "Transformers have sprung up in the field of computer vision. In this work, we\nexplore whether the core self-attention module in Transformer is the key to\nachieving excellent performance in image recognition. To this end, we build an\nattention-free network called sMLPNet based on the existing MLP-based vision\nmodels. Specifically, we replace the MLP module in the token-mixing step with a\nnovel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along\nthe axial directions and the parameters are shared among rows or columns. By\nsparse connection and weight sharing, sMLP module significantly reduces the\nnumber of model parameters and computational complexity, avoiding the common\nover-fitting problem that plagues the performance of MLP-like models. When only\ntrained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1\naccuracy with only 24M parameters, which is much better than most CNNs and\nvision Transformers under the same model size constraint. When scaling up to\n66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the\nstate-of-the-art Swin Transformer. The success of sMLPNet suggests that the\nself-attention mechanism is not necessarily a silver bullet in computer vision.\nCode will be made publicly available.",
          "link": "http://arxiv.org/abs/2109.05422",
          "publishedOn": "2021-09-14T07:20:10.919Z",
          "wordCount": 645,
          "title": "Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05496",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhui Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Liangcai Cao</a>",
          "description": "This paper considers the constrained total variation (TV) denoising problem\nfor complex-valued images. We extend the definition of TV seminorms for\nreal-valued images to dealing with complex-valued ones. In particular, we\nintroduce two types of complex TV in both isotropic and anisotropic forms. To\nsolve the constrained denoising problem, we adopt a dual approach and derive an\naccelerated gradient projection algorithm. We further generalize the proposed\ndenoising algorithm as a key building block of the proximal gradient scheme to\nsolve a vast class of complex constrained optimization problems with TV\nregularizers. As an example, we apply the proposed algorithmic framework to\nphase retrieval. We combine the complex TV regularizer with the conventional\nprojection-based method within the constraint complex TV model. Initial results\nfrom both simulated and optical experiments demonstrate the validity of the\nconstrained TV model in extracting sparsity priors within complex-valued\nimages, while also utilizing physically tractable constraints that help speed\nup convergence.",
          "link": "http://arxiv.org/abs/2109.05496",
          "publishedOn": "2021-09-14T07:20:10.901Z",
          "wordCount": 618,
          "title": "A Complex Constrained Total Variation Image Denoising Algorithm with Application to Phase Retrieval. (arXiv:2109.05496v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05287",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_R/0/1/0/all/0/1\">Ruiying Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1\">Guanliang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_Z/0/1/0/all/0/1\">Ziheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_M/0/1/0/all/0/1\">Mu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>",
          "description": "Dual-view snapshot compressive imaging (SCI) aims to capture videos from two\nfield-of-views (FoVs) using a 2D sensor (detector) in a single snapshot,\nachieving joint FoV and temporal compressive sensing, and thus enjoying the\nadvantages of low-bandwidth, low-power, and low-cost. However, it is\nchallenging for existing model-based decoding algorithms to reconstruct each\nindividual scene, which usually require exhaustive parameter tuning with\nextremely long running time for large scale data. In this paper, we propose an\noptical flow-aided recurrent neural network for dual video SCI systems, which\nprovides high-quality decoding in seconds. Firstly, we develop a diversity\namplification method to enlarge the differences between scenes of two FoVs, and\ndesign a deep convolutional neural network with dual branches to separate\ndifferent scenes from the single measurement. Secondly, we integrate the\nbidirectional optical flow extracted from adjacent frames with the recurrent\nneural network to jointly reconstruct each video in a sequential manner.\nExtensive results on both simulation and real data demonstrate the superior\nperformance of our proposed model in a short inference time. The code and data\nare available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.",
          "link": "http://arxiv.org/abs/2109.05287",
          "publishedOn": "2021-09-14T07:20:10.840Z",
          "wordCount": 644,
          "title": "Dual-view Snapshot Compressive Imaging via Optical Flow Aided Recurrent Neural Network. (arXiv:2109.05287v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haijun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>",
          "description": "Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.",
          "link": "http://arxiv.org/abs/2109.05523",
          "publishedOn": "2021-09-14T07:20:10.772Z",
          "wordCount": 635,
          "title": "Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rithish_H/0/1/0/all/0/1\">Harish Rithish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modhugu_R/0/1/0/all/0/1\">Raghava Modhugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1\">Ranjith Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>",
          "description": "Conventional approaches for addressing road safety rely on manual\ninterventions or immobile CCTV infrastructure. Such methods are expensive in\nenforcing compliance to traffic rules and do not scale to large road networks.\nThis paper proposes a simple mobile imaging setup to address several common\nproblems in road safety at scale. We use recent computer vision techniques to\nidentify possible irregularities on roads, the absence of street lights, and\ndefective traffic signs using videos from a moving camera-mounted vehicle.\nBeyond the inspection of static road infrastructure, we also demonstrate the\nmobile imaging solution's applicability to spot traffic violations. Before\ndeploying our system in the real-world, we investigate the strengths and\nshortcomings of computer vision techniques on thirteen condition-based\nhierarchical labels. These conditions include different timings, road type,\ntraffic density, and state of road damage. Our demonstrations are then carried\nout on 2000 km of unconstrained road scenes, captured across an entire city.\nThrough this, we quantitatively measure the overall safety of roads in the city\nthrough carefully constructed metrics. We also show an interactive dashboard\nfor visually inspecting and initiating action in a time, labor and\ncost-efficient manner. Code, models, and datasets used in this work will be\npublicly released.",
          "link": "http://arxiv.org/abs/2109.05226",
          "publishedOn": "2021-09-14T07:20:10.763Z",
          "wordCount": 657,
          "title": "Evaluating Computer Vision Techniques for Urban Mobility on Large-Scale, Unconstrained Roads. (arXiv:2109.05226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05211",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Shiyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Ruihao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aishan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fengwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Deep neural networks (DNNs) are vulnerable to adversarial noises, which\nmotivates the benchmark of model robustness. Existing benchmarks mainly focus\non evaluating the defenses, but there are no comprehensive studies of how\narchitecture design and general training techniques affect robustness.\nComprehensively benchmarking their relationships will be highly beneficial for\nbetter understanding and developing robust DNNs. Thus, we propose RobustART,\nthe first comprehensive Robustness investigation benchmark on ImageNet\n(including open-source toolkit, pre-trained model zoo, datasets, and analyses)\nregarding ARchitecture design (44 human-designed off-the-shelf architectures\nand 1200+ networks from neural architecture search) and Training techniques\n(10+ general techniques, e.g., data augmentation) towards diverse noises\n(adversarial, natural, and system noises). Extensive experiments revealed and\nsubstantiated several insights for the first time, for example: (1) adversarial\ntraining largely improves the clean accuracy and all types of robustness for\nTransformers and MLP-Mixers; (2) with comparable sizes, CNNs > Transformers >\nMLP-Mixers on robustness against natural and system noises; Transformers >\nMLP-Mixers > CNNs on adversarial robustness; (3) for some light-weight\narchitectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing\nmodel sizes or using extra training data cannot improve robustness. Our\nbenchmark this http URL : (1) presents an open-source platform for\nconducting comprehensive evaluation on diverse robustness types; (2) provides a\nvariety of pre-trained models with different training techniques to facilitate\nrobustness evaluation; (3) proposes a new view to better understand the\nmechanism towards designing robust DNN architectures, backed up by the\nanalysis. We will continuously contribute to building this ecosystem for the\ncommunity.",
          "link": "http://arxiv.org/abs/2109.05211",
          "publishedOn": "2021-09-14T07:20:10.752Z",
          "wordCount": 709,
          "title": "RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Muhamamd Ishfaq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>",
          "description": "Stereoscopy exposits a natural perception of distance in a scene, and its\nmanifestation in 3D world understanding is an intuitive phenomenon. However, an\ninnate rigid calibration of binocular vision sensors is crucial for accurate\ndepth estimation. Alternatively, a monocular camera alleviates the limitation\nat the expense of accuracy in estimating depth, and the challenge exacerbates\nin harsh environmental conditions. Moreover, an optical sensor often fails to\nacquire vital signals in harsh environments, and radar is used instead, which\ngives coarse but more accurate signals. This work explores the utility of\ncoarse signals from radar when fused with fine-grained data from a monocular\ncamera for depth estimation in harsh environmental conditions. A variant of\nfeature pyramid network (FPN) extensively operates on fine-grained image\nfeatures at multiple scales with a fewer number of parameters. FPN feature maps\nare fused with sparse radar features extracted with a Convolutional neural\nnetwork. The concatenated hierarchical features are used to predict the depth\nwith ordinal regression. We performed experiments on the nuScenes dataset, and\nthe proposed architecture stays on top in quantitative evaluations with reduced\nparameters and faster inference. The depth estimation results suggest that the\nproposed techniques can be used as an alternative to stereo depth estimation in\ncritical applications in robotics and self-driving cars. The source code will\nbe available in the following: \\url{https://github.com/MI-Hussain/RVMDE}.",
          "link": "http://arxiv.org/abs/2109.05265",
          "publishedOn": "2021-09-14T07:20:10.743Z",
          "wordCount": 671,
          "title": "RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jun Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>",
          "description": "Estimating the articulated 3D hand-object pose from a single RGB image is a\nhighly ambiguous and challenging problem requiring large-scale datasets that\ncontain diverse hand poses, object poses, and camera viewpoints. Most\nreal-world datasets lack this diversity. In contrast, synthetic datasets can\neasily ensure vast diversity, but learning from them is inefficient and suffers\nfrom heavy training consumption. To address the above issues, we propose\nArtiBoost, a lightweight online data enrichment method that boosts articulated\nhand-object pose estimation from the data perspective. ArtiBoost is employed\nalong with a real-world source dataset. During training, ArtiBoost\nalternatively performs data exploration and synthesis. ArtiBoost can cover\nvarious hand-object poses and camera viewpoints based on a Compositional\nhand-object Configuration and Viewpoint space (CCV-space) and can adaptively\nenrich the current hard-discernable samples by a mining strategy. We apply\nArtiBoost on a simple learning baseline network and demonstrate the performance\nboost on several hand-object benchmarks. As an illustrative example, with\nArtiBoost, even a simple baseline network can outperform the previous\nstart-of-the-art based on Transformer on the HO3D dataset. Our code is\navailable at https://github.com/MVIG-SJTU/ArtiBoost.",
          "link": "http://arxiv.org/abs/2109.05488",
          "publishedOn": "2021-09-14T07:20:10.734Z",
          "wordCount": 643,
          "title": "ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05409",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Macar_U/0/1/0/all/0/1\">Uzay Macar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthik_E/0/1/0/all/0/1\">Enamundram Naga Karthik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andr&#xe9;anne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "This paper gives a detailed description of the pipelines used for the 2nd\nedition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.\nAn overview of the data preprocessing steps applied is provided along with a\nbrief description of the pipelines used, in terms of the architecture and the\nhyperparameters. Our code for this work can be found at:\nhttps://github.com/ivadomed/ms-challenge-2021.",
          "link": "http://arxiv.org/abs/2109.05409",
          "publishedOn": "2021-09-14T07:20:10.714Z",
          "wordCount": 557,
          "title": "Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05443",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Woo_B/0/1/0/all/0/1\">Boyeong Woo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Siyu Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marques_M/0/1/0/all/0/1\">Matthew Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engstrom_C/0/1/0/all/0/1\">Craig B. Engstrom</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greer_P/0/1/0/all/0/1\">Peter B. Greer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1\">Stuart Crozier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dowling_J/0/1/0/all/0/1\">Jason A. Dowling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandras_S/0/1/0/all/0/1\">Shekhar S. Chandras</a>",
          "description": "Direct automatic segmentation of objects from 3D medical imaging, such as\nmagnetic resonance (MR) imaging, is challenging as it often involves accurately\nidentifying a number of individual objects with complex geometries within a\nlarge volume under investigation. To address these challenges, most deep\nlearning approaches typically enhance their learning capability by\nsubstantially increasing the complexity or the number of trainable parameters\nwithin their models. Consequently, these models generally require long\ninference time on standard workstations operating clinical MR systems and are\nrestricted to high-performance computing hardware due to their large memory\nrequirement. Further, to fit 3D dataset through these large models using\nlimited computer memory, trade-off techniques such as patch-wise training are\noften used which sacrifice the fine-scale geometric information from input\nimages which could be clinically significant for diagnostic purposes. To\naddress these challenges, we present a compact convolutional neural network\nwith a shallow memory footprint to efficiently reduce the number of model\nparameters required for state-of-art performance. This is critical for\npractical employment as most clinical environments only have low-end hardware\nwith limited computing power and memory. The proposed network can maintain data\nintegrity by directly processing large full-size 3D input volumes with no\npatches required and significantly reduces the computational time required for\nboth training and inference. We also propose a novel loss function with extra\nshape constraint to improve the accuracy for imbalanced classes in 3D MR\nimages.",
          "link": "http://arxiv.org/abs/2109.05443",
          "publishedOn": "2021-09-14T07:20:10.706Z",
          "wordCount": 709,
          "title": "CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.",
          "link": "http://arxiv.org/abs/2109.05281",
          "publishedOn": "2021-09-14T07:20:10.657Z",
          "wordCount": 630,
          "title": "COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05263",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenivasan_L/0/1/0/all/0/1\">Lalithkumar Seenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1\">Ben Glocker</a>",
          "description": "Despite impressive accuracy, deep neural networks are often miscalibrated and\ntend to overly confident predictions. Recent techniques like temperature\nscaling (TS) and label smoothing (LS) show effectiveness in obtaining a\nwell-calibrated model by smoothing logits and hard labels with scalar factors,\nrespectively. However, the use of uniform TS or LS factor may not be optimal\nfor calibrating models trained on a long-tailed dataset where the model\nproduces overly confident probabilities for high-frequency classes. In this\nstudy, we propose class-distribution-aware TS (CDA-TS) and LS (CDA-LS) by\nincorporating class frequency information in model calibration in the context\nof long-tailed distribution. In CDA-TS, the scalar temperature value is\nreplaced with the CDA temperature vector encoded with class frequency to\ncompensate for the over-confidence. Similarly, CDA-LS uses a vector smoothing\nfactor and flattens the hard labels according to their corresponding class\ndistribution. We also integrate CDA optimal temperature vector with\ndistillation loss, which reduces miscalibration in self-distillation (SD). We\nempirically show that class-distribution-aware TS and LS can accommodate the\nimbalanced data distribution yielding superior performance in both calibration\nerror and predictive accuracy. We also observe that SD with an extremely\nimbalanced dataset is less effective in terms of calibration performance. Code\nis available in https://github.com/mobarakol/Class-Distribution-Aware-TS-LS.",
          "link": "http://arxiv.org/abs/2109.05263",
          "publishedOn": "2021-09-14T07:20:10.618Z",
          "wordCount": 658,
          "title": "Class-Distribution-Aware Calibration for Long-Tailed Visual Recognition. (arXiv:2109.05263v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>",
          "description": "Deep hashing approaches, including deep quantization and deep binary hashing,\nhave become a common solution to large-scale image retrieval due to high\ncomputation and storage efficiency. Most existing hashing methods can not\nproduce satisfactory results for fine-grained retrieval, because they usually\nadopt the outputs of the last CNN layer to generate binary codes, which is less\neffective to capture subtle but discriminative visual details. To improve\nfine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization\n(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to\ncapture and preserve fine-grained semantic information from multi-level\nfeatures. Besides, we propose a learnable quantization module with a partial\nattention mechanism, which helps to optimize the most relevant codewords and\nimproves the quantization. Comprehensive experiments demonstrate that PHPQ\noutperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05206",
          "publishedOn": "2021-09-14T07:20:10.610Z",
          "wordCount": 583,
          "title": "Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1\">Faisal Muhammad Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humaira_M/0/1/0/all/0/1\">Mayeesha Humaira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jim_M/0/1/0/all/0/1\">Md Abidur Rahman Khan Jim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ami_A/0/1/0/all/0/1\">Amit Saha Ami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shimul Paul</a>",
          "description": "Image captioning using Encoder-Decoder based approach where CNN is used as\nthe Encoder and sequence generator like RNN as Decoder has proven to be very\neffective. However, this method has a drawback that is sequence needs to be\nprocessed in order. To overcome this drawback some researcher has utilized the\nTransformer model to generate captions from images using English datasets.\nHowever, none of them generated captions in Bengali using the transformer\nmodel. As a result, we utilized three different Bengali datasets to generate\nBengali captions from images using the Transformer model. Additionally, we\ncompared the performance of the transformer-based model with a visual\nattention-based Encoder-Decoder approach. Finally, we compared the result of\nthe transformer-based model with other models that employed different Bengali\nimage captioning datasets.",
          "link": "http://arxiv.org/abs/2109.05218",
          "publishedOn": "2021-09-14T07:20:10.566Z",
          "wordCount": 578,
          "title": "Bornon: Bengali Image Captioning with Transformer-based Deep learning approach. (arXiv:2109.05218v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sucar_E/0/1/0/all/0/1\">Edgar Sucar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Joseph Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>",
          "description": "We show for the first time that a multilayer perceptron (MLP) can serve as\nthe only scene representation in a real-time SLAM system for a handheld RGB-D\ncamera. Our network is trained in live operation without prior data, building a\ndense, scene-specific implicit 3D model of occupancy and colour which is also\nimmediately used for tracking.\n\nAchieving real-time SLAM via continual training of a neural network against a\nlive image stream requires significant innovation. Our iMAP algorithm uses a\nkeyframe structure and multi-processing computation flow, with dynamic\ninformation-guided pixel sampling for speed, with tracking at 10 Hz and global\nmap updating at 2 Hz. The advantages of an implicit MLP over standard dense\nSLAM techniques include efficient geometry representation with automatic detail\ncontrol and smooth, plausible filling-in of unobserved regions such as the back\nsurfaces of objects.",
          "link": "http://arxiv.org/abs/2103.12352",
          "publishedOn": "2021-09-14T07:20:10.463Z",
          "wordCount": 615,
          "title": "iMAP: Implicit Mapping and Positioning in Real-Time. (arXiv:2103.12352v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Molodetskikh_I/0/1/0/all/0/1\">Ivan Molodetskikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_M/0/1/0/all/0/1\">Mikhail Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moskalenko_A/0/1/0/all/0/1\">Andrey Moskalenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1\">Dmitry Vatolin</a>",
          "description": "We propose a novel neural-network-based method to perform matting of videos\ndepicting people that does not require additional user input such as trimaps.\nOur architecture achieves temporal stability of the resulting alpha mattes by\nusing motion-estimation-based smoothing of image-segmentation algorithm\noutputs, combined with convolutional-LSTM modules on U-Net skip connections.\n\nWe also propose a fake-motion algorithm that generates training clips for the\nvideo-matting network given photos with ground-truth alpha mattes and\nbackground videos. We apply random motion to photos and their mattes to\nsimulate movement one would find in real videos and composite the result with\nthe background clips. It lets us train a deep neural network operating on\nvideos in an absence of a large annotated video dataset and provides\nground-truth training-clip foreground optical flow for use in loss functions.",
          "link": "http://arxiv.org/abs/2109.04843",
          "publishedOn": "2021-09-13T07:20:36.708Z",
          "wordCount": 586,
          "title": "Temporally Coherent Person Matting Trained on Fake-Motion Dataset. (arXiv:2109.04843v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>",
          "description": "Self-training is a simple semi-supervised learning approach: Unlabelled\nexamples that attract high-confidence predictions are labelled with their\npredictions and added to the training set, with this process being repeated\nmultiple times. Recently, self-supervision -- learning without manual\nsupervision by solving an automatically-generated pretext task -- has gained\nprominence in deep learning. This paper investigates three different ways of\nincorporating self-supervision into self-training to improve accuracy in image\nclassification: self-supervision as pretraining only, self-supervision\nperformed exclusively in the first iteration of self-training, and\nself-supervision added to every iteration of self-training. Empirical results\non the SVHN, CIFAR-10, and PlantVillage datasets, using both training from\nscratch, and Imagenet-pretrained weights, show that applying self-supervision\nonly in the first iteration of self-training can greatly improve accuracy, for\na modest increase in computation time.",
          "link": "http://arxiv.org/abs/2109.00778",
          "publishedOn": "2021-09-13T07:20:26.173Z",
          "wordCount": 591,
          "title": "Better Self-training for Image Classification through Self-supervision. (arXiv:2109.00778v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seunghwan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>",
          "description": "The task of image-based virtual try-on aims to transfer a target clothing\nitem onto the corresponding region of a person, which is commonly tackled by\nfitting the item to the desired body part and fusing the warped item with the\nperson. While an increasing number of studies have been conducted, the\nresolution of synthesized images is still limited to low (e.g., 256x192), which\nacts as the critical limitation against satisfying online consumers. We argue\nthat the limitation stems from several challenges: as the resolution increases,\nthe artifacts in the misaligned areas between the warped clothes and the\ndesired clothing regions become noticeable in the final results; the\narchitectures used in existing methods have low performance in generating\nhigh-quality body parts and maintaining the texture sharpness of the clothes.\nTo address the challenges, we propose a novel virtual try-on method called\nVITON-HD that successfully synthesizes 1024x768 virtual try-on images.\nSpecifically, we first prepare the segmentation map to guide our virtual try-on\nsynthesis, and then roughly fit the target clothing item to a given person's\nbody. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS\ngenerator to handle the misaligned areas and preserve the details of 1024x768\ninputs. Through rigorous comparison with existing methods, we demonstrate that\nVITON-HD highly surpasses the baselines in terms of synthesized image quality\nboth qualitatively and quantitatively. Code is available at\nhttps://github.com/shadow2496/VITON-HD.",
          "link": "http://arxiv.org/abs/2103.16874",
          "publishedOn": "2021-09-13T07:20:26.101Z",
          "wordCount": 714,
          "title": "VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. (arXiv:2103.16874v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parola_M/0/1/0/all/0/1\">Marco Parola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nannini_A/0/1/0/all/0/1\">Alice Nannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poleggi_S/0/1/0/all/0/1\">Stefano Poleggi</a>",
          "description": "To implement a good Content Based Image Retrieval (CBIR) system, it is\nessential to adopt efficient search methods. One way to achieve this results is\nby exploiting approximate search techniques. In fact, when we deal with very\nlarge collections of data, using an exact search method makes the system very\nslow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to\nimplement a CBIR system that allows us to perform fast similarity search on\ndeep features. Specifically, we exploit transfer learning techniques to extract\ndeep features from images; this phase is done using two famous Convolutional\nNeural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both\npre-trained on ImageNet. Then we try out several fully connected deep neural\nnetworks, built on top of both of the previously mentioned CNNs in order to\nfine-tuned them on our dataset. In both of previous cases, we index the\nfeatures within our LSH index implementation and within a sequential scan, to\nbetter understand how much the introduction of the index affects the results.\nFinally, we carry out a performance analysis: we evaluate the relevance of the\nresult set, computing the mAP (mean Average Precision) value obtained during\nthe different experiments with respect to the number of done comparison and\nvarying the hyper-parameter values of the LSH index.",
          "link": "http://arxiv.org/abs/2108.13301",
          "publishedOn": "2021-09-13T07:20:26.052Z",
          "wordCount": 668,
          "title": "Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Belousov_S/0/1/0/all/0/1\">Sergei Belousov</a>",
          "description": "In recent years, the use of Generative Adversarial Networks (GANs) has become\nvery popular in generative image modeling. While style-based GAN architectures\nyield state-of-the-art results in high-fidelity image synthesis,\ncomputationally, they are highly complex. In our work, we focus on the\nperformance optimization of style-based generative models. We analyze the most\ncomputationally hard parts of StyleGAN2, and propose changes in the generator\nnetwork to make it possible to deploy style-based generative networks in the\nedge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer\nparameters and is x9.5 less computationally complex than StyleGAN2, while\nproviding comparable quality.",
          "link": "http://arxiv.org/abs/2104.04767",
          "publishedOn": "2021-09-13T07:20:25.956Z",
          "wordCount": 570,
          "title": "MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis. (arXiv:2104.04767v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1\">Gobinda Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>",
          "description": "Artificial learning systems aspire to mimic human intelligence by continually\nlearning from a stream of tasks without forgetting past knowledge. One way to\nenable such learning is to store past experiences in the form of input examples\nin episodic memory and replay them when learning new tasks. However,\nperformance of such method suffers as the size of the memory becomes smaller.\nIn this paper, we propose a new approach for experience replay, where we select\nthe past experiences by looking at the saliency maps which provide visual\nexplanations for the model's decision. Guided by these saliency maps, we pack\nthe memory with only the parts or patches of the input images important for the\nmodel's prediction. While learning a new task, we replay these memory patches\nwith appropriate zero-padding to remind the model about its past decisions. We\nevaluate our algorithm on diverse image classification datasets and report\nbetter performance than the state-of-the-art approaches. With qualitative and\nquantitative analyses we show that our method captures richer summary of past\nexperiences without any memory increase, and hence performs well with small\nepisodic memory.",
          "link": "http://arxiv.org/abs/2109.04954",
          "publishedOn": "2021-09-13T07:20:25.903Z",
          "wordCount": 631,
          "title": "Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naively training all tasks\ntogether in one model often degrades performance, and exhaustively searching\nthrough combinations of task groupings can be prohibitively expensive. As a\nresult, efficiently identifying the tasks that would benefit from co-training\nremains a challenging design question without a clear solution. In this paper,\nwe suggest an approach to select which tasks should train together in\nmulti-task learning models. Our method determines task groupings in a single\ntraining run by co-training all tasks together and quantifying the effect to\nwhich one task's gradient would affect another task's loss. On the large-scale\nTaskonomy computer vision dataset, we find this method can decrease test loss\nby 10.0\\% compared to simply training all tasks together while operating 11.6\ntimes faster than a state-of-the-art task grouping method.",
          "link": "http://arxiv.org/abs/2109.04617",
          "publishedOn": "2021-09-13T07:20:25.897Z",
          "wordCount": 597,
          "title": "Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1\">Kuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haiyun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gaopan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1\">Honglin Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>",
          "description": "In person re-identification, extracting part-level features from person\nimages has been verified to be crucial. Most of existing CNN-based methods only\nlocate the human parts coarsely, or rely on pre-trained human parsing models\nand fail in locating the identifiable non-human parts (e.g., knapsack). In this\npaper, we introduce an alignment scheme in Transformer architecture for the\nfirst time and propose the Auto-Aligned Transformer (AAformer) to automatically\nlocate both the human parts and non-human ones at patch-level. We introduce the\n\"part tokens\", which are learnable vectors, to extract part features in\nTransformer. A part token only interacts with a local subset of patches in\nself-attention and learns to be the part representation. To adaptively group\nthe image patches into different subsets, we design the Auto-Alignment.\nAuto-Alignment employs a fast variant of Optimal Transport algorithm to online\ncluster the patch embeddings into several groups with the part tokens as their\nprototypes. We harmoniously integrate the part alignment into the\nself-attention and the output part tokens can be directly used for retrieval.\nExtensive experiments validate the effectiveness of part tokens and the\nsuperiority of AAformer over various state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2104.00921",
          "publishedOn": "2021-09-13T07:20:25.839Z",
          "wordCount": 671,
          "title": "AAformer: Auto-Aligned Transformer for Person Re-Identification. (arXiv:2104.00921v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alamayreh_O/0/1/0/all/0/1\">Omran Alamayreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>",
          "description": "Research on the detection of AI-generated videos has focused almost\nexclusively on face videos, usually referred to as deepfakes. Manipulations\nlike face swapping, face reenactment and expression manipulation have been the\nsubject of an intense research with the development of a number of efficient\ntools to distinguish artificial videos from genuine ones. Much less attention\nhas been paid to the detection of artificial non-facial videos. Yet, new tools\nfor the generation of such kind of videos are being developed at a fast pace\nand will soon reach the quality level of deepfake videos. The goal of this\npaper is to investigate the detectability of a new kind of AI-generated videos\nframing driving street sequences (here referred to as DeepStreets videos),\nwhich, by their nature, can not be analysed with the same tools used for facial\ndeepfakes. Specifically, we present a simple frame-based detector, achieving\nvery good performance on state-of-the-art DeepStreets videos generated by the\nVid2vid architecture. Noticeably, the detector retains very good performance on\ncompressed videos, even when the compression level used during training does\nnot match that used for the test videos.",
          "link": "http://arxiv.org/abs/2109.04991",
          "publishedOn": "2021-09-13T07:20:25.833Z",
          "wordCount": 631,
          "title": "Detection of GAN-synthesized street videos. (arXiv:2109.04991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04573",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bottcher_W/0/1/0/all/0/1\">Wolfgang Bottcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lama_N/0/1/0/all/0/1\">Nikesh Lama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGinnity_T/0/1/0/all/0/1\">T.M. McGinnity</a>",
          "description": "Robots need to exploit high-quality information on grasped objects to\ninteract with the physical environment. Haptic data can therefore be used for\nsupplementing the visual modality. This paper investigates the use of\nConvolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) neural\nnetwork architectures for object classification on Spatio-temporal tactile\ngrasping data. Furthermore, we compared these methods using data from two\ndifferent fingertip sensors (namely the BioTac SP and WTS-FT) in the same\nphysical setup, allowing for a realistic comparison across methods and sensors\nfor the same tactile object classification dataset. Additionally, we propose a\nway to create more training examples from the recorded data. The results show\nthat the proposed method improves the maximum accuracy from 82.4% (BioTac SP\nfingertips) and 90.7% (WTS-FT fingertips) with complete time-series data to\nabout 94% for both sensor types.",
          "link": "http://arxiv.org/abs/2109.04573",
          "publishedOn": "2021-09-13T07:20:25.811Z",
          "wordCount": 601,
          "title": "Object recognition for robotics from tactile time series data utilising different neural network architectures. (arXiv:2109.04573v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>",
          "description": "We present the first method capable of photorealistically reconstructing\ndeformable scenes using photos/videos captured casually from mobile phones. Our\napproach augments neural radiance fields (NeRF) by optimizing an additional\ncontinuous volumetric deformation field that warps each observed point into a\ncanonical 5D NeRF. We observe that these NeRF-like deformation fields are prone\nto local minima, and propose a coarse-to-fine optimization method for\ncoordinate-based models that allows for more robust optimization. By adapting\nprinciples from geometry processing and physical simulation to NeRF-like\nmodels, we propose an elastic regularization of the deformation field that\nfurther improves robustness. We show that our method can turn casually captured\nselfie photos/videos into deformable NeRF models that allow for photorealistic\nrenderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We\nevaluate our method by collecting time-synchronized data using a rig with two\nmobile phones, yielding train/validation images of the same pose at different\nviewpoints. We show that our method faithfully reconstructs non-rigidly\ndeforming scenes and reproduces unseen views with high fidelity.",
          "link": "http://arxiv.org/abs/2011.12948",
          "publishedOn": "2021-09-13T07:20:25.806Z",
          "wordCount": 688,
          "title": "Nerfies: Deformable Neural Radiance Fields. (arXiv:2011.12948v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Temporal grounding aims to temporally localize a video moment in the video\nwhose semantics are related to a given natural language query. Existing methods\ntypically apply a detection or regression pipeline on the fused representation\nwith a focus on designing complicated heads and fusion strategies. Instead,\nfrom a perspective on temporal grounding as a metric-learning problem, we\npresent a Dual Matching Network (DMN), to directly model the relations between\nlanguage queries and video moments in a joint embedding space. This new\nmetric-learning framework enables fully exploiting negative samples from two\nnew aspects: constructing negative cross-modal pairs from a dual matching\nscheme and mining negative pairs across different videos. These new negative\nsamples could enhance the joint representation learning of two modalities via\ncross-modal pair discrimination to maximize their mutual information.\nExperiments show that DMN achieves highly competitive performance compared with\nstate-of-the-art methods on four video grounding benchmarks. Based on DMN, we\npresent a winner solution for STVG challenge of the 3rd PIC workshop. This\nsuggests that metric-learning is still a promising method for temporal\ngrounding via capturing the essential cross-modal correlation in a joint\nembedding space.",
          "link": "http://arxiv.org/abs/2109.04872",
          "publishedOn": "2021-09-13T07:20:25.764Z",
          "wordCount": 661,
          "title": "Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding. (arXiv:2109.04872v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-09-13T07:20:23.616Z",
          "wordCount": 602,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04883",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Birjukovs_M/0/1/0/all/0/1\">Mihails Birjukovs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Trtik_P/0/1/0/all/0/1\">Pavel Trtik</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kaestner_A/0/1/0/all/0/1\">Anders Kaestner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hovind_J/0/1/0/all/0/1\">Jan Hovind</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klevs_M/0/1/0/all/0/1\">Martins Klevs</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Thomsen_K/0/1/0/all/0/1\">Knud Thomsen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jakovics_A/0/1/0/all/0/1\">Andris Jakovics</a>",
          "description": "We demonstrate a new image processing methodology for resolving gas bubbles\ntravelling through liquid metal from dynamic neutron radiography images with\nintrinsically low signal-to-noise ratio. Image pre-processing, denoising and\nbubble segmentation are described in detail, with practical recommendations.\nExperimental validation is presented - stationary and moving reference bodies\nwith neutron-transparent cavities are radiographed with imaging conditions\nsimilar to the cases with bubbles in liquid metal. The new methods are applied\nto our experimental data from previous and recent imaging campaigns, and the\nperformance of the methods proposed in this paper is compared against our\npreviously developed methods. Significant improvements are observed as well as\nthe capacity to reliably extract physically meaningful information from\nmeasurements performed under highly adverse imaging conditions. The showcased\nimage processing solution and separate elements thereof are readily extendable\nbeyond the present application, and have been made open-source.",
          "link": "http://arxiv.org/abs/2109.04883",
          "publishedOn": "2021-09-13T07:20:23.590Z",
          "wordCount": 601,
          "title": "Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images. (arXiv:2109.04883v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recent studies indicate that hierarchical Vision Transformer with a macro\narchitecture of interleaved non-overlapped window-based self-attention \\&\nshifted-window operation is able to achieve state-of-the-art performance in\nvarious visual recognition tasks, and challenges the ubiquitous convolutional\nneural networks (CNNs) using densely slid kernels. Most follow-up works attempt\nto replace the shifted-window operation with other kinds of cross-window\ncommunication paradigms, while treating self-attention as the de-facto standard\nfor window-based information aggregation. In this manuscript, we question\nwhether self-attention is the only choice for hierarchical Vision Transformer\nto attain strong performance, and the effects of different kinds of\ncross-window communication. To this end, we replace self-attention layers with\nembarrassingly simple linear mapping layers, and the resulting proof-of-concept\narchitecture termed as LinMapper can achieve very strong performance in\nImageNet-1k image recognition. Moreover, we find that LinMapper is able to\nbetter leverage the pre-trained representations from image recognition and\ndemonstrates excellent transfer learning properties on downstream dense\nprediction tasks such as object detection and instance segmentation. We also\nexperiment with other alternatives to self-attention for content aggregation\ninside each non-overlapped window under different cross-window communication\napproaches, which all give similar competitive results. Our study reveals that\nthe \\textbf{macro architecture} of Swin model families, other than specific\naggregation layers or specific means of cross-window communication, may be more\nresponsible for its strong performance and is the real challenger to the\nubiquitous CNN's dense sliding window paradigm. Code and models will be\npublicly available to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.02174",
          "publishedOn": "2021-09-13T07:20:23.545Z",
          "wordCount": 720,
          "title": "What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>",
          "description": "Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space",
          "link": "http://arxiv.org/abs/2109.04993",
          "publishedOn": "2021-09-13T07:20:23.510Z",
          "wordCount": 581,
          "title": "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qianhao Ning</a>",
          "description": "To eliminate the problems of large dimensional differences, big semantic gap,\nand mutual interference caused by hybrid features, in this paper, we propose a\nnovel Multi-Features Guidance Network for partial-to-partial point cloud\nregistration(MFG). The proposed network mainly includes four parts: keypoints'\nfeature extraction, correspondences searching, correspondences credibility\ncomputation, and SVD, among which correspondences searching and correspondence\ncredibility computation are the cores of the network. Unlike the previous work,\nwe utilize the shape features and the spatial coordinates to guide\ncorrespondences search independently and fusing the matching results to obtain\nthe final matching matrix. In the correspondences credibility computation\nmodule, based on the conflicted relationship between the features matching\nmatrix and the coordinates matching matrix, we score the reliability for each\ncorrespondence, which can reduce the impact of mismatched or non-matched\npoints. Experimental results show that our network outperforms the current\nstate-of-the-art while maintaining computational efficiency.",
          "link": "http://arxiv.org/abs/2011.12079",
          "publishedOn": "2021-09-13T07:20:23.466Z",
          "wordCount": 643,
          "title": "Multi-Features Guidance Network for partial-to-partial point cloud registration. (arXiv:2011.12079v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Temporal action localization (TAL) is an important and challenging problem in\nvideo understanding. However, most existing TAL benchmarks are built upon the\ncoarse granularity of action classes, which exhibits two major limitations in\nthis task. First, coarse-level actions can make the localization models overfit\nin high-level context information, and ignore the atomic action details in the\nvideo. Second, the coarse action classes often lead to the ambiguous\nannotations of temporal boundaries, which are inappropriate for temporal action\nlocalization. To tackle these problems, we develop a novel large-scale and\nfine-grained video dataset, coined as FineAction, for temporal action\nlocalization. In total, FineAction contains 103K temporal instances of 106\naction categories, annotated in 17K untrimmed videos. FineAction introduces new\nopportunities and challenges for temporal action localization, thanks to its\ndistinct characteristics of fine action classes with rich diversity, dense\nannotations of multiple instances, and co-occurring actions of different\nclasses. To benchmark FineAction, we systematically investigate the performance\nof several popular temporal localization methods on it, and deeply analyze the\ninfluence of short-duration and fine-grained instances in temporal action\nlocalization. We believe that FineAction can advance research of temporal\naction localization and beyond.",
          "link": "http://arxiv.org/abs/2105.11107",
          "publishedOn": "2021-09-13T07:20:23.432Z",
          "wordCount": 679,
          "title": "FineAction: A Fine-Grained Video Dataset for Temporal Action Localization. (arXiv:2105.11107v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.",
          "link": "http://arxiv.org/abs/2104.05845",
          "publishedOn": "2021-09-13T07:20:23.422Z",
          "wordCount": 616,
          "title": "Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>",
          "description": "Neural networks have been successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are more difficult to train successfully for\nsemi-supervised problems where small amounts of labeled instances are available\nalong with a large number of unlabeled instances. This work explores a new\ntraining method for semi-supervised learning that is based on similarity\nfunction learning using a Siamese network to obtain a suitable embedding. The\nlearned representations are discriminative in Euclidean space, and hence can be\nused for labeling unlabeled instances using a nearest-neighbor classifier.\nConfident predictions of unlabeled instances are used as true labels for\nretraining the Siamese network on the expanded training set. This process is\napplied iteratively. We perform an empirical study of this iterative\nself-training algorithm. For improving unlabeled predictions, local learning\nwith global consistency [22] is also evaluated.",
          "link": "http://arxiv.org/abs/2109.00794",
          "publishedOn": "2021-09-13T07:20:23.405Z",
          "wordCount": 627,
          "title": "Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>",
          "description": "Most recent self-supervised learning (SSL) algorithms learn features by\ncontrasting between instances of images or by clustering the images and then\ncontrasting between the image clusters. We introduce a simple mean-shift\nalgorithm that learns representations by grouping images together without\ncontrasting between them or adopting much of prior on the structure of the\nclusters. We simply \"shift\" the embedding of each image to be close to the\n\"mean\" of its neighbors. Since in our setting, the closest neighbor is always\nanother augmentation of the same image, our model will be identical to BYOL\nwhen using only one nearest neighbor instead of 5 as used in our experiments.\nOur model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200\nepochs outperforming BYOL. Our code is available here:\nhttps://github.com/UMBCvision/MSF",
          "link": "http://arxiv.org/abs/2105.07269",
          "publishedOn": "2021-09-13T07:20:23.386Z",
          "wordCount": 593,
          "title": "Mean Shift for Self-Supervised Learning. (arXiv:2105.07269v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naive formulations often\ndegrade performance and in particular, identifying the tasks that would benefit\nfrom co-training remains a challenging design question. In this paper, we\nanalyze the dynamics of information transfer, or transference, across tasks\nthroughout training. Specifically, we develop a similarity measure that can\nquantify transference among tasks and use this quantity to both better\nunderstand the optimization dynamics of multi-task learning as well as improve\noverall learning performance. In the latter case, we propose two methods to\nleverage our transference metric. The first operates at a macro-level by\nselecting which tasks should train together while the second functions at a\nmicro-level by determining how to combine task gradients at each training step.\nWe find these methods can lead to significant improvement over prior work on\nthree supervised multi-task learning benchmarks and one multi-task\nreinforcement learning paradigm.",
          "link": "http://arxiv.org/abs/2010.15413",
          "publishedOn": "2021-09-13T07:20:23.378Z",
          "wordCount": 652,
          "title": "Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03679",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chotzoglou_E/0/1/0/all/0/1\">Elisa Chotzoglou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Day_T/0/1/0/all/0/1\">Thomas Day</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matthew_J/0/1/0/all/0/1\">Jacqueline Matthew</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lloyd_D/0/1/0/all/0/1\">David Lloyd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Simpson_J/0/1/0/all/0/1\">John Simpson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Congenital heart disease is considered as one the most common groups of\ncongenital malformations which affects $6-11$ per $1000$ newborns. In this\nwork, an automated framework for detection of cardiac anomalies during\nultrasound screening is proposed and evaluated on the example of Hypoplastic\nLeft Heart Syndrome (HLHS), a sub-category of congenital heart disease. We\npropose an unsupervised approach that learns healthy anatomy exclusively from\nclinically confirmed normal control patients. We evaluate a number of known\nanomaly detection frameworks together with a model architecture based on the\n$\\alpha$-GAN network and find evidence that the proposed model performs\nsignificantly better than the state-of-the-art in image-based anomaly\ndetection, yielding average $0.81$ AUC \\emph{and} a better robustness towards\ninitialisation compared to previous works.",
          "link": "http://arxiv.org/abs/2012.03679",
          "publishedOn": "2021-09-13T07:20:23.344Z",
          "wordCount": 616,
          "title": "Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome. (arXiv:2012.03679v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>",
          "description": "Hidden Markov Models (HMMs) comprise a powerful generative approach for\nmodeling sequential data and time-series in general. However, the commonly\nemployed assumption of the dependence of the current time frame to a single or\nmultiple immediately preceding frames is unrealistic; more complicated dynamics\npotentially exist in real world scenarios. This paper revisits conventional\nsequential modeling approaches, aiming to address the problem of capturing\ntime-varying temporal dependency patterns. To this end, we propose a different\nformulation of HMMs, whereby the dependence on past frames is dynamically\ninferred from the data. Specifically, we introduce a hierarchical extension by\npostulating an additional latent variable layer; therein, the (time-varying)\ntemporal dependence patterns are treated as latent variables over which\ninference is performed. We leverage solid arguments from the Variational Bayes\nframework and derive a tractable inference algorithm based on the\nforward-backward algorithm. As we experimentally show, our approach can model\nhighly complex sequential data and can effectively handle data with missing\nvalues.",
          "link": "http://arxiv.org/abs/2002.05809",
          "publishedOn": "2021-09-13T07:20:23.323Z",
          "wordCount": 649,
          "title": "Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Recovering texture information from the aliasing regions has always been a\nmajor challenge for Single Image Super Resolution (SISR) task. These regions\nare often submerged in noise so that we have to restore texture details while\nsuppressing noise. To address this issue, we propose a Balanced Attention\nMechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and\nMaxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to\nsuppress extreme noise in the large scale feature maps while MSAM preserves\nhigh-frequency texture details. Thanks to the parallel structure, these two\nmodules not only conduct self-optimization, but also mutual optimization to\nobtain the balance of noise reduction and high-frequency texture restoration\nduring the back propagation process, and the parallel structure makes the\ninference faster. To verify the effectiveness and robustness of BAM, we applied\nit to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently\nimprove the networks performance, and for those originally with attention\nmechanism, the substitution with BAM further reduces the amount of parameters\nand increases the inference speed. Moreover, we present a dataset with rich\ntexture aliasing regions in real scenes, named realSR7. Experiments prove that\nBAM achieves better super-resolution results on the aliasing area.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-09-13T07:20:23.290Z",
          "wordCount": 689,
          "title": "BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06009",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1\">Fengming Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xuelei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_T/0/1/0/all/0/1\">Tianhang Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yiguang Liu</a>",
          "description": "Detection and tracking of fast-moving objects have widespread utility in many\nfields. However, fulfilling this demand for fast and efficient detecting and\ntracking using image-based techniques is problematic, owing to the complex\ncalculations and limited data processing capabilities. To tackle this problem,\nwe propose an image-free method to achieve real-time detection and tracking of\nfast-moving objects. It employs the Hadamard pattern to illuminate the\nfast-moving object by a spatial light modulator, in which the resulting light\nsignal is collected by a single-pixel detector. The single-pixel measurement\nvalues are directly used to reconstruct the position information without image\nreconstruction. Furthermore, a new sampling method is used to optimize the\npattern projection way for achieving an ultra-low sampling rate. Compared with\nthe state-of-the-art methods, our approach is not only capable of handling\nreal-time detection and tracking, but also it has a small amount of calculation\nand high efficiency. We experimentally demonstrate that the proposed method,\nusing a 22kHz digital micro-mirror device, can implement a 105fps frame rate at\na 1.28% sampling rate when tracks. Our method breaks through the traditional\ntracking ways, which can implement the object real-time tracking without image\nreconstruction.",
          "link": "http://arxiv.org/abs/2108.06009",
          "publishedOn": "2021-09-13T07:20:23.272Z",
          "wordCount": 670,
          "title": "Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>",
          "description": "Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.",
          "link": "http://arxiv.org/abs/2106.07806",
          "publishedOn": "2021-09-13T07:20:23.246Z",
          "wordCount": 772,
          "title": "Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Keunhong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_U/0/1/0/all/0/1\">Utkarsh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedman_P/0/1/0/all/0/1\">Peter Hedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouaziz_S/0/1/0/all/0/1\">Sofien Bouaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldman_D/0/1/0/all/0/1\">Dan B Goldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_Brualla_R/0/1/0/all/0/1\">Ricardo Martin-Brualla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Steven M. Seitz</a>",
          "description": "Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for\ninterpolation and 8.6% for novel-view synthesis, as measured by LPIPS.\nAdditional videos, results, and visualizations are available at\nhttps://hypernerf.github.io.",
          "link": "http://arxiv.org/abs/2106.13228",
          "publishedOn": "2021-09-13T07:20:23.238Z",
          "wordCount": 717,
          "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>",
          "description": "We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.",
          "link": "http://arxiv.org/abs/2104.08384",
          "publishedOn": "2021-09-13T07:20:23.229Z",
          "wordCount": 702,
          "title": "\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Liangzhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang-Chieh Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroff_F/0/1/0/all/0/1\">Florian Schroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1\">Hartwig Adam</a>",
          "description": "Recognition of human poses and actions is crucial for autonomous systems to\ninteract smoothly with people. However, cameras generally capture human poses\nin 2D as images and videos, which can have significant appearance variations\nacross viewpoints that make the recognition tasks challenging. To address this,\nwe explore recognizing similarity in 3D human body poses from 2D information,\nwhich has not been well-studied in existing works. Here, we propose an approach\nto learning a compact view-invariant embedding space from 2D body joint\nkeypoints, without explicitly predicting 3D poses. Input ambiguities of 2D\nposes from projection and occlusion are difficult to represent through a\ndeterministic mapping, and therefore we adopt a probabilistic formulation for\nour embedding space. Experimental results show that our embedding model\nachieves higher accuracy when retrieving similar poses across different camera\nviews, in comparison with 3D pose estimation models. We also show that by\ntraining a simple temporal embedding model, we achieve superior performance on\npose sequence retrieval and largely reduce the embedding dimension from\nstacking frame-based embeddings for efficient large-scale retrieval.\nFurthermore, in order to enable our embeddings to work with partially visible\ninput, we further investigate different keypoint occlusion augmentation\nstrategies during training. We demonstrate that these occlusion augmentations\nsignificantly improve retrieval performance on partial 2D input poses. Results\non action recognition and video alignment demonstrate that using our embeddings\nwithout any additional training achieves competitive performance relative to\nother models specifically trained for each task.",
          "link": "http://arxiv.org/abs/2010.13321",
          "publishedOn": "2021-09-13T07:20:23.212Z",
          "wordCount": 757,
          "title": "View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose. (arXiv:2010.13321v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junteng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>",
          "description": "Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a\n3D solid volume, exhibits advantages in numerous application domains. However,\nexisting methods generally synthesize solid texture with specific features,\nwhich may result in the failure of capturing diversified textural information.\nIn this paper, we propose a novel generative adversarial nets-based approach\n(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our\nmulti-scale discriminators evaluate the similarity between patch from exemplar\nand slice from the generated volume, promoting the generator to synthesize\nrealistic solid textures. Experimental results demonstrate that the proposed\nmethod can generate high-quality solid textures with similar visual\ncharacteristics to the exemplar.",
          "link": "http://arxiv.org/abs/2102.03973",
          "publishedOn": "2021-09-13T07:20:23.206Z",
          "wordCount": 612,
          "title": "Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_P/0/1/0/all/0/1\">Pit Schneider</a>",
          "description": "Text line segmentation is one of the pre-stages of modern optical character\nrecognition systems. The algorithmic approach proposed by this paper has been\ndesigned for this exact purpose. Its main characteristic is the combination of\ntwo different techniques, morphological image operations and horizontal\nhistogram projections. The method was developed to be applied on a historic\ndata collection that commonly features quality issues, such as degraded paper,\nblurred text, or presence of noise. For that reason, the segmenter in question\ncould be of particular interest for cultural institutions, that want access to\nrobust line bounding boxes for a given historic document. Because of the\npromising segmentation results that are joined by low computational cost, the\nalgorithm was incorporated into the OCR pipeline of the National Library of\nLuxembourg, in the context of the initiative of reprocessing their historic\nnewspaper collection. The general contribution of this paper is to outline the\napproach and to evaluate the gains in terms of accuracy and speed, comparing it\nto the segmentation algorithm bundled with the used open source OCR software.",
          "link": "http://arxiv.org/abs/2103.08922",
          "publishedOn": "2021-09-13T07:20:23.192Z",
          "wordCount": 656,
          "title": "Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Debasrita Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ashish Ghosh</a>",
          "description": "Binary change detection in bi-temporal co-registered hyperspectral images is\na challenging task due to a large number of spectral bands present in the data.\nResearchers, therefore, try to handle it by reducing dimensions. The proposed\nwork aims to build a novel feature extraction system using a feature fusion\ndeep convolutional autoencoder for detecting changes between a pair of such\nbi-temporal co-registered hyperspectral images. The feature fusion considers\nfeatures across successive levels and multiple receptive fields and therefore\nadds a competitive edge over the existing feature extraction methods. The\nchange detection technique described is completely unsupervised and is much\nmore elegant than other supervised or semi-supervised methods which require\nsome amount of label information. Different methods have been applied to the\nextracted features to find the changes in the two images and it is found that\nthe proposed method clearly outperformed the state of the art methods in\nunsupervised change detection for all the datasets.",
          "link": "http://arxiv.org/abs/2109.04990",
          "publishedOn": "2021-09-13T07:20:23.185Z",
          "wordCount": 610,
          "title": "Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Multitask learning is a common approach in machine learning, which allows to\ntrain multiple objectives with a shared architecture. It has been shown that by\ntraining multiple tasks together inference time and compute resources can be\nsaved, while the objectives performance remains on a similar or even higher\nlevel. However, in perception related multitask networks only closely related\ntasks can be found, such as object detection, instance and semantic\nsegmentation or depth estimation. Multitask networks with diverse tasks and\ntheir effects with respect to efficiency on one another are not well studied.\nIn this paper we augment the CenterNet anchor-free approach for training\nmultiple diverse perception related tasks together, including the task of\nobject detection and semantic segmentation as well as human pose estimation. We\nrefer to this DNN as Multitask-CenterNet (MCN). Additionally, we study\ndifferent MCN settings for efficiency. The MCN can perform several tasks at\nonce while maintaining, and in some cases even exceeding, the performance\nvalues of its corresponding single task networks. More importantly, the MCN\narchitecture decreases inference time and reduces network size when compared to\na composition of single task networks.",
          "link": "http://arxiv.org/abs/2108.05060",
          "publishedOn": "2021-09-13T07:20:23.169Z",
          "wordCount": 680,
          "title": "MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach. (arXiv:2108.05060v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sariyildiz_M/0/1/0/all/0/1\">Mert Bulent Sariyildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1\">Yannis Kalantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>",
          "description": "Measuring concept generalization, i.e., the extent to which models trained on\na set of (seen) visual concepts can be leveraged to recognize a new set of\n(unseen) concepts, is a popular way of evaluating visual representations,\nespecially in a self-supervised learning framework. Nonetheless, the choice of\nunseen concepts for such an evaluation is usually made arbitrarily, and\nindependently from the seen concepts used to train representations, thus\nignoring any semantic relationships between the two. In this paper, we argue\nthat the semantic relationships between seen and unseen concepts affect\ngeneralization performance and propose ImageNet-CoG, a novel benchmark on the\nImageNet-21K (IN-21K) dataset that enables measuring concept generalization in\na principled way. Our benchmark leverages expert knowledge that comes from\nWordNet in order to define a sequence of unseen IN-21K concept sets that are\nsemantically more and more distant from the ImageNet-1K (IN-1K) subset, a\nubiquitous training set. This allows us to benchmark visual representations\nlearned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31\nconvolution and transformer-based models and show how different architectures,\nlevels of supervision, regularization techniques and use of web data impact the\nconcept generalization performance.",
          "link": "http://arxiv.org/abs/2012.05649",
          "publishedOn": "2021-09-13T07:20:23.157Z",
          "wordCount": 683,
          "title": "Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_V/0/1/0/all/0/1\">Vipin Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1\">Paolo Favaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>",
          "description": "Recently, contrastive learning has achieved great results in self-supervised\nlearning, where the main idea is to push two augmentations of an image\n(positive pairs) closer compared to other random images (negative pairs). We\nargue that not all random images are equal. Hence, we introduce a self\nsupervised learning algorithm where we use a soft similarity for the negative\nimages rather than a binary distinction between positive and negative pairs. We\niteratively distill a slowly evolving teacher model to the student model by\ncapturing the similarity of a query image to some random images and\ntransferring that knowledge to the student. We argue that our method is less\nconstrained compared to recent contrastive learning methods, so it can learn\nbetter features. Specifically, our method should handle unbalanced and\nunlabeled data better than existing contrastive learning methods, because the\nrandomly chosen negative set might include many samples that are semantically\nsimilar to the query image. In this case, our method labels them as highly\nsimilar while standard contrastive methods label them as negative pairs. Our\nmethod achieves comparable results to the state-of-the-art models. We also show\nthat our method performs better in the settings where the unlabeled data is\nunbalanced. Our code is available here: https://github.com/UMBCvision/ISD.",
          "link": "http://arxiv.org/abs/2012.09259",
          "publishedOn": "2021-09-13T07:20:23.150Z",
          "wordCount": 688,
          "title": "ISD: Self-Supervised Learning by Iterative Similarity Distillation. (arXiv:2012.09259v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingpei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richter_F/0/1/0/all/0/1\">Florian Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1\">Michael Yip</a>",
          "description": "Keypoint detection is an essential building block for many robotic\napplications like motion capture and pose estimation. Historically, keypoints\nare detected using uniquely engineered markers such as checkerboards or\nfiducials. More recently, deep learning methods have been explored as they have\nthe ability to detect user-defined keypoints in a marker-less manner. However,\ndifferent manually selected keypoints can have uneven performance when it comes\nto detection and localization. An example of this can be found on symmetric\nrobotic tools where DNN detectors cannot solve the correspondence problem\ncorrectly. In this work, we propose a new and autonomous way to define the\nkeypoint locations that overcomes these challenges. The approach involves\nfinding the optimal set of keypoints on robotic manipulators for robust visual\ndetection and localization. Using a robotic simulator as a medium, our\nalgorithm utilizes synthetic data for DNN training, and the proposed algorithm\nis used to optimize the selection of keypoints through an iterative approach.\nThe results show that when using the optimized keypoints, the detection\nperformance of the DNNs improved significantly. We further use the optimized\nkeypoints for real robotic applications by using domain randomization to bridge\nthe reality gap between the simulator and the physical world. The physical\nworld experiments show how the proposed method can be applied to the\nwide-breadth of robotic applications that require visual feedback, such as\ncamera-to-robot calibration, robotic tool tracking, and end-effector pose\nestimation.",
          "link": "http://arxiv.org/abs/2010.08054",
          "publishedOn": "2021-09-13T07:20:23.143Z",
          "wordCount": 712,
          "title": "Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer. (arXiv:2010.08054v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>",
          "description": "Deep neural networks produce state-of-the-art results when trained on a large\nnumber of labeled examples but tend to overfit when small amounts of labeled\nexamples are used for training. Creating a large number of labeled examples\nrequires considerable resources, time, and effort. If labeling new data is not\nfeasible, so-called semi-supervised learning can achieve better generalisation\nthan purely supervised learning by employing unlabeled instances as well as\nlabeled ones. The work presented in this paper is motivated by the observation\nthat transfer learning provides the opportunity to potentially further improve\nperformance by exploiting models pretrained on a similar domain. More\nspecifically, we explore the use of transfer learning when performing\nsemi-supervised learning using self-learning. The main contribution is an\nempirical evaluation of transfer learning using different combinations of\nsimilarity metric learning methods and label propagation algorithms in\nsemi-supervised learning. We find that transfer learning always substantially\nimproves the model's accuracy when few labeled examples are available,\nregardless of the type of loss used for training the neural network. This\nfinding is obtained by performing extensive experiments on the SVHN, CIFAR10,\nand Plant Village image classification datasets and applying pretrained weights\nfrom Imagenet for transfer learning.",
          "link": "http://arxiv.org/abs/2109.00788",
          "publishedOn": "2021-09-13T07:20:23.136Z",
          "wordCount": 689,
          "title": "Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification. (arXiv:2109.00788v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.07739",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiamian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>",
          "description": "The study of 3D hyperspectral image (HSI) reconstruction refers to the\ninverse process of snapshot compressive imaging, during which the optical\nsystem, e.g., the coded aperture snapshot spectral imaging (CASSI) system,\ncaptures the 3D spatial-spectral signal and encodes it to a 2D measurement.\nWhile numerous sophisticated neural networks have been elaborated for\nend-to-end reconstruction, trade-offs still need to be made among performance,\nefficiency (training and inference time), and feasibility (the ability of\nrestoring high resolution HSI on limited GPU memory). This raises a challenge\nto design a new baseline to conjointly meet the above requirements. In this\npaper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual\nU-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)\nscale/spectral-invariant learning, 2) nested residual learning, and 3)\ncomputational efficiency. Benefiting from these three modules, the proposed\nSSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3\ndB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To\nthe greatest extent, SSI-ResU-Net achieves competing performance with over\n77.3% reduction in terms of floating-point operations (FLOPs), which for the\nfirst time, makes high-resolution HSI reconstruction feasible under practical\napplication scenarios. Code and pre-trained models are made available at\nhttps://github.com/Jiamian-Wang/HSI_baseline.",
          "link": "http://arxiv.org/abs/2108.07739",
          "publishedOn": "2021-09-13T07:20:23.080Z",
          "wordCount": 676,
          "title": "A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junjie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiagang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dalong Du</a>",
          "description": "Recently, face recognition in the wild has achieved remarkable success and\none key engine is the increasing size of training data. For example, the\nlargest face dataset, WebFace42M contains about 2 million identities and 42\nmillion faces. However, a massive number of faces raise the constraints in\ntraining time, computing resources, and memory cost. The current research on\nthis problem mainly focuses on designing an efficient Fully-connected layer\n(FC) to reduce GPU memory consumption caused by a large number of identities.\nIn this work, we relax these constraints by resolving the redundancy problem of\nthe up-to-date face datasets caused by the greedily collecting operation (i.e.\nthe core-set selection perspective). As the first attempt in this perspective\non the face recognition problem, we find that existing methods are limited in\nboth performance and efficiency. For superior cost-efficiency, we contribute a\nnovel filtering strategy dubbed Face-NMS. Face-NMS works on feature space and\nsimultaneously considers the local and global sparsity in generating core sets.\nIn practice, Face-NMS is analogous to Non-Maximum Suppression (NMS) in the\nobject detection community. It ranks the faces by their potential contribution\nto the overall sparsity and filters out the superfluous face in the pairs with\nhigh similarity for local sparsity. With respect to the efficiency aspect,\nFace-NMS accelerates the whole pipeline by applying a smaller but sufficient\nproxy dataset in training the proxy model. As a result, with Face-NMS, we\nsuccessfully scale down the WebFace42M dataset to 60% while retaining its\nperformance on the main benchmarks, offering a 40% resource-saving and 1.64\ntimes acceleration. The code is publicly available for reference at\nhttps://github.com/HuangJunJie2017/Face-NMS.",
          "link": "http://arxiv.org/abs/2109.04698",
          "publishedOn": "2021-09-13T07:20:22.992Z",
          "wordCount": 727,
          "title": "Face-NMS: A Core-set Selection Approach for Efficient Face Recognition. (arXiv:2109.04698v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lekkala_S/0/1/0/all/0/1\">Shanthi Lekkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motwani_T/0/1/0/all/0/1\">Tanya Motwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Manojkumar Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phadke_A/0/1/0/all/0/1\">Amit Phadke</a>",
          "description": "Artificial Intelligence has made a significant contribution to autonomous\nvehicles, from object detection to path planning. However, AI models require a\nlarge amount of sensitive training data and are usually computationally\nintensive to build. The commercial value of such models motivates attackers to\nmount various attacks. Adversaries can launch model extraction attacks for\nmonetization purposes or step-ping-stone towards other attacks like model\nevasion. In specific cases, it even results in destroying brand reputation,\ndifferentiation, and value proposition. In addition, IP laws and AI-related\nlegalities are still evolving and are not uniform across countries. We discuss\nmodel extraction attacks in detail with two use-cases and a generic kill-chain\nthat can compromise autonomous cars. It is essential to investigate strategies\nto manage and mitigate the risk of model theft.",
          "link": "http://arxiv.org/abs/2109.04865",
          "publishedOn": "2021-09-13T07:20:22.984Z",
          "wordCount": 595,
          "title": "Emerging AI Security Threats for Autonomous Cars -- Case Studies. (arXiv:2109.04865v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05014",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>",
          "description": "Knowledge-based visual question answering (VQA) involves answering questions\nthat require external knowledge not present in the image. Existing methods\nfirst retrieve knowledge from external resources, then reason over the selected\nknowledge, the input image, and question for answer prediction. However, this\ntwo-step approach could lead to mismatches that potentially limit the VQA\nperformance. For example, the retrieved knowledge might be noisy and irrelevant\nto the question, and the re-embedded knowledge features during reasoning might\ndeviate from their original meanings in the knowledge base (KB). To address\nthis challenge, we propose PICa, a simple yet effective method that Prompts\nGPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by\nGPT-3's power in knowledge retrieval and question answering, instead of using\nstructured KBs as in previous work, we treat GPT-3 as an implicit and\nunstructured KB that can jointly acquire and process relevant knowledge.\nSpecifically, we first convert the image into captions (or tags) that GPT-3 can\nunderstand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just\nproviding a few in-context VQA examples. We further boost performance by\ncarefully investigating: (i) what text formats best describe the image content,\nand (ii) how in-context examples can be better selected and used. PICa unlocks\nthe first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa\nsurpasses the supervised state of the art by an absolute +8.6 points on the\nOK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent\nfew-shot performance.",
          "link": "http://arxiv.org/abs/2109.05014",
          "publishedOn": "2021-09-13T07:20:22.962Z",
          "wordCount": 704,
          "title": "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. (arXiv:2109.05014v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04970",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhongze Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Liguang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yangsheng Xu</a>",
          "description": "In recent years, self-supervised denoising methods have shown impressive\nperformance, which circumvent painstaking collection procedure of noisy-clean\nimage pairs in supervised denoising methods and boost denoising applicability\nin real world. One of well-known self-supervised denoising strategies is the\nblind-spot training scheme. However, a few works attempt to improve blind-spot\nbased self-denoiser in the aspect of network architecture. In this paper, we\ntake an intuitive view of blind-spot strategy and consider its process of using\nneighbor pixels to predict manipulated pixels as an inpainting process.\nTherefore, we propose a novel Mask Guided Residual Convolution (MGRConv) into\ncommon convolutional neural networks, e.g. U-Net, to promote blind-spot based\ndenoising. Our MGRConv can be regarded as soft partial convolution and find a\ntrade-off among partial convolution, learnable attention maps, and gated\nconvolution. It enables dynamic mask learning with appropriate mask constrain.\nDifferent from partial convolution and gated convolution, it provides moderate\nfreedom for network learning. It also avoids leveraging external learnable\nparameters for mask activation, unlike learnable attention maps. The\nexperiments show that our proposed plug-and-play MGRConv can assist blind-spot\nbased denoising network to reach promising results on both existing\nsingle-image based and dataset-based methods.",
          "link": "http://arxiv.org/abs/2109.04970",
          "publishedOn": "2021-09-13T07:20:22.880Z",
          "wordCount": 652,
          "title": "View Blind-spot as Inpainting: Self-Supervised Denoising with Mask Guided Residual Convolution. (arXiv:2109.04970v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chuanqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1\">Pinzhuo Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tiexin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuesong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Few-shot learning, especially few-shot image classification, has received\nincreasing attention and witnessed significant advances in recent years. Some\nrecent studies implicitly show that many generic techniques or ``tricks'', such\nas data augmentation, pre-training, knowledge distillation, and\nself-supervision, may greatly boost the performance of a few-shot learning\nmethod. Moreover, different works may employ different software platforms,\ndifferent training schedules, different backbone architectures and even\ndifferent input image sizes, making fair comparisons difficult and\npractitioners struggle with reproducibility. To address these situations, we\npropose a comprehensive library for few-shot learning (LibFewShot) by\nre-implementing seventeen state-of-the-art few-shot learning methods in a\nunified framework with the same single codebase in PyTorch. Furthermore, based\non LibFewShot, we provide comprehensive evaluations on multiple benchmark\ndatasets with multiple backbone architectures to evaluate common pitfalls and\neffects of different training tricks. In addition, given the recent doubts on\nthe necessity of meta- or episodic-training mechanism, our evaluation results\nshow that such kind of mechanism is still necessary especially when combined\nwith pre-training. We hope our work can not only lower the barriers for\nbeginners to work on few-shot learning but also remove the effects of the\nnontrivial tricks to facilitate intrinsic research on few-shot learning. The\nsource code is available from https://github.com/RL-VIG/LibFewShot.",
          "link": "http://arxiv.org/abs/2109.04898",
          "publishedOn": "2021-09-13T07:20:22.872Z",
          "wordCount": 666,
          "title": "LibFewShot: A Comprehensive Library for Few-shot Learning. (arXiv:2109.04898v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rui Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhatkuli_A/0/1/0/all/0/1\">Ajad Chhatkuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Traditional domain adaptation addresses the task of adapting a model to a\nnovel target domain under limited or no additional supervision. While tackling\nthe input domain gap, the standard domain adaptation settings assume no domain\nchange in the output space. In semantic prediction tasks, different datasets\nare often labeled according to different semantic taxonomies. In many\nreal-world settings, the target domain task requires a different taxonomy than\nthe one imposed by the source domain. We therefore introduce the more general\ntaxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent\ntaxonomies between the two domains. We further propose an approach that jointly\naddresses the image-level and label-level domain adaptation. On the\nlabel-level, we employ a bilateral mixed sampling strategy to augment the\ntarget domain, and a relabelling method to unify and align the label spaces. We\naddress the image-level domain gap by proposing an uncertainty-rectified\ncontrastive learning method, leading to more domain-invariant and class\ndiscriminative features. We extensively evaluate the effectiveness of our\nframework under different TADA settings: open taxonomy, coarse-to-fine\ntaxonomy, and partially-overlapping taxonomy. Our framework outperforms\nprevious state-of-the-art by a large margin, while capable of adapting to new\ntarget domain taxonomies.",
          "link": "http://arxiv.org/abs/2109.04813",
          "publishedOn": "2021-09-13T07:20:22.865Z",
          "wordCount": 648,
          "title": "TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunzhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinrui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hesheng Wang</a>",
          "description": "Scene flow estimation is the task to predict the point-wise 3D displacement\nvector between two consecutive frames of point clouds, which has important\napplication in fields such as service robots and autonomous driving. Although\nmany previous works have explored greatly on scene flow estimation based on\npoint clouds, we point out two problems that have not been noticed or well\nsolved before: 1) Points of adjacent frames in repetitive patterns may be\nwrongly associated due to similar spatial structure in their neighbourhoods; 2)\nScene flow between adjacent frames of point clouds with long-distance movement\nmay be inaccurately estimated. To solve the first problem, we propose a novel\ncontext-aware set conv layer to exploit contextual structure information of\nEuclidean space and learn soft aggregation weights for local point features.\nOur design is inspired by human perception of contextual structure information\nduring scene understanding. We incorporate the context-aware set conv layer in\na context-aware point feature pyramid module of 3D point clouds for scene flow\nestimation. For the second problem, we propose an explicit residual flow\nlearning structure in the residual flow refinement layer to cope with\nlong-distance movement. The experiments and ablation study on FlyingThings3D\nand KITTI scene flow datasets demonstrate the effectiveness of each proposed\ncomponent and that we solve problem of ambiguous inter-frame association and\nlong-distance movement estimation. Quantitative results on both FlyingThings3D\nand KITTI scene flow datasets show that our method achieves state-of-the-art\nperformance, surpassing all other previous works to the best of our knowledge\nby at least 25%.",
          "link": "http://arxiv.org/abs/2109.04685",
          "publishedOn": "2021-09-13T07:20:22.858Z",
          "wordCount": 705,
          "title": "Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinchao Zhu</a>",
          "description": "The reasonable employment of RGB and depth data show great significance in\npromoting the development of computer vision tasks and robot-environment\ninteraction. However, there are different advantages and disadvantages in the\nearly and late fusion of the two types of data. Besides, due to the diversity\nof object information, using a single type of data in a specific scenario tends\nto result in semantic misleading. Based on the above considerations, we propose\nan adaptively-cooperative fusion network (ACFNet) with ResinRes structure for\nsalient object detection. This structure is designed to flexibly utilize the\nadvantages of feature fusion in early and late stages. Secondly, an\nadaptively-cooperative semantic guidance (ACG) scheme is designed to suppress\ninaccurate features in the guidance phase. Further, we proposed a type-based\nattention module (TAM) to optimize the network and enhance the multi-scale\nperception of different objects. For different objects, the features generated\nby different types of convolution are enhanced or suppressed by the gated\nmechanism for segmentation optimization. ACG and TAM optimize the transfer of\nfeature streams according to their data attributes and convolution attributes,\nrespectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate\nthat the proposed network performs favorably against 18 state-of-the-art\nalgorithms.",
          "link": "http://arxiv.org/abs/2109.04627",
          "publishedOn": "2021-09-13T07:20:22.852Z",
          "wordCount": 637,
          "title": "ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection. (arXiv:2109.04627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_M/0/1/0/all/0/1\">Mitsuki Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_R/0/1/0/all/0/1\">Ryogo Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kanji Tanaka</a>",
          "description": "In this paper, we address the problem of image sequence-based\nself-localization (ISS) from a new highly compressive scene representation\ncalled sequential semantic scene graph (S3G). Recent developments in deep graph\nconvolutional neural networks (GCNs) have enabled a highly compressive visual\nplace classifier (VPC) that can use a scene graph as the input modality.\nHowever, in such a highly compressive application, the amount of information\nlost in the image-to-graph mapping is significant and can damage the\nclassification performance. To address this issue, we propose a pair of\nsimilarity-preserving mappings, image-to-nodes and image-to-edges, such that\nthe nodes and edges act as absolute and relative features, respectively, that\ncomplement each other. Moreover, the proposed GCN-VPC is applied to a new task\nof viewpoint planning (VP) of the query image sequence, which contributes to\nfurther improvement in the VPC performance. Experiments using the public NCLT\ndataset validated the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2109.04569",
          "publishedOn": "2021-09-13T07:20:22.846Z",
          "wordCount": 613,
          "title": "S3G-ARM: Highly Compressive Visual Self-localization from Sequential Semantic Scene Graph Using Absolute and Relative Measurements. (arXiv:2109.04569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qiqi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Charlie Wang</a>",
          "description": "Our automatic portrait video matting method does not require extra inputs.\nMost state-of-the-art matting methods rely on semantic segmentation methods to\nautomatically generate the trimap. Their performance is compromised due to the\nlack of temporal information. Our method exploits semantic information as well\nas temporal information from optical flow and produces high-quality results.",
          "link": "http://arxiv.org/abs/2109.04598",
          "publishedOn": "2021-09-13T07:20:22.827Z",
          "wordCount": 493,
          "title": "Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Pim de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>",
          "description": "Computational fluid dynamics (CFD) is a valuable tool for personalised,\nnon-invasive evaluation of hemodynamics in arteries, but its complexity and\ntime-consuming nature prohibit large-scale use in practice. Recently, the use\nof deep learning for rapid estimation of CFD parameters like wall shear stress\n(WSS) on surface meshes has been investigated. However, existing approaches\ntypically depend on a hand-crafted re-parametrisation of the surface mesh to\nmatch convolutional neural network architectures. In this work, we propose to\ninstead use mesh convolutional neural networks that directly operate on the\nsame finite-element surface mesh as used in CFD. We train and evaluate our\nmethod on two datasets of synthetic coronary artery models with and without\nbifurcation, using a ground truth obtained from CFD simulation. We show that\nour flexible deep learning model can accurately predict 3D WSS vectors on this\nsurface mesh. Our method processes new meshes in less than 5 [s], consistently\nachieves a normalised mean absolute error of $\\leq$ 1.6 [%], and peaks at 90.5\n[%] median approximation accuracy over the held-out test set, comparing\nfavorably to previously published work. This shows the feasibility of CFD\nsurrogate modelling using mesh convolutional neural networks for hemodynamic\nparameter estimation in artery models.",
          "link": "http://arxiv.org/abs/2109.04797",
          "publishedOn": "2021-09-13T07:20:22.821Z",
          "wordCount": 676,
          "title": "Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Ziluo Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Ruiqin Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhaofei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>",
          "description": "Event camera has offered promising alternative for visual perception,\nespecially in high speed and high dynamic range scenes. Recently, many deep\nlearning methods have shown great success in providing model-free solutions to\nmany event-based problems, such as optical flow estimation. However, existing\ndeep learning methods did not address the importance of temporal information\nwell from the perspective of architecture design and cannot effectively extract\nspatio-temporal features. Another line of research that utilizes Spiking Neural\nNetwork suffers from training issues for deeper architecture. To address these\npoints, a novel input representation is proposed that captures the events\ntemporal distribution for signal enhancement. Moreover, we introduce a\nspatio-temporal recurrent encoding-decoding neural network architecture for\nevent-based optical flow estimation, which utilizes Convolutional Gated\nRecurrent Units to extract feature maps from a series of event images. Besides,\nour architecture allows some traditional frame-based core modules, such as\ncorrelation layer and iterative residual refine scheme, to be incorporated. The\nnetwork is end-to-end trained with self-supervised learning on the\nMulti-Vehicle Stereo Event Camera dataset. We have shown that it outperforms\nall the existing state-of-the-art methods by a large margin.",
          "link": "http://arxiv.org/abs/2109.04871",
          "publishedOn": "2021-09-13T07:20:22.814Z",
          "wordCount": 633,
          "title": "Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04760",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yu_K/0/1/0/all/0/1\">Ke Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zexian Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yue Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jinwei Gu</a>",
          "description": "Image Signal Processor (ISP) is a crucial component in digital cameras that\ntransforms sensor signals into images for us to perceive and understand.\nExisting ISP designs always adopt a fixed architecture, e.g., several\nsequential modules connected in a rigid order. Such a fixed ISP architecture\nmay be suboptimal for real-world applications, where camera sensors, scenes and\ntasks are diverse. In this study, we propose a novel Reconfigurable ISP\n(ReconfigISP) whose architecture and parameters can be automatically tailored\nto specific data and tasks. In particular, we implement several ISP modules,\nand enable backpropagation for each module by training a differentiable proxy,\nhence allowing us to leverage the popular differentiable neural architecture\nsearch and effectively search for the optimal ISP architecture. A proxy tuning\nmechanism is adopted to maintain the accuracy of proxy networks in all cases.\nExtensive experiments conducted on image restoration and object detection, with\ndifferent sensors, light conditions and efficiency constraints, validate the\neffectiveness of ReconfigISP. Only hundreds of parameters need tuning for every\ntask.",
          "link": "http://arxiv.org/abs/2109.04760",
          "publishedOn": "2021-09-13T07:20:22.798Z",
          "wordCount": 622,
          "title": "ReconfigISP: Reconfigurable Camera Image Processing Pipeline. (arXiv:2109.04760v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">C. Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayobi_N/0/1/0/all/0/1\">N. Ayobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_I/0/1/0/all/0/1\">I. Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1\">J. Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">J. Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">P. Arbel&#xe1;ez</a>",
          "description": "This paper proposes Panoptic Narrative Grounding, a spatially fine and\ngeneral formulation of the natural language visual grounding problem. We\nestablish an experimental framework for the study of this new task, including\nnew ground truth and metrics, and we propose a strong baseline method to serve\nas stepping stone for future work. We exploit the intrinsic semantic richness\nin an image by including panoptic categories, and we approach visual grounding\nat a fine-grained level by using segmentations. In terms of ground truth, we\npropose an algorithm to automatically transfer Localized Narratives annotations\nto specific regions in the panoptic segmentations of the MS COCO dataset. To\nguarantee the quality of our annotations, we take advantage of the semantic\nstructure contained in WordNet to exclusively incorporate noun phrases that are\ngrounded to a meaningfully related panoptic segmentation region. The proposed\nbaseline achieves a performance of 55.4 absolute Average Recall points. This\nresult is a suitable foundation to push the envelope further in the development\nof methods for Panoptic Narrative Grounding.",
          "link": "http://arxiv.org/abs/2109.04988",
          "publishedOn": "2021-09-13T07:20:22.780Z",
          "wordCount": 619,
          "title": "Panoptic Narrative Grounding. (arXiv:2109.04988v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_M/0/1/0/all/0/1\">Min Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang-Dong Zhou</a>",
          "description": "Video question answering (VideoQA) is challenging given its multimodal\ncombination of visual understanding and natural language understanding. While\nexisting approaches seldom leverage the appearance-motion information in the\nvideo at multiple temporal scales, the interaction between the question and the\nvisual information for textual semantics extraction is frequently ignored.\nTargeting these issues, this paper proposes a novel Temporal Pyramid\nTransformer (TPT) model with multimodal interaction for VideoQA. The TPT model\ncomprises two modules, namely Question-specific Transformer (QT) and Visual\nInference (VI). Given the temporal pyramid constructed from a video, QT builds\nthe question semantics from the coarse-to-fine multimodal co-occurrence between\neach word and the visual content. Under the guidance of such question-specific\nsemantics, VI infers the visual clues from the local-to-global multi-level\ninteractions between the question and the video. Within each module, we\nintroduce a multimodal attention mechanism to aid the extraction of\nquestion-video interactions, with residual connections adopted for the\ninformation passing across different levels. Through extensive experiments on\nthree VideoQA datasets, we demonstrate better performances of the proposed\nmethod in comparison with the state-of-the-arts.",
          "link": "http://arxiv.org/abs/2109.04735",
          "publishedOn": "2021-09-13T07:20:22.773Z",
          "wordCount": 630,
          "title": "Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering. (arXiv:2109.04735v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>",
          "description": "While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.",
          "link": "http://arxiv.org/abs/2109.04699",
          "publishedOn": "2021-09-13T07:20:22.765Z",
          "wordCount": 594,
          "title": "EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarzadeh_A/0/1/0/all/0/1\">Ara Jafarzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antequera_M/0/1/0/all/0/1\">Manuel Lopez Antequera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gargallo_P/0/1/0/all/0/1\">Pau Gargallo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1\">Yubin Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toft_C/0/1/0/all/0/1\">Carl Toft</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>",
          "description": "Visual localization is the problem of estimating the position and orientation\nfrom which a given image (or a sequence of images) is taken in a known scene.\nIt is an important part of a wide range of computer vision and robotics\napplications, from self-driving cars to augmented/virtual reality systems.\nVisual localization techniques should work reliably and robustly under a wide\nrange of conditions, including seasonal, weather, illumination and man-made\nchanges. Recent benchmarking efforts model this by providing images under\ndifferent conditions, and the community has made rapid progress on these\ndatasets since their inception. However, they are limited to a few geographical\nregions and often recorded with a single device. We propose a new benchmark for\nvisual localization in outdoor scenes, using crowd-sourced data to cover a wide\nrange of geographical regions and camera devices with a focus on the failure\ncases of current algorithms. Experiments with state-of-the-art localization\napproaches show that our dataset is very challenging, with all evaluated\nmethods failing on its hardest parts. As part of the dataset release, we\nprovide the tooling used to generate it, enabling efficient and effective 2D\ncorrespondence annotation to obtain reference poses.",
          "link": "http://arxiv.org/abs/2109.04527",
          "publishedOn": "2021-09-13T07:20:22.758Z",
          "wordCount": 645,
          "title": "CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization. (arXiv:2109.04527v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chong_T/0/1/0/all/0/1\">Toby Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1\">I-Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umetani_N/0/1/0/all/0/1\">Nobuyuki Umetani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>",
          "description": "Virtual try-on is a promising application of computer graphics and human\ncomputer interaction that can have a profound real-world impact especially\nduring this pandemic. Existing image-based works try to synthesize a try-on\nimage from a single image of a target garment, but it inherently limits the\nability to react to possible interactions. It is difficult to reproduce the\nchange of wrinkles caused by pose and body size change, as well as pulling and\nstretching of the garment by hand. In this paper, we propose an alternative per\ngarment capture and synthesis workflow to handle such rich interactions by\ntraining the model with many systematically captured images. Our workflow is\ncomposed of two parts: garment capturing and clothed person image synthesis. We\ndesigned an actuated mannequin and an efficient capturing process that collects\nthe detailed deformations of the target garments under diverse body sizes and\nposes. Furthermore, we proposed to use a custom-designed measurement garment,\nand we captured paired images of the measurement garment and the target\ngarments. We then learn a mapping between the measurement garment and the\ntarget garments using deep image-to-image translation. The customer can then\ntry on the target garments interactively during online shopping.",
          "link": "http://arxiv.org/abs/2109.04654",
          "publishedOn": "2021-09-13T07:20:22.744Z",
          "wordCount": 651,
          "title": "Per Garment Capture and Synthesis for Real-time Virtual Try-on. (arXiv:2109.04654v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04960",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bai_Y/0/1/0/all/0/1\">Yongsheng Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abduallah_R/0/1/0/all/0/1\">Ramzi M. Abduallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezen_H/0/1/0/all/0/1\">Halil Sezen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>",
          "description": "This paper proposes a pipeline to automatically track and measure\ndisplacement and vibration of structural specimens during laboratory\nexperiments. The latest Mask Regional Convolutional Neural Network (Mask R-CNN)\ncan locate the targets and monitor their movement from videos recorded by a\nstationary camera. To improve precision and remove the noise, techniques such\nas Scale-invariant Feature Transform (SIFT) and various filters for signal\nprocessing are included. Experiments on three small-scale reinforced concrete\nbeams and a shaking table test are utilized to verify the proposed method.\nResults show that the proposed deep learning method can achieve the goal to\nautomatically and precisely measure the motion of tested structural members\nduring laboratory experiments.",
          "link": "http://arxiv.org/abs/2109.04960",
          "publishedOn": "2021-09-13T07:20:22.718Z",
          "wordCount": 579,
          "title": "Automatic Displacement and Vibration Measurement in Laboratory Experiments with A Deep Learning Method. (arXiv:2109.04960v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.14556",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Rongtao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shibiao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_W/0/1/0/all/0/1\">Weiliang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_Q/0/1/0/all/0/1\">Qimin Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>",
          "description": "Accurate lung nodules segmentation from Computed Tomography (CT) images is\ncrucial to the analysis and diagnosis of lung diseases such as COVID-19 and\nlung cancer. However, due to the smallness and variety of lung nodules and the\nlack of high-quality labeling, accurate lung nodule segmentation is still a\nchallenging problem. To address these issues, we propose a complete paradigm\nfor accurate lung nodules segmentation. First, we introduce a new segmentation\nmask named Soft Mask which has richer and more accurate edge details\ndescription and better visualization. Correspondingly, we develop a universal\nsemi-automatic Soft Mask annotation pipeline to deal with different datasets.\nSecond, a novel Network with Detailed representation transfer and Soft Mask\nsupervision (DSNet) is proposed to process the input low-resolution images of\nlung nodules into high-quality segmentation results. In our DSNet, we design a\nnovel Selective Detailed Representation Fusion Module to reconstruct the\ndetailed representation to alleviate the small size of lung nodules images. In\naddition, the adversarial training framework with Soft Mask is proposed to\nfurther improve the accuracy of segmentation. Extensive experiments validate\nthat our DSNet outperforms the state-of-the-art methods for accurate lung\nnodules segmentation. And our method also demonstrates competitive results in\nother accurate medical segmentation tasks. Besides, we provide a new\nchallenging lung nodules segmentation dataset for further studies.",
          "link": "http://arxiv.org/abs/2007.14556",
          "publishedOn": "2021-09-13T07:20:22.709Z",
          "wordCount": 744,
          "title": "Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiafei Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Samson Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheston Tan</a>",
          "description": "To align advanced artificial intelligence (AI) with human values and promote\nsafe AI, it is important for AI to predict the outcome of physical\ninteractions. Even with the ongoing debates on how humans predict the outcomes\nof physical interactions among objects in the real world, there are works\nattempting to tackle this task via cognitive-inspired AI approaches. However,\nthere is still a lack of AI approaches that mimic the mental imagery humans use\nto predict physical interactions in the real world. In this work, we propose a\nnovel PIP scheme: Physical Interaction Prediction via Mental Imagery with Span\nSelection. PIP utilizes a deep generative model to output future frames of\nphysical interactions among objects before extracting crucial information for\npredicting physical interactions by focusing on salient frames using span\nselection. To evaluate our model, we propose a large-scale SPACE+ dataset of\nsynthetic video frames, including three physical interaction events in a 3D\nenvironment. Our experiments show that PIP outperforms baselines and human\nperformance in physical interaction prediction for both seen and unseen\nobjects. Furthermore, PIP's span selection scheme can effectively identify the\nframes where physical interactions among objects occur within the generated\nframes, allowing for added interpretability.",
          "link": "http://arxiv.org/abs/2109.04683",
          "publishedOn": "2021-09-13T07:20:22.696Z",
          "wordCount": 652,
          "title": "PIP: Physical Interaction Prediction via Mental Imagery with Span Selection. (arXiv:2109.04683v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lulu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>",
          "description": "Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.",
          "link": "http://arxiv.org/abs/2109.04600",
          "publishedOn": "2021-09-13T07:20:22.686Z",
          "wordCount": 605,
          "title": "EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.11861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1\">Philipp Henzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>",
          "description": "We learn a latent space for easy capture, consistent interpolation, and\nefficient reproduction of visual material appearance. When users provide a\nphoto of a stationary natural material captured under flashlight illumination,\nfirst it is converted into a latent material code. Then, in the second step,\nconditioned on the material code, our method produces an infinite and diverse\nspatial field of BRDF model parameters (diffuse albedo, normals, roughness,\nspecular albedo) that subsequently allows rendering in complex scenes and\nilluminations, matching the appearance of the input photograph. Technically, we\njointly embed all flash images into a latent space using a convolutional\nencoder, and -- conditioned on these latent codes -- convert random spatial\nfields into fields of BRDF parameters using a convolutional neural network\n(CNN). We condition these BRDF parameters to match the visual characteristics\n(statistics and spectra of visual features) of the input under matching light.\nA user study compares our approach favorably to previous work, even those with\naccess to BRDF supervision.",
          "link": "http://arxiv.org/abs/2102.11861",
          "publishedOn": "2021-09-13T07:20:22.675Z",
          "wordCount": 641,
          "title": "Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungho Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ayoung Kim</a>",
          "description": "Along with feature points for image matching, line features provide\nadditional constraints to solve visual geometric problems in robotics and\ncomputer vision (CV). Although recent convolutional neural network (CNN)-based\nline descriptors are promising for viewpoint changes or dynamic environments,\nwe claim that the CNN architecture has innate disadvantages to abstract\nvariable line length into the fixed-dimensional descriptor. In this paper, we\neffectively introduce Line-Transformers dealing with variable lines. Inspired\nby natural language processing (NLP) tasks where sentences can be understood\nand abstracted well in neural nets, we view a line segment as a sentence that\ncontains points (words). By attending to well-describable points on aline\ndynamically, our descriptor performs excellently on variable line length. We\nalso propose line signature networks sharing the line's geometric attributes to\nneighborhoods. Performing as group descriptors, the networks enhance line\ndescriptors by understanding lines' relative geometries. Finally, we present\nthe proposed line descriptor and matching in a Point and Line Localization\n(PL-Loc). We show that the visual localization with feature points can be\nimproved using our line features. We validate the proposed method for\nhomography estimation and visual localization.",
          "link": "http://arxiv.org/abs/2109.04753",
          "publishedOn": "2021-09-13T07:20:22.658Z",
          "wordCount": 633,
          "title": "Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization. (arXiv:2109.04753v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Ke Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "As an essential ingredient of modern deep learning, attention mechanism,\nespecially self-attention, plays a vital role in the global correlation\ndiscovery. However, is hand-crafted attention irreplaceable when modeling the\nglobal context? Our intriguing finding is that self-attention is not better\nthan the matrix decomposition (MD) model developed 20 years ago regarding the\nperformance and computational cost for encoding the long-distance dependencies.\nWe model the global context issue as a low-rank recovery problem and show that\nits optimization algorithms can help design global information blocks. This\npaper then proposes a series of Hamburgers, in which we employ the optimization\nalgorithms for solving MDs to factorize the input representations into\nsub-matrices and reconstruct a low-rank embedding. Hamburgers with different\nMDs can perform favorably against the popular global context module\nself-attention when carefully coping with gradients back-propagated through\nMDs. Comprehensive experiments are conducted in the vision tasks where it is\ncrucial to learn the global context, including semantic segmentation and image\ngeneration, demonstrating significant improvements over self-attention and its\nvariants.",
          "link": "http://arxiv.org/abs/2109.04553",
          "publishedOn": "2021-09-13T07:20:22.649Z",
          "wordCount": 618,
          "title": "Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.12205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morin_G/0/1/0/all/0/1\">Gr&#xe9;goire Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_E/0/1/0/all/0/1\">Eyy&#xfc;b Sari</a>",
          "description": "Neural network models are resource hungry. It is difficult to deploy such\ndeep networks on devices with limited resources, like smart wearables,\ncellphones, drones, and autonomous vehicles. Low bit quantization such as\nbinary and ternary quantization is a common approach to alleviate this resource\nrequirements. Ternary quantization provides a more flexible model and\noutperforms binary quantization in terms of accuracy, however doubles the\nmemory footprint and increases the computational cost. Contrary to these\napproaches, mixed quantized models allow a trade-off between accuracy and\nmemory footprint. In such models, quantization depth is often chosen manually,\nor is tuned using a separate optimization routine. The latter requires training\na quantized network multiple times. Here, we propose an adaptive combination of\nbinary and ternary quantization, namely Smart Quantization (SQ), in which the\nquantization depth is modified directly via a regularization function, so that\nthe model is trained only once. Our experimental results show that the proposed\nmethod adapts quantization depth successfully while keeping the model accuracy\nhigh on MNIST and CIFAR10 benchmarks.",
          "link": "http://arxiv.org/abs/1909.12205",
          "publishedOn": "2021-09-10T07:20:14.765Z",
          "wordCount": null,
          "title": "Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hezheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dong Shen</a>",
          "description": "Employing large-scale pre-trained model CLIP to conduct video-text retrieval\ntask (VTR) has become a new trend, which exceeds previous VTR methods. Though,\ndue to the heterogeneity of structures and contents between video and text,\nprevious CLIP-based models are prone to overfitting in the training phase,\nresulting in relatively poor retrieval performance. In this paper, we propose a\nmulti-stream Corpus Alignment network with single gate Mixture-of-Experts\n(CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The\nCAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video\nrepresentations, including action, entity, scene, etc., then align them with\nthe corresponding part of the text. In this stage, we conduct massive\nexplorations towards the feature extraction module and feature alignment\nmodule. DSL is proposed to avoid the one-way optimum-match which occurs in\nprevious contrastive methods. Introducing the intrinsic prior of each pair in a\nbatch, DSL serves as a reviser to correct the similarity matrix and achieves\nthe dual optimal match. DSL is easy to implement with only one-line code but\nimproves significantly. The results show that the proposed CAMoE and DSL are of\nstrong efficiency, and each of them is capable of achieving State-of-The-Art\n(SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC.\nFurther, with both of them, the performance is advanced to a big extend,\nsurpassing the previous SOTA methods for around 4.6\\% R@1 in MSR-VTT.",
          "link": "http://arxiv.org/abs/2109.04290",
          "publishedOn": "2021-09-10T07:20:14.593Z",
          "wordCount": 681,
          "title": "Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss. (arXiv:2109.04290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2001.01290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareian_A/0/1/0/all/0/1\">Alireza Zareian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>",
          "description": "We formulate a practical yet challenging problem: General Partial Label\nLearning (GPLL). Compared to the traditional Partial Label Learning (PLL)\nproblem, GPLL relaxes the supervision assumption from instance-level -- a label\nset partially labels an instance -- to group-level: 1) a label set partially\nlabels a group of instances, where the within-group instance-label link\nannotations are missing, and 2) cross-group links are allowed -- instances in a\ngroup may be partially linked to the label set from another group. Such\nambiguous group-level supervision is more practical in real-world scenarios as\nadditional annotation on the instance-level is no longer required, e.g.,\nface-naming in videos where the group consists of faces in a frame, labeled by\na name set in the corresponding caption. In this paper, we propose a novel\ngraph convolutional network (GCN) called Dual Bipartite Graph Autoencoder\n(DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the\ncross-group correlations to represent the instance groups as dual bipartite\ngraphs: within-group and cross-group, which reciprocally complements each other\nto resolve the linking ambiguities. Second, we design a GCN autoencoder to\nencode and decode them, where the decodings are considered as the refined\nresults. It is worth noting that DB-GAE is self-supervised and transductive, as\nit only uses the group-level supervision without a separate offline training\nstage. Extensive experiments on two real-world datasets demonstrate that DB-GAE\nsignificantly outperforms the best baseline over absolute 0.159 F1-score and\n24.8% accuracy. We further offer analysis on various levels of label\nambiguities.",
          "link": "http://arxiv.org/abs/2001.01290",
          "publishedOn": "2021-09-10T07:20:14.080Z",
          "wordCount": 743,
          "title": "General Partial Label Learning via Dual Bipartite Graph Autoencoder. (arXiv:2001.01290v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xin_P/0/1/0/all/0/1\">Peng Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wangting_X/0/1/0/all/0/1\">Xu Wangting</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaqi_Y/0/1/0/all/0/1\">Yang Jiaqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_K/0/1/0/all/0/1\">Kneip Laurent</a>",
          "description": "Event cameras trigger events asynchronously and independently upon a\nsufficient change of the logarithmic brightness level. The neuromorphic sensor\nhas several advantages over standard cameras including low latency, absence of\nmotion blur, and high dynamic range. Event cameras are particularly well suited\nto sense motion dynamics in agile scenarios. We propose the continuous\nevent-line constraint, which relies on a constant-velocity motion assumption as\nwell as trifocal tensor geometry in order to express a relationship between\nline observations given by event clusters as well as first-order camera\ndynamics. Our core result is a closed-form solver for up-to-scale linear camera\nvelocity {with known angular velocity}. Nonlinear optimization is adopted to\nimprove the performance of the algorithm. The feasibility of the approach is\ndemonstrated through a careful analysis on both simulated and real data.",
          "link": "http://arxiv.org/abs/2109.04313",
          "publishedOn": "2021-09-10T07:20:14.002Z",
          "wordCount": 574,
          "title": "Continuous Event-Line Constraint for Closed-Form Velocity Initialization. (arXiv:2109.04313v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Shehroz S. Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Ziting Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haoying Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ax Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abedi_A/0/1/0/all/0/1\">Ali Abedi</a>",
          "description": "Detecting distracted driving behaviours is important to reduce millions of\ndeaths and injuries occurring worldwide. Distracted or anomalous driving\nbehaviours are deviations from the 'normal' driving that need to be identified\ncorrectly to alert the driver. However, these driving behaviours do not\ncomprise of one specific type of driving style and their distribution can be\ndifferent during training and testing phases of a classifier. We formulate this\nproblem as a supervised contrastive learning approach to learn a visual\nrepresentation to detect normal, and seen and unseen anomalous driving\nbehaviours. We made a change to the standard contrastive loss function to\nadjust the similarity of negative pairs to aid the optimization. Normally, the\n(self) supervised contrastive framework contains an encoder followed by a\nprojection head, which is omitted during testing phase as the encoding layers\nare considered to contain general visual representative information. However,\nwe assert that for supervised contrastive learning task, including projection\nhead will be beneficial. We showed our results on a Driver Anomaly Detection\ndataset that contains 783 minutes of video recordings of normal and anomalous\ndriving behaviours of 31 drivers from various from top and front cameras (both\ndepth and infrared). We also performed an extra step of fine tuning the labels\nin this dataset. Out of 9 video modalities combinations, our modified\ncontrastive approach improved the ROC AUC on 7 in comparison to the baseline\nmodels (from 3.12% to 8.91% for different modalities); the remaining two models\nalso had manual labelling. We performed statistical tests that showed evidence\nthat our modifications perform better than the baseline contrastive models.\nFinally, the results showed that the fusion of depth and infrared modalities\nfrom top and front view achieved the best AUC ROC of 0.9738 and AUC PR of\n0.9772.",
          "link": "http://arxiv.org/abs/2109.04021",
          "publishedOn": "2021-09-10T07:20:13.891Z",
          "wordCount": 747,
          "title": "Modified Supervised Contrastive Learning for Detecting Anomalous Driving Behaviours. (arXiv:2109.04021v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.03372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Lingyu Duan</a>",
          "description": "In object re-identification (ReID), the development of deep learning\ntechniques often involves model updates and deployment. It is unbearable to\nre-embedding and re-index with the system suspended when deploying new models.\nTherefore, backward-compatible representation is proposed to enable \"new\"\nfeatures to be compared with \"old\" features directly, which means that the\ndatabase is active when there are both \"new\" and \"old\" features in it. Thus we\ncan scroll-refresh the database or even do nothing on the database to update.\n\nThe existing backward-compatible methods either require a strong overlap\nbetween old and new training data or simply conduct constraints at the instance\nlevel. Thus they are difficult in handling complicated cluster structures and\nare limited in eliminating the impact of outliers in old embeddings, resulting\nin a risk of damaging the discriminative capability of new features. In this\nwork, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.\nWith no assumptions about the new training data, we estimate the sub-cluster\nstructures of old embeddings. A new embedding is constrained with multiple old\nembeddings in both embedding space and discrimination space at the sub-class\nlevel. The effect of outliers diminished, as the multiple samples serve as\n\"mean teachers\". Besides, we also propose a scheme to filter the old embeddings\nwith low credibility, further improving the compatibility robustness. Our\nmethod ensures backward compatibility without impairing the accuracy of the new\nmodel. And it can even improve the new model's accuracy in most scenarios.",
          "link": "http://arxiv.org/abs/2108.03372",
          "publishedOn": "2021-09-10T07:20:13.872Z",
          "wordCount": 720,
          "title": "Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "Efficient reasoning about the semantic, spatial, and temporal structure of a\nscene is a crucial prerequisite for autonomous driving. We present NEural\nATtention fields (NEAT), a novel representation that enables such reasoning for\nend-to-end imitation learning models. NEAT is a continuous function which maps\nlocations in Bird's Eye View (BEV) scene coordinates to waypoints and\nsemantics, using intermediate attention maps to iteratively compress\nhigh-dimensional 2D image features into a compact representation. This allows\nour model to selectively attend to relevant regions in the input while ignoring\ninformation irrelevant to the driving task, effectively associating the images\nwith the BEV representation. In a new evaluation setting involving adverse\nenvironmental conditions and challenging scenarios, NEAT outperforms several\nstrong baselines and achieves driving scores on par with the privileged CARLA\nexpert used to generate its training data. Furthermore, visualizing the\nattention maps for models with NEAT intermediate representations provides\nimproved interpretability.",
          "link": "http://arxiv.org/abs/2109.04456",
          "publishedOn": "2021-09-10T07:20:13.719Z",
          "wordCount": 605,
          "title": "NEAT: Neural Attention Fields for End-to-End Autonomous Driving. (arXiv:2109.04456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jiangtao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Benjia Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>",
          "description": "Vehicle Re-identification (ReID) is an important yet challenging problem in\ncomputer vision. Compared to other visual objects like faces and persons,\nvehicles simultaneously exhibit much larger intraclass viewpoint variations and\ninterclass visual similarities, making most exiting loss functions designed for\nface recognition and person ReID unsuitable for vehicle ReID. To obtain a\nhigh-performance vehicle ReID model, we present a novel Distance Shrinking with\nAngular Marginalizing (DSAM) loss function to perform hybrid learning in both\nthe Original Feature Space (OFS) and the Feature Angular Space (FAS) using the\nlocal verification and the global identification information. Specifically, it\nshrinks the distance between samples of the same class locally in the Original\nFeature Space while keeps samples of different classes far away in the Feature\nAngular Space. The shrinking and marginalizing operations are performed during\neach iteration of the training process and are suitable for different SoftMax\nbased loss functions. We evaluate the DSAM loss function on three large vehicle\nReID datasets with detailed analyses and extensive comparisons with many\ncompeting vehicle ReID methods. Experimental results show that our DSAM loss\nenhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset:\n10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is\nincreased by 9.34% on the PKU-VehicleID dataset and 6.13% on the VeRi-776\ndataset. Source code will be released to facilitate further studies in this\nresearch direction.",
          "link": "http://arxiv.org/abs/2011.06228",
          "publishedOn": "2021-09-10T07:20:13.592Z",
          "wordCount": 724,
          "title": "DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio. (arXiv:2011.06228v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Zhi Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Nowadays, scene text recognition has attracted more and more attention due to\nits various applications. Most state-of-the-art methods adopt an\nencoder-decoder framework with attention mechanism, which generates text\nautoregressively from left to right. Despite the convincing performance, the\nspeed is limited because of the one-by-one decoding strategy. As opposed to\nautoregressive models, non-autoregressive models predict the results in\nparallel with a much shorter inference time, but the accuracy falls behind the\nautoregressive counterpart considerably. In this paper, we propose a Parallel,\nIterative and Mimicking Network (PIMNet) to balance accuracy and efficiency.\nSpecifically, PIMNet adopts a parallel attention mechanism to predict the text\nfaster and an iterative generation mechanism to make the predictions more\naccurate. In each iteration, the context information is fully explored. To\nimprove learning of the hidden layer, we exploit the mimicking learning in the\ntraining phase, where an additional autoregressive decoder is adopted and the\nparallel decoder mimics the autoregressive decoder with fitting outputs of the\nhidden layer. With the shared backbone between the two decoders, the proposed\nPIMNet can be trained end-to-end without pre-training. During inference, the\nbranch of the autoregressive decoder is removed for a faster speed. Extensive\nexperiments on public benchmarks demonstrate the effectiveness and efficiency\nof PIMNet. Our code will be available at https://github.com/Pay20Y/PIMNet.",
          "link": "http://arxiv.org/abs/2109.04145",
          "publishedOn": "2021-09-10T07:20:13.583Z",
          "wordCount": 678,
          "title": "PIMNet: A Parallel, Iterative and Mimicking Network for Scene Text Recognition. (arXiv:2109.04145v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "Deep learning has the potential to augment many components of the clinical\nworkflow, such as medical image interpretation. However, the translation of\nthese black box algorithms into clinical practice has been marred by the\nrelative lack of transparency compared to conventional machine learning\nmethods, hindering in clinician trust in the systems for critical medical\ndecision-making. Specifically, common deep learning approaches do not have\nintuitive ways of expressing uncertainty with respect to cases that might\nrequire further human review. Furthermore, the possibility of algorithmic bias\nhas caused hesitancy regarding the use of developed algorithms in clinical\nsettings. To these ends, we explore how conformal methods can complement deep\nlearning models by providing both clinically intuitive way (by means of\nconfidence prediction sets) of expressing model uncertainty as well as\nfacilitating model transparency in clinical workflows. In this paper, we\nconduct a field survey with clinicians to assess clinical use-cases of\nconformal predictions. Next, we conduct experiments with a mammographic breast\ndensity and dermatology photography datasets to demonstrate the utility of\nconformal predictions in \"rule-in\" and \"rule-out\" disease scenarios. Further,\nwe show that conformal predictors can be used to equalize coverage with respect\nto patient demographics such as race and skin tone. We find that a conformal\npredictions to be a promising framework with potential to increase clinical\nusability and transparency for better collaboration between deep learning\nalgorithms and clinicians.",
          "link": "http://arxiv.org/abs/2109.04392",
          "publishedOn": "2021-09-10T07:20:13.564Z",
          "wordCount": 687,
          "title": "Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.05582",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1\">Timo Milbich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1\">Karsten Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brattoli_B/0/1/0/all/0/1\">Biagio Brattoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Learning the similarity between images constitutes the foundation for\nnumerous vision tasks. The common paradigm is discriminative metric learning,\nwhich seeks an embedding that separates different training classes. However,\nthe main challenge is to learn a metric that not only generalizes from training\nto novel, but related, test samples. It should also transfer to different\nobject classes. So what complementary information is missed by the\ndiscriminative paradigm? Besides finding characteristics that separate between\nclasses, we also need them to likely occur in novel categories, which is\nindicated if they are shared across training classes. This work investigates\nhow to learn such characteristics without the need for extra annotations or\ntraining data. By formulating our approach as a novel triplet sampling\nstrategy, it can be easily applied on top of recent ranking loss frameworks.\nExperiments show that, independent of the underlying network architecture and\nthe specific ranking loss, our approach significantly improves performance in\ndeep metric learning, leading to new the state-of-the-art results on various\nstandard benchmark datasets. Preliminary early access page can be found here:\nhttps://ieeexplore.ieee.org/document/9141449",
          "link": "http://arxiv.org/abs/2004.05582",
          "publishedOn": "2021-09-10T07:20:13.543Z",
          "wordCount": 672,
          "title": "Sharing Matters for Generalization in Deep Metric Learning. (arXiv:2004.05582v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hao_H/0/1/0/all/0/1\">Hanxiang Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaTourette_K/0/1/0/all/0/1\">Kevin LaTourette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konz_L/0/1/0/all/0/1\">Latisha Konz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_M/0/1/0/all/0/1\">Moses Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comer_M/0/1/0/all/0/1\">Mary L. Comer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Automatic building segmentation is an important task for satellite imagery\nanalysis and scene understanding. Most existing segmentation methods focus on\nthe case where the images are taken from directly overhead (i.e., low\noff-nadir/viewing angle). These methods often fail to provide accurate results\non satellite images with larger off-nadir angles due to the higher noise level\nand lower spatial resolution. In this paper, we propose a method that is able\nto provide accurate building segmentation for satellite imagery captured from a\nlarge range of off-nadir angles. Based on Bayesian deep learning, we explicitly\ndesign our method to learn the data noise via aleatoric and epistemic\nuncertainty modeling. Satellite image metadata (e.g., off-nadir angle and\nground sample distance) is also used in our model to further improve the\nresult. We show that with uncertainty modeling and metadata injection, our\nmethod achieves better performance than the baseline method, especially for\nnoisy images taken from large off-nadir angles.",
          "link": "http://arxiv.org/abs/2109.03961",
          "publishedOn": "2021-09-10T07:20:13.511Z",
          "wordCount": 617,
          "title": "Improving Building Segmentation for Off-Nadir Satellite Imagery. (arXiv:2109.03961v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In this paper, we introduce the \\textit{Layer-Peeled Model}, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts of the network. We demonstrate that the\nLayer-Peeled Model, albeit simple, inherits many characteristics of\nwell-trained neural networks, thereby offering an effective tool for explaining\nand predicting common empirical patterns of deep learning training. First, when\nworking on class-balanced datasets, we prove that any solution to this model\nforms a simplex equiangular tight frame, which in part explains the recently\ndiscovered phenomenon of neural collapse \\cite{papyan2020prevalence}. More\nimportantly, when moving to the imbalanced case, our analysis of the\nLayer-Peeled Model reveals a hitherto unknown phenomenon that we term\n\\textit{Minority Collapse}, which fundamentally limits the performance of deep\nlearning models on the minority classes. In addition, we use the Layer-Peeled\nModel to gain insights into how to mitigate Minority Collapse. Interestingly,\nthis phenomenon is first predicted by the Layer-Peeled Model before being\nconfirmed by our computational experiments.",
          "link": "http://arxiv.org/abs/2101.12699",
          "publishedOn": "2021-09-10T07:20:13.416Z",
          "wordCount": 717,
          "title": "Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. (arXiv:2101.12699v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04202",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>",
          "description": "Like many scientific fields, new chemistry literature has grown at a\nstaggering pace, with thousands of papers released every month. A large portion\nof chemistry literature focuses on new molecules and reactions between\nmolecules. Most vital information is conveyed through 2-D images of molecules,\nrepresenting the underlying molecules or reactions described. In order to\nensure reproducible and machine-readable molecule representations, text-based\nmolecule descriptors like SMILES and SELFIES were created. These text-based\nmolecule representations provide molecule generation but are unfortunately\nrarely present in published literature. In the absence of molecule descriptors,\nthe generation of molecule descriptors from the 2-D images present in the\nliterature is necessary to understand chemistry literature at scale. Successful\nmethods such as Optical Structure Recognition Application (OSRA), and\nChemSchematicResolver are able to extract the locations of molecules structures\nin chemistry papers and infer molecular descriptions and reactions. While\neffective, existing systems expect chemists to correct outputs, making them\nunsuitable for unsupervised large-scale data mining. Leveraging the task\nformulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a\nmodel which leverages Deep Residual Networks for image feature extraction and\nan encoder-decoder Transformer layers for molecule description generation.\nUnlike previous Neural Network-based systems, IMG2SMI builds around the task of\nmolecule description generation, which enables IMG2SMI to outperform OSRA-based\nsystems by 163% in molecule similarity prediction as measured by the molecular\nMACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further\nresearch on this task, we release a new molecule prediction dataset. including\n81 million molecules for molecule description generation",
          "link": "http://arxiv.org/abs/2109.04202",
          "publishedOn": "2021-09-10T07:20:13.186Z",
          "wordCount": 706,
          "title": "IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System. (arXiv:2109.04202v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1\">Emre Akbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vural_F/0/1/0/all/0/1\">Fatos T. Yarman Vural</a>",
          "description": "In this study, we introduce a measure for machine perception, inspired by the\nconcept of Just Noticeable Difference (JND) of human perception. Based on this\nmeasure, we suggest an adversarial image generation algorithm, which\niteratively distorts an image by an additive noise until the model detects the\nchange in the image by outputting a false label. The noise added to the\noriginal image is defined as the gradient of the cost function of the model. A\nnovel cost function is defined to explicitly minimize the amount of\nperturbation applied to the input image while enforcing the perceptual\nsimilarity between the adversarial and input images. For this purpose, the cost\nfunction is regularized by the well-known total variation and bounded range\nterms to meet the natural appearance of the adversarial image. We evaluate the\nadversarial images generated by our algorithm both qualitatively and\nquantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on\nimage classification and object detection tasks show that adversarial images\ngenerated by our JND method are both more successful in deceiving the\nrecognition/detection models and less perturbed compared to the images\ngenerated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool\nmethods.",
          "link": "http://arxiv.org/abs/2102.08079",
          "publishedOn": "2021-09-10T07:20:13.157Z",
          "wordCount": 703,
          "title": "Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04047",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinsoo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>",
          "description": "A common problem in the task of human-object interaction (HOI) detection is\nthat numerous HOI classes have only a small number of labeled examples,\nresulting in training sets with a long-tailed distribution. The lack of\npositive labels can lead to low classification accuracy for these classes.\nTowards addressing this issue, we observe that there exist natural correlations\nand anti-correlations among human-object interactions. In this paper, we model\nthe correlations as action co-occurrence matrices and present techniques to\nlearn these priors and leverage them for more effective training, especially on\nrare classes. The efficacy of our approach is demonstrated experimentally,\nwhere the performance of our approach consistently improves over the\nstate-of-the-art methods on both of the two leading HOI detection benchmark\ndatasets, HICO-Det and V-COCO.",
          "link": "http://arxiv.org/abs/2109.04047",
          "publishedOn": "2021-09-10T07:20:13.128Z",
          "wordCount": 593,
          "title": "ACP++: Action Co-occurrence Priors for Human-Object Interaction Detection. (arXiv:2109.04047v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>",
          "description": "Vision transformers (ViTs) have recently received explosive popularity, but\nthe huge computational cost is still a severe issue. Since the computation\ncomplexity of ViT is quadratic with respect to the input sequence length, a\nmainstream paradigm for computation reduction is to reduce the number of\ntokens. Existing designs include structured spatial compression that uses a\nprogressive shrinking pyramid to reduce the computations of large feature maps,\nand unstructured token pruning that dynamically drops redundant tokens.\nHowever, the limitation of existing token pruning lies in two folds: 1) the\nincomplete spatial structure caused by pruning is not compatible with\nstructured spatial compression that is commonly used in modern deep-narrow\ntransformers; 2) it usually requires a time-consuming pre-training procedure.\nTo tackle the limitations and expand the applicable scenario of token pruning,\nwe present Evo-ViT, a self-motivated slow-fast token evolution approach for\nvision transformers. Specifically, we conduct unstructured instance-wise token\nselection by taking advantage of the simple and effective global class\nattention that is native to vision transformers. Then, we propose to update the\nselected informative tokens and uninformative tokens with different computation\npaths, namely, slow-fast updating. Since slow-fast updating mechanism maintains\nthe spatial structure and information flow, Evo-ViT can accelerate vanilla\ntransformers of both flat and deep-narrow structures from the very beginning of\nthe training process. Experimental results demonstrate that our method\nsignificantly reduces the computational cost of vision transformers while\nmaintaining comparable performance on image classification.",
          "link": "http://arxiv.org/abs/2108.01390",
          "publishedOn": "2021-09-10T07:20:13.119Z",
          "wordCount": 754,
          "title": "Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Guannan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Network pruning is an effective approach to reduce network complexity without\nperformance compromise. Existing studies achieve the sparsity of neural\nnetworks via time-consuming weight tuning or complex search on networks with\nexpanded width, which greatly limits the applications of network pruning. In\nthis paper, we show that high-performing and sparse sub-networks without the\ninvolvement of weight tuning, termed \"lottery jackpots\", exist in pre-trained\nmodels with unexpanded width. For example, we obtain a lottery jackpot that has\nonly 10% parameters and still reaches the performance of the original dense\nVGGNet-19 without any modifications on the pre-trained weights. Furthermore, we\nobserve that the sparse masks derived from many existing pruning criteria have\na high overlap with the searched mask of our lottery jackpot, among which, the\nmagnitude-based pruning results in the most similar mask with ours. Based on\nthis insight, we initialize our sparse mask using the magnitude pruning,\nresulting in at least 3x cost reduction on the lottery jackpot search while\nachieves comparable or even better performance. Specifically, our\nmagnitude-based lottery jackpot removes 90% weights in the ResNet-50, while\neasily obtains more than 70% top-1 accuracy using only 10 searching epochs on\nImageNet.",
          "link": "http://arxiv.org/abs/2104.08700",
          "publishedOn": "2021-09-10T07:20:13.085Z",
          "wordCount": 691,
          "title": "Efficient Weight Pruning using Pre-trained Lottery Jackpots. (arXiv:2104.08700v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1\">Sherif A.S. Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghbayan_M/0/1/0/all/0/1\">Mohammad-Hashem Haghbayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miele_A/0/1/0/all/0/1\">Antonio Miele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plosila_J/0/1/0/all/0/1\">Juha Plosila</a>",
          "description": "We propose an energy-efficient controller to minimize the energy consumption\nof a mobile robot by dynamically manipulating the mechanical and computational\nactuators of the robot. The mobile robot performs real-time vision-based\napplications based on an event-based camera. The actuators of the controller\nare CPU voltage/frequency for the computation part and motor voltage for the\nmechanical part. We show that independently considering speed control of the\nrobot and voltage/frequency control of the CPU does not necessarily result in\nan energy-efficient solution. In fact, to obtain the highest efficiency, the\ncomputation and mechanical parts should be controlled together in synergy. We\npropose a fast hill-climbing optimization algorithm to allow the controller to\nfind the best CPU/motor configuration at run-time and whenever the mobile robot\nis facing a new environment during its travel. Experimental results on a robot\nwith Brushless DC Motors, Jetson TX2 board as the computing unit, and a\nDAVIS-346 event-based camera show that the proposed control algorithm can save\nbattery energy by an average of 50.5%, 41%, and 30%, in low-complexity,\nmedium-complexity, and high-complexity environments, over baselines.",
          "link": "http://arxiv.org/abs/2109.04285",
          "publishedOn": "2021-09-10T07:20:13.002Z",
          "wordCount": 648,
          "title": "Energy-Efficient Mobile Robot Control via Run-time Monitoring of Environmental Complexity and Computing Workload. (arXiv:2109.04285v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sibei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Preserving maximal information is one of principles of designing\nself-supervised learning methodologies. To reach this goal, contrastive\nlearning adopts an implicit way which is contrasting image pairs. However, we\nbelieve it is not fully optimal to simply use the contrastive estimation for\npreservation. Moreover, it is necessary and complemental to introduce an\nexplicit solution to preserve more information. From this perspective, we\nintroduce Preservational Learning to reconstruct diverse image contexts in\norder to preserve more information in learned representations. Together with\nthe contrastive loss, we present Preservational Contrastive Representation\nLearning (PCRL) for learning self-supervised medical representations. PCRL\nprovides very competitive results under the pretraining-finetuning protocol,\noutperforming both self-supervised and supervised counterparts in 5\nclassification/segmentation tasks substantially.",
          "link": "http://arxiv.org/abs/2109.04379",
          "publishedOn": "2021-09-10T07:20:12.935Z",
          "wordCount": 580,
          "title": "Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. (arXiv:2109.04379v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiujie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junjie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuangmin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shiqing Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1\">Changhe Tu</a>",
          "description": "Surface reconstruction from noisy, non-uniformly, and unoriented point clouds\nis a fascinating yet difficult problem in computer vision and computer\ngraphics. In this paper, we propose Neural-IMLS, a novel approach that learning\nnoise-resistant signed distance function (SDF) for reconstruction. Instead of\nexplicitly learning priors with the ground-truth signed distance values, our\nmethod learns the SDF from raw point clouds directly in a self-supervised\nfashion by minimizing the loss between the couple of SDFs, one obtained by the\nimplicit moving least-square function (IMLS) and the other by our network.\nFinally, a watertight and smooth 2-manifold triangle mesh is yielded by running\nMarching Cubes. We conduct extensive experiments on various benchmarks to\ndemonstrate the performance of Neural-IMLS, especially for point clouds with\nnoise.",
          "link": "http://arxiv.org/abs/2109.04398",
          "publishedOn": "2021-09-10T07:20:12.635Z",
          "wordCount": 587,
          "title": "Neural-IMLS: Learning Implicit Moving Least-Squares for Surface Reconstruction from Unoriented Point clouds. (arXiv:2109.04398v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_D/0/1/0/all/0/1\">Dimitri Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1\">Ignacio Rocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnberger_J/0/1/0/all/0/1\">Johannes L. Schnberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekin_B/0/1/0/all/0/1\">Bugra Tekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>",
          "description": "Narrated instructional videos often show and describe manipulations of\nsimilar objects, e.g., repairing a particular model of a car or laptop. In this\nwork we aim to reconstruct such objects and to localize associated narrations\nin 3D. Contrary to the standard scenario of instance-level 3D reconstruction,\nwhere identical objects or scenes are present in all views, objects in\ndifferent instructional videos may have large appearance variations given\nvarying conditions and versions of the same product. Narrations may also have\nlarge variation in natural language expressions. We address these challenges by\nthree contributions. First, we propose an approach for correspondence\nestimation combining learnt local features and dense flow. Second, we design a\ntwo-step divide and conquer reconstruction approach where the initial 3D\nreconstructions of individual videos are combined into a 3D alignment graph.\nFinally, we propose an unsupervised approach to ground natural language in\nobtained 3D reconstructions. We demonstrate the effectiveness of our approach\nfor the domain of car maintenance. Given raw instructional videos and no manual\nsupervision, our method successfully reconstructs engines of different car\nmodels and associates textual descriptions with corresponding objects in 3D.",
          "link": "http://arxiv.org/abs/2109.04409",
          "publishedOn": "2021-09-10T07:20:12.594Z",
          "wordCount": 637,
          "title": "Reconstructing and grounding narrated instructional videos in 3D. (arXiv:2109.04409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">YiMin Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianbing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yingjie Xi</a>",
          "description": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training.",
          "link": "http://arxiv.org/abs/2105.03857",
          "publishedOn": "2021-09-10T07:20:12.587Z",
          "wordCount": 772,
          "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shuang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>",
          "description": "3D point cloud semantic segmentation is a challenging topic in the computer\nvision field. Most of the existing methods in literature require a large amount\nof fully labeled training data, but it is extremely time-consuming to obtain\nthese training data by manually labeling massive point clouds. Addressing this\nproblem, we propose a superpoint-guided semi-supervised segmentation network\nfor 3D point clouds, which jointly utilizes a small portion of labeled scene\npoint clouds and a large number of unlabeled point clouds for network training.\nThe proposed network is iteratively updated with its predicted pseudo labels,\nwhere a superpoint generation module is introduced for extracting superpoints\nfrom 3D point clouds, and a pseudo-label optimization module is explored for\nautomatically assigning pseudo labels to the unlabeled points under the\nconstraint of the extracted superpoints. Additionally, there are some 3D points\nwithout pseudo-label supervision. We propose an edge prediction module to\nconstrain features of edge points. A superpoint feature aggregation module and\na superpoint feature consistency loss function are introduced to smooth\nsuperpoint features. Extensive experimental results on two 3D public datasets\ndemonstrate that our method can achieve better performance than several\nstate-of-the-art point cloud segmentation networks and several popular\nsemi-supervised segmentation methods with few labeled scenes.",
          "link": "http://arxiv.org/abs/2107.03601",
          "publishedOn": "2021-09-10T07:20:12.509Z",
          "wordCount": 682,
          "title": "Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2107.03601v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeisen_M/0/1/0/all/0/1\">Michael Keckeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Development in the field of Single Board Computers (SBC) have been increasing\nfor several years. They provide a good balance between computing performance\nand power consumption which is usually required for mobile platforms, like\napplication in vehicles for Advanced Driver Assistance Systems (ADAS) and\nAutonomous Driving (AD). However, there is an ever-increasing need of more\npowerful and efficient SBCs which can run power intensive Deep Neural Networks\n(DNNs) in real-time and can also satisfy necessary functional safety\nrequirements such as Automotive Safety Integrity Level (ASIL). ProAI is being\ndeveloped by ZF mainly to run powerful and efficient applications such as\nmultitask DNNs and on top of that it also has the required safety certification\nfor AD. In this work, we compare and discuss state of the art SBC on the basis\nof power intensive multitask DNN architecture called Multitask-CenterNet with\nrespect to performance measures such as, FPS and power efficiency. As an\nautomotive supercomputer, ProAI delivers an excellent combination of\nperformance and efficiency, managing nearly twice the number of FPS per watt\nthan a modern workstation laptop and almost four times compared to the Jetson\nNano. Furthermore, it was also shown that there is still power in reserve for\nfurther and more complex tasks on the ProAI, based on the CPU and GPU\nutilization during the benchmark.",
          "link": "http://arxiv.org/abs/2108.05170",
          "publishedOn": "2021-09-10T07:20:12.457Z",
          "wordCount": 720,
          "title": "ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study. (arXiv:2108.05170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15607",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joong-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyung-Il Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongjin Kwon</a>",
          "description": "Since many safety-critical systems, such as surgical robots and autonomous\ndriving cars operate in unstable environments with sensor noise and incomplete\ndata, it is desirable for object detectors to take the localization uncertainty\ninto account. However, there are several limitations of the existing\nuncertainty estimation methods for anchor-based object detection. 1) They model\nthe uncertainty of the heterogeneous object properties with different\ncharacteristics and scales, such as location (center point) and scale (width,\nheight), which could be difficult to estimate. 2) They model box offsets as\nGaussian distributions, which is not compatible with the ground truth bounding\nboxes that follow the Dirac delta distribution. 3) Since anchor-based methods\nare sensitive to anchor hyperparameters, the localization uncertainty for them\ncould be also highly sensitive to the choice of hyperparameters as well. To\ntackle these limitations, we propose a new localization uncertainty estimation\nmethod called UAD for anchor-free object detection. Our method captures the\nuncertainty in four directions of box offsets~(left, right, top, bottom) that\nare homogeneous, so that it can tell which direction is uncertain, and provides\na quantitative value of uncertainty in $[0, 1]$. To enable such uncertainty\nestimation, we design a new uncertainty loss, negative power log-likelihood\nloss, to measure the localization uncertainty by weighting the likelihood loss\nby its IoU, which alleviates the model misspecification problem. Furthermore,\nwe propose an uncertainty-aware focal loss for reflecting the estimated\nuncertainty to the classification score. Experimental results on COCO datasets\ndemonstrate that our method significantly improves FCOS, by up to 1.8 points,\nwithout sacrificing computational efficiency.",
          "link": "http://arxiv.org/abs/2006.15607",
          "publishedOn": "2021-09-10T07:20:12.063Z",
          "wordCount": 756,
          "title": "Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>",
          "description": "A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.",
          "link": "http://arxiv.org/abs/2010.03060",
          "publishedOn": "2021-09-10T07:20:12.045Z",
          "wordCount": 704,
          "title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Ka Leong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yueqi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "Reversible image conversion (RIC) aims to build a reversible transformation\nbetween specific visual content (e.g., short videos) and an embedding image,\nwhere the original content can be restored from the embedding when necessary.\nThis work develops Invertible Image Conversion Net (IICNet) as a generic\nsolution to various RIC tasks due to its strong capacity and task-independent\ndesign. Unlike previous encoder-decoder based methods, IICNet maintains a\nhighly invertible structure based on invertible neural networks (INNs) to\nbetter preserve the information during conversion. We use a relation module and\na channel squeeze layer to improve the INN nonlinearity to extract cross-image\nrelations and the network flexibility, respectively. Experimental results\ndemonstrate that IICNet outperforms the specifically-designed methods on\nexisting RIC tasks and can generalize well to various newly-explored tasks.\nWith our generic IICNet, we no longer need to hand-engineer task-specific\nembedding networks for rapidly occurring visual content. Our source codes are\navailable at: https://github.com/felixcheng97/IICNet.",
          "link": "http://arxiv.org/abs/2109.04242",
          "publishedOn": "2021-09-10T07:20:12.037Z",
          "wordCount": 603,
          "title": "IICNet: A Generic Framework for Reversible Image Conversion. (arXiv:2109.04242v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kyra Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantipongpipat_U/0/1/0/all/0/1\">Uthaipon Tantipongpipat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>",
          "description": "Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities because of an effect we term\nargmax bias. However, we demonstrate that formalized fairness metrics and\nquantitative analysis on their own are insufficient for capturing the risk of\nrepresentational harm in automatic cropping. We suggest the removal of\nsaliency-based cropping in favor of a solution that better preserves user\nagency. For developing a new solution that sufficiently address concerns\nrelated to representational harm, our critique motivates a combination of\nquantitative and qualitative methods that include human-centered design.",
          "link": "http://arxiv.org/abs/2105.08667",
          "publishedOn": "2021-09-10T07:20:12.023Z",
          "wordCount": 685,
          "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency. (arXiv:2105.08667v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiyue Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_W/0/1/0/all/0/1\">Wan Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1\">Michael Foshey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Tactile sensing is critical for humans to perform everyday tasks. While\nsignificant progress has been made in analyzing object grasping from vision, it\nremains unclear how we can utilize tactile sensing to reason about and model\nthe dynamics of hand-object interactions. In this work, we employ a\nhigh-resolution tactile glove to perform four different interactive activities\non a diversified set of objects. We build our model on a cross-modal learning\nframework and generate the labels using a visual processing pipeline to\nsupervise the tactile model, which can then be used on its own during the test\ntime. The tactile model aims to predict the 3d locations of both the hand and\nthe object purely from the touch data by combining a predictive model and a\ncontrastive learning module. This framework can reason about the interaction\npatterns from the tactile data, hallucinate the changes in the environment,\nestimate the uncertainty of the prediction, and generalize to unseen objects.\nWe also provide detailed ablation studies regarding different system designs as\nwell as visualizations of the predicted trajectories. This work takes a step on\ndynamics modeling in hand-object interactions from dense tactile sensing, which\nopens the door for future applications in activity learning, human-computer\ninteractions, and imitation learning for robotics.",
          "link": "http://arxiv.org/abs/2109.04378",
          "publishedOn": "2021-09-10T07:20:12.016Z",
          "wordCount": 683,
          "title": "Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. (arXiv:2109.04378v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>",
          "description": "We present HoHoNet, a versatile and efficient framework for holistic\nunderstanding of an indoor 360-degree panorama using a Latent Horizontal\nFeature (LHFeat). The compact LHFeat flattens the features along the vertical\ndirection and has shown success in modeling per-column modality for room layout\nreconstruction. HoHoNet advances in two important aspects. First, the deep\narchitecture is redesigned to run faster with improved accuracy. Second, we\npropose a novel horizon-to-dense module, which relaxes the per-column output\nshape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is\nfast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones\nrespectively, for modeling dense modalities from a high-resolution $512 \\times\n1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and\nsemantic segmentation, HoHoNet achieves results on par with current\nstate-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior\narts by a large margin.",
          "link": "http://arxiv.org/abs/2011.11498",
          "publishedOn": "2021-09-10T07:20:12.009Z",
          "wordCount": 638,
          "title": "HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features. (arXiv:2011.11498v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DellEva_A/0/1/0/all/0/1\">Anthony Dell&#x27;Eva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1\">Massimo Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image (i2i) networks struggle to capture local changes because they\ndo not affect the global scene structure. For example, translating from highway\nscenes to offroad, i2i networks easily focus on global color features but\nignore obvious traits for humans like the absence of lane markings. In this\npaper, we leverage human knowledge about spatial domain characteristics which\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\nGAN on few source data and hallucinate a new unseen domain which subsequently\neases transfer learning to target. We experiment on three tasks ranging from\nunstructured environments to adverse weather. Our comprehensive evaluation\nsetting shows we are able to generate realistic translations, with minimal\npriors, and training only on a few images. Furthermore, when trained on our\ntranslations images we show that all tested proxy tasks are significantly\nimproved, without ever seeing target domain at training.",
          "link": "http://arxiv.org/abs/2109.04468",
          "publishedOn": "2021-09-10T07:20:11.960Z",
          "wordCount": 613,
          "title": "Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiquier_M/0/1/0/all/0/1\">Mia Chiquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>",
          "description": "We find that images contain intrinsic structure that enables the reversal of\nmany adversarial attacks. Attack vectors cause not only image classifiers to\nfail, but also collaterally disrupt incidental structure in the image. We\ndemonstrate that modifying the attacked image to restore the natural structure\nwill reverse many types of attacks, providing a defense. Experiments\ndemonstrate significantly improved robustness for several state-of-the-art\nmodels across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results\nshow that our defense is still effective even if the attacker is aware of the\ndefense mechanism. Since our defense is deployed during inference instead of\ntraining, it is compatible with pre-trained networks as well as most other\ndefenses. Our results suggest deep networks are vulnerable to adversarial\nexamples partly because their representations do not enforce the natural\nstructure of images.",
          "link": "http://arxiv.org/abs/2103.14222",
          "publishedOn": "2021-09-10T07:20:11.936Z",
          "wordCount": 626,
          "title": "Adversarial Attacks are Reversible with Natural Supervision. (arXiv:2103.14222v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1\">Bo Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Reconstructing 3D object from a single image (RGB or depth) is a fundamental\nproblem in visual scene understanding and yet remains challenging due to its\nill-posed nature and complexity in real-world scenes. To address those\nchallenges, we adopt a primitive-based representation for 3D object, and\npropose a two-stage graph network for primitive-based 3D object estimation,\nwhich consists of a sequential proposal module and a graph reasoning module.\nGiven a 2D image, our proposal module first generates a sequence of 3D\nprimitives from input image with local feature attention. Then the graph\nreasoning module performs joint reasoning on a primitive graph to capture the\nglobal shape context for each primitive. Such a framework is capable of taking\ninto account rich geometry and semantic constraints during 3D structure\nrecovery, producing 3D objects with more coherent structure even under\nchallenging viewing conditions. We train the entire graph neural network in a\nstage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and\nNYU Depth V2. Extensive experiments show that our approach outperforms the\nprevious state of the arts with a considerable margin.",
          "link": "http://arxiv.org/abs/2109.04153",
          "publishedOn": "2021-09-10T07:20:11.904Z",
          "wordCount": 635,
          "title": "Single Image 3D Object Estimation with Primitive Graph Networks. (arXiv:2109.04153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14273",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basher_A/0/1/0/all/0/1\">Abol Basher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarmad_M/0/1/0/all/0/1\">Muhammad Sarmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutellier_J/0/1/0/all/0/1\">Jani Boutellier</a>",
          "description": "Recently, several works have addressed modeling of 3D shapes using deep\nneural networks to learn implicit surface representations. Up to now, the\nmajority of works have concentrated on reconstruction quality, paying little or\nno attention to model size or training time. This work proposes LightSAL, a\nnovel deep convolutional architecture for learning 3D shapes; the proposed work\nconcentrates on efficiency both in network training time and resulting model\nsize. We build on the recent concept of Sign Agnostic Learning for training the\nproposed network, relying on signed distance fields, with unsigned distance as\nground truth. In the experimental section of the paper, we demonstrate that the\nproposed architecture outperforms previous work in model size and number of\nrequired training iterations, while achieving equivalent accuracy. Experiments\nare based on the D-Faust dataset that contains 41k 3D scans of human shapes.\nThe proposed model has been implemented in PyTorch.",
          "link": "http://arxiv.org/abs/2103.14273",
          "publishedOn": "2021-09-10T07:20:11.884Z",
          "wordCount": 619,
          "title": "LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface Representation. (arXiv:2103.14273v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>",
          "description": "In this paper, we aim to advance the research of multi-modal pre-training on\nE-commerce and subsequently contribute a large-scale dataset, named M5Product,\nwhich consists of over 6 million multimodal pairs, covering more than 6,000\ncategories and 5,000 attributes. Generally, existing multi-modal datasets are\neither limited in scale or modality diversity. Differently, our M5Product is\nfeatured from the following aspects. First, the M5Product dataset is 500 times\nlarger than the public multimodal dataset with the same number of modalities\nand nearly twice larger compared with the largest available text-image\ncross-modal dataset. Second, the dataset contains rich information of multiple\nmodalities including image, text, table, video and audio, in which each\nmodality can capture different views of semantic information (e.g. category,\nattributes, affordance, brand, preference) and complements the other. Third, to\nbetter accommodate with real-world problems, a few portion of M5Product\ncontains incomplete modality pairs and noises while having the long-tailed\ndistribution, which aligns well with real-world scenarios. Finally, we provide\na baseline model M5-MMT that makes the first attempt to integrate the different\nmodality configuration into an unified model for feature fusion to address the\ngreat challenge for semantic alignment. We also evaluate various multi-model\npre-training state-of-the-arts for benchmarking their capabilities in learning\nfrom unlabeled data under the different number of modalities on the M5Product\ndataset. We conduct extensive experiments on four downstream tasks and provide\nsome interesting findings on these modalities. Our dataset and related code are\navailable at https://xiaodongsuper.github.io/M5Product_dataset.",
          "link": "http://arxiv.org/abs/2109.04275",
          "publishedOn": "2021-09-10T07:20:11.877Z",
          "wordCount": 699,
          "title": "M5Product: A Multi-modal Pretraining Benchmark for E-commercial Product Downstream Tasks. (arXiv:2109.04275v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shilin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinghong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shulu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yuejia Han</a>",
          "description": "The aim of this paper is to improve the accuracy of copy-move forgery\ndetection (CMFD) in image forensics by proposing a novel scheme. The proposed\nscheme integrates both block-based and keypoint-based forgery detection\nmethods. Firstly, speed-up robust feature (SURF) descriptor in log-polar space\nand scale invariant feature transform (SIFT) descriptor are extracted from an\nentire forged image. Secondly, generalized 2 nearest neighbor (g2NN) is\nemployed to get massive matched pairs. Then, random sample consensus (RANSAC)\nalgorithm is employed to filter out mismatched pairs, thus allowing rough\nlocalization of the counterfeit areas. To present more accurately these forgery\nareas more accurately, we propose an efficient and accurate algorithm, evolving\ncircular domains coverage (ECDC), to cover present them. This algorithm aims to\nfind satisfactory threshold areas by extracting block features from jointly\nevolving circular domains, which are centered on the matched pairs. Finally,\nmorphological operation is applied to refine the detected forgery areas. The\nexperimental results indicate that the proposed CMFD scheme can achieve better\ndetection performance under various attacks compared with other\nstate-of-the-art CMFD schemes.",
          "link": "http://arxiv.org/abs/2109.04381",
          "publishedOn": "2021-09-10T07:20:11.854Z",
          "wordCount": 626,
          "title": "Copy-Move Image Forgery Detection Based on Evolving Circular Domains Coverage. (arXiv:2109.04381v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04352",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salehi_Y/0/1/0/all/0/1\">Yasmin Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giannacopoulos_D/0/1/0/all/0/1\">Dennis Giannacopoulos</a>",
          "description": "Correctly capturing intraoperative brain shift in image-guided neurosurgical\nprocedures is a critical task for aligning preoperative data with\nintraoperative geometry, ensuring effective surgical navigation and optimal\nsurgical precision. While the finite element method (FEM) is a proven technique\nto effectively approximate soft tissue deformation through biomechanical\nformulations, their degree of success boils down to a trade-off between\naccuracy and speed. To circumvent this problem, the most recent works in this\ndomain have proposed leveraging data-driven models obtained by training various\nmachine learning algorithms, e.g. random forests, artificial neural networks\n(ANNs), with the results of finite element analysis (FEA) to speed up tissue\ndeformation approximations by prediction. These methods, however, do not\naccount for the structure of the finite element (FE) mesh during training that\nprovides information on node connectivities as well as the distance between\nthem, which can aid with approximating tissue deformation based on the\nproximity of force load points with the rest of the mesh nodes. Therefore, this\nwork proposes a novel framework, PhysGNN, a data-driven model that approximates\nthe solution of FEA by leveraging graph neural networks (GNNs), which are\ncapable of accounting for the mesh structural information and inductive\nlearning over unstructured grids and complex topological structures.\nEmpirically, we demonstrate that the proposed architecture, PhysGNN, promises\naccurate and fast soft tissue deformation approximations while remaining\ncomputationally feasible, suitable for neurosurgical settings.",
          "link": "http://arxiv.org/abs/2109.04352",
          "publishedOn": "2021-09-10T07:20:11.835Z",
          "wordCount": 697,
          "title": "PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning-Hsu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>",
          "description": "Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&V-planes (referred to as\nPanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.",
          "link": "http://arxiv.org/abs/2106.14166",
          "publishedOn": "2021-09-10T07:20:11.829Z",
          "wordCount": 633,
          "title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruoxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Borui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yangzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenglong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>",
          "description": "In this work we propose Energy Attack, a transfer-based black-box\n$L_\\infty$-adversarial attack. The attack is parameter-free and does not\nrequire gradient approximation. In particular, we first obtain white-box\nadversarial perturbations of a surrogate model and divide these perturbations\ninto small patches. Then we extract the unit component vectors and eigenvalues\nof these patches with principal component analysis (PCA). Base on the\neigenvalues, we can model the energy distribution of adversarial perturbations.\nWe then perform black-box attacks by sampling from the perturbation patches\naccording to their energy distribution, and tiling the sampled patches to form\na full-size adversarial perturbation. This can be done without the available\naccess to victim models. Extensive experiments well demonstrate that the\nproposed Energy Attack achieves state-of-the-art performance in black-box\nattacks on various models and several datasets. Moreover, the extracted\ndistribution is able to transfer among different model architectures and\ndifferent datasets, and is therefore intrinsic to vision architectures.",
          "link": "http://arxiv.org/abs/2109.04300",
          "publishedOn": "2021-09-10T07:20:11.822Z",
          "wordCount": 601,
          "title": "Energy Attack: On Transferring Adversarial Examples. (arXiv:2109.04300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "MLP-based architectures, which consist of a sequence of consecutive\nmulti-layer perceptron blocks, have recently been found to reach comparable\nresults to convolutional and transformer-based methods. However, most adopt\nspatial MLPs which take fixed dimension inputs, therefore making it difficult\nto apply them to downstream tasks, such as object detection and semantic\nsegmentation. Moreover, single-stage designs further limit performance in other\ncomputer vision tasks and fully connected layers bear heavy computation. To\ntackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for\nvisual recognition, which is a light-weight, stage-wise, co-design of\nconvolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1\naccuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of\nMLP-Mixer-B/16, respectively). Experiments on object detection and semantic\nsegmentation further show that visual representation learned by ConvMLP can be\nseamlessly transferred and achieve competitive results with fewer parameters.\nOur code and pre-trained models are publicly available at\nhttps://github.com/SHI-Labs/Convolutional-MLPs.",
          "link": "http://arxiv.org/abs/2109.04454",
          "publishedOn": "2021-09-10T07:20:11.814Z",
          "wordCount": 595,
          "title": "ConvMLP: Hierarchical Convolutional MLPs for Vision. (arXiv:2109.04454v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.10091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behl_A/0/1/0/all/0/1\">Aseem Behl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohn_Bar_E/0/1/0/all/0/1\">Eshed Ohn-Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "It is well known that semantic segmentation can be used as an effective\nintermediate representation for learning driving policies. However, the task of\nstreet scene semantic segmentation requires expensive annotations. Furthermore,\nsegmentation algorithms are often trained irrespective of the actual driving\ntask, using auxiliary image-space loss functions which are not guaranteed to\nmaximize driving metrics such as safety or distance traveled per intervention.\nIn this work, we seek to quantify the impact of reducing segmentation\nannotation costs on learned behavior cloning agents. We analyze several\nsegmentation-based intermediate representations. We use these visual\nabstractions to systematically study the trade-off between annotation\nefficiency and driving performance, i.e., the types of classes labeled, the\nnumber of image samples used to learn the visual abstraction model, and their\ngranularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers\nseveral practical insights into how segmentation-based visual abstractions can\nbe exploited in a more label efficient manner. Surprisingly, we find that\nstate-of-the-art driving performance can be achieved with orders of magnitude\nreduction in annotation cost. Beyond label efficiency, we find several\nadditional training benefits when leveraging visual abstractions, such as a\nsignificant reduction in the variance of the learned policy when compared to\nstate-of-the-art end-to-end driving models.",
          "link": "http://arxiv.org/abs/2005.10091",
          "publishedOn": "2021-09-10T07:20:11.808Z",
          "wordCount": 703,
          "title": "Label Efficient Visual Abstractions for Autonomous Driving. (arXiv:2005.10091v2 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>",
          "description": "An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve\nthe network performance significantly compared to the widely used activations\nlike ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have\n5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in\nCIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on\nPreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.",
          "link": "http://arxiv.org/abs/2109.04386",
          "publishedOn": "2021-09-10T07:20:11.743Z",
          "wordCount": 572,
          "title": "ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Steitz_J/0/1/0/all/0/1\">Jan-Martin O. Steitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_S/0/1/0/all/0/1\">Stefan Roth</a>",
          "description": "Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),\nrequires an alignment of semantic concepts across domains. Despite the\nwidespread success of end-to-end learning, today's multimodal pipelines by and\nlarge leverage pre-extracted, fixed features from object detectors, typically\nFaster R-CNN, as representations of the visual world. The obvious downside is\nthat the visual representation is not specifically tuned to the multimodal task\nat hand. At the same time, while transformer-based object detectors have gained\npopularity, they have not been employed in today's multimodal pipelines. We\naddress both shortcomings with TxT, a transformer-based crossmodal pipeline\nthat enables fine-tuning both language and visual components on the downstream\ntask in a fully end-to-end manner. We overcome existing limitations of\ntransformer-based detectors for multimodal reasoning regarding the integration\nof global context and their scalability. Our transformer-based multimodal model\nachieves considerable gains from end-to-end learning for multimodal question\nanswering.",
          "link": "http://arxiv.org/abs/2109.04422",
          "publishedOn": "2021-09-10T07:20:11.691Z",
          "wordCount": 607,
          "title": "TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kanggeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Won-Ki Jeong</a>",
          "description": "With the advent of advances in self-supervised learning, paired clean-noisy\ndata are no longer required in deep learning-based image denoising. However,\nexisting blind denoising methods still require the assumption with regard to\nnoise characteristics, such as zero-mean noise distribution and pixel-wise\nnoise-signal independence; this hinders wide adaptation of the method in the\nmedical domain. On the other hand, unpaired learning can overcome limitations\nrelated to the assumption on noise characteristics, which makes it more\nfeasible for collecting the training data in real-world scenarios. In this\npaper, we propose a novel image denoising scheme, Interdependent\nSelf-Cooperative Learning (ISCL), that leverages unpaired learning by combining\ncyclic adversarial learning with self-supervised residual learning. Unlike the\nexisting unpaired image denoising methods relying on matching data\ndistributions in different domains, the two architectures in ISCL, designed for\ndifferent tasks, complement each other and boost the learning process. To\nassess the performance of the proposed method, we conducted extensive\nexperiments in various biomedical image degradation scenarios, such as noise\ncaused by physical characteristics of electron microscopy (EM) devices (film\nand charging noise), and structural noise found in low-dose computer tomography\n(CT). We demonstrate that the image quality of our method is superior to\nconventional and current state-of-the-art deep learning-based image denoising\nmethods, including supervised learning.",
          "link": "http://arxiv.org/abs/2102.09858",
          "publishedOn": "2021-09-10T07:20:11.668Z",
          "wordCount": 686,
          "title": "ISCL: Interdependent Self-Cooperative Learning for Unpaired Image Denoising. (arXiv:2102.09858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lingxiao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kecheng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xingyu Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Peng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Existing person re-identification (re-id) methods are stuck when deployed to\na new unseen scenario despite the success in cross-camera person matching.\nRecent efforts have been substantially devoted to domain adaptive person re-id\nwhere extensive unlabeled data in the new scenario are utilized in a\ntransductive learning manner. However, for each scenario, it is required to\nfirst collect enough data and then train such a domain adaptive re-id model,\nthus restricting their practical application. Instead, we aim to explore\nmultiple labeled datasets to learn generalized domain-invariant representations\nfor person re-id, which is expected universally effective for each new-coming\nre-id scenario. To pursue practicability in real-world systems, we collect all\nthe person re-id datasets (20 datasets) in this field and select the three most\nfrequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen\ntarget domains. In addition, we develop DataHunter that collects over 300K+\nweak annotated images named YouTube-Human from YouTube street-view videos,\nwhich joins 17 remaining full labeled datasets to form multiple source domains.\nOn such a large and challenging benchmark called FastHuman (~440K+ labeled\nimages), we further propose a simple yet effective Semi-Supervised Knowledge\nDistillation (SSKD) framework. SSKD effectively exploits the weakly annotated\ndata by assigning soft pseudo labels to YouTube-Human to improve models'\ngeneralization ability. Experiments on several protocols verify the\neffectiveness of the proposed SSKD framework on domain generalizable person\nre-id, which is even comparable to supervised learning on the target domains.\nLastly, but most importantly, we hope the proposed benchmark FastHuman could\nbring the next development of domain generalizable person re-id algorithms.",
          "link": "http://arxiv.org/abs/2108.05045",
          "publishedOn": "2021-09-10T07:20:11.527Z",
          "wordCount": 732,
          "title": "Semi-Supervised Domain Generalizable Person Re-Identification. (arXiv:2108.05045v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanakoyeu_A/0/1/0/all/0/1\">Artsiom Sanakoyeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingchuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschernezki_V/0/1/0/all/0/1\">Vadim Tschernezki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>",
          "description": "Deep metric learning (DML) is a cornerstone of many computer vision\napplications. It aims at learning a mapping from the input domain to an\nembedding space, where semantically similar objects are located nearby and\ndissimilar objects far from another. The target similarity on the training data\nis defined by user in form of ground-truth class labels. However, while the\nembedding space learns to mimic the user-provided similarity on the training\ndata, it should also generalize to novel categories not seen during training.\nBesides user-provided groundtruth training labels, a lot of additional visual\nfactors (such as viewpoint changes or shape peculiarities) exist and imply\ndifferent notions of similarity between objects, affecting the generalization\non the images unseen during training. However, existing approaches usually\ndirectly learn a single embedding space on all available training data,\nstruggling to encode all different types of relationships, and do not\ngeneralize well. We propose to build a more expressive representation by\njointly splitting the embedding space and the data hierarchically into smaller\nsub-parts. We successively focus on smaller subsets of the training data,\nreducing its variance and learning a different embedding subspace for each data\nsubset. Moreover, the subspaces are learned jointly to cover not only the\nintricacies, but the breadth of the data as well. Only after that, we build the\nfinal embedding from the subspaces in the conquering stage. The proposed\nalgorithm acts as a transparent wrapper that can be placed around arbitrary\nexisting DML methods. Our approach significantly improves upon the\nstate-of-the-art on image retrieval, clustering, and re-identification tasks\nevaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop\nClothes, and PKU VehicleID datasets.",
          "link": "http://arxiv.org/abs/2109.04003",
          "publishedOn": "2021-09-10T07:20:11.507Z",
          "wordCount": 722,
          "title": "Improving Deep Metric Learning by Divide and Conquer. (arXiv:2109.04003v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>",
          "description": "Pretrained vision-and-language BERTs aim to learn representations that\ncombine information from both modalities. We propose a diagnostic method based\non cross-modal input ablation to assess the extent to which these models\nactually integrate cross-modal information. This method involves ablating\ninputs from one modality, either entirely or selectively based on cross-modal\ngrounding alignments, and evaluating the model prediction performance on the\nother modality. Model performance is measured by modality-specific tasks that\nmirror the model pretraining objectives (e.g. masked language modelling for\ntext). Models that have learned to construct cross-modal representations using\nboth modalities are expected to perform worse when inputs are missing from a\nmodality. We find that recently proposed models have much greater relative\ndifficulty predicting text when visual information is ablated, compared to\npredicting visual object categories when text is ablated, indicating that these\nmodels are not symmetrically cross-modal.",
          "link": "http://arxiv.org/abs/2109.04448",
          "publishedOn": "2021-09-10T07:20:11.499Z",
          "wordCount": 591,
          "title": "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhipeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>",
          "description": "Vision transformers (ViTs) have demonstrated impressive performance on a\nseries of computer vision tasks, yet they still suffer from adversarial\nexamples. In this paper, we posit that adversarial attacks on transformers\nshould be specially tailored for their architecture, jointly considering both\npatches and self-attention, in order to achieve high transferability. More\nspecifically, we introduce a dual attack framework, which contains a Pay No\nAttention (PNA) attack and a PatchOut attack, to improve the transferability of\nadversarial samples across different ViTs. We show that skipping the gradients\nof attention during backpropagation can generate adversarial examples with high\ntransferability. In addition, adversarial perturbations generated by optimizing\nrandomly sampled subsets of patches at each iteration achieve higher attack\nsuccess rates than attacks using all patches. We evaluate the transferability\nof attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The\nresults of these experiments demonstrate that the proposed dual attack can\ngreatly boost transferability between ViTs and from ViTs to CNNs. In addition,\nthe proposed method can easily be combined with existing transfer methods to\nboost performance.",
          "link": "http://arxiv.org/abs/2109.04176",
          "publishedOn": "2021-09-10T07:20:11.448Z",
          "wordCount": 624,
          "title": "Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>",
          "description": "Facial editing is an important task in vision and graphics with numerous\napplications. However, existing works are incapable to deliver a continuous and\nfine-grained editing mode (e.g., editing a slightly smiling face to a big\nlaughing one) with natural interactions with users. In this work, we propose\nTalk-to-Edit, an interactive facial editing framework that performs\nfine-grained attribute manipulation through dialog between the user and the\nsystem. Our key insight is to model a continual \"semantic field\" in the GAN\nlatent space. 1) Unlike previous works that regard the editing as traversing\nstraight lines in the latent space, here the fine-grained editing is formulated\nas finding a curving trajectory that respects fine-grained attribute landscape\non the semantic field. 2) The curvature at each step is location-specific and\ndetermined by the input image as well as the users' language requests. 3) To\nengage the users in a meaningful dialog, our system generates language feedback\nby considering both the user request and the current state of the semantic\nfield.\n\nWe also contribute CelebA-Dialog, a visual-language facial editing dataset to\nfacilitate large-scale study. Specifically, each image has manually annotated\nfine-grained attribute annotations as well as template-based textual\ndescriptions in natural language. Extensive quantitative and qualitative\nexperiments demonstrate the superiority of our framework in terms of 1) the\nsmoothness of fine-grained editing, 2) the identity/attribute preservation, and\n3) the visual photorealism and dialog fluency. Notably, user study validates\nthat our overall system is consistently favored by around 80% of the\nparticipants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.",
          "link": "http://arxiv.org/abs/2109.04425",
          "publishedOn": "2021-09-10T07:20:11.436Z",
          "wordCount": 711,
          "title": "Talk-to-Edit: Fine-Grained Facial Editing via Dialog. (arXiv:2109.04425v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sheshkus_A/0/1/0/all/0/1\">A. Sheshkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirvonaya_A/0/1/0/all/0/1\">A. Chirvonaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">V.L. Arlazarov</a>",
          "description": "In this paper, we study the problem of feature points description in the\ncontext of document analysis and template matching. Our study shows that the\nspecific training data is required for the task especially if we are to train a\nlightweight neural network that will be usable on devices with limited\ncomputational resources. In this paper, we construct and provide a dataset with\na method of training patches retrieval. We prove the effectiveness of this data\nby training a lightweight neural network and show how it performs in both\ndocuments and general patches matching. The training was done on the provided\ndataset in comparison with HPatches training dataset and for the testing we use\nHPatches testing framework and two publicly available datasets with various\ndocuments pictured on complex backgrounds: MIDV-500 and MIDV-2019.",
          "link": "http://arxiv.org/abs/2109.04134",
          "publishedOn": "2021-09-10T07:20:11.412Z",
          "wordCount": 591,
          "title": "Tiny CNN for feature point description for document analysis: approach and dataset. (arXiv:2109.04134v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>",
          "description": "Deep learning has achieved remarkable progress for visual recognition on\nlarge-scale balanced datasets but still performs poorly on real-world\nlong-tailed data. Previous methods often adopt class re-balanced training\nstrategies to effectively alleviate the imbalance issue, but might be a risk of\nover-fitting tail classes. The recent decoupling method overcomes over-fitting\nissues by using a multi-stage training scheme, yet, it is still incapable of\ncapturing tail class information in the feature learning stage. In this paper,\nwe show that soft label can serve as a powerful solution to incorporate label\ncorrelation into a multi-stage training scheme for long-tailed recognition. The\nintrinsic relation between classes embodied by soft labels turns out to be\nhelpful for long-tailed recognition by transferring knowledge from head to tail\nclasses.\n\nSpecifically, we propose a conceptually simple yet particularly effective\nmulti-stage training scheme, termed as Self Supervised to Distillation (SSD).\nThis scheme is composed of two parts. First, we introduce a self-distillation\nframework for long-tailed recognition, which can mine the label relation\nautomatically. Second, we present a new distillation label generation module\nguided by self-supervision. The distilled labels integrate information from\nboth label and data domains that can model long-tailed distribution\neffectively. We conduct extensive experiments and our method achieves the\nstate-of-the-art results on three long-tailed recognition benchmarks:\nImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong\nLWS baseline by from $2.7\\%$ to $4.5\\%$ on various datasets. The code is\navailable at https://github.com/MCG-NJU/SSD-LT.",
          "link": "http://arxiv.org/abs/2109.04075",
          "publishedOn": "2021-09-10T07:20:11.386Z",
          "wordCount": 685,
          "title": "Self Supervision to Distillation for Long-Tailed Visual Recognition. (arXiv:2109.04075v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dingheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weisheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jianbo Shi</a>",
          "description": "Visual recognition is currently one of the most important and active research\nareas in computer vision, pattern recognition, and even the general field of\nartificial intelligence. It has great fundamental importance and strong\nindustrial needs. Deep neural networks (DNNs) have largely boosted their\nperformances on many concrete tasks, with the help of large amounts of training\ndata and new powerful computation resources. Though recognition accuracy is\nusually the first concern for new progresses, efficiency is actually rather\nimportant and sometimes critical for both academic research and industrial\napplications. Moreover, insightful views on the opportunities and challenges of\nefficiency are also highly required for the entire community. While general\nsurveys on the efficiency issue of DNNs have been done from various\nperspectives, as far as we are aware, scarcely any of them focused on visual\nrecognition systematically, and thus it is unclear which progresses are\napplicable to it and what else should be concerned. In this paper, we present\nthe review of the recent advances with our suggestions on the new possible\ndirections towards improving the efficiency of DNN-related visual recognition\napproaches. We investigate not only from the model but also the data point of\nview (which is not the case in existing surveys), and focus on three most\nstudied data types (images, videos and points). This paper attempts to provide\na systematic summary via a comprehensive survey which can serve as a valuable\nreference and inspire both researchers and practitioners who work on visual\nrecognition problems.",
          "link": "http://arxiv.org/abs/2108.13055",
          "publishedOn": "2021-09-10T07:20:11.356Z",
          "wordCount": 728,
          "title": "Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions. (arXiv:2108.13055v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04374",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Serna_I/0/1/0/all/0/1\">Ignacio Serna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1\">Javier Ortega-Garcia</a>",
          "description": "This paper is the first to explore an automatic way to detect bias in deep\nconvolutional neural networks by simply looking at their weights. Furthermore,\nit is also a step towards understanding neural networks and how they work. We\nshow that it is indeed possible to know if a model is biased or not simply by\nlooking at its weights, without the model inference for an specific input. We\nanalyze how bias is encoded in the weights of deep networks through a toy\nexample using the Colored MNIST database and we also provide a realistic case\nstudy in gender detection from face images using state-of-the-art methods and\nexperimental resources. To do so, we generated two databases with 36K and 48K\nbiased models each. In the MNIST models we were able to detect whether they\npresented a strong or low bias with more than 99% accuracy, and we were also\nable to classify between four levels of bias with more than 70% accuracy. For\nthe face models, we achieved 90% accuracy in distinguishing between models\nbiased towards Asian, Black, or Caucasian ethnicity.",
          "link": "http://arxiv.org/abs/2109.04374",
          "publishedOn": "2021-09-10T07:20:11.335Z",
          "wordCount": 617,
          "title": "IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R.Zaiane</a>",
          "description": "Most recent semantic segmentation methods adopt a U-Net framework with an\nencoder-decoder architecture. It is still challenging for U-Net with a simple\nskip connection scheme to model the global multi-scale context: 1) Not each\nskip connection setting is effective due to the issue of incompatible feature\nsets of encoder and decoder stage, even some skip connection negatively\ninfluence the segmentation performance; 2) The original U-Net is worse than the\none without any skip connection on some datasets. Based on our findings, we\npropose a new segmentation framework, named UCTransNet (with a proposed CTrans\nmodule in U-Net), from the channel perspective with attention mechanism.\nSpecifically, the CTrans module is an alternate of the U-Net skip connections,\nwhich consists of a sub-module to conduct the multi-scale Channel Cross fusion\nwith Transformer (named CCT) and a sub-module Channel-wise Cross-Attention\n(named CCA) to guide the fused multi-scale channel-wise information to\neffectively connect to the decoder features for eliminating the ambiguity.\nHence, the proposed connection consisting of the CCT and CCA is able to replace\nthe original skip connection to solve the semantic gaps for an accurate\nautomatic medical image segmentation. The experimental results suggest that our\nUCTransNet produces more precise segmentation performance and achieves\nconsistent improvements over the state-of-the-art for semantic segmentation\nacross different datasets and conventional architectures involving transformer\nor U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.",
          "link": "http://arxiv.org/abs/2109.04335",
          "publishedOn": "2021-09-10T07:20:11.317Z",
          "wordCount": 689,
          "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mandalapu_H/0/1/0/all/0/1\">Hareesh Mandalapu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_A/0/1/0/all/0/1\">Aravinda Reddy P N</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">K Sreenivasa Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Pabitra Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_S/0/1/0/all/0/1\">S R Mahadeva Prasanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Smartphones have been employed with biometric-based verification systems to\nprovide security in highly sensitive applications. Audio-visual biometrics are\ngetting popular due to the usability and also it will be challenging to spoof\nbecause of multi-modal nature. In this work, we present an audio-visual\nsmartphone dataset captured in five different recent smartphones. This new\ndataset contains 103 subjects captured in three different sessions considering\nthe different real-world scenarios. Three different languages are acquired in\nthis dataset to include the problem of language dependency of the speaker\nrecognition systems. These unique characteristics of this dataset will pave the\nway to implement novel state-of-the-art unimodal or audio-visual speaker\nrecognition systems. We also report the performance of the bench-marked\nbiometric verification systems on our dataset. The robustness of biometric\nalgorithms is evaluated towards multiple dependencies like signal noise,\ndevice, language and presentation attacks like replay and synthesized signals\nwith extensive experiments. The obtained results raised many concerns about the\ngeneralization properties of state-of-the-art biometrics methods in\nsmartphones.",
          "link": "http://arxiv.org/abs/2109.04138",
          "publishedOn": "2021-09-10T07:20:11.297Z",
          "wordCount": 617,
          "title": "Multilingual Audio-Visual Smartphone Dataset And Evaluation. (arXiv:2109.04138v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wentao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Sequential manipulation tasks require a robot to perceive the state of an\nenvironment and plan a sequence of actions leading to a desired goal state,\nwhere the ability to reason about spatial relationships among object entities\nfrom raw sensor inputs is crucial. Prior works relying on explicit state\nestimation or end-to-end learning struggle with novel objects. In this work, we\npropose SORNet (Spatial Object-Centric Representation Network), which extracts\nobject-centric representations from RGB images conditioned on canonical views\nof the objects of interest. We show that the object embeddings learned by\nSORNet generalize zero-shot to unseen object entities on three spatial\nreasoning tasks: spatial relationship classification, skill precondition\nclassification and relative direction regression, significantly outperforming\nbaselines. Further, we present real-world robotic experiments demonstrating the\nusage of the learned object embeddings in task planning for sequential\nmanipulation.",
          "link": "http://arxiv.org/abs/2109.03891",
          "publishedOn": "2021-09-10T07:20:11.263Z",
          "wordCount": 581,
          "title": "SORNet: Spatial Object-Centric Representations for Sequential Manipulation. (arXiv:2109.03891v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paliwal_S/0/1/0/all/0/1\">Shubham Paliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Monika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>",
          "description": "Piping and Instrumentation Diagrams (P&ID) are ubiquitous in several\nmanufacturing, oil and gas enterprises for representing engineering schematics\nand equipment layout. There is an urgent need to extract and digitize\ninformation from P&IDs without the cost of annotating a varying set of symbols\nfor each new use case. A robust one-shot learning approach for symbol\nrecognition i.e., localization followed by classification, would therefore go a\nlong way towards this goal. Our method works by sampling pixels sequentially\nalong the different contour boundaries in the image. These sampled points form\npaths which are used in the prototypical line diagram to construct a graph that\ncaptures the structure of the contours. Subsequently, the prototypical graphs\nare fed into a Dynamic Graph Convolutional Neural Network (DGCNN) which is\ntrained to classify graphs into one of the given symbol classes. Further, we\nappend embeddings from a Resnet-34 network which is trained on symbol images\ncontaining sampled points to make the classification network more robust.\nSince, many symbols in P&ID are structurally very similar to each other, we\nutilize Arcface loss during DGCNN training which helps in maximizing symbol\nclass separability by producing highly discriminative embeddings. The images\nconsist of components attached on the pipeline (straight line). The sampled\npoints segregated around the symbol regions are used for the classification\ntask. The proposed pipeline, named OSSR-PID, is fast and gives outstanding\nperformance for recognition of symbols on a synthetic dataset of 100 P&ID\ndiagrams. We also compare our method against prior-work on a real-world private\ndataset of 12 P&ID sheets and obtain comparable/superior results. Remarkably,\nit is able to achieve such excellent performance using only one prototypical\nexample per symbol.",
          "link": "http://arxiv.org/abs/2109.03849",
          "publishedOn": "2021-09-10T07:20:11.227Z",
          "wordCount": 735,
          "title": "OSSR-PID: One-Shot Symbol Recognition in P&ID Sheets using Path Sampling and GCN. (arXiv:2109.03849v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhaojing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiping Zheng</a>",
          "description": "Deep learning models usually require a large amount of labeled data to\nachieve satisfactory performance. In multimedia analysis, domain adaptation\nstudies the problem of cross-domain knowledge transfer from a label rich source\ndomain to a label scarce target domain, thus potentially alleviates the\nannotation requirement for deep learning models. However, we find that\ncontemporary domain adaptation methods for cross-domain image understanding\nperform poorly when source domain is noisy. Weakly Supervised Domain Adaptation\n(WSDA) studies the domain adaptation problem under the scenario where source\ndata can be noisy. Prior methods on WSDA remove noisy source data and align the\nmarginal distribution across domains without considering the fine-grained\nsemantic structure in the embedding space, which have the problem of class\nmisalignment, e.g., features of cats in the target domain might be mapped near\nfeatures of dogs in the source domain. In this paper, we propose a novel\nmethod, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we\nadopt the cluster assumption and learn cluster discriminatively with class\nprototypes in the embedding space. We propose to leverage the location\ninformation of the data points in the embedding space and model the location\ninformation with a Gaussian mixture model to identify noisy source data. We\nthen design a network which incorporates the Gaussian mixture noise model as a\nsub-module for unsupervised noise removal and propose a novel cluster-level\nadversarial adaptation method which aligns unlabeled target data with the less\nnoisy class prototypes for mapping the semantic structure across domains. We\nconduct extensive experiments to evaluate the effectiveness of our method on\nboth general images and medical images from COVID-19 and e-commerce datasets.\nThe results show that our method significantly outperforms state-of-the-art\nWSDA methods.",
          "link": "http://arxiv.org/abs/2109.04284",
          "publishedOn": "2021-09-10T07:20:11.220Z",
          "wordCount": 793,
          "title": "Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal. (arXiv:2109.04284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mengzhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "While post-training quantization receives popularity mostly due to its\nevasion in accessing the original complete training dataset, its poor\nperformance also stems from this limitation. To alleviate this limitation, in\nthis paper, we leverage the synthetic data introduced by zero-shot quantization\nwith calibration dataset and we propose a fine-grained data distribution\nalignment (FDDA) method to boost the performance of post-training quantization.\nThe method is based on two important properties of batch normalization\nstatistics (BNS) we observed in deep layers of the trained network, i.e.,\ninter-class separation and intra-class incohesion. To preserve this\nfine-grained distribution information: 1) We calculate the per-class BNS of the\ncalibration dataset as the BNS centers of each class and propose a\nBNS-centralized loss to force the synthetic data distributions of different\nclasses to be close to their own centers. 2) We add Gaussian noise into the\ncenters to imitate the incohesion and propose a BNS-distorted loss to force the\nsynthetic data distribution of the same class to be close to the distorted\ncenters. By introducing these two fine-grained losses, our method shows the\nstate-of-the-art performance on ImageNet, especially when the first and last\nlayers are quantized to low-bit as well. Our project is available at\nhttps://github.com/viperit/FDDA.",
          "link": "http://arxiv.org/abs/2109.04186",
          "publishedOn": "2021-09-10T07:20:11.134Z",
          "wordCount": 655,
          "title": "Fine-grained Data Distribution Alignment for Post-Training Quantization. (arXiv:2109.04186v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junha Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>",
          "description": "Point cloud registration is the task of estimating the rigid transformation\nthat aligns a pair of point cloud fragments. We present an efficient and robust\nframework for pairwise registration of real-world 3D scans, leveraging Hough\nvoting in the 6D transformation parameter space. First, deep geometric features\nare extracted from a point cloud pair to compute putative correspondences. We\nthen construct a set of triplets of correspondences to cast votes on the 6D\nHough space, representing the transformation parameters in sparse tensors.\nNext, a fully convolutional refinement module is applied to refine the noisy\nvotes. Finally, we identify the consensus among the correspondences from the\nHough space, which we use to predict our final transformation parameters. Our\nmethod outperforms state-of-the-art methods on 3DMatch and 3DLoMatch benchmarks\nwhile achieving comparable performance on KITTI odometry dataset. We further\ndemonstrate the generalizability of our approach by setting a new\nstate-of-the-art on ICL-NUIM dataset, where we integrate our module into a\nmulti-way registration pipeline.",
          "link": "http://arxiv.org/abs/2109.04310",
          "publishedOn": "2021-09-10T07:20:11.058Z",
          "wordCount": 606,
          "title": "Deep Hough Voting for Robust Global Registration. (arXiv:2109.04310v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Chang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>",
          "description": "This work studies the problem of high-dimensional data (referred to tensors)\ncompletion from partially observed samplings. We consider that a tensor is a\nsuperposition of multiple low-rank components. In particular, each component\ncan be represented as multilinear connections over several latent factors and\nnaturally mapped to a specific tensor network (TN) topology. In this paper, we\npropose a fundamental tensor decomposition (TD) framework: Multi-Tensor Network\nRepresentation (MTNR), which can be regarded as a linear combination of a range\nof TD models, e.g., CANDECOMP/PARAFAC (CP) decomposition, Tensor Train (TT),\nand Tensor Ring (TR). Specifically, MTNR represents a high-order tensor as the\naddition of multiple TN models, and the topology of each TN is automatically\ngenerated instead of manually pre-designed. For the optimization phase, an\nadaptive topology learning (ATL) algorithm is presented to obtain latent\nfactors of each TN based on a rank incremental strategy and a projection error\nmeasurement strategy. In addition, we theoretically establish the fundamental\nmultilinear operations for the tensors with TN representation, and reveal the\nstructural transformation of MTNR to a single TN. Finally, MTNR is applied to a\ntypical task, tensor completion, and two effective algorithms are proposed for\nthe exact recovery of incomplete data based on the Alternating Least Squares\n(ALS) scheme and Alternating Direction Method of Multiplier (ADMM) framework.\nExtensive numerical experiments on synthetic data and real-world datasets\ndemonstrate the effectiveness of MTNR compared with the start-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2109.04022",
          "publishedOn": "2021-09-10T07:20:10.950Z",
          "wordCount": 674,
          "title": "Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_A/0/1/0/all/0/1\">Andreas Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_J/0/1/0/all/0/1\">Joao Filipe Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGinnity_T/0/1/0/all/0/1\">T.M. McGinnity</a>",
          "description": "The detection of moving objects is a trivial task performed by vertebrate\nretinas, yet a complex computer vision task. Object-motion-sensitive ganglion\ncells (OMS-GC) are specialised cells in the retina that sense moving objects.\nOMS-GC take as input continuous signals and produce spike patterns as output,\nthat are transmitted to the Visual Cortex via the optic nerve. The Hybrid\nSensitive Motion Detector (HSMD) algorithm proposed in this work enhances the\nGSOC dynamic background subtraction (DBS) algorithm with a customised 3-layer\nspiking neural network (SNN) that outputs spiking responses akin to the OMS-GC.\nThe algorithm was compared against existing background subtraction (BS)\napproaches, available on the OpenCV library, specifically on the 2012 change\ndetection (CDnet2012) and the 2014 change detection (CDnet2014) benchmark\ndatasets. The results show that the HSMD was ranked overall first among the\ncompeting approaches and has performed better than all the other algorithms on\nfour of the categories across all the eight test metrics. Furthermore, the HSMD\nproposed in this paper is the first to use an SNN to enhance an existing state\nof the art DBS (GSOC) algorithm and the results demonstrate that the SNN\nprovides near real-time performance in realistic applications.",
          "link": "http://arxiv.org/abs/2109.04119",
          "publishedOn": "2021-09-10T07:20:10.933Z",
          "wordCount": 662,
          "title": "HSMD: An object motion detection algorithm using a Hybrid Spiking Neural Network Architecture. (arXiv:2109.04119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sovetkin_E/0/1/0/all/0/1\">Evgenii Sovetkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieters_B/0/1/0/all/0/1\">Bart E. Pieters</a>",
          "description": "This paper discusses an application of the singular spectrum analysis method\n(SSA) in the context of electroluminescence (EL) images of thin-film\nphotovoltaic (PV) modules. We propose an EL image decomposition as a sum of\nthree components: global intensity, cell, and aperiodic components. A\nparametric model of the extracted signal is used to perform several image\nprocessing tasks. The cell component is used to identify interconnection lines\nbetween PV cells at sub-pixel accuracy, as well as to correct incorrect\nstitching of EL images. Furthermore, an explicit expression of the cell\ncomponent signal is used to estimate the inverse characteristic length, a\nphysical parameter related to the resistances in a PV module.",
          "link": "http://arxiv.org/abs/2109.04048",
          "publishedOn": "2021-09-10T07:20:10.906Z",
          "wordCount": 565,
          "title": "Application of the Singular Spectrum Analysis on electroluminescence images of thin-film photovoltaic modules. (arXiv:2109.04048v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Llaneza_D/0/1/0/all/0/1\">Daniel Fernandez-Llaneza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gondova_A/0/1/0/all/0/1\">Andrea Gondova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vince_H/0/1/0/all/0/1\">Harris Vince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zurek_M/0/1/0/all/0/1\">Magdalena Zurek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konings_P/0/1/0/all/0/1\">Peter Konings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kagelid_P/0/1/0/all/0/1\">Patrik Kagelid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hultin_L/0/1/0/all/0/1\">Leif Hultin</a>",
          "description": "Automated segmentation of human cardiac magnetic resonance datasets has been\nsteadily improving during recent years. However, these methods are not directly\napplicable in preclinical context due to limited datasets and lower image\nresolution. Successful application of deep architectures for rat cardiac\nsegmentation, although of critical importance for preclinical evaluation of\ncardiac function, has to our knowledge not yet been reported. We developed\nsegmentation models that expand on the standard U-Net architecture and\nevaluated separate models for systole and diastole phases, 2MSA, and one model\nfor all timepoints, 1MSA. Furthermore, we calibrated model outputs using a\nGaussian Process (GP)-based prior to improve phase selection. Resulting models\napproach human performance in terms of left ventricular segmentation quality\nand ejection fraction (EF) estimation in both 1MSA and 2MSA settings\n(S{\\o}rensen-Dice score 0.91 +/- 0.072 and 0.93 +/- 0.032, respectively). 2MSA\nachieved a mean absolute difference between estimated and reference EF of 3.5\n+/- 2.5 %, while 1MSA resulted in 4.1 +/- 3.0 %. Applying Gaussian Processes to\n1MSA allows to automate the selection of systole and diastole phases. Combined\nwith a novel cardiac phase selection strategy, our work presents an important\nfirst step towards a fully automated segmentation pipeline in the context of\nrat cardiac analysis.",
          "link": "http://arxiv.org/abs/2109.04188",
          "publishedOn": "2021-09-10T07:20:10.840Z",
          "wordCount": 692,
          "title": "Towards Fully Automated Segmentation of Rat Cardiac MRI by Leveraging Deep Learning Frameworks. (arXiv:2109.04188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shengxi Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "In this paper, we propose a model-driven method that reconstructs LoD-2\nbuilding models following a \"decomposition-optimization-fitting\" paradigm. The\nproposed method starts building detection results through a deep learning-based\ndetector and vectorizes individual segments into polygons using a \"three-step\"\npolygon extraction method, followed by a novel grid-based decomposition method\nthat decomposes the complex and irregularly shaped building polygons to tightly\ncombined elementary building rectangles ready to fit elementary building\nmodels. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC)\nlabeling to further refine the orientation of 2D building rectangle. The 3D\nmodeling step takes building-specific parameters such as hip lines, as well as\nnon-rigid and regularized transformations to optimize the flexibility for using\na minimal set of elementary models. Finally, roof type of building models s\nrefined and adjacent building models in one building segment are merged into\nthe complex polygonal model. Our proposed method has addressed a few technical\ncaveats over existing methods, resulting in practically high-quality results,\nbased on our evaluation and comparative study on a diverse set of experimental\ndatasets of cities with different urban patterns.",
          "link": "http://arxiv.org/abs/2109.03876",
          "publishedOn": "2021-09-10T07:20:10.805Z",
          "wordCount": 631,
          "title": "Automated LoD-2 Model Reconstruction from Very-HighResolution Satellite-derived Digital Surface Model and Orthophoto. (arXiv:2109.03876v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1\">Matthew Johnson-Roberson</a>",
          "description": "Robot localization remains a challenging task in GPS denied environments.\nState estimation approaches based on local sensors, e.g. cameras or IMUs, are\ndrifting-prone for long-range missions as error accumulates. In this study, we\naim to address this problem by localizing image observations in a 2D\nmulti-modal geospatial map. We introduce the cross-scale dataset and a\nmethodology to produce additional data from cross-modality sources. We propose\na framework that learns cross-scale visual representations without supervision.\nExperiments are conducted on data from two different domains, underwater and\naerial. In contrast to existing studies in cross-view image geo-localization,\nour approach a) performs better on smaller-scale multi-modal maps; b) is more\ncomputationally efficient for real-time applications; c) can serve directly in\nconcert with state estimation pipelines.",
          "link": "http://arxiv.org/abs/2109.04087",
          "publishedOn": "2021-09-10T07:20:10.737Z",
          "wordCount": 566,
          "title": "Learning Cross-Scale Visual Representations for Real-Time Image Geo-Localization. (arXiv:2109.04087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haozhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Biometric systems are vulnerable to the Presentation Attacks (PA) performed\nusing various Presentation Attack Instruments (PAIs). Even though there are\nnumerous Presentation Attack Detection (PAD) techniques based on both deep\nlearning and hand-crafted features, the generalization of PAD for unknown PAI\nis still a challenging problem. The common problem with existing deep\nlearning-based PAD techniques is that they may struggle with local optima,\nresulting in weak generalization against different PAs. In this work, we\npropose to use self-supervised learning to find a reasonable initialization\nagainst local trap, so as to improve the generalization ability in detecting\nPAs on the biometric system.The proposed method, denoted as IF-OM, is based on\na global-local view coupled with De-Folding and De-Mixing to derive the\ntask-specific representation for PAD.During De-Folding, the proposed technique\nwill learn region-specific features to represent samples in a local pattern by\nexplicitly maximizing cycle consistency. While, De-Mixing drives detectors to\nobtain the instance-specific features with global information for more\ncomprehensive representation by maximizing topological consistency. Extensive\nexperimental results show that the proposed method can achieve significant\nimprovements in terms of both face and fingerprint PAD in more complicated and\nhybrid datasets, when compared with the state-of-the-art methods. Specifically,\nwhen training in CASIA-FASD and Idiap Replay-Attack, the proposed method can\nachieve 18.60% Equal Error Rate (EER) in OULU-NPU and MSU-MFSD, exceeding\nbaseline performance by 9.54%. Code will be made publicly available.",
          "link": "http://arxiv.org/abs/2109.04100",
          "publishedOn": "2021-09-10T07:20:10.695Z",
          "wordCount": 690,
          "title": "Taming Self-Supervised Learning for Presentation Attack Detection: In-Image De-Folding and Out-of-Image De-Mixing. (arXiv:2109.04100v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangpil Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jihyun Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1\">Hyunggun Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sunghee Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_B/0/1/0/all/0/1\">Byoung Soo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramani_K/0/1/0/all/0/1\">Karthik Ramani</a>",
          "description": "We introduce a multi-stage framework that uses mean curvature on a hand\nsurface and focuses on learning interaction between hand and object by\nanalyzing hand grasp type for hand action recognition in egocentric videos. The\nproposed method does not require 3D information of objects including 6D object\nposes which are difficult to annotate for learning an object's behavior while\nit interacts with hands. Instead, the framework synthesizes the mean curvature\nof the hand mesh model to encode the hand surface geometry in 3D space.\nAdditionally, our method learns the hand grasp type which is highly correlated\nwith the hand action. From our experiment, we notice that using hand grasp type\nand mean curvature of hand increases the performance of the hand action\nrecognition.",
          "link": "http://arxiv.org/abs/2109.03783",
          "publishedOn": "2021-09-09T07:20:44.186Z",
          "wordCount": 587,
          "title": "Egocentric View Hand Action Recognition by Leveraging Hand Surface and Hand Grasp Type. (arXiv:2109.03783v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schulze_H/0/1/0/all/0/1\">Henning Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>",
          "description": "Generating images according to natural language descriptions is a challenging\ntask. Prior research has mainly focused to enhance the quality of generation by\ninvestigating the use of spatial attention and/or textual attention thereby\nneglecting the relationship between channels. In this work, we propose the\nCombined Attention Generative Adversarial Network (CAGAN) to generate\nphoto-realistic images according to textual descriptions. The proposed CAGAN\nutilises two attention models: word attention to draw different sub-regions\nconditioned on related words; and squeeze-and-excitation attention to capture\nnon-linear interaction among channels. With spectral normalisation to stabilise\ntraining, our proposed CAGAN improves the state of the art on the IS and FID on\nthe CUB dataset and the FID on the more challenging COCO dataset. Furthermore,\nwe demonstrate that judging a model by a single evaluation metric can be\nmisleading by developing an additional model adding local self-attention which\nscores a higher IS, outperforming the state of the art on the CUB dataset, but\ngenerates unrealistic images through feature repetition.",
          "link": "http://arxiv.org/abs/2104.12663",
          "publishedOn": "2021-09-09T07:20:43.385Z",
          "wordCount": null,
          "title": "CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03812",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stewart_R/0/1/0/all/0/1\">Russell Stewart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dixon_A/0/1/0/all/0/1\">Austin Dixon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_Y/0/1/0/all/0/1\">Yvonne W. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_M/0/1/0/all/0/1\">Michael S. Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>",
          "description": "Improving speed and image quality of Magnetic Resonance Imaging (MRI) via\nnovel reconstruction approaches remains one of the highest impact applications\nfor deep learning in medical imaging. The fastMRI dataset, unique in that it\ncontains large volumes of raw MRI data, has enabled significant advances in\naccelerating MRI using deep learning-based reconstruction methods. While the\nimpact of the fastMRI dataset on the field of medical imaging is unquestioned,\nthe dataset currently lacks clinical expert pathology annotations, critical to\naddressing clinically relevant reconstruction frameworks and exploring\nimportant questions regarding rendering of specific pathology using such novel\napproaches. This work introduces fastMRI+, which consists of 16154\nsubspecialist expert bounding box annotations and 13 study-level labels for 22\ndifferent pathology categories on the fastMRI knee dataset, and 7570\nsubspecialist expert bounding box annotations and 643 study-level labels for 30\ndifferent pathology categories for the fastMRI brain dataset. The fastMRI+\ndataset is open access and aims to support further research and advancement of\nmedical imaging in MRI reconstruction and beyond.",
          "link": "http://arxiv.org/abs/2109.03812",
          "publishedOn": "2021-09-09T07:20:43.377Z",
          "wordCount": null,
          "title": "fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tuan-Anh Nguyen Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat-Thanh Nguyen</a>",
          "description": "Information extraction from document images has received a lot of attention\nrecently, due to the need for digitizing a large volume of unstructured\ndocuments such as invoices, receipts, bank transfers, etc. In this paper, we\npropose a novel deep learning architecture for end-to-end information\nextraction on the 2D character-grid embedding of the document, namely the\n\\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and\nspatial relations between 2D elements, our model leverages a specialized\nmulti-stage encoder-decoders design, in conjunction with efficient uses of the\nself-attention mechanism and the box convolution. Experimental results on\ndifferent datasets show that our model outperforms the baseline U-Net\narchitecture by a large margin while using 40\\% fewer parameters. Moreover, it\nalso significantly improved the baseline in erroneous OCR and limited training\ndata scenario, thus becomes practical for real-world applications.",
          "link": "http://arxiv.org/abs/2106.00952",
          "publishedOn": "2021-09-09T07:20:43.371Z",
          "wordCount": null,
          "title": "End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01986",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhipeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1\">Yong Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>",
          "description": "Dual-energy computed tomography (DECT) has been widely used in many\napplications that need material decomposition. Image-domain methods directly\ndecompose material images from high- and low-energy attenuation images, and\nthus, are susceptible to noise and artifacts on attenuation images. The purpose\nof this study is to develop an improved iterative neural network (INN) for\nhigh-quality image-domain material decomposition in DECT, and to study its\nproperties. We propose a new INN architecture for DECT material decomposition.\nThe proposed INN architecture uses distinct cross-material convolutional neural\nnetwork (CNN) in image refining modules, and uses image decomposition physics\nin image reconstruction modules. The distinct cross-material CNN refiners\nincorporate distinct encoding-decoding filters and cross-material model that\ncaptures correlations between different materials. We study the distinct\ncross-material CNN refiner with patch-based reformulation and tight-frame\ncondition. Numerical experiments with extended cardiactorso (XCAT) phantom and\nclinical data show that the proposed INN significantly improves the image\nquality over several image-domain material decomposition methods, including a\nconventional model-based image decomposition (MBID) method using an\nedge-preserving regularizer, a recent MBID method using pre-learned\nmaterial-wise sparsifying transforms, and a noniterative deep CNN method. Our\nstudy with patch-based reformulations reveals that learned filters of distinct\ncross-material CNN refiners can approximately satisfy the tight-frame\ncondition.",
          "link": "http://arxiv.org/abs/2012.01986",
          "publishedOn": "2021-09-09T07:20:43.370Z",
          "wordCount": null,
          "title": "An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Image translation methods typically aim to manipulate a set of labeled\nattributes (given as supervision at training time e.g. domain label) while\nleaving the unlabeled attributes intact. Current methods achieve either: (i)\ndisentanglement, which exhibits low visual fidelity and can only be satisfied\nwhere the attributes are perfectly uncorrelated. (ii) visually-plausible\ntranslations, which are clearly not disentangled. In this work, we propose\nOverLORD, a single framework for disentangling labeled and unlabeled attributes\nas well as synthesizing high-fidelity images, which is composed of two stages;\n(i) Disentanglement: Learning disentangled representations with latent\noptimization. Differently from previous approaches, we do not rely on\nadversarial training or any architectural biases. (ii) Synthesis: Training\nfeed-forward encoders for inferring the learned attributes and tuning the\ngenerator in an adversarial manner to increase the perceptual quality. When the\nlabeled and unlabeled attributes are correlated, we model an additional\nrepresentation that accounts for the correlated attributes and improves\ndisentanglement. We highlight that our flexible framework covers multiple\nsettings as disentangling labeled attributes, pose and appearance, localized\nconcepts, and shape and texture. We present significantly better\ndisentanglement with higher translation quality and greater output diversity\nthan state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2103.14017",
          "publishedOn": "2021-09-09T07:20:43.369Z",
          "wordCount": null,
          "title": "Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zihao Huang</a>",
          "description": "One-stage object detectors rely on a point feature to predict the detection\nresults. However, the point feature often lacks the information of the whole\nobject, thereby leading to a misalignment between the object and the point\nfeature. Meanwhile, the classification and regression tasks are sensitive to\ndifferent object regions, but their features are spatially aligned. Both of\nthese two problems hinder the detection performance. In order to solve these\ntwo problems, we propose a simple and plug-in operator that can generate\naligned and disentangled features for each task, respectively, without breaking\nthe fully convolutional manner. By predicting two task-aware point sets that\nare located in each sensitive region, the proposed operator can align the point\nfeature with the object and disentangle the two tasks from the spatial\ndimension. We also reveal an interesting finding of the opposite effect of the\nlong-range skip connection for classification and regression. On the basis of\nthe Object-Aligned and Task-disentangled operator (OAT), we propose OAT-Net,\nwhich explicitly exploits point-set features for accurate detection results.\nExtensive experiments on the MS-COCO dataset show that OAT can consistently\nboost different state-of-the-art one-stage detectors by $\\sim$2 AP. Notably,\nOAT-Net with Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.",
          "link": "http://arxiv.org/abs/2108.12176",
          "publishedOn": "2021-09-09T07:20:43.369Z",
          "wordCount": null,
          "title": "Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Albeit current salient object detection (SOD) works have achieved fantastic\nprogress, they are cast into the shade when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both the micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object, while at the macro level,\nthe model needs to discover all salient objects from the given image scene. To\nfacilitate integrity learning for salient object detection, we design a novel\nIntegrity Cognition Network (ICON), which explores three important components\nto learn strong integrity features. 1) Unlike the existing models that focus\nmore on feature discriminability, we introduce a diverse feature aggregation\n(DFA) component to aggregate features with various receptive fields (i.e.,,\nkernel shape and context) and increase the feature diversity. Such diversity is\nthe foundation for mining the integral salient objects. 2) Based on the DFA\nfeatures, we introduce the integrity channel enhancement (ICE) component with\nthe goal of enhancing feature channels that highlight the integral salient\nobjects at the macro level, while suppressing the other distracting ones. 3)\nAfter extracting the enhanced features, the part-whole verification (PWV)\nmethod is employed to determine whether the part and whole object features have\nstrong agreement. Such part-whole agreements can further improve the\nmicro-level integrity for each salient object. To demonstrate the effectiveness\nof ICON, comprehensive experiments are conducted on seven challenging\nbenchmarks, where promising results are achieved.",
          "link": "http://arxiv.org/abs/2101.07663",
          "publishedOn": "2021-09-09T07:20:43.363Z",
          "wordCount": null,
          "title": "Salient Object Detection via Integrity Learning. (arXiv:2101.07663v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09035",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengxu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Senbo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>",
          "description": "Monocular 3D detection currently struggles with extremely lower detection\nrates compared to LiDAR-based methods. The poor accuracy is mainly caused by\nthe absence of accurate location cues due to the ill-posed nature of monocular\nimagery. LiDAR point clouds, which provide precise spatial measurement, can\noffer beneficial information for the training of monocular methods. To make use\nof LiDAR point clouds, prior works project them to form depth map labels,\nsubsequently training a dense depth estimator to extract explicit location\nfeatures. This indirect and complicated way introduces intermediate products,\ni.e., depth map predictions, taking much computation costs as well as leading\nto suboptimal performances. In this paper, we propose LPCG (LiDAR point cloud\nguided monocular 3D object detection), which is a general framework for guiding\nthe training of monocular 3D detectors with LiDAR point clouds. Specifically,\nwe use LiDAR point clouds to generate pseudo labels, allowing monocular 3D\ndetectors to benefit from easy-collected massive unlabeled data. LPCG works\nwell under both supervised and unsupervised setups. Thanks to a general design,\nLPCG can be plugged into any monocular 3D detector, significantly boosting the\nperformance. As a result, we take the first place on KITTI monocular 3D/BEV\n(bird's-eye-view) detection benchmark with a considerable margin. The code will\nbe made publicly available soon.",
          "link": "http://arxiv.org/abs/2104.09035",
          "publishedOn": "2021-09-09T07:20:43.360Z",
          "wordCount": null,
          "title": "Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1\">Milan Sonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural\nnetwork by maximizing the AUC score of the model on a dataset. Most previous\nworks of AUC maximization focus on the perspective of optimization by designing\nefficient stochastic algorithms, and studies on generalization performance of\nlarge-scale DAM on difficult tasks are missing. In this work, we aim to make\nDAM more practical for interesting real-world applications (e.g., medical image\nclassification). First, we propose a new margin-based min-max surrogate loss\nfunction for the AUC score (named as AUC min-max-margin loss or simply AUC\nmargin loss for short). It is more robust than the commonly used AUC square\nloss, while enjoying the same advantage in terms of large-scale stochastic\noptimization. Second, we conduct extensive empirical studies of our DAM method\non four difficult medical image classification tasks, namely (i) classification\nof chest x-ray images for identifying many threatening diseases, (ii)\nclassification of images of skin lesions for identifying melanoma, (iii)\nclassification of mammogram for breast cancer screening, and (iv)\nclassification of microscopic images for identifying tumor tissue. Our studies\ndemonstrate that the proposed DAM method improves the performance of optimizing\ncross-entropy loss by a large margin, and also achieves better performance than\noptimizing the existing AUC square loss on these medical image classification\ntasks. Specifically, our DAM method has achieved the 1st place on Stanford\nCheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is\nthe first work that makes DAM succeed on large-scale medical image datasets. We\nalso conduct extensive ablation studies to demonstrate the advantages of the\nnew AUC margin loss over the AUC square loss on benchmark datasets. The\nproposed method is implemented in our open-sourced library LibAUC\n(www.libauc.org).",
          "link": "http://arxiv.org/abs/2012.03173",
          "publishedOn": "2021-09-09T07:20:43.358Z",
          "wordCount": null,
          "title": "Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-09-09T07:20:43.352Z",
          "wordCount": null,
          "title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02369",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopanas_G/0/1/0/all/0/1\">Georgios Kopanas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philip_J/0/1/0/all/0/1\">Julien Philip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leimkuhler_T/0/1/0/all/0/1\">Thomas Leimk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drettakis_G/0/1/0/all/0/1\">George Drettakis</a>",
          "description": "There has recently been great interest in neural rendering methods. Some\napproaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but\ncannot recover from the errors of this process, while others directly learn a\nvolumetric neural representation, but suffer from expensive training and\ninference. We introduce a general approach that is initialized with MVS, but\nallows further optimization of scene properties in the space of input views,\nincluding depth and reprojected features, resulting in improved novel-view\nsynthesis. A key element of our approach is our new differentiable point-based\npipeline, based on bi-directional Elliptical Weighted Average splatting, a\nprobabilistic depth test and effective camera selection. We use these elements\ntogether in our neural renderer, that outperforms all previous methods both in\nquality and speed in almost all scenes we tested. Our pipeline can be applied\nto multi-view harmonization and stylization in addition to novel-view\nsynthesis.",
          "link": "http://arxiv.org/abs/2109.02369",
          "publishedOn": "2021-09-09T07:20:43.340Z",
          "wordCount": null,
          "title": "Point-Based Neural Rendering with Per-View Optimization. (arXiv:2109.02369v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03778",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_Mengin_M/0/1/0/all/0/1\">Marius Schmidt-Mengin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ricigliano_V/0/1/0/all/0/1\">Vito A.G. Ricigliano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodini_B/0/1/0/all/0/1\">Benedetta Bodini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morena_E/0/1/0/all/0/1\">Emanuele Morena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colombi_A/0/1/0/all/0/1\">Annalisa Colombi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamzaoui_M/0/1/0/all/0/1\">Mariem Hamzaoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panah_A/0/1/0/all/0/1\">Arya Yazdan Panah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stankoff_B/0/1/0/all/0/1\">Bruno Stankoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colliot_O/0/1/0/all/0/1\">Olivier Colliot</a>",
          "description": "Choroid plexuses (CP) are structures of the ventricles of the brain which\nproduce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo\nstudies have pointed towards their role in the inflammatory process in multiple\nsclerosis (MS). Automatic segmentation of CP from MRI thus has high value for\nstudying their characteristics in large cohorts of patients. To the best of our\nknowledge, the only freely available tool for CP segmentation is FreeSurfer but\nits accuracy for this specific structure is poor. In this paper, we propose to\nautomatically segment CP from non-contrast enhanced T1-weighted MRI. To that\nend, we introduce a new model called \"Axial-MLP\" based on an assembly of Axial\nmulti-layer perceptrons (MLPs). This is inspired by recent works which showed\nthat the self-attention layers of Transformers can be replaced with MLPs. This\napproach is systematically compared with a standard 3D U-Net, nnU-Net,\nFreesurfer and FastSurfer. For our experiments, we make use of a dataset of 141\nsubjects (44 controls and 97 patients with MS). We show that all the tested\ndeep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs\n0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is\nslightly less accurate. The conclusions of our paper are two-fold: 1) the\nstudied deep learning methods could be useful tools to study CP in large\ncohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to\nconvolutional neural networks for such tasks, although it could benefit from\nfurther improvements.",
          "link": "http://arxiv.org/abs/2109.03778",
          "publishedOn": "2021-09-09T07:20:43.332Z",
          "wordCount": null,
          "title": "Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dobshik_A/0/1/0/all/0/1\">Anna Dobshik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulupov_A/0/1/0/all/0/1\">Andrey Tulupov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berikov_V/0/1/0/all/0/1\">Vladimir Berikov</a>",
          "description": "This paper presents an automatic algorithm for the segmentation of areas\naffected by an acute stroke on the non-contrast computed tomography brain\nimages. The proposed algorithm is designed for learning in a weakly supervised\nscenario when some images are labeled accurately, and some images are labeled\ninaccurately. Wrong labels appear as a result of inaccuracy made by a\nradiologist in the process of manual annotation of computed tomography images.\nWe propose methods for solving the segmentation problem in the case of\ninaccurately labeled training data. We use the U-Net neural network\narchitecture with several modifications. Experiments on real computed\ntomography scans show that the proposed methods increase the segmentation\naccuracy.",
          "link": "http://arxiv.org/abs/2109.01887",
          "publishedOn": "2021-09-09T07:20:43.329Z",
          "wordCount": null,
          "title": "Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1\">Mustafa Ekrem Erak&#x131;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">U&#x11f;ur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>",
          "description": "Facial appearance variations due to occlusion has been one of the main\nchallenges for face recognition systems. To facilitate further research in this\narea, it is necessary and important to have occluded face datasets collected\nfrom real-world, as synthetically generated occluded faces cannot represent the\nnature of the problem. In this paper, we present the Real World Occluded Faces\n(ROF) dataset, that contains faces with both upper face occlusion, due to\nsunglasses, and lower face occlusion, due to masks. We propose two evaluation\nprotocols for this dataset. Benchmark experiments on the dataset have shown\nthat no matter how powerful the deep face representation models are, their\nperformance degrades significantly when they are tested on real-world occluded\nfaces. It is observed that the performance drop is far less when the models are\ntested on synthetically generated occluded faces. The ROF dataset and the\nassociated evaluation protocols are publicly available at the following link\nhttps://github.com/ekremerakin/RealWorldOccludedFaces.",
          "link": "http://arxiv.org/abs/2109.03672",
          "publishedOn": "2021-09-09T07:20:43.325Z",
          "wordCount": null,
          "title": "On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1\">Medha Wyawahare</a>",
          "description": "One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.",
          "link": "http://arxiv.org/abs/2106.10698",
          "publishedOn": "2021-09-09T07:20:43.314Z",
          "wordCount": null,
          "title": "Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sai Phani Kumar Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_J/0/1/0/all/0/1\">Jayanta Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_C/0/1/0/all/0/1\">Chaker Larabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>",
          "description": "Deep neural networks have shown their profound impact on achieving human\nlevel performance in visual saliency prediction. However, it is still unclear\nhow they learn the task and what it means in terms of understanding human\nvisual system. In this work, we develop a technique to derive explainable\nsaliency models from their corresponding deep neural architecture based\nsaliency models by applying human perception theories and the conventional\nconcepts of saliency. This technique helps us understand the learning pattern\nof the deep network at its intermediate layers through their activation maps.\nInitially, we consider two state-of-the-art deep saliency models, namely UNISAL\nand MSI-Net for our interpretation. We use a set of biologically plausible\nlog-gabor filters for identifying and reconstructing the activation maps of\nthem using our explainable saliency model. The final saliency map is generated\nusing these reconstructed activation maps. We also build our own deep saliency\nmodel named cross-concatenated multi-scale residual block based network\n(CMRNet) for saliency prediction. Then, we evaluate and compare the performance\nof the explainable models derived from UNISAL, MSI-Net and CMRNet on three\nbenchmark datasets with other state-of-the-art methods. Hence, we propose that\nthis approach of explainability can be applied to any deep visual saliency\nmodel for interpretation which makes it a generic one.",
          "link": "http://arxiv.org/abs/2109.03575",
          "publishedOn": "2021-09-09T07:20:43.303Z",
          "wordCount": null,
          "title": "Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00802",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Onishi_Y/0/1/0/all/0/1\">Yuya Onishi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ohba_H/0/1/0/all/0/1\">Hiroyuki Ohba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ota_R/0/1/0/all/0/1\">Ryosuke Ota</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoshikawa_E/0/1/0/all/0/1\">Etsuji Yoshikawa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ouchi_Y/0/1/0/all/0/1\">Yasuomi Ouchi</a>",
          "description": "Although supervised convolutional neural networks (CNNs) often outperform\nconventional alternatives for denoising positron emission tomography (PET)\nimages, they require many low- and high-quality reference PET image pairs.\nHerein, we propose an unsupervised 3D PET image denoising method based on an\nanatomical information-guided attention mechanism. The proposed magnetic\nresonance-guided deep decoder (MR-GDD) utilizes the spatial details and\nsemantic features of MR-guidance image more effectively by introducing\nencoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and\npatterns of the guidance image do not affect the denoised PET image, because\nthe guidance image is input to the network through an attention gate. In a\nMonte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the\nproposed method achieved the highest peak signal-to-noise ratio and structural\nsimilarity (27.92 $\\pm$ 0.44 dB/0.886 $\\pm$ 0.007), as compared with Gaussian\nfiltering (26.68 $\\pm$ 0.10 dB/0.807 $\\pm$ 0.004), image guided filtering\n(27.40 $\\pm$ 0.11 dB/0.849 $\\pm$ 0.003), deep image prior (DIP) (24.22 $\\pm$\n0.43 dB/0.737 $\\pm$ 0.017), and MR-DIP (27.65 $\\pm$ 0.42 dB/0.879 $\\pm$ 0.007).\nFurthermore, we experimentally visualized the behavior of the optimization\nprocess, which is often unknown in unsupervised CNN-based restoration problems.\nFor preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical\n(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates\nstate-of-the-art denoising performance while retaining spatial resolution and\nquantitative accuracy, despite using a common network architecture for various\nnoisy PET images with 1/10th of the full counts. These results suggest that the\nproposed MR-GDD can reduce PET scan times and PET tracer doses considerably\nwithout impacting patients.",
          "link": "http://arxiv.org/abs/2109.00802",
          "publishedOn": "2021-09-09T07:20:43.287Z",
          "wordCount": null,
          "title": "Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03569",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartoccioni_F/0/1/0/all/0/1\">Florent Bartoccioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zablocki_E/0/1/0/all/0/1\">&#xc9;loi Zablocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1\">Patrick P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>",
          "description": "Vision-based depth estimation is a key feature in autonomous systems, which\noften relies on a single camera or several independent ones. In such a\nmonocular setup, dense depth is obtained with either additional input from one\nor several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which\nsuffer from scale-ambiguity and infinite-depth problems. In this paper, we\npropose a new alternative of densely estimating metric depth by combining a\nmonocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of\ntoday's automotive-grade mass-produced laser scanners. Inspired by recent\nself-supervised methods, we introduce a novel framework, called LiDARTouch, to\nestimate dense depth maps from monocular images with the help of ``touches'' of\nLiDAR, i.e., without the need for dense ground-truth depth. In our setup, the\nminimal LiDAR input contributes on three different levels: as an additional\nmodel's input, in a self-supervised LiDAR reconstruction objective function,\nand to estimate changes of pose (a key component of self-supervised depth\nestimation architectures). Our LiDARTouch framework achieves new state of the\nart in self-supervised depth estimation on the KITTI dataset, thus supporting\nour choices of integrating the very sparse LiDAR signal with other visual\nfeatures. Moreover, we show that the use of a few-beam LiDAR alleviates scale\nambiguity and infinite-depth issues that camera-only methods suffer from. We\nalso demonstrate that methods from the fully-supervised depth-completion\nliterature can be adapted to a self-supervised regime with a minimal LiDAR\nsignal.",
          "link": "http://arxiv.org/abs/2109.03569",
          "publishedOn": "2021-09-09T07:20:43.254Z",
          "wordCount": null,
          "title": "LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR. (arXiv:2109.03569v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yali Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wenxiong Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Self-supervised Multi-view stereo (MVS) with a pretext task of image\nreconstruction has achieved significant progress recently. However, previous\nmethods are built upon intuitions, lacking comprehensive explanations about the\neffectiveness of the pretext task in self-supervised MVS. To this end, we\npropose to estimate epistemic uncertainty in self-supervised MVS, accounting\nfor what the model ignores. Specially, the limitations can be categorized into\ntwo types: ambiguious supervision in foreground and invalid supervision in\nbackground. To address these issues, we propose a novel Uncertainty reduction\nMulti-view Stereo (UMVS) framework for self-supervised learning. To alleviate\nambiguous supervision in foreground, we involve extra correspondence prior with\na flow-depth consistency loss. The dense 2D correspondence of optical flows is\nused to regularize the 3D stereo correspondence in MVS. To handle the invalid\nsupervision in background, we use Monte-Carlo Dropout to acquire the\nuncertainty map and further filter the unreliable supervision signals on\ninvalid regions. Extensive experiments on DTU and Tank&Temples benchmark show\nthat our U-MVS framework achieves the best performance among unsupervised MVS\nmethods, with competitive performance with its supervised opponents.",
          "link": "http://arxiv.org/abs/2108.12966",
          "publishedOn": "2021-09-09T07:20:43.248Z",
          "wordCount": null,
          "title": "Digging into Uncertainty in Self-supervised Multi-view Stereo. (arXiv:2108.12966v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dertli_H/0/1/0/all/0/1\">Hasan Dertli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>",
          "description": "Face anti-spoofing is essential to prevent false facial verification by using\na photo, video, mask, or a different substitute for an authorized person's\nface. Most of the state-of-the-art presentation attack detection (PAD) systems\nsuffer from overfitting, where they achieve near-perfect scores on a single\ndataset but fail on a different dataset with more realistic data. This problem\ndrives researchers to develop models that perform well under real-world\nconditions. This is an especially challenging problem for frame-based\npresentation attack detection systems that use convolutional neural networks\n(CNN). To this end, we propose a new PAD approach, which combines pixel-wise\nbinary supervision with patch-based CNN. We believe that training a CNN with\nface patches allows the model to distinguish spoofs without learning background\nor dataset-specific traces. We tested the proposed method both on the standard\nbenchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.\nThe proposed approach shows its superiority on challenging experimental setups.\nNamely, it achieves higher performance on OULU-NPU protocol 3, 4 and on\ninter-dataset real-world experiments.",
          "link": "http://arxiv.org/abs/2109.03484",
          "publishedOn": "2021-09-09T07:20:43.245Z",
          "wordCount": null,
          "title": "Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.10042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zili Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guodong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Honghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kuoliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>",
          "description": "3D object detector based on Hough voting achieves great success and derives\nmany follow-up works. Despite constantly refreshing the detection accuracy,\nthese works suffer from handcrafted components used to eliminate redundant\nboxes, and thus are non-end-to-end and time-consuming. In this work, we propose\na suppress-and-refine framework to remove these handcrafted components. To\nfully utilize full-resolution information and achieve real-time speed, it\ndirectly consumes feature points and redundant 3D proposals. Specifically, it\nfirst suppresses noisy 3D feature points and then feeds them to 3D proposals\nfor the following RoI-aware refinement. With the gating mechanism to build fine\nproposal features and the self-attention mechanism to model relationships, our\nmethod can produce high-quality predictions with a small computation budget in\nan end-to-end manner. To this end, we present the first fully end-to-end 3D\ndetector, SRDet, on the basis of VoteNet. It achieves state-of-the-art\nperformance on the challenging ScanNetV2 and SUN RGB-D datasets with the\nfastest speed ever. Our code will be available at\nhttps://github.com/ZJULearning/SRDet.",
          "link": "http://arxiv.org/abs/2103.10042",
          "publishedOn": "2021-09-09T07:20:43.244Z",
          "wordCount": null,
          "title": "Suppress-and-Refine Framework for End-to-End 3D Object Detection. (arXiv:2103.10042v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>",
          "description": "This paper proposes an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis. The construction of large and publicly\navailable datasets containing labeled images of healthy and diseased crop\nplants led to growing interest in computer vision techniques for automatic\nplant disease diagnosis. Although supervised image classifiers based on deep\nlearning can be a powerful tool for plant disease diagnosis, they require a\nhuge amount of labeled data. The data mining technique of anomaly detection\nincludes unsupervised approaches that do not require rare samples for training\nclassifiers. We propose an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis that is based on the reconstructability of\ncolors; a deep encoder-decoder network trained to reconstruct the colors of\n\\textit{healthy} plant images should fail to reconstruct colors of symptomatic\nregions. Our proposed method includes a new image-based framework for plant\ndisease detection that utilizes a conditional adversarial network called\npix2pix and a new anomaly score based on CIEDE2000 color difference.\nExperiments with PlantVillage dataset demonstrated the superiority of our\nproposed method compared to an existing anomaly detector at identifying\ndiseased crop images in terms of accuracy, interpretability and computational\nefficiency.",
          "link": "http://arxiv.org/abs/2011.14306",
          "publishedOn": "2021-09-09T07:20:43.242Z",
          "wordCount": null,
          "title": "Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anupam K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1\">Nathan F. Lepora</a>",
          "description": "Robotic touch, particularly when using soft optical tactile sensors, suffers\nfrom distortion caused by motion-dependent shear. The manner in which the\nsensor contacts a stimulus is entangled with the tactile information about the\ngeometry of the stimulus. In this work, we propose a supervised convolutional\ndeep neural network model that learns to disentangle, in the latent space, the\ncomponents of sensor deformations caused by contact geometry from those due to\nsliding-induced shear. The approach is validated by reconstructing unsheared\ntactile images from sheared images and showing they match unsheared tactile\nimages collected with no sliding motion. In addition, the unsheared tactile\nimages give a faithful reconstruction of the contact geometry that is not\npossible from the sheared data, and robust estimation of the contact pose that\ncan be used for servo control sliding around various 2D shapes. Finally, the\ncontact geometry reconstruction in conjunction with servo control sliding were\nused for faithful full object reconstruction of various 2D shapes. The methods\nhave broad applicability to deep learning models for robots with a\nshear-sensitive sense of touch.",
          "link": "http://arxiv.org/abs/2109.03615",
          "publishedOn": "2021-09-09T07:20:43.239Z",
          "wordCount": null,
          "title": "Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03426",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xugong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Youhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dayan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhihong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongbin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Due to the large success in object detection and instance segmentation, Mask\nR-CNN attracts great attention and is widely adopted as a strong baseline for\narbitrary-shaped scene text detection and spotting. However, two issues remain\nto be settled. The first is dense text case, which is easy to be neglected but\nquite practical. There may exist multiple instances in one proposal, which\nmakes it difficult for the mask head to distinguish different instances and\ndegrades the performance. In this work, we argue that the performance\ndegradation results from the learning confusion issue in the mask head. We\npropose to use an MLP decoder instead of the \"deconv-conv\" decoder in the mask\nhead, which alleviates the issue and promotes robustness significantly. And we\npropose instance-aware mask learning in which the mask head learns to predict\nthe shape of the whole instance rather than classify each pixel to text or\nnon-text. With instance-aware mask learning, the mask branch can learn\nseparated and compact masks. The second is that due to large variations in\nscale and aspect ratio, RPN needs complicated anchor settings, making it hard\nto maintain and transfer across different datasets. To settle this issue, we\npropose an adaptive label assignment in which all instances especially those\nwith extreme aspect ratios are guaranteed to be associated with enough anchors.\nEquipped with these components, the proposed method named MAYOR achieves\nstate-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500,\nICDAR2015, CTW1500, and Total-Text.",
          "link": "http://arxiv.org/abs/2109.03426",
          "publishedOn": "2021-09-09T07:20:43.237Z",
          "wordCount": null,
          "title": "Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection. (arXiv:2109.03426v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuai Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Congzhentao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Richard YiDa Xu</a>",
          "description": "Clothing changes and lack of data labels are both crucial challenges in\nperson ReID. For the former challenge, people may occur multiple times at\ndifferent locations wearing different clothing. However, most of the current\nperson ReID research works focus on the benchmarks in which a person's clothing\nis kept the same all the time. For the last challenge, some researchers try to\nmake model learn information from a labeled dataset as a source to an unlabeled\ndataset. Whereas purely unsupervised training is less used. In this paper, we\naim to solve both problems at the same time. We design a novel unsupervised\nmodel, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person\nReID problem. We developer a purely unsupervised clothing change person ReID\npipeline with person sync augmentation operation and same person feature\nrestriction. The person sync augmentation is to supply additional same person\nresources. These same person's resources can be used as part supervised input\nby same person feature restriction. The extensive experiments on clothing\nchange ReID datasets show the out-performance of our methods.",
          "link": "http://arxiv.org/abs/2109.03702",
          "publishedOn": "2021-09-09T07:20:43.236Z",
          "wordCount": null,
          "title": "Unsupervised clothing change adaptive person ReID. (arXiv:2109.03702v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Paliwal_S/0/1/0/all/0/1\">Shubham Paliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arushi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Monika Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vig_L/0/1/0/all/0/1\">Lovekesh Vig</a>",
          "description": "Digitization of scanned Piping and Instrumentation diagrams(P&ID), widely\nused in manufacturing or mechanical industries such as oil and gas over several\ndecades, has become a critical bottleneck in dynamic inventory management and\ncreation of smart P&IDs that are compatible with the latest CAD tools.\nHistorically, P&ID sheets have been manually generated at the design stage,\nbefore being scanned and stored as PDFs. Current digitization initiatives\ninvolve manual processing and are consequently very time consuming, labour\nintensive and error-prone.Thanks to advances in image processing, machine and\ndeep learning techniques there are emerging works on P&ID digitization.\nHowever, existing solutions face several challenges owing to the variation in\nthe scale, size and noise in the P&IDs, sheer complexity and crowdedness within\ndrawings, domain knowledge required to interpret the drawings. This motivates\nour current solution called Digitize-PID which comprises of an end-to-end\npipeline for detection of core components from P&IDs like pipes, symbols and\ntextual information, followed by their association with each other and\neventually, the validation and correction of output data based on inherent\ndomain knowledge. A novel and efficient kernel-based line detection and a\ntwo-step method for detection of complex symbols based on a fine-grained deep\nrecognition technique is presented in the paper. In addition, we have created\nan annotated synthetic dataset, Dataset-P&ID, of 500 P&IDs by incorporating\ndifferent types of noise and complex symbols which is made available for public\nuse (currently there exists no public P&ID dataset). We evaluate our proposed\nmethod on this synthetic dataset and a real-world anonymized private dataset of\n12 P&ID sheets. Results show that Digitize-PID outperforms the existing\nstate-of-the-art for P&ID digitization.",
          "link": "http://arxiv.org/abs/2109.03794",
          "publishedOn": "2021-09-09T07:20:43.232Z",
          "wordCount": null,
          "title": "Digitize-PID: Automatic Digitization of Piping and Instrumentation Diagrams. (arXiv:2109.03794v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meel_V/0/1/0/all/0/1\">Vidushi Meel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodepudi_A/0/1/0/all/0/1\">Asritha Bodepudi</a>",
          "description": "Malignant melanoma is a common skin cancer that is mostly curable before\nmetastasis, where melanoma growths spawn in organs away from the original site.\nMelanoma is the most dangerous type of skin cancer if left untreated due to the\nhigh chance of metastasis. This paper presents Melatect, a machine learning\nmodel that identifies potential malignant melanoma. A recursive computer image\nanalysis algorithm was used to create a machine learning model which is capable\nof detecting likely melanoma. The comparison is performed using 20,000 raw\nimages of benign and malignant lesions from the International Skin Imaging\nCollaboration (ISIC) archive that were augmented to 60,000 images. Tests of the\nalgorithm using subsets of the ISIC images suggest it accurately classifies\nlesions as malignant or benign over 95% of the time with no apparent bias or\noverfitting. The Melatect iOS app was later created (unpublished), in which the\nmachine learning model was embedded. With the app, users have the ability to\ntake pictures of skin lesions (moles) using the app, which are then processed\nthrough the machine learning model, and users are notified whether their lesion\ncould be abnormal or not. Melatect provides a convenient way to get free advice\non lesions and track these lesions over time.",
          "link": "http://arxiv.org/abs/2109.03310",
          "publishedOn": "2021-09-09T07:20:43.231Z",
          "wordCount": null,
          "title": "Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13971",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ciga_O/0/1/0/all/0/1\">Ozan Ciga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1\">Tony Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>",
          "description": "Unsupervised learning has been a long-standing goal of machine learning and\nis especially important for medical image analysis, where the learning can\ncompensate for the scarcity of labeled datasets. A promising subclass of\nunsupervised learning is self-supervised learning, which aims to learn salient\nfeatures using the raw input as the learning signal. In this paper, we use a\ncontrastive self-supervised learning method called SimCLR that achieved\nstate-of-the-art results on natural-scene images and apply this method to\ndigital histopathology by collecting and pretraining on 57 histopathology\ndatasets without any labels. We find that combining multiple multi-organ\ndatasets with different types of staining and resolution properties improves\nthe quality of the learned features. Furthermore, we find using more images for\npretraining leads to a better performance in multiple downstream tasks. Linear\nclassifiers trained on top of the learned features show that networks\npretrained on digital histopathology datasets perform better than ImageNet\npretrained networks, boosting task performances by more than 28% in F1 scores\non average. These findings may also be useful when applying newer contrastive\ntechniques to histopathology data. Pretrained PyTorch models are made publicly\navailable at https://github.com/ozanciga/self-supervised-histopathology.",
          "link": "http://arxiv.org/abs/2011.13971",
          "publishedOn": "2021-09-09T07:20:43.221Z",
          "wordCount": null,
          "title": "Self supervised contrastive learning for digital histopathology. (arXiv:2011.13971v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiano_L/0/1/0/all/0/1\">Luca Maiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amerini_I/0/1/0/all/0/1\">Irene Amerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celsi_L/0/1/0/all/0/1\">Lorenzo Ricciardi Celsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anagnostopoulos_A/0/1/0/all/0/1\">Aris Anagnostopoulos</a>",
          "description": "Videos have become a powerful tool for spreading illegal content such as\nmilitary propaganda, revenge porn, or bullying through social networks. To\ncounter these illegal activities, it has become essential to try new methods to\nverify the origin of videos from these platforms. However, collecting datasets\nlarge enough to train neural networks for this task has become difficult\nbecause of the privacy regulations that have been enacted in recent years. To\nmitigate this limitation, in this work we propose two different solutions based\non transfer learning and multitask learning to determine whether a video has\nbeen uploaded from or downloaded to a specific social platform through the use\nof shared features with images trained on the same task. By transferring\nfeatures from the shallowest to the deepest levels of the network from the\nimage task to videos, we measure the amount of information shared between these\ntwo tasks. Then, we introduce a model based on multitask learning, which learns\nfrom both tasks simultaneously. The promising experimental results show, in\nparticular, the effectiveness of the multitask approach. According to our\nknowledge, this is the first work that addresses the problem of social media\nplatform identification of videos through the use of shared features.",
          "link": "http://arxiv.org/abs/2109.03598",
          "publishedOn": "2021-09-09T07:20:43.210Z",
          "wordCount": null,
          "title": "Identification of Social-Media Platform of Videos through the Use of Shared Features. (arXiv:2109.03598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casey_E/0/1/0/all/0/1\">Evan Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_V/0/1/0/all/0/1\">V&#xed;ctor P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teitelman_H/0/1/0/all/0/1\">Harry Teitelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyajian_N/0/1/0/all/0/1\">Nick Boyajian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulver_T/0/1/0/all/0/1\">Tim Pulver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manh_M/0/1/0/all/0/1\">Mike Manh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grisaitis_W/0/1/0/all/0/1\">William Grisaitis</a>",
          "description": "Visual correspondence is a fundamental building block on the way to building\nassistive tools for hand-drawn animation. However, while a large body of work\nhas focused on learning visual correspondences at the pixel-level, few\napproaches have emerged to learn correspondence at the level of line enclosures\n(segments) that naturally occur in hand-drawn animation. Exploiting this\nstructure in animation has numerous benefits: it avoids the intractable memory\ncomplexity of attending to individual pixels in high resolution images and\nenables the use of real-world animation datasets that contain correspondence\ninformation at the level of per-segment colors. To that end, we propose the\nAnimation Transformer (AnT) which uses a transformer-based architecture to\nlearn the spatial and visual relationships between segments across a sequence\nof images. AnT enables practical ML-assisted colorization for professional\nanimation workflows and is publicly accessible as a creative tool in Cadmium.",
          "link": "http://arxiv.org/abs/2109.02614",
          "publishedOn": "2021-09-09T07:20:43.200Z",
          "wordCount": null,
          "title": "The Animation Transformer: Visual Correspondence via Segment Matching. (arXiv:2109.02614v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03478",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_G/0/1/0/all/0/1\">Geng-Xin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_Z/0/1/0/all/0/1\">Zhongxiang Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_F/0/1/0/all/0/1\">Feng Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_M/0/1/0/all/0/1\">Man Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yaozong Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>",
          "description": "Early and accurate severity assessment of Coronavirus disease 2019 (COVID-19)\nbased on computed tomography (CT) images offers a great help to the estimation\nof intensive care unit event and the clinical decision of treatment planning.\nTo augment the labeled data and improve the generalization ability of the\nclassification model, it is necessary to aggregate data from multiple sites.\nThis task faces several challenges including class imbalance between mild and\nsevere infections, domain distribution discrepancy between sites, and presence\nof heterogeneous features. In this paper, we propose a novel domain adaptation\n(DA) method with two components to address these problems. The first component\nis a stochastic class-balanced boosting sampling strategy that overcomes the\nimbalanced learning problem and improves the classification performance on\npoorly-predicted classes. The second component is a representation learning\nthat guarantees three properties: 1) domain-transferability by prototype\ntriplet loss, 2) discriminant by conditional maximum mean discrepancy loss, and\n3) completeness by multi-view reconstruction loss. Particularly, we propose a\ndomain translator and align the heterogeneous data to the estimated class\nprototypes (i.e., class centers) in a hyper-sphere manifold. Experiments on\ncross-site severity assessment of COVID-19 from CT images show that the\nproposed method can effectively tackle the imbalanced learning problem and\noutperform recent DA approaches.",
          "link": "http://arxiv.org/abs/2109.03478",
          "publishedOn": "2021-09-09T07:20:43.194Z",
          "wordCount": null,
          "title": "Cross-Site Severity Assessment of COVID-19 from CT Images via Domain Adaptation. (arXiv:2109.03478v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a method of learning Local-GlObal Contextual Adaptation\nfor fully end-to-end and fast bottom-up human Pose estimation, dubbed as\nLOGO-CAP. It is built on the conceptually simple center-offset formulation that\nlacks inaccuracy for pose estimation. When revisiting the bottom-up human pose\nestimation with the thought of \"thinking, fast and slow\" by D. Kahneman, we\nintroduce a \"slow keypointer\" to remedy the lack of sufficient accuracy of the\n\"fast keypointer\". In learning the \"slow keypointer\", the proposed LOGO-CAP\nlifts the initial \"fast\" keypoints by offset predictions to keypoint expansion\nmaps (KEMs) to counter their uncertainty in two modules. Firstly, the local\nKEMs (e.g., 11x11) are extracted from a low-dimensional feature map. A proposed\nconvolutional message passing module learns to \"re-focus\" the local KEMs to the\nkeypoint attraction maps (KAMs) by accounting for the structured output\nprediction nature of human pose estimation, which is directly supervised by the\nobject keypoint similarity (OKS) loss in training. Secondly, the global KEMs\nare extracted, with a sufficiently large region-of-interest (e.g., 97x97), from\nthe keypoint heatmaps that are computed by a direct map-to-map regression.\nThen, a local-global contextual adaptation module is proposed to convolve the\nglobal KEMs using the learned KAMs as the kernels. This convolution can be\nunderstood as the learnable offsets guided deformable and dynamic convolution\nin a pose-sensitive way. The proposed method is end-to-end trainable with near\nreal-time inference speed, obtaining state-of-the-art performance on the COCO\nkeypoint benchmark for bottom-up human pose estimation. With the COCO trained\nmodel, our LOGO-CAP also outperforms prior arts by a large margin on the\nchallenging OCHuman dataset.",
          "link": "http://arxiv.org/abs/2109.03622",
          "publishedOn": "2021-09-09T07:20:43.168Z",
          "wordCount": null,
          "title": "Learning Local-Global Contextual Adaptation for Fully End-to-End Bottom-Up Human Pose Estimation. (arXiv:2109.03622v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chang-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chia-Yi Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chia-Mu Yu</a>",
          "description": "Deep neural networks have developed rapidly and have achieved outstanding\nperformance in several tasks, such as image classification and natural language\nprocessing. However, recent studies have indicated that both digital and\nphysical adversarial examples can fool neural networks. Face-recognition\nsystems are used in various applications that involve security threats from\nphysical adversarial examples. Herein, we propose a physical adversarial attack\nwith the use of full-face makeup. The presence of makeup on the human face is a\nreasonable possibility, which possibly increases the imperceptibility of\nattacks. In our attack framework, we combine the cycle-adversarial generative\nnetwork (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to\ngenerate adversarial makeup, and the architecture of the victimized classifier\nis VGG 16. Our experimental results show that our attack can effectively\novercome manual errors in makeup application, such as color and\nposition-related errors. We also demonstrate that the approaches used to train\nthe models can influence physical attacks; the adversarial perturbations\ncrafted from the pre-trained model are affected by the corresponding training\ndata.",
          "link": "http://arxiv.org/abs/2109.03329",
          "publishedOn": "2021-09-09T07:20:43.158Z",
          "wordCount": null,
          "title": "Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2101.01169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.",
          "link": "http://arxiv.org/abs/2101.01169",
          "publishedOn": "2021-09-09T07:20:43.158Z",
          "wordCount": null,
          "title": "Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03793",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Karnes_M/0/1/0/all/0/1\">Michael Karnes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1\">Shehan Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adhikari_S/0/1/0/all/0/1\">Srikar Adhikari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Alper Yilmaz</a>",
          "description": "This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19\ndiagnostic system. The adaptive visual diagnostics utilize few-shot learning\n(FSL) to generate encoded disease state models that are stored and classified\nusing a dictionary of knowns. The novel vocabulary based feature processing of\nthe pipeline adapts the knowledge of a pretrained deep neural network to\ncompress the ultrasound images into discrimative descriptions. The\ncomputational efficiency of the FSL approach enables high diagnostic deep\nlearning performance in PoC settings, where training data is limited and the\nannotation process is not strictly controlled. The algorithm performance is\nevaluated on the open source COVID-19 POCUS Dataset to validate the system's\nability to distinguish COVID-19, pneumonia, and healthy disease states. The\nresults of the empirical analyses demonstrate the appropriate efficiency and\naccuracy for scalable PoC use. The code for this work will be made publicly\navailable on GitHub upon acceptance.",
          "link": "http://arxiv.org/abs/2109.03793",
          "publishedOn": "2021-09-09T07:20:43.154Z",
          "wordCount": null,
          "title": "Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System. (arXiv:2109.03793v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "We propose the first general and scalable framework to design certifiable\nalgorithms for robust geometric perception in the presence of outliers. Our\nfirst contribution is to show that estimation using common robust costs, such\nas truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey's\nbiweight, among others, can be reformulated as polynomial optimization problems\n(POPs). By focusing on the TLS cost, our second contribution is to exploit\nsparsity in the POP and propose a sparse semidefinite programming (SDP)\nrelaxation that is much smaller than the standard Lasserre's hierarchy while\npreserving exactness, i.e., the SDP recovers the optimizer of the nonconvex POP\nwith an optimality certificate. Our third contribution is to solve the SDP\nrelaxations at an unprecedented scale and accuracy by presenting STRIDE, a\nsolver that blends global descent on the convex SDP with fast local search on\nthe nonconvex POP. Our fourth contribution is an evaluation of the proposed\nframework on six geometric perception problems including single and multiple\nrotation averaging, point cloud and mesh registration, absolute pose\nestimation, and category-level object pose and shape estimation. Our\nexperiments demonstrate that (i) our sparse SDP relaxation is exact with up to\n60%-90% outliers across applications; (ii) while still being far from\nreal-time, STRIDE is up to 100 times faster than existing SDP solvers on\nmedium-scale problems, and is the only solver that can solve large-scale SDPs\nwith hundreds of thousands of constraints to high accuracy; (iii) STRIDE\nprovides a safeguard to existing fast heuristics for robust estimation (e.g.,\nRANSAC or Graduated Non-Convexity), i.e., it certifies global optimality if the\nheuristic estimates are optimal, or detects and allows escaping local optima\nwhen the heuristic estimates are suboptimal.",
          "link": "http://arxiv.org/abs/2109.03349",
          "publishedOn": "2021-09-09T07:20:43.152Z",
          "wordCount": null,
          "title": "Certifiable Outlier-Robust Geometric Perception: Exact Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03444",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1\">Jaeha Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Husan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>",
          "description": "Most image super-resolution (SR) methods are developed on synthetic\nlow-resolution (LR) and high-resolution (HR) image pairs that are constructed\nby a predetermined operation, e.g., bicubic downsampling. As existing methods\ntypically learn an inverse mapping of the specific function, they produce\nblurry results when applied to real-world images whose exact formulation is\ndifferent and unknown. Therefore, several methods attempt to synthesize much\nmore diverse LR samples or learn a realistic downsampling model. However, due\nto restrictive assumptions on the downsampling process, they are still biased\nand less generalizable. This study proposes a novel method to simulate an\nunknown downsampling process without imposing restrictive prior knowledge. We\npropose a generalizable low-frequency loss (LFL) in the adversarial training\nframework to imitate the distribution of target LR images without using any\npaired examples. Furthermore, we design an adaptive data loss (ADL) for the\ndownsampler, which can be adaptively learned and updated from the data during\nthe training loops. Extensive experiments validate that our downsampling model\ncan facilitate existing SR methods to perform more accurate reconstructions on\nvarious synthetic and real-world examples than the conventional approaches.",
          "link": "http://arxiv.org/abs/2109.03444",
          "publishedOn": "2021-09-09T07:20:43.151Z",
          "wordCount": null,
          "title": "Toward Real-World Super-Resolution via Adaptive Downsampling Models. (arXiv:2109.03444v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03237",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Guan_Y/0/1/0/all/0/1\">Yu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zongjiang Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_D/0/1/0/all/0/1\">Dong Liang</a>",
          "description": "Purpose: Although recent deep energy-based generative models (EBMs) have\nshown encouraging results in many image generation tasks, how to take advantage\nof the self-adversarial cogitation in deep EBMs to boost the performance of\nMagnetic Resonance Imaging (MRI) reconstruction is still desired.\n\nMethods: With the successful application of deep learning in a wide range of\nMRI reconstruction, a line of emerging research involves formulating an\noptimization-based reconstruction method in the space of a generative model.\nLeveraging this, a novel regularization strategy is introduced in this article\nwhich takes advantage of self-adversarial cogitation of the deep energy-based\nmodel. More precisely, we advocate for alternative learning a more powerful\nenergy-based model with maximum likelihood estimation to obtain the deep\nenergy-based information, represented as image prior. Simultaneously, implicit\ninference with Langevin dynamics is a unique property of re-construction. In\ncontrast to other generative models for reconstruction, the proposed method\nutilizes deep energy-based information as the image prior in reconstruction to\nimprove the quality of image.\n\nResults: Experiment results that imply the proposed technique can obtain\nremarkable performance in terms of high reconstruction accuracy that is\ncompetitive with state-of-the-art methods, and does not suffer from mode\ncollapse.\n\nConclusion: Algorithmically, an iterative approach was presented to\nstrengthen EBM training with the gradient of energy network. The robustness and\nthe reproducibility of the algorithm were also experimentally validated. More\nimportantly, the proposed reconstruction framework can be generalized for most\nMRI reconstruction scenarios.",
          "link": "http://arxiv.org/abs/2109.03237",
          "publishedOn": "2021-09-09T07:20:43.118Z",
          "wordCount": null,
          "title": "MRI Reconstruction Using Deep Energy-Based Model. (arXiv:2109.03237v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03398",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1\">S&#xe9;bastien Marcel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>",
          "description": "Face authentication is now widely used, especially on mobile devices, rather\nthan authentication using a personal identification number or an unlock\npattern, due to its convenience. It has thus become a tempting target for\nattackers using a presentation attack. Traditional presentation attacks use\nfacial images or videos of the victim. Previous work has proven the existence\nof master faces, i.e., faces that match multiple enrolled templates in face\nrecognition systems, and their existence extends the ability of presentation\nattacks. In this paper, we perform an extensive study on latent variable\nevolution (LVE), a method commonly used to generate master faces. We run an LVE\nalgorithm for various scenarios and with more than one database and/or face\nrecognition system to study the properties of the master faces and to\nunderstand in which conditions strong master faces could be generated.\nMoreover, through analysis, we hypothesize that master faces come from some\ndense areas in the embedding spaces of the face recognition systems. Last but\nnot least, simulated presentation attacks using generated master faces\ngenerally preserve the false-matching ability of their original digital forms,\nthus demonstrating that the existence of master faces poses an actual threat.",
          "link": "http://arxiv.org/abs/2109.03398",
          "publishedOn": "2021-09-09T07:20:43.118Z",
          "wordCount": null,
          "title": "Master Face Attacks on Face Recognition Systems. (arXiv:2109.03398v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Man Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zeyu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xueyang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Aiping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>",
          "description": "Deep learning provides a new avenue for image restoration, which demands a\ndelicate balance between fine-grained details and high-level contextualized\ninformation during recovering the latent clear image. In practice, however,\nexisting methods empirically construct encapsulated end-to-end mapping networks\nwithout deepening into the rationality, and neglect the intrinsic prior\nknowledge of restoration task. To solve the above problems, inspired by\nTaylor's Approximations, we unfold Taylor's Formula to construct a novel\nframework for image restoration. We find the main part and the derivative part\nof Taylor's Approximations take the same effect as the two competing goals of\nhigh-level contextualized information and spatial details of image restoration\nrespectively. Specifically, our framework consists of two steps,\ncorrespondingly responsible for the mapping and derivative functions. The\nformer first learns the high-level contextualized information and the later\ncombines it with the degraded input to progressively recover local high-order\nspatial details. Our proposed framework is orthogonal to existing methods and\nthus can be easily integrated with them for further improvement, and extensive\nexperiments demonstrate the effectiveness and scalability of our proposed\nframework.",
          "link": "http://arxiv.org/abs/2109.03442",
          "publishedOn": "2021-09-09T07:20:43.111Z",
          "wordCount": null,
          "title": "Unfolding Taylor's Approximations for Image Restoration. (arXiv:2109.03442v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03299",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Boyd_J/0/1/0/all/0/1\">Joseph Boyd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liashuha_M/0/1/0/all/0/1\">Mykola Liashuha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deutsch_E/0/1/0/all/0/1\">Eric Deutsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paragios_N/0/1/0/all/0/1\">Nikos Paragios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christodoulidis_S/0/1/0/all/0/1\">Stergios Christodoulidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>",
          "description": "The examination of histopathology images is considered to be the gold\nstandard for the diagnosis and stratification of cancer patients. A key\nchallenge in the analysis of such images is their size, which can run into the\ngigapixels and can require tedious screening by clinicians. With the recent\nadvances in computational medicine, automatic tools have been proposed to\nassist clinicians in their everyday practice. Such tools typically process\nthese large images by slicing them into tiles that can then be encoded and\nutilized for different clinical models. In this study, we propose a novel\ngenerative framework that can learn powerful representations for such tiles by\nlearning to plausibly expand their visual field. In particular, we developed a\nprogressively grown generative model with the objective of visual field\nexpansion. Thus trained, our model learns to generate different tissue types\nwith fine details, while simultaneously learning powerful representations that\ncan be used for different clinical endpoints, all in a self-supervised way. To\nevaluate the performance of our model, we conducted classification experiments\non CAMELYON17 and CRC benchmark datasets, comparing favorably to other\nself-supervised and pre-trained strategies that are commonly used in digital\npathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.",
          "link": "http://arxiv.org/abs/2109.03299",
          "publishedOn": "2021-09-09T07:20:43.102Z",
          "wordCount": null,
          "title": "Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology. (arXiv:2109.03299v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01088",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haisheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>",
          "description": "Efficient spatiotemporal modeling is an important yet challenging problem for\nvideo action recognition. Existing state-of-the-art methods exploit motion\nclues to assist in short-term temporal modeling through temporal difference\nover consecutive frames. However, insignificant noises will be inevitably\nintroduced due to the camera movement. Besides, movements of different actions\ncan vary greatly. In this paper, we propose a Temporal Saliency Integration\n(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and\na Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to\nhighlight the motion-sensitive area through local-global motion modeling, where\nthe saliency alignment and pyramidal feature difference are conducted\nsuccessively between neighboring frames to capture motion dynamics with less\nnoises caused by misaligned background. CTI is designed to perform multi-scale\ntemporal modeling through a group of separate 1D convolutions respectively.\nMeanwhile, temporal interactions across different scales are integrated with\nattention mechanism. Through these two modules, long short-term temporal\nrelationships can be encoded efficiently by introducing limited additional\nparameters. Extensive experiments are conducted on several popular benchmarks\n(i.e., Something-Something V1 & V2, Kinetics-400, UCF-101, and HMDB-51), which\ndemonstrate the effectiveness and superiority of our proposed method.",
          "link": "http://arxiv.org/abs/2106.01088",
          "publishedOn": "2021-09-09T07:20:42.935Z",
          "wordCount": null,
          "title": "TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingqian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1\">Marcelo H. Ang Jr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>",
          "description": "The central idea of contrastive learning is to discriminate between different\ninstances and force different views of the same instance to share the same\nrepresentation. To avoid trivial solutions, augmentation plays an important\nrole in generating different views, among which random cropping is shown to be\neffective for the model to learn a strong and generalized representation.\nCommonly used random crop operation keeps the difference between two views\nstatistically consistent along the training process. In this work, we challenge\nthis convention by showing that adaptively controlling the disparity between\ntwo augmented views along the training process enhances the quality of the\nlearnt representation. Specifically, we present a parametric cubic cropping\noperation, ParamCrop, for video contrastive learning, which automatically crops\na 3D cubic from the video by differentiable 3D affine transformations.\nParamCrop is trained simultaneously with the video backbone using an\nadversarial objective and learns an optimal cropping strategy from the data.\nThe visualizations show that the center distance and the IoU between two\naugmented views are adaptively controlled by ParamCrop and the learned change\nin the disparity along the training process is beneficial to learning a strong\nrepresentation. Extensive ablation studies demonstrate the effectiveness of the\nproposed ParamCrop on multiple contrastive learning frameworks and video\nbackbones. With ParamCrop, we improve the state-of-the-art performance on both\nHMDB51 and UCF101 datasets.",
          "link": "http://arxiv.org/abs/2108.10501",
          "publishedOn": "2021-09-09T07:20:42.895Z",
          "wordCount": null,
          "title": "ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning. (arXiv:2108.10501v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.11005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Guan_Y/0/1/0/all/0/1\">Yue Shang-Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>",
          "description": "Online continual learning from data streams in dynamic environments is a\ncritical direction in the computer vision field. However, realistic benchmarks\nand fundamental studies in this line are still missing. To bridge the gap, we\npresent a new online continual object detection benchmark with an egocentric\nvideo dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,\nan ego-centric video stream collected over nine months by a graduate student.\nOAK provides exhaustive bounding box annotations of 80 video snippets (~17.5\nhours) for 105 object categories in outdoor scenes. The emergence of new object\ncategories in our benchmark follows a pattern similar to what a single person\nmight see in their day-to-day life. The dataset also captures the natural\ndistribution shifts as the person travels to different places. These egocentric\nlong-running videos provide a realistic playground for continual learning\nalgorithms, especially in online embodied settings. We also introduce new\nevaluation metrics to evaluate the model performance and catastrophic\nforgetting and provide baseline studies for online continual object detection.\nWe believe this benchmark will pose new exciting challenges for learning from\nnon-stationary data in continual learning. The OAK dataset and the associated\nbenchmark are released at https://oakdata.github.io/.",
          "link": "http://arxiv.org/abs/2108.11005",
          "publishedOn": "2021-09-09T07:20:42.871Z",
          "wordCount": null,
          "title": "Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaowu Chen</a>",
          "description": "Conventional RGB-D salient object detection methods aim to leverage depth as\ncomplementary information to find the salient regions in both modalities.\nHowever, the salient object detection results heavily rely on the quality of\ncaptured depth data which sometimes are unavailable. In this work, we make the\nfirst attempt to solve the RGB-D salient object detection problem with a novel\ndepth-awareness framework. This framework only relies on RGB data in the\ntesting phase, utilizing captured depth data as supervision for representation\nlearning. To construct our framework as well as achieving accurate salient\ndetection results, we propose a Ubiquitous Target Awareness (UTA) network to\nsolve three important challenges in RGB-D SOD task: 1) a depth awareness module\nto excavate depth information and to mine ambiguous regions via adaptive\ndepth-error weights, 2) a spatial-aware cross-modal interaction and a\nchannel-aware cross-level interaction, exploiting the low-level boundary cues\nand amplifying high-level salient channels, and 3) a gated multi-scale\npredictor module to perceive the object saliency in different contextual\nscales. Besides its high performance, our proposed UTA network is depth-free\nfor inference and runs in real-time with 43 FPS. Experimental evidence\ndemonstrates that our proposed network not only surpasses the state-of-the-art\nmethods on five public RGB-D SOD benchmarks by a large margin, but also\nverifies its extensibility on five public RGB SOD benchmarks.",
          "link": "http://arxiv.org/abs/2109.03425",
          "publishedOn": "2021-09-09T07:20:42.833Z",
          "wordCount": null,
          "title": "RGB-D Salient Object Detection with Ubiquitous Target Awareness. (arXiv:2109.03425v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion, zoom, rotation, image cut and Gaussian perturbations\nimproves, while significantly improving the performance on clean images without\nany data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-09-09T07:20:42.820Z",
          "wordCount": null,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Gyujoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1\">Ahmed Abdulkadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erus_G/0/1/0/all/0/1\">Guray Erus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habes_M/0/1/0/all/0/1\">Mohamad Habes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomponio_R/0/1/0/all/0/1\">Raymond Pomponio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_H/0/1/0/all/0/1\">Haochang Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_J/0/1/0/all/0/1\">Jimit Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamourian_E/0/1/0/all/0/1\">Elizabeth Mamourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tanweer Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgel_M/0/1/0/all/0/1\">Murat Bilgel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotiras_A/0/1/0/all/0/1\">Aristeidis Sotiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_D/0/1/0/all/0/1\">Dhivya Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John C. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_D/0/1/0/all/0/1\">Daniel Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albert_M/0/1/0/all/0/1\">Marilyn S. Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_N/0/1/0/all/0/1\">Nick R. Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnick_S/0/1/0/all/0/1\">Susan M. Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_I/0/1/0/all/0/1\">Ilya M. Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a> (from the iSTAGING consortium, for the ADNI)",
          "description": "Neuroimaging biomarkers that distinguish between typical brain aging and\nAlzheimer's disease (AD) are valuable for determining how much each contributes\nto cognitive decline. Machine learning models can derive multi-variate brain\nchange patterns related to the two processes, including the SPARE-AD (Spatial\nPatterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of\nBrain Aging) investigated herein. However, substantial overlap between brain\nregions affected in the two processes confounds measuring them independently.\nWe present a methodology toward disentangling the two. T1-weighted MRI images\nof 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),\nor cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based\ncoordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were\nanalyzed. First, a subset of AD patients and CN adults were selected based\npurely on clinical diagnoses to train SPARE-BA1 (regression of age using CN\nindividuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous\ngroups were selected based on clinical and molecular markers to train SPARE-BA2\nand SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,\nA+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN\ngroup. Finally, the combined group of the AD continuum and A-/CN individuals\nwas used to train SPARE-BA3, with the intention to estimate brain age\nregardless of AD-related brain changes. Disentangled SPARE models derived brain\npatterns that were more specific to the two types of the brain changes.\nCorrelation between the SPARE-BA and SPARE-AD was significantly reduced.\nCorrelation of disentangled SPARE-AD was non-inferior to the molecular\nmeasurements and to the number of APOE4 alleles, but was less to AD-related\npsychometric test scores, suggesting contribution of advanced brain aging to\nthese scores.",
          "link": "http://arxiv.org/abs/2109.03723",
          "publishedOn": "2021-09-09T07:20:42.814Z",
          "wordCount": null,
          "title": "Disentangling Alzheimer's disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03492",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Can Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Teng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lovell_B/0/1/0/all/0/1\">Brian Lovell</a>",
          "description": "With the excellent disentanglement properties of state-of-the-art generative\nmodels, image editing has been the dominant approach to control the attributes\nof synthesised face images. However, these edited results often suffer from\nartifacts or incorrect feature rendering, especially when there is a large\ndiscrepancy between the image to be edited and the desired feature set.\nTherefore, we propose a new approach to mapping the latent vectors of the\ngenerative model to the scaling factors through solving a set of multivariate\nlinear equations. The coefficients of the equations are the eigenvectors of the\nweight parameters of the pre-trained model, which form the basis of a hyper\ncoordinate system. The qualitative and quantitative results both show that the\nproposed method outperforms the baseline in terms of image diversity. In\naddition, the method is much more time-efficient because you can obtain\nsynthesised images with desirable features directly from the latent vectors,\nrather than the former process of editing randomly generated images requiring\nmany processing steps.",
          "link": "http://arxiv.org/abs/2109.03492",
          "publishedOn": "2021-09-09T07:20:42.744Z",
          "wordCount": null,
          "title": "FaceCook: Face Generation Based on Linear Scaling Factors. (arXiv:2109.03492v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03393",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sumin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eun_H/0/1/0/all/0/1\">Hyunjun Eun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seokeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonhyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_C/0/1/0/all/0/1\">Chanho Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Changick Kim</a>",
          "description": "Online action detection, which aims to identify an ongoing action from a\nstreaming video, is an important subject in real-world applications. For this\ntask, previous methods use recurrent neural networks for modeling temporal\nrelations in an input sequence. However, these methods overlook the fact that\nthe input image sequence includes not only the action of interest but\nbackground and irrelevant actions. This would induce recurrent units to\naccumulate unnecessary information for encoding features on the action of\ninterest. To overcome this problem, we propose a novel recurrent unit, named\nInformation Discrimination Unit (IDU), which explicitly discriminates the\ninformation relevancy between an ongoing action and others to decide whether to\naccumulate the input information. This enables learning more discriminative\nrepresentations for identifying an ongoing action. In this paper, we further\npresent a new recurrent unit, called Information Integration Unit (IIU), for\naction anticipation. Our IIU exploits the outputs from IDU as pseudo action\nlabels as well as RGB frames to learn enriched features of observed actions\neffectively. In experiments on TVSeries and THUMOS-14, the proposed methods\noutperform state-of-the-art methods by a significant margin in online action\ndetection and action anticipation. Moreover, we demonstrate the effectiveness\nof the proposed units by conducting comprehensive ablation studies.",
          "link": "http://arxiv.org/abs/2109.03393",
          "publishedOn": "2021-09-09T07:20:42.728Z",
          "wordCount": null,
          "title": "Learning to Discriminate Information for Online Action Detection: Analysis and Application. (arXiv:2109.03393v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingkai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>",
          "description": "Vision transformers (ViTs) have been an alternative design paradigm to\nconvolutional neural networks (CNNs). However, the training of ViTs is much\nharder than CNNs, as it is sensitive to the training parameters, such as\nlearning rate, optimizer and warmup epoch. The reasons for training difficulty\nare empirically analysed in ~\\cite{xiao2021early}, and the authors conjecture\nthat the issue lies with the \\textit{patchify-stem} of ViT models and propose\nthat early convolutions help transformers see better. In this paper, we further\ninvestigate this problem and extend the above conclusion: only early\nconvolutions do not help for stable training, but the scaled ReLU operation in\nthe \\textit{convolutional stem} (\\textit{conv-stem}) matters. We verify, both\ntheoretically and empirically, that scaled ReLU in \\textit{conv-stem} not only\nimproves training stabilization, but also increases the diversity of patch\ntokens, thus boosting peak performance with a large margin via adding few\nparameters and flops. In addition, extensive experiments are conducted to\ndemonstrate that previous ViTs are far from being well trained, further showing\nthat ViTs have great potential to be a better substitute of CNNs.",
          "link": "http://arxiv.org/abs/2109.03810",
          "publishedOn": "2021-09-09T07:20:42.703Z",
          "wordCount": null,
          "title": "Scaled ReLU Matters for Training Vision Transformers. (arXiv:2109.03810v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "We present Panoptic SegFormer, a general framework for end-to-end panoptic\nsegmentation with Transformers. The proposed method extends Deformable DETR\nwith a unified mask prediction workflow for both things and stuff, making the\npanoptic segmentation pipeline concise and effective. With a ResNet-50\nbackbone, our method achieves 50.0\\% PQ on the COCO test-dev split, surpassing\nprevious state-of-the-art methods by significant margins without bells and\nwhistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves\na new record of 54.1\\%PQ and 54.4\\% PQ on the COCO val and test-dev splits with\nsingle scale input.",
          "link": "http://arxiv.org/abs/2109.03814",
          "publishedOn": "2021-09-09T07:20:42.680Z",
          "wordCount": null,
          "title": "Panoptic SegFormer. (arXiv:2109.03814v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_T/0/1/0/all/0/1\">Tao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1\">Qi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Huamin Feng</a>",
          "description": "Video object detection is challenging in the presence of appearance\ndeterioration in certain video frames. Therefore, it is a natural choice to\naggregate temporal information from other frames of the same video into the\ncurrent frame. However, RoI Align, as one of the most core procedures of video\ndetectors, still remains extracting features from a single-frame feature map\nfor proposals, making the extracted RoI features lack temporal information from\nvideos. In this work, considering the features of the same object instance are\nhighly similar among frames in a video, a novel Temporal RoI Align operator is\nproposed to extract features from other frames feature maps for current frame\nproposals by utilizing feature similarity. The proposed Temporal RoI Align\noperator can extract temporal information from the entire video for proposals.\nWe integrate it into single-frame video detectors and other state-of-the-art\nvideo detectors, and conduct quantitative experiments to demonstrate that the\nproposed Temporal RoI Align operator can consistently and significantly boost\nthe performance. Besides, the proposed Temporal RoI Align can also be applied\ninto video instance segmentation.",
          "link": "http://arxiv.org/abs/2109.03495",
          "publishedOn": "2021-09-09T07:20:42.678Z",
          "wordCount": null,
          "title": "Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lubing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for the panoptic tracking problem and propose a novel\ninstance-centric metric that addresses the concerns. We present extensive\nexperiments that demonstrate the utility of Panoptic nuScenes compared to\nexisting datasets and make the online evaluation server available at\n\\url{nuScenes.org}. We believe that this extension will accelerate the research\nof novel methods for scene understanding of dynamic urban environments.",
          "link": "http://arxiv.org/abs/2109.03805",
          "publishedOn": "2021-09-09T07:20:42.649Z",
          "wordCount": null,
          "title": "Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinjiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Southon_A/0/1/0/all/0/1\">Anthony Southon</a>",
          "description": "With the rapid development of intelligent detection algorithms based on deep\nlearning, much progress has been made in automatic road defect recognition and\nroad marking parsing. This can effectively address the issue of an expensive\nand time-consuming process for professional inspectors to review the street\nmanually. Towards this goal, we present RoadAtlas, a novel end-to-end\nintegrated system that can support 1) road defect detection, 2) road marking\nparsing, 3) a web-based dashboard for presenting and inputting data by users,\nand 4) a backend containing a well-structured database and developed APIs.",
          "link": "http://arxiv.org/abs/2109.03385",
          "publishedOn": "2021-09-09T07:20:42.604Z",
          "wordCount": null,
          "title": "RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Youhui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xugong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Scene text detection has drawn the close attention of researchers. Though\nmany methods have been proposed for horizontal and oriented texts, previous\nmethods may not perform well when dealing with arbitrary-shaped texts such as\ncurved texts. In particular, confusion problem arises in the case of nearby\ntext instances. In this paper, we propose a simple yet effective method for\naccurate arbitrary-shaped nearby scene text detection. Firstly, a One-to-Many\nTraining Scheme (OMTS) is designed to eliminate confusion and enable the\nproposals to learn more appropriate groundtruths in the case of nearby text\ninstances. Secondly, we propose a Proposal Feature Attention Module (PFAM) to\nexploit more effective features for each proposal, which can better adapt to\narbitrary-shaped text instances. Finally, we propose a baseline that is based\non Faster R-CNN and outputs the curve representation directly. Equipped with\nPFAM and OMTS, the detector can achieve state-of-the-art or competitive\nperformance on several challenging benchmarks.",
          "link": "http://arxiv.org/abs/2109.03451",
          "publishedOn": "2021-09-09T07:20:42.528Z",
          "wordCount": null,
          "title": "Which and Where to Focus: A Simple yet Accurate Framework for Arbitrary-Shaped Nearby Text Detection in Scene Images. (arXiv:2109.03451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>",
          "description": "Similarity learning has been recognized as a crucial step for object\ntracking. However, existing multiple object tracking methods only use sparse\nground truth matching as the training objective, while ignoring the majority of\nthe informative regions on the images. In this paper, we present Quasi-Dense\nSimilarity Learning, which densely samples hundreds of region proposals on a\npair of images for contrastive learning. We can directly combine this\nsimilarity learning with existing detection methods to build Quasi-Dense\nTracking (QDTrack) without turning to displacement regression or motion priors.\nWe also find that the resulting distinctive feature space admits a simple\nnearest neighbor search at the inference time. Despite its simplicity, QDTrack\noutperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking\nbenchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external\ntraining data. Compared to methods with similar detectors, it boosts almost 10\npoints of MOTA and significantly decreases the number of ID switches on BDD100K\nand Waymo datasets. Our code and trained models are available at\nthis http URL",
          "link": "http://arxiv.org/abs/2006.06664",
          "publishedOn": "2021-09-09T07:20:42.135Z",
          "wordCount": 687,
          "title": "Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinming Huang</a>",
          "description": "Projecting the point cloud on the 2D spherical range image transforms the\nLiDAR semantic segmentation to a 2D segmentation task on the range image.\nHowever, the LiDAR range image is still naturally different from the regular 2D\nRGB image; for example, each position on the range image encodes the unique\ngeometry information. In this paper, we propose a new projection-based LiDAR\nsemantic segmentation pipeline that consists of a novel network structure and\nan efficient post-processing step. In our network structure, we design a FID\n(fully interpolation decoding) module that directly upsamples the\nmulti-resolution feature maps using bilinear interpolation. Inspired by the 3D\ndistance interpolation used in PointNet++, we argue this FID module is a 2D\nversion distance interpolation on $(\\theta, \\phi)$ space. As a parameter-free\ndecoding module, the FID largely reduces the model complexity by maintaining\ngood performance. Besides the network structure, we empirically find that our\nmodel predictions have clear boundaries between different semantic classes.\nThis makes us rethink whether the widely used K-nearest-neighbor\npost-processing is still necessary for our pipeline. Then, we realize the\nmany-to-one mapping causes the blurring effect that some points are mapped into\nthe same pixel and share the same label. Therefore, we propose to process those\noccluded points by assigning the nearest predicted label to them. This NLA\n(nearest label assignment) post-processing step shows a better performance than\nKNN with faster inference speed in the ablation study. On the SemanticKITTI\ndataset, our pipeline achieves the best performance among all projection-based\nmethods with $64 \\times 2048$ resolution and all point-wise solutions. With a\nResNet-34 as the backbone, both the training and testing of our model can be\nfinished on a single RTX 2080 Ti with 11G memory. The code is released.",
          "link": "http://arxiv.org/abs/2109.03787",
          "publishedOn": "2021-09-09T07:20:42.068Z",
          "wordCount": 750,
          "title": "FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding. (arXiv:2109.03787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>",
          "description": "The problem of generalizing deep neural networks from multiple source domains\nto a target one is studied under two settings: When unlabeled target data is\navailable, it is a multi-source unsupervised domain adaptation (UDA) problem,\notherwise a domain generalization (DG) problem. We propose a unified framework\ntermed domain adaptive ensemble learning (DAEL) to address both problems. A\nDAEL model is composed of a CNN feature extractor shared across domains and\nmultiple classifier heads each trained to specialize in a particular source\ndomain. Each such classifier is an expert to its own domain and a non-expert to\nothers. DAEL aims to learn these experts collaboratively so that when forming\nan ensemble, they can leverage complementary information from each other to be\nmore effective for an unseen target domain. To this end, each source domain is\nused in turn as a pseudo-target-domain with its own expert providing\nsupervisory signal to the ensemble of non-experts learned from the other\nsources. For unlabeled target data under the UDA setting where real expert does\nnot exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive\nexperiments on three multi-source UDA datasets and two DG datasets show that\nDAEL improves the state of the art on both problems, often by significant\nmargins. The code is released at\n\\url{https://github.com/KaiyangZhou/Dassl.pytorch}.",
          "link": "http://arxiv.org/abs/2003.07325",
          "publishedOn": "2021-09-09T07:20:42.061Z",
          "wordCount": 698,
          "title": "Domain Adaptive Ensemble Learning. (arXiv:2003.07325v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>",
          "description": "Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global feature is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method outperforms the existing\nstate-of-the-arts on both fine-grained and generalized datasets.",
          "link": "http://arxiv.org/abs/2103.08160",
          "publishedOn": "2021-09-09T07:20:41.999Z",
          "wordCount": 643,
          "title": "DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1\">Aneesh Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "Encouraged by the success of contrastive learning on image classification\ntasks, we propose a new self-supervised method for the structured regression\ntask of 3D hand pose estimation. Contrastive learning makes use of unlabeled\ndata for the purpose of representation learning via a loss formulation that\nencourages the learned feature representations to be invariant under any image\ntransformation. For 3D hand pose estimation, it too is desirable to have\ninvariance to appearance transformation such as color jitter. However, the task\nrequires equivariance under affine transformations, such as rotation and\ntranslation. To address this issue, we propose an equivariant contrastive\nobjective and demonstrate its effectiveness in the context of 3D hand pose\nestimation. We experimentally investigate the impact of invariant and\nequivariant contrastive objectives and show that learning equivariant features\nleads to better representations for the task of 3D hand pose estimation.\nFurthermore, we show that standard ResNets with sufficient depth, trained on\nadditional unlabeled data, attain improvements of up to 14.5% in PA-EPE on\nFreiHAND and thus achieves state-of-the-art performance without any task\nspecific, specialized architectures. Code and models are available at\nhttps://ait.ethz.ch/projects/2021/PeCLR/",
          "link": "http://arxiv.org/abs/2106.05953",
          "publishedOn": "2021-09-09T07:20:41.975Z",
          "wordCount": null,
          "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhongxing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yifan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>",
          "description": "Person Re-Identification (Re-Id) in occlusion scenarios is a challenging\nproblem because a pedestrian can be partially occluded. The use of local\ninformation for feature extraction and matching is still necessary. Therefore,\nwe propose a Pose-guided inter-and intra-part relational transformer (Pirt) for\noccluded person Re-Id, which builds part-aware long-term correlations by\nintroducing transformers. In our framework, we firstly develop a pose-guided\nfeature extraction module with regional grouping and mask construction for\nrobust feature representations. The positions of a pedestrian in the image\nunder surveillance scenarios are relatively fixed, hence we propose an\nintra-part and inter-part relational transformer. The intra-part module creates\nlocal relations with mask-guided features, while the inter-part relationship\nbuilds correlations with transformers, to develop cross relationships between\npart nodes. With the collaborative learning inter- and intra-part\nrelationships, experiments reveal that our proposed Pirt model achieves a new\nstate of the art on the public occluded dataset, and further extensions on\nstandard non-occluded person Re-Id datasets also reveal our comparable\nperformances.",
          "link": "http://arxiv.org/abs/2109.03483",
          "publishedOn": "2021-09-09T07:20:41.336Z",
          "wordCount": 623,
          "title": "Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification. (arXiv:2109.03483v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03435",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+R_A/0/1/0/all/0/1\">Ammu R</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinha_N/0/1/0/all/0/1\">Neelam Sinha</a>",
          "description": "Automatic image segmentation is a critical component of medical image\nanalysis, and hence quantifying segmentation performance is crucial. Challenges\nin medical image segmentation are mainly due to spatial variations of regions\nto be segmented and imbalance in distribution of classes. Commonly used metrics\ntreat all detected pixels, indiscriminately. However, pixels in smaller\nsegments must be treated differently from pixels in larger segments, as\ndetection of smaller ones aid in early treatment of associated disease and are\nalso easier to miss. To address this, we propose a novel evaluation metric for\nsegmentation performance, emphasizing smaller segments, by assigning higher\nweightage to smaller segment pixels. Weighted false positives are also\nconsidered in deriving the new metric named, \"SSEGEP\"(Small SEGment Emphasized\nPerformance evaluation metric), (range : 0(Bad) to 1(Good)). The experiments\nwere performed on diverse anatomies(eye, liver, pancreas and breast) from\npublicly available datasets to show applicability of the proposed metric across\ndifferent imaging techniques. Mean opinion score (MOS) and statistical\nsignificance testing is used to quantify the relevance of proposed approach.\nAcross 33 fundus images, where the largest exudate is 1.41%, and the smallest\nis 0.0002% of the image, the proposed metric is 30% closer to MOS, as compared\nto Dice Similarity Coefficient (DSC). Statistical significance testing resulted\nin promising p-value of order 10^{-18} with SSEGEP for hepatic tumor compared\nto DSC. The proposed metric is found to perform better for the images having\nmultiple segments for a single label.",
          "link": "http://arxiv.org/abs/2109.03435",
          "publishedOn": "2021-09-09T07:20:41.327Z",
          "wordCount": 698,
          "title": "SSEGEP: Small SEGment Emphasized Performance evaluation metric for medical image segmentation. (arXiv:2109.03435v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03327",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1\">Shengyu Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sammak_S/0/1/0/all/0/1\">Shervin Sammak</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1\">Peyman Givi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yurko1_J/0/1/0/all/0/1\">Joseph P.Yurko1</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>",
          "description": "Direct numerical simulation (DNS) of turbulent flows is computationally\nexpensive and cannot be applied to flows with large Reynolds numbers. Large\neddy simulation (LES) is an alternative that is computationally less demanding,\nbut is unable to capture all of the scales of turbulent transport accurately.\nOur goal in this work is to build a new data-driven methodology based on\nsuper-resolution techniques to reconstruct DNS data from LES predictions. We\nleverage the underlying physical relationships to regularize the relationships\namongst different physical variables. We also introduce a hierarchical\ngenerative process and a reverse degradation process to fully explore the\ncorrespondence between DNS and LES data. We demonstrate the effectiveness of\nour method through a single-snapshot experiment and a cross-time experiment.\nThe results confirm that our method can better reconstruct high-resolution DNS\ndata over space and over time in terms of pixel-wise reconstruction error and\nstructural similarity. Visual comparisons show that our method performs much\nbetter in capturing fine-level flow dynamics.",
          "link": "http://arxiv.org/abs/2109.03327",
          "publishedOn": "2021-09-09T07:20:41.319Z",
          "wordCount": 616,
          "title": "Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zickler_T/0/1/0/all/0/1\">Todd Zickler</a>",
          "description": "Localizing stereo boundaries and predicting nearby disparities are difficult\nbecause stereo boundaries induce occluded regions where matching cues are\nabsent. Most modern computer vision algorithms treat occlusions secondarily\n(e.g., via left-right consistency checks after matching) or rely on high-level\ncues to improve nearby disparities (e.g., via deep networks and large training\nsets). They ignore the geometry of stereo occlusions, which dictates that the\nspatial extent of occlusion must equal the amplitude of the disparity jump that\ncauses it. This paper introduces an energy and level-set optimizer that\nimproves boundaries by encoding occlusion geometry. Our model applies to\ntwo-layer, figure-ground scenes, and it can be implemented cooperatively using\nmessages that pass predominantly between parents and children in an undecimated\nhierarchy of multi-scale image patches. In a small collection of figure-ground\nscenes curated from Middlebury and Falling Things stereo datasets, our model\nprovides more accurate boundaries than previous occlusion-handling stereo\ntechniques. This suggests new directions for creating cooperative stereo\nsystems that incorporate occlusion cues in a human-like manner.",
          "link": "http://arxiv.org/abs/2109.03464",
          "publishedOn": "2021-09-09T07:20:41.303Z",
          "wordCount": 621,
          "title": "Level Set Binocular Stereo with Occlusions. (arXiv:2109.03464v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cvisic_I/0/1/0/all/0/1\">Igor Cvi&#x161;i&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_I/0/1/0/all/0/1\">Ivan Markovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovic_I/0/1/0/all/0/1\">Ivan Petrovi&#x107;</a>",
          "description": "Over the last decade, one of the most relevant public datasets for evaluating\nodometry accuracy is the KITTI dataset. Beside the quality and rich sensor\nsetup, its success is also due to the online evaluation tool, which enables\nresearchers to benchmark and compare algorithms. The results are evaluated on\nthe test subset solely, without any knowledge about the ground truth, yielding\nunbiased, overfit free and therefore relevant validation for robot localization\nbased on cameras, 3D laser or combination of both. However, as any sensor\nsetup, it requires prior calibration and rectified stereo images are provided,\nintroducing dependence on the default calibration parameters. Given that, a\nnatural question arises if a better set of calibration parameters can be found\nthat would yield higher odometry accuracy. In this paper, we propose a new\napproach for one shot calibration of the KITTI dataset multiple camera setup.\nThe approach yields better calibration parameters, both in the sense of lower\ncalibration reprojection errors and lower visual odometry error. We conducted\nexperiments where we show for three different odometry algorithms, namely\nSOFT2, ORB-SLAM2 and VISO2, that odometry accuracy is significantly improved\nwith the proposed calibration parameters. Moreover, our odometry, SOFT2, in\nconjunction with the proposed calibration method achieved the highest accuracy\non the official KITTI scoreboard with 0.53% translational and 0.0009 deg/m\nrotational error, outperforming even 3D laser-based methods.",
          "link": "http://arxiv.org/abs/2109.03462",
          "publishedOn": "2021-09-09T07:20:41.275Z",
          "wordCount": 682,
          "title": "Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy. (arXiv:2109.03462v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanaa_D/0/1/0/all/0/1\">David Kanaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Despite having been studied to a great extent, the task of conditional\ngeneration of sequences of frames, or videos, remains extremely challenging. It\nis a common belief that a key step towards solving this task resides in\nmodelling accurately both spatial and temporal information in video signals. A\npromising direction to do so has been to learn latent variable models that\npredict the future in latent space and project back to pixels, as suggested in\nrecent literature. Following this line of work and building on top of a family\nof models introduced in prior work, Neural ODE, we investigate an approach that\nmodels time-continuous dynamics over a continuous latent space with a\ndifferential equation with respect to time. The intuition behind this approach\nis that these trajectories in latent space could then be extrapolated to\ngenerate video frames beyond the time steps for which the model is trained. We\nshow that our approach yields promising results in the task of future frame\nprediction on the Moving MNIST dataset with 1 and 2 digits.",
          "link": "http://arxiv.org/abs/2109.03292",
          "publishedOn": "2021-09-09T07:20:41.222Z",
          "wordCount": 644,
          "title": "Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Luquin_F/0/1/0/all/0/1\">Fernanda Hern&#xe1;ndez-Luquin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1\">Hugo Jair Escalante</a>",
          "description": "Emotion recognition (ER) from facial images is one of the landmark tasks in\naffective computing with major developments in the last decade. Initial efforts\non ER relied on handcrafted features that were used to characterize facial\nimages and then feed to standard predictive models. Recent methodologies\ncomprise end-to-end trainable deep learning methods that simultaneously learn\nboth, features and predictive model. Perhaps the most successful models are\nbased on convolutional neural networks (CNNs). While these models have excelled\nat this task, they still fail at capturing local patterns that could emerge in\nthe learning process. We hypothesize these patterns could be captured by\nvariants based on locally weighted learning. Specifically, in this paper we\npropose a CNN based architecture enhanced with multiple branches formed by\nradial basis function (RBF) units that aims at exploiting local information at\nthe final stage of the learning process. Intuitively, these RBF units capture\nlocal patterns shared by similar instances using an intermediate\nrepresentation, then the outputs of the RBFs are feed to a softmax layer that\nexploits this information to improve the predictive performance of the model.\nThis feature could be particularly advantageous in ER as cultural / ethnicity\ndifferences may be identified by the local units. We evaluate the proposed\nmethod in several ER datasets and show the proposed methodology achieves\nstate-of-the-art in some of them, even when we adopt a pre-trained VGG-Face\nmodel as backbone. We show it is the incorporation of local information what\nmakes the proposed model competitive.",
          "link": "http://arxiv.org/abs/2109.03336",
          "publishedOn": "2021-09-09T07:20:41.213Z",
          "wordCount": 707,
          "title": "Multi-Branch Deep Radial Basis Function Networks for Facial Emotion Recognition. (arXiv:2109.03336v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Cheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Ye Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Kunpeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zongming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "Quantization has been proven to be a vital method for improving the inference\nefficiency of deep neural networks (DNNs). However, it is still challenging to\nstrike a good balance between accuracy and efficiency while quantizing DNN\nweights or activation values from high-precision formats to their quantized\ncounterparts. We propose a new method called elastic significant bit\nquantization (ESB) that controls the number of significant bits of quantized\nvalues to obtain better inference accuracy with fewer resources. We design a\nunified mathematical formula to constrain the quantized values of the ESB with\na flexible number of significant bits. We also introduce a distribution\ndifference aligner (DDA) to quantitatively align the distributions between the\nfull-precision weight or activation values and quantized values. Consequently,\nESB is suitable for various bell-shaped distributions of weights and activation\nof DNNs, thus maintaining a high inference accuracy. Benefitting from fewer\nsignificant bits of quantized values, ESB can reduce the multiplication\ncomplexity. We implement ESB as an accelerator and quantitatively evaluate its\nefficiency on FPGAs. Extensive experimental results illustrate that ESB\nquantization consistently outperforms state-of-the-art methods and achieves\naverage accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet,\nResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can\nachieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx\nZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators\non FPGAs, the ESB accelerator can improve the energy efficiency by up to 65x,\n11x, and 26x, respectively.",
          "link": "http://arxiv.org/abs/2109.03513",
          "publishedOn": "2021-09-09T07:20:41.206Z",
          "wordCount": 715,
          "title": "Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks. (arXiv:2109.03513v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">W. Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">M. Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">X. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimobayashi_N/0/1/0/all/0/1\">N. Shimobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruta_D/0/1/0/all/0/1\">D. Maruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">T. Okatani</a>",
          "description": "This paper considers matching images of low-light scenes, aiming to widen the\nfrontier of SfM and visual SLAM applications. Recent image sensors can record\nthe brightness of scenes with more than eight-bit precision, available in their\nRAW-format image. We are interested in making full use of such high-precision\ninformation to match extremely low-light scene images that conventional methods\ncannot handle. For extreme low-light scenes, even if some of their brightness\ninformation exists in the RAW format images' low bits, the standard raw image\nprocessing on cameras fails to utilize them properly. As was recently shown by\nChen et al., CNNs can learn to produce images with a natural appearance from\nsuch RAW-format images. To consider if and how well we can utilize such\ninformation stored in RAW-format images for image matching, we have created a\nnew dataset named MID (matching in the dark). Using it, we experimentally\nevaluated combinations of eight image-enhancing methods and eleven image\nmatching methods consisting of classical/neural local descriptors and\nclassical/neural initial point-matching methods. The results show the advantage\nof using the RAW-format images and the strengths and weaknesses of the above\ncomponent methods. They also imply there is room for further research.",
          "link": "http://arxiv.org/abs/2109.03585",
          "publishedOn": "2021-09-09T07:20:41.196Z",
          "wordCount": 670,
          "title": "Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes. (arXiv:2109.03585v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Deqian Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kei_Y/0/1/0/all/0/1\">Yik Lun Kei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>",
          "description": "We study the understanding of embodied reference: One agent uses both\nlanguage and gesture to refer to an object to another agent in a shared\nphysical environment. Of note, this new visual task requires understanding\nmultimodal cues with perspective-taking to identify which object is being\nreferred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced\ndataset of embodied reference collected in various physical scenes; the dataset\ncontains 4,195 unique reference clips in 432 indoor scenes. To the best of our\nknowledge, this is the first embodied reference dataset that allows us to study\nreferring expressions in daily physical scenes to understand referential\nbehavior, human communication, and human-robot interaction. We further devise\ntwo benchmarks for image-based and video-based embodied reference\nunderstanding. Comprehensive baselines and extensive experiments provide the\nvery first result of machine perception on how the referring expressions and\ngestures affect the embodied reference understanding. Our results provide\nessential evidence that gestural cues are as critical as language cues in\nunderstanding the embodied reference.",
          "link": "http://arxiv.org/abs/2109.03413",
          "publishedOn": "2021-09-09T07:20:41.134Z",
          "wordCount": 628,
          "title": "YouRefIt: Embodied Reference Understanding with Language and Gesture. (arXiv:2109.03413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03351",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Peters_B/0/1/0/all/0/1\">Benjamin Peters</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kriegeskorte_N/0/1/0/all/0/1\">Nikolaus Kriegeskorte</a>",
          "description": "Human visual perception carves a scene at its physical joints, decomposing\nthe world into objects, which are selectively attended, tracked, and predicted\nas we engage our surroundings. Object representations emancipate perception\nfrom the sensory input, enabling us to keep in mind that which is out of sight\nand to use perceptual content as a basis for action and symbolic cognition.\nHuman behavioral studies have documented how object representations emerge\nthrough grouping, amodal completion, proto-objects, and object files. Deep\nneural network (DNN) models of visual object recognition, by contrast, remain\nlargely tethered to the sensory input, despite achieving human-level\nperformance at labeling objects. Here, we review related work in both fields\nand examine how these fields can help each other. The cognitive literature\nprovides a starting point for the development of new experimental tasks that\nreveal mechanisms of human object perception and serve as benchmarks driving\ndevelopment of deep neural network models that will put the object into object\nrecognition.",
          "link": "http://arxiv.org/abs/2109.03351",
          "publishedOn": "2021-09-09T07:20:41.088Z",
          "wordCount": 614,
          "title": "Capturing the objects of vision with neural networks. (arXiv:2109.03351v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03551",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_M/0/1/0/all/0/1\">Ming-Chi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shu-Wei Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\nspeech enhancement, a task that aims to improve the quality of the artificial\nvoice from an electrolarynx device. In frame-based VC methods, time alignment\nneeds to be performed prior to model training, and the dynamic time warping\n(DTW) algorithm is widely adopted to compute the best time alignment between\neach utterance pair. The validity is based on the assumption that the same\nphonemes of the speakers have similar features and can be mapped by measuring a\npre-defined distance between speech frames of the source and the target.\nHowever, the special characteristics of the EL speech can break the assumption,\nresulting in a sub-optimal DTW alignment. In this work, we propose to use lip\nimages for time alignment, as we assume that the lip movements of laryngectomee\nremain normal compared to healthy people. We investigate two naive lip\nrepresentations and distance metrics, and experimental results demonstrate that\nthe proposed method can significantly outperform the audio-only alignment in\nterms of objective and subjective evaluations.",
          "link": "http://arxiv.org/abs/2109.03551",
          "publishedOn": "2021-09-09T07:20:41.082Z",
          "wordCount": 650,
          "title": "Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_E/0/1/0/all/0/1\">Enrique Dunn</a>",
          "description": "We present GTT-Net, a supervised learning framework for the reconstruction of\nsparse dynamic 3D geometry. We build on a graph-theoretic formulation of the\ngeneralized trajectory triangulation problem, where non-concurrent multi-view\nimaging geometry is known but global image sequencing is not provided. GTT-Net\nlearns pairwise affinities modeling the spatio-temporal relationships among our\ninput observations and leverages them to determine 3D geometry estimates.\nExperiments reconstructing 3D motion-capture sequences show GTT-Net outperforms\nthe state of the art in terms of accuracy and robustness. Within the context of\narticulated motion reconstruction, our proposed architecture is 1) able to\nlearn and enforce semantic 3D motion priors for shared training and test\ndomains, while being 2) able to generalize its performance across different\ntraining and test domains. Moreover, GTT-Net provides a computationally\nstreamlined framework for trajectory triangulation with applications to\nmulti-instance reconstruction and event segmentation.",
          "link": "http://arxiv.org/abs/2109.03408",
          "publishedOn": "2021-09-09T07:20:41.047Z",
          "wordCount": 579,
          "title": "GTT-Net: Learned Generalized Trajectory Triangulation. (arXiv:2109.03408v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02700",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gona_S/0/1/0/all/0/1\">Sai Nikhil Gona</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandhakavi_P/0/1/0/all/0/1\">Prithvi Raj Bandhakavi</a>",
          "description": "There are few industries which use manually controlled robots for carrying\nmaterial and this cannot be used all the time in all the places. So, it is very\ntranquil to have robots which can follow a specific human by following the\nunique coloured object held by that person. So, we propose a robotic system\nwhich uses robot vision and deep learning to get the required linear and\nangular velocities which are {\\nu} and {\\omega}, respectively. Which in turn\nmakes the robot to avoid obstacles when following the unique coloured object\nheld by the human. The novel methodology that we are proposing is accurate in\ndetecting the position of the unique coloured object in any kind of lighting\nand tells us the horizontal pixel value where the robot is present and also\ntells if the object is close to or far from the robot. Moreover, the artificial\nneural networks that we have used in this problem gave us a meagre error in\nlinear and angular velocity prediction and the PI controller which was used to\ncontrol the linear and angular velocities, which in turn controls the position\nof the robot gave us impressive results and this methodology outperforms all\nother methodologies.",
          "link": "http://arxiv.org/abs/2109.02700",
          "publishedOn": "2021-09-08T07:20:12.703Z",
          "wordCount": null,
          "title": "Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance. (arXiv:2109.02700v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "Out-of-distribution (OOD) detection and lossless compression constitute two\nproblems that can be solved by the training of probabilistic models on a first\ndataset with subsequent likelihood evaluation on a second dataset, where data\ndistributions differ. By defining the generalization of probabilistic models in\nterms of likelihood we show that, in the case of image models, the OOD\ngeneralization ability is dominated by local features. This motivates our\nproposal of a Local Autoregressive model that exclusively models local image\nfeatures towards improving OOD performance. We apply the proposed model to OOD\ndetection tasks and achieve state-of-the-art unsupervised OOD detection\nperformance without the introduction of additional data. Additionally, we\nemploy our model to build a new lossless image compressor: NeLLoC (Neural Local\nLossless Compressor) and report state-of-the-art compression rates and model\nsize.",
          "link": "http://arxiv.org/abs/2109.02639",
          "publishedOn": "2021-09-08T07:20:12.692Z",
          "wordCount": null,
          "title": "On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walke_H/0/1/0/all/0/1\">Homer Walke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, editing, and more. Training such inference models is challenging\ndue to the lack of paired (shape, program) data in most domains. A popular\napproach is to pre-train a model on synthetic data and then fine-tune on real\nshapes using slow, unstable reinforcement learning. In this paper, we argue\nthat self-training is a viable alternative for fine-tuning such models.\nSelf-training is a semi-supervised learning paradigm where a model assigns\npseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)\npairs as the new ground truth. We show that for constructive solid geometry and\nassembly-based modeling, self-training outperforms state-of-the-art\nreinforcement learning approaches. Additionally, shape program inference has a\nunique property that circumvents a potential downside of self-training\n(incorrect pseudo-label assignment): inferred programs are executable. For a\ngiven shape from our distribution of interest $\\mathbf{x}^*$ and its predicted\nprogram $\\mathbf{z}$, one can execute $\\mathbf{z}$ to obtain a shape\n$\\mathbf{x}$ and train on $(\\mathbf{z}, \\mathbf{x})$ pairs, rather than\n$(\\mathbf{z}, \\mathbf{x}^*)$ pairs. We term this procedure latent execution\nself training (LEST). We demonstrate that self training infers shape programs\nwith higher shape reconstruction accuracy and converges significantly faster\nthan reinforcement learning approaches, and in some domains, LEST can further\nimprove this performance.",
          "link": "http://arxiv.org/abs/2011.13045",
          "publishedOn": "2021-09-08T07:20:12.662Z",
          "wordCount": null,
          "title": "Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>",
          "description": "Domain adaptation aims to leverage information from the source domain to\nimprove the classification performance in the target domain. It mainly utilizes\ntwo schemes: sample reweighting and feature matching. While the first scheme\nallocates different weights to individual samples, the second scheme matches\nthe feature of two domains using global structural statistics. The two schemes\nare complementary with each other, which are expected to jointly work for\nrobust domain adaptation. Several methods combine the two schemes, but the\nunderlying relationship of samples is insufficiently analyzed due to the\nneglect of the hierarchy of samples and the geometric properties between\nsamples. To better combine the advantages of the two schemes, we propose a\nGrassmannian graph-attentional landmark selection (GGLS) framework for domain\nadaptation. GGLS presents a landmark selection scheme using attention-induced\nneighbors of the graphical structure of samples and performs distribution\nadaptation and knowledge adaptation over Grassmann manifold. the former treats\nthe landmarks of each sample differently, and the latter avoids feature\ndistortion and achieves better geometric properties. Experimental results on\ndifferent real-world cross-domain visual recognition tasks demonstrate that\nGGLS provides better classification accuracies compared with state-of-the-art\ndomain adaptation methods.",
          "link": "http://arxiv.org/abs/2109.02990",
          "publishedOn": "2021-09-08T07:20:12.658Z",
          "wordCount": null,
          "title": "Grassmannian Graph-attentional Landmark Selection for Domain Adaptation. (arXiv:2109.02990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.02846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Pu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>",
          "description": "Aerial scene recognition is a fundamental research problem in interpreting\nhigh-resolution aerial imagery. Over the past few years, most studies focus on\nclassifying an image into one scene category, while in real-world scenarios, it\nis more often that a single image contains multiple scenes. Therefore, in this\npaper, we investigate a more practical yet underexplored task -- multi-scene\nrecognition in single images. To this end, we create a large-scale dataset,\ncalled MultiScene, composed of 100,000 unconstrained high-resolution aerial\nimages. Considering that manually labeling such images is extremely arduous, we\nresort to low-cost annotations from crowdsourcing platforms, e.g.,\nOpenStreetMap (OSM). However, OSM data might suffer from incompleteness and\nincorrectness, which introduce noise into image labels. To address this issue,\nwe visually inspect 14,000 images and correct their scene labels, yielding a\nsubset of cleanly-annotated images, named MultiScene-Clean. With it, we can\ndevelop and evaluate deep networks for multi-scene recognition using clean\ndata. Moreover, we provide crowdsourced annotations of all images for the\npurpose of studying network learning with noisy labels. We conduct experiments\nwith extensive baseline models on both MultiScene-Clean and MultiScene to offer\nbenchmarks for multi-scene recognition in single images and learning from noisy\nlabels for this task, respectively. To facilitate progress, we make our dataset\nand trained models available on\nhttps://gitlab.lrz.de/ai4eo/reasoning/multiscene.",
          "link": "http://arxiv.org/abs/2104.02846",
          "publishedOn": "2021-09-08T07:20:12.432Z",
          "wordCount": 709,
          "title": "MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-09-08T07:20:12.411Z",
          "wordCount": 622,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Canqun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhennan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenbin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chen Xu</a>",
          "description": "Parsing an image into a hierarchy of objects, parts, and relations is\nimportant and also challenging in many computer vision tasks. This paper\nproposes a simple and effective capsule autoencoder to address this issue,\ncalled DPR-CAE. In our approach, the encoder parses the input into a set of\npart capsules, including pose, intensity, and dynamic vector. The decoder\nintroduces a novel dynamic part representation (DPR) by combining the dynamic\nvector and a shared template bank. These part representations are then\nregulated by corresponding capsules to composite the final output in an\ninterpretable way. Besides, an extra translation-invariant module is proposed\nto avoid directly learning the uncertain scene-part relationship in our\nDPR-CAE, which makes the resulting method achieves a promising performance gain\non $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship\nDPR-CAE can be easily combined with the existing stacked capsule autoencoder\nand experimental results show it significantly improves performance in terms of\nunsupervised object classification. Our code is available in the Appendix.",
          "link": "http://arxiv.org/abs/2104.14735",
          "publishedOn": "2021-09-08T07:20:12.277Z",
          "wordCount": 664,
          "title": "DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing. (arXiv:2104.14735v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-09-08T07:20:12.261Z",
          "wordCount": 740,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Training sample re-weighting is an effective approach for tackling data\nbiases such as imbalanced and corrupted labels. Recent methods develop\nlearning-based algorithms to learn sample re-weighting strategies jointly with\nmodel training based on the frameworks of reinforcement learning and meta\nlearning. However, depending on additional unbiased reward data is limiting\ntheir general applicability. Furthermore, existing learning-based sample\nre-weighting methods require nested optimizations of models and weighting\nparameters, which requires expensive second-order computation. This paper\naddresses these two problems and presents a novel learning-based fast sample\nre-weighting (FSR) method that does not require additional reward data. The\nmethod is based on two key ideas: learning from history to build proxy reward\ndata and feature sharing to reduce the optimization cost. Our experiments show\nthe proposed method achieves competitive results compared to state of the arts\non label noise robustness and long-tailed recognition, and does so while\nachieving significantly improved training efficiency. The source code is\npublicly available at\nhttps://github.com/google-research/google-research/tree/master/ieg.",
          "link": "http://arxiv.org/abs/2109.03216",
          "publishedOn": "2021-09-08T07:20:12.253Z",
          "wordCount": 602,
          "title": "Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferreri_A/0/1/0/all/0/1\">Andrea Ferreri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucci_S/0/1/0/all/0/1\">Silvia Bucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>",
          "description": "Scene recognition is one of the basic problems in computer vision research\nwith extensive applications in robotics. When available, depth images provide\nhelpful geometric cues that complement the RGB texture information and help to\nidentify discriminative scene image features. Depth sensing technology\ndeveloped fast in the last years and a great variety of 3D cameras have been\nintroduced, each with different acquisition properties. However, those\nproperties are often neglected when targeting big data collections, so\nmulti-modal images are gathered disregarding their original nature. In this\nwork, we put under the spotlight the existence of a possibly severe domain\nshift issue within multi-modality scene recognition datasets. As a consequence,\na scene classification model trained on one camera may not generalize on data\nfrom a different camera, only providing a low recognition performance. Starting\nfrom the well-known SUN RGB-D dataset, we designed an experimental testbed to\nstudy this problem and we use it to benchmark the performance of existing\nmethods. Finally, we introduce a novel adaptive scene recognition approach that\nleverages self-supervised translation between modalities. Indeed, learning to\ngo from RGB to depth and vice-versa is an unsupervised procedure that can be\ntrained jointly on data of multiple cameras and may help to bridge the gap\namong the extracted feature distributions. Our experimental results confirm the\neffectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2103.14672",
          "publishedOn": "2021-09-08T07:20:12.227Z",
          "wordCount": 701,
          "title": "Multi-Modal RGB-D Scene Recognition Across Domains. (arXiv:2103.14672v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vo_Ho_V/0/1/0/all/0/1\">Viet-Khoa Vo-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1\">Akihiro Sugimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>",
          "description": "Temporal action proposal generation is an essential and challenging task that\naims at localizing temporal intervals containing human actions in untrimmed\nvideos. Most of existing approaches are unable to follow the human cognitive\nprocess of understanding the video context due to lack of attention mechanism\nto express the concept of an action or an agent who performs the action or the\ninteraction between the agent and the environment. Based on the action\ndefinition that a human, known as an agent, interacts with the environment and\nperforms an action that affects the environment, we propose a contextual\nAgent-Environment Network. Our proposed contextual AEN involves (i) agent\npathway, operating at a local level to tell about which humans/agents are\nacting and (ii) environment pathway operating at a global level to tell about\nhow the agents interact with the environment. Comprehensive evaluations on\n20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different\nbackbone networks, i.e C3D and SlowFast, show that our method robustly exhibits\noutperformance against state-of-the-art methods regardless of the employed\nbackbone network.",
          "link": "http://arxiv.org/abs/2107.08323",
          "publishedOn": "2021-09-08T07:20:12.212Z",
          "wordCount": 650,
          "title": "Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Futao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "More transformer blocks with residual connections have recently achieved\nimpressive results on various tasks. To achieve better performance with fewer\ntrainable parameters, recent methods are proposed to go shallower by parameter\nsharing or model compressing along with the depth. However, weak modeling\ncapacity limits their performance. Contrastively, going wider by inducing more\ntrainable matrixes and parameters would produce a huge model requiring advanced\nparallelism to train and inference.\n\nIn this paper, we propose a parameter-efficient framework, going wider\ninstead of deeper. Specially, following existing works, we adapt parameter\nsharing to compress along depth. But, such deployment would limit the\nperformance. To maximize modeling capacity, we scale along model width by\nreplacing feed-forward network (FFN) with mixture-of-experts (MoE). Across\ntransformer blocks, instead of sharing normalization layers, we propose to use\nindividual layernorms to transform various semantic representations in a more\nparameter-efficient way. To evaluate our plug-and-run framework, we design\nWideNet and conduct comprehensive experiments on popular computer vision and\nnatural language processing benchmarks. On ImageNet-1K, our best model\noutperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable\nparameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can\nstill surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four\nnatural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on\naverage and surpass BERT using factorized embedding parameterization by $0.8\\%$\nwith fewer parameters.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-09-08T07:20:12.188Z",
          "wordCount": 709,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weikang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1\">Peiguang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>",
          "description": "As moving objects always draw more attention of human eyes, the temporal\nmotive information is always exploited complementarily with spatial information\nto detect salient objects in videos. Although efficient tools such as optical\nflow have been proposed to extract temporal motive information, it often\nencounters difficulties when used for saliency detection due to the movement of\ncamera or the partial movement of salient objects. In this paper, we\ninvestigate the complimentary roles of spatial and temporal information and\npropose a novel dynamic spatiotemporal network (DS-Net) for more effective\nfusion of spatiotemporal information. We construct a symmetric two-bypass\nnetwork to explicitly extract spatial and temporal features. A dynamic weight\ngenerator (DWG) is designed to automatically learn the reliability of\ncorresponding saliency branch. And a top-down cross attentive aggregation (CAA)\nprocedure is designed so as to facilitate dynamic complementary aggregation of\nspatiotemporal features. Finally, the features are modified by spatial\nattention with the guidance of coarse saliency map and then go through decoder\npart for final saliency map. Experimental results on five benchmarks VOS,\nDAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method\nachieves superior performance than state-of-the-art algorithms. The source code\nis available at https://github.com/TJUMMG/DS-Net.",
          "link": "http://arxiv.org/abs/2012.04886",
          "publishedOn": "2021-09-08T07:20:12.149Z",
          "wordCount": 683,
          "title": "DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1\">Dehui Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Baocai Yin</a>",
          "description": "3D action recognition is referred to as the classification of action\nsequences which consist of 3D skeleton joints. While many research work are\ndevoted to 3D action recognition, it mainly suffers from three problems: highly\ncomplicated articulation, a great amount of noise, and a low implementation\nefficiency. To tackle all these problems, we propose a real-time 3D action\nrecognition framework by integrating the locally aggregated kinematic-guided\nskeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first\ndefine the skeletonlet as a few combinations of joint offsets grouped in terms\nof kinematic principle, and then represent an action sequence using LAKS, which\nconsists of a denoising phase and a locally aggregating phase. The denoising\nphase detects the noisy action data and adjust it by replacing all the features\nwithin it with the features of the corresponding previous frame, while the\nlocally aggregating phase sums the difference between an offset feature of the\nskeletonlet and its cluster center together over all the offset features of the\nsequence. Finally, the SHA model which combines sparse representation with a\nhashing model, aiming at promoting the recognition accuracy while maintaining a\nhigh efficiency. Experimental results on MSRAction3D, UTKinectAction3D and\nFlorence3DAction datasets demonstrate that the proposed method outperforms\nstate-of-the-art methods in both recognition accuracy and implementation\nefficiency.",
          "link": "http://arxiv.org/abs/2105.11312",
          "publishedOn": "2021-09-08T07:20:12.140Z",
          "wordCount": 706,
          "title": "Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1\">Lucas Fernando Alvarenga e Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedronette_D/0/1/0/all/0/1\">Daniel Carlos Guimar&#xe3;es Pedronette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faria_F/0/1/0/all/0/1\">F&#xe1;bio Augusto Faria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o Paulo Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1\">Jurandy Almeida</a>",
          "description": "Deep learning (DL) has been the primary approach used in various computer\nvision tasks due to its relevant results achieved on many tasks. However, on\nreal-world scenarios with partially or no labeled data, DL methods are also\nprone to the well-known domain shift problem. Multi-source unsupervised domain\nadaptation (MSDA) aims at learning a predictor for an unlabeled domain by\nassigning weak knowledge from a bag of source models. However, most works\nconduct domain adaptation leveraging only the extracted features and reducing\ntheir domain shift from the perspective of loss function designs. In this\npaper, we argue that it is not sufficient to handle domain shift only based on\ndomain-level features, but it is also essential to align such information on\nthe feature space. Unlike previous works, we focus on the network design and\npropose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at\ndifferent levels of the predictor. These layers are designed to match the\nfeature distributions between different domains and can be easily applied to\nvarious MSDA methods. To show the robustness of our approach, we conducted an\nextensive experimental evaluation considering two challenging scenarios: digit\nrecognition and object classification. The experimental results indicated that\nour approach can improve state-of-the-art MSDA methods, yielding relative gains\nof up to +30.64% on their classification accuracies.",
          "link": "http://arxiv.org/abs/2109.02693",
          "publishedOn": "2021-09-08T07:20:12.117Z",
          "wordCount": 673,
          "title": "Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02688",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianze Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>",
          "description": "Annotated images are required for both supervised model training and\nevaluation in image classification. Manually annotating images is arduous and\nexpensive, especially for multi-labeled images. A recent trend for conducting\nsuch laboursome annotation tasks is through crowdsourcing, where images are\nannotated by volunteers or paid workers online (e.g., workers of Amazon\nMechanical Turk) from scratch. However, the quality of crowdsourcing image\nannotations cannot be guaranteed, and incompleteness and incorrectness are two\nmajor concerns for crowdsourcing annotations. To address such concerns, we have\na rethinking of crowdsourcing annotations: Our simple hypothesis is that if the\nannotators only partially annotate multi-label images with salient labels they\nare confident in, there will be fewer annotation errors and annotators will\nspend less time on uncertain labels. As a pleasant surprise, with the same\nannotation budget, we show a multi-label image classifier supervised by images\nwith salient annotations can outperform models supervised by fully annotated\nimages. Our method contributions are 2-fold: An active learning way is proposed\nto acquire salient labels for multi-label images; and a novel Adaptive\nTemperature Associated Model (ATAM) specifically using partial annotations is\nproposed for multi-label image classification. We conduct experiments on\npractical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark\ndataset COCO 2014. When compared with state-of-the-art classification methods\ntrained on fully annotated images, the proposed ATAM can achieve higher\naccuracy. The proposed idea is promising for crowdsourcing data annotation. Our\ncode will be publicly available.",
          "link": "http://arxiv.org/abs/2109.02688",
          "publishedOn": "2021-09-08T07:20:12.093Z",
          "wordCount": 690,
          "title": "Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification. (arXiv:2109.02688v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12995",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1\">Zhanghui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wayne Zhang</a>",
          "description": "Most weakly supervised semantic segmentation (WSSS) methods follow the\npipeline that generates pseudo-masks initially and trains the segmentation\nmodel with the pseudo-masks in fully supervised manner after. However, we find\nsome matters related to the pseudo-masks, including high quality pseudo-masks\ngeneration from class activation maps (CAMs), and training with noisy\npseudo-mask supervision. For these matters, we propose the following designs to\npush the performance to new state-of-art: (i) Coefficient of Variation\nSmoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask\nGeneration to project the expanded CAMs to pseudo-mask based on a new metric\nindicating the importance of each class on each location, instead of the scores\ntrained from binary classifiers. (iii) Pretended Under-Fitting strategy to\nsuppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to\nboost the pseudo-masks during training of fully supervised semantic\nsegmentation (FSSS). Experiments based on our methods achieve new state-of-art\nresults on two changeling weakly supervised semantic segmentation datasets,\npushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014\nrespectively. Codes including segmentation framework are released at\nhttps://github.com/Eli-YiLi/PMM",
          "link": "http://arxiv.org/abs/2108.12995",
          "publishedOn": "2021-09-08T07:20:12.071Z",
          "wordCount": 644,
          "title": "Pseudo-mask Matters in Weakly-supervised Semantic Segmentation. (arXiv:2108.12995v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gesnouin_J/0/1/0/all/0/1\">Joseph Gesnouin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechberti_S/0/1/0/all/0/1\">Steve Pechberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>",
          "description": "Understanding the behaviors and intentions of pedestrians is still one of the\nmain challenges for vehicle autonomy, as accurate predictions of their\nintentions can guarantee their safety and driving comfort of vehicles. In this\npaper, we address pedestrian crossing prediction in urban traffic environments\nby linking the dynamics of a pedestrian's skeleton to a binary crossing\nintention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch\npredictor. TrouSPI-Net extracts spatio-temporal features for different time\nresolutions by encoding pseudo-images sequences of skeletal joints' positions\nand processes them with parallel attention modules and atrous convolutions. The\nproposed approach is then enhanced by processing features such as relative\ndistances of skeletal joints, bounding box positions, or ego-vehicle speed with\nU-GRUs. Using the newly proposed evaluation procedures for two large public\nnaturalistic data sets for studying pedestrian behavior in traffic: JAAD and\nPIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results\nshow that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,\ntherefore outperforming current state-of-the-art while being lightweight and\ncontext-free.",
          "link": "http://arxiv.org/abs/2109.00953",
          "publishedOn": "2021-09-08T07:20:12.004Z",
          "wordCount": 670,
          "title": "TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianchuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "We present animatable neural radiance fields (animatable NeRF) for detailed\nhuman avatar creation from monocular videos. Our approach extends neural\nradiance fields (NeRF) to the dynamic scenes with human movements via\nintroducing explicit pose-guided deformation while learning the scene\nrepresentation network. In particular, we estimate the human pose for each\nframe and learn a constant canonical space for the detailed human template,\nwhich enables natural shape deformation from the observation space to the\ncanonical space under the explicit control of the pose parameters. To\ncompensate for inaccurate pose estimation, we introduce the pose refinement\nstrategy that updates the initial pose during the learning process, which not\nonly helps to learn more accurate human reconstruction but also accelerates the\nconvergence. In experiments we show that the proposed approach achieves 1)\nimplicit human geometry and appearance reconstruction with high-quality\ndetails, 2) photo-realistic rendering of the human from novel views, and 3)\nanimation of the human with novel poses.",
          "link": "http://arxiv.org/abs/2106.13629",
          "publishedOn": "2021-09-08T07:20:11.960Z",
          "wordCount": 642,
          "title": "Animatable Neural Radiance Fields from Monocular RGB Videos. (arXiv:2106.13629v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-09-08T07:20:11.877Z",
          "wordCount": 631,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Teuscher_A/0/1/0/all/0/1\">Adam Teuscher</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1\">Connor Anderson</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farrell_R/0/1/0/all/0/1\">Ryan Farrell</a> (1) ((1) Brigham Young University, (2) University of Maryland)",
          "description": "For the task of image classification, researchers work arduously to develop\nthe next state-of-the-art (SOTA) model, each bench-marking their own\nperformance against that of their predecessors and of their peers.\nUnfortunately, the metric used most frequently to describe a model's\nperformance, average categorization accuracy, is often used in isolation. As\nthe number of classes increases, such as in fine-grained visual categorization\n(FGVC), the amount of information conveyed by average accuracy alone dwindles.\nWhile its most glaring weakness is its failure to describe the model's\nperformance on a class-by-class basis, average accuracy also fails to describe\nhow performance may vary from one trained model of the same architecture, on\nthe same dataset, to another (both averaged across all categories and at the\nper-class level). We first demonstrate the magnitude of these variations across\nmodels and across class distributions based on attributes of the data,\ncomparing results on different visual domains and different per-class image\ndistributions, including long-tailed distributions and few-shot subsets. We\nthen analyze the impact various FGVC methods have on overall and per-class\nvariance. From this analysis, we both highlight the importance of reporting and\ncomparing methods based on information beyond overall accuracy, as well as\npoint out techniques that mitigate variance in FGVC results.",
          "link": "http://arxiv.org/abs/2109.03156",
          "publishedOn": "2021-09-08T07:20:11.812Z",
          "wordCount": 699,
          "title": "Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zocco_F/0/1/0/all/0/1\">Federico Zocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1\">Beatrice Smyth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1\">Se&#xe1;n McLoone</a>",
          "description": "Long-term availability of minerals and industrial materials is a necessary\ncondition for sustainable development as they are the constituents of any\nmanufacturing product. To enhance the efficiency of material management, we\ndefine a computer-vision-enabled material measurement system and provide a\nsurvey of works relevant to its development with particular emphasis on the\nfoundations. A network of such systems for wide-area material stock monitoring\nis also covered. Finally, challenges and future research directions are\ndiscussed. As the first article bridging industrial ecology and advanced\ncomputer vision, this survey is intended to support both research communities\ntowards more sustainable manufacturing.",
          "link": "http://arxiv.org/abs/2103.01997",
          "publishedOn": "2021-09-08T07:20:11.805Z",
          "wordCount": 577,
          "title": "Material Measurement Units for a Circular Economy: Foundations through a Survey. (arXiv:2103.01997v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1\">Dominic Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danielyan_A/0/1/0/all/0/1\">Ara Danielyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Hang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golparvar_Fard_M/0/1/0/all/0/1\">Mani Golparvar-Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>",
          "description": "Generative models for 3D shapes represented by hierarchies of parts can\ngenerate realistic and diverse sets of outputs. However, existing models suffer\nfrom the key practical limitation of modelling shapes holistically and thus\ncannot perform conditional sampling, i.e. they are not able to generate\nvariants on individual parts of generated shapes without modifying the rest of\nthe shape. This is limiting for applications such as 3D CAD design that involve\nadjusting created shapes at multiple levels of detail. To address this, we\nintroduce LSD-StructureNet, an augmentation to the StructureNet architecture\nthat enables re-generation of parts situated at arbitrary positions in the\nhierarchies of its outputs. We achieve this by learning individual,\nprobabilistic conditional decoders for each hierarchy depth. We evaluate\nLSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes\nrepresented by hierarchies of parts. Our results show that contrarily to\nexisting methods, LSD-StructureNet can perform conditional sampling without\nimpacting inference speed or the realism and diversity of its outputs.",
          "link": "http://arxiv.org/abs/2108.13459",
          "publishedOn": "2021-09-08T07:20:11.799Z",
          "wordCount": 636,
          "title": "LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murillo_R/0/1/0/all/0/1\">Raul Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrio_A/0/1/0/all/0/1\">Alberto A. Del Barrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botella_G/0/1/0/all/0/1\">Guillermo Botella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyunJin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_N/0/1/0/all/0/1\">Nader Bagherzadeh</a>",
          "description": "The Posit Number System was introduced in 2017 as a replacement for\nfloating-point numbers. Since then, the community has explored its application\nin Neural Network related tasks and produced some unit designs which are still\nfar from being competitive with their floating-point counterparts. This paper\nproposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to\nsignificantly reduce the complexity of posit multipliers, the most power-hungry\nunits within Deep Neural Network architectures. When comparing with\nstate-of-the-art posit multipliers, experiments show that the proposed\ntechnique reduces the area, power, and delay of hardware multipliers up to\n72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.",
          "link": "http://arxiv.org/abs/2102.09262",
          "publishedOn": "2021-09-08T07:20:11.399Z",
          "wordCount": 579,
          "title": "PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06201",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">He-Liang Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhao_Y/0/1/0/all/0/1\">Youwei Zhao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1\">Yulin Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_S/0/1/0/all/0/1\">Shaowei Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liang_F/0/1/0/all/0/1\">Futian Liang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_J/0/1/0/all/0/1\">Jin Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1\">Min-Hsiu Hsieh</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rong_H/0/1/0/all/0/1\">Hao Rong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Peng_C/0/1/0/all/0/1\">Cheng-Zhi Peng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lu_C/0/1/0/all/0/1\">Chao-Yang Lu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Ao Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobo Zhu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_J/0/1/0/all/0/1\">Jian-Wei Pan</a>",
          "description": "Quantum machine learning is expected to be one of the first practical\napplications of near-term quantum devices. Pioneer theoretical works suggest\nthat quantum generative adversarial networks (GANs) may exhibit a potential\nexponential advantage over classical GANs, thus attracting widespread\nattention. However, it remains elusive whether quantum GANs implemented on\nnear-term quantum devices can actually solve real-world learning tasks. Here,\nwe devise a flexible quantum GAN scheme to narrow this knowledge gap, which\ncould accomplish image generation with arbitrarily high-dimensional features,\nand could also take advantage of quantum superposition to train multiple\nexamples in parallel. For the first time, we experimentally achieve the\nlearning and generation of real-world hand-written digit images on a\nsuperconducting quantum processor. Moreover, we utilize a gray-scale bar\ndataset to exhibit the competitive performance between quantum GANs and the\nclassical GANs based on multilayer perceptron and convolutional neural network\narchitectures, respectively, benchmarked by the Fr\\'echet Distance score. Our\nwork provides guidance for developing advanced quantum generative models on\nnear-term quantum devices and opens up an avenue for exploring quantum\nadvantages in various GAN-related learning tasks.",
          "link": "http://arxiv.org/abs/2010.06201",
          "publishedOn": "2021-09-08T07:20:11.319Z",
          "wordCount": 718,
          "title": "Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15753",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.",
          "link": "http://arxiv.org/abs/2106.15753",
          "publishedOn": "2021-09-08T07:20:11.293Z",
          "wordCount": 683,
          "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1\">Fernando Pereira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_G/0/1/0/all/0/1\">Gabriel Biscaro Cavallari</a>",
          "description": "Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.",
          "link": "http://arxiv.org/abs/2109.02752",
          "publishedOn": "2021-09-08T07:20:11.278Z",
          "wordCount": 590,
          "title": "Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "This paper studies the model compression problem of vision transformers.\nBenefit from the self-attention module, transformer architectures have shown\nextraordinary performance on many computer vision tasks. Although the network\nperformance is boosted, transformers are often required more computational\nresources including memory usage and the inference complexity. Compared with\nthe existing knowledge distillation approaches, we propose to excavate useful\ninformation from the teacher transformer through the relationship between\nimages and the divided patches. We then explore an efficient fine-grained\nmanifold distillation approach that simultaneously calculates cross-images,\ncross-patch, and random-selected manifolds in teacher and student models.\nExperimental results conducted on several benchmarks demonstrate the\nsuperiority of the proposed algorithm for distilling portable transformer\nmodels with higher performance. For example, our approach achieves 75.06% Top-1\naccuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which\noutperforms other ViT distillation methods.",
          "link": "http://arxiv.org/abs/2107.01378",
          "publishedOn": "2021-09-08T07:20:11.272Z",
          "wordCount": 627,
          "title": "Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>",
          "description": "In this paper, we propose a self-supervised approach for tumor segmentation.\nSpecifically, we advocate a zero-shot setting, where models from\nself-supervised learning should be directly applicable for the downstream task,\nwithout using any manual annotations whatsoever. We make the following\ncontributions. First, with careful examination on existing self-supervised\nlearning approaches, we reveal the surprising result that, given suitable data\naugmentation, models trained from scratch in fact achieve comparable\nperformance to those pre-trained with self-supervised learning. Second,\ninspired by the fact that tumors tend to be characterized independently to the\ncontexts, we propose a scalable pipeline for generating synthetic tumor data,\nand train a self-supervised model that minimises the generalisation gap with\nthe downstream task. Third, we conduct extensive ablation studies on different\ndownstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for\nliver tumor segmentation. While evaluating the model transferability for tumor\nsegmentation under a low-annotation regime, including an extreme case of\nzero-shot segmentation, the proposed approach demonstrates state-of-the-art\nperformance, substantially outperforming all existing self-supervised\napproaches, and opening up the usage of self-supervised learning in practical\nscenarios.",
          "link": "http://arxiv.org/abs/2109.03230",
          "publishedOn": "2021-09-08T07:20:11.239Z",
          "wordCount": 630,
          "title": "Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03141",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guanxiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_A/0/1/0/all/0/1\">Abbas Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreishah_A/0/1/0/all/0/1\">Abdallah Khreishah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jo Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ansari_N/0/1/0/all/0/1\">Nirwan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chengjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousef_M/0/1/0/all/0/1\">Mustafa Yousef</a>",
          "description": "Traffic management systems capture tremendous video data and leverage\nadvances in video processing to detect and monitor traffic incidents. The\ncollected data are traditionally forwarded to the traffic management center\n(TMC) for in-depth analysis and may thus exacerbate the network paths to the\nTMC. To alleviate such bottlenecks, we propose to utilize edge computing by\nequipping edge nodes that are close to cameras with computing resources (e.g.\ncloudlets). A cloudlet, with limited computing resources as compared to TMC,\nprovides limited video processing capabilities. In this paper, we focus on two\ncommon traffic monitoring tasks, congestion detection, and speed detection, and\npropose a two-tier edge computing based model that takes into account of both\nthe limited computing capability in cloudlets and the unstable network\ncondition to the TMC. Our solution utilizes two algorithms for each task, one\nimplemented at the edge and the other one at the TMC, which are designed with\nthe consideration of different computing resources. While the TMC provides\nstrong computation power, the video quality it receives depends on the\nunderlying network conditions. On the other hand, the edge processes very\nhigh-quality video but with limited computing resources. Our model captures\nthis trade-off. We evaluate the performance of the proposed two-tier model as\nwell as the traffic monitoring algorithms via test-bed experiments under\ndifferent weather as well as network conditions and show that our proposed\nhybrid edge-cloud solution outperforms both the cloud-only and edge-only\nsolutions.",
          "link": "http://arxiv.org/abs/2109.03141",
          "publishedOn": "2021-09-08T07:20:11.217Z",
          "wordCount": 695,
          "title": "Smart Traffic Monitoring System using Computer Vision and Edge Computing. (arXiv:2109.03141v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.02394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.",
          "link": "http://arxiv.org/abs/2007.02394",
          "publishedOn": "2021-09-08T07:20:11.195Z",
          "wordCount": 685,
          "title": "Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1\">Maja Taseska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Successful active speaker detection requires a three-stage pipeline: (i)\naudio-visual encoding for all speakers in the clip, (ii) inter-speaker relation\nmodeling between a reference speaker and the background speakers within each\nframe, and (iii) temporal modeling for the reference speaker. Each stage of\nthis pipeline plays an important role for the final performance of the created\narchitecture. Based on a series of controlled experiments, this work presents\nseveral practical guidelines for audio-visual active speaker detection.\nCorrespondingly, we present a new architecture called ASDNet, which achieves a\nnew state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%\noutperforming the second best with a large margin of 4.7%. Our code and\npretrained models are publicly available.",
          "link": "http://arxiv.org/abs/2106.03932",
          "publishedOn": "2021-09-08T07:20:11.133Z",
          "wordCount": 615,
          "title": "How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02965",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postnikov_A/0/1/0/all/0/1\">Aleksey Postnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamayunov_A/0/1/0/all/0/1\">Aleksander Gamayunov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>",
          "description": "The correct characterization of uncertainty when predicting human motion is\nequally important as the accuracy of this prediction. We present a new method\nto correctly predict the uncertainty associated with the predicted distribution\nof future trajectories. Our approach, CovariaceNet, is based on a Conditional\nGenerative Model with Gaussian latent variables in order to predict the\nparameters of a bi-variate Gaussian distribution. The combination of\nCovarianceNet with a motion prediction model results in a hybrid approach that\noutputs a uni-modal distribution. We will show how some state of the art\nmethods in motion prediction become overconfident when predicting uncertainty,\naccording to our proposed metric and validated in the ETH data-set\n\\cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which\nmakes our method suitable for applications that use predicted distributions,\ne.g., planning or decision making.",
          "link": "http://arxiv.org/abs/2109.02965",
          "publishedOn": "2021-09-08T07:20:11.099Z",
          "wordCount": 583,
          "title": "CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction. (arXiv:2109.02965v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawley_S/0/1/0/all/0/1\">Scott H. Hawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_A/0/1/0/all/0/1\">Andrew C. Morrison</a>",
          "description": "We train an object detector built from convolutional neural networks to count\ninterference fringes in elliptical antinode regions in frames of high-speed\nvideo recordings of transient oscillations in Caribbean steelpan drums\nilluminated by electronic speckle pattern interferometry (ESPI). The\nannotations provided by our model aim to contribute to the understanding of\ntime-dependent behavior in such drums by tracking the development of\nsympathetic vibration modes. The system is trained on a dataset of crowdsourced\nhuman-annotated images obtained from the Zooniverse Steelpan Vibrations\nProject. Due to the small number of human-annotated images and the ambiguity of\nthe annotation task, we also evaluate the model on a large corpus of synthetic\nimages whose properties have been matched to the real images by style transfer\nusing a Generative Adversarial Network. Applying the model to thousands of\nunlabeled video frames, we measure oscillations consistent with audio\nrecordings of these drum strikes. One unanticipated result is that sympathetic\noscillations of higher-octave notes significantly precede the rise in sound\nintensity of the corresponding second harmonic tones; the mechanism responsible\nfor this remains unidentified. This paper primarily concerns the development of\nthe predictive model; further exploration of the steelpan images and deeper\nphysical insights await its further application.",
          "link": "http://arxiv.org/abs/2102.00632",
          "publishedOn": "2021-09-08T07:20:11.078Z",
          "wordCount": 714,
          "title": "ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02920",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xin Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_H/0/1/0/all/0/1\">Hong Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_X/0/1/0/all/0/1\">Xiangran Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1\">Yun Gu</a>",
          "description": "3D Convolutional Neural Networks (CNNs) have been widely adopted for airway\nsegmentation. The performance of 3D CNNs is greatly influenced by the dataset\nwhile the public airway datasets are mainly clean CT scans with coarse\nannotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19\nCT scans). In this work, we proposed a new dual-stream network to address the\nvariability between the clean domain and noisy domain, which utilizes the clean\nCT scans and a small amount of labeled noisy CT scans for airway segmentation.\nWe designed two different encoders to extract the transferable clean features\nand the unique noisy features separately, followed by two independent decoders.\nFurther on, the transferable features are refined by the channel-wise feature\nrecalibration and Signed Distance Map (SDM) regression. The feature\nrecalibration module emphasizes critical features and the SDM pays more\nattention to the bronchi, which is beneficial to extracting the transferable\ntopological features robust to the coarse labels. Extensive experimental\nresults demonstrated the obvious improvement brought by our proposed method.\nCompared to other state-of-the-art transfer learning methods, our method\naccurately segmented more bronchi in the noisy CT scans.",
          "link": "http://arxiv.org/abs/2109.02920",
          "publishedOn": "2021-09-08T07:20:11.054Z",
          "wordCount": 700,
          "title": "FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation. (arXiv:2109.02920v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02785",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Banerjee_S/0/1/0/all/0/1\">Subhashis Banerjee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mitra_S/0/1/0/all/0/1\">Sushmita Mitra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hall_L/0/1/0/all/0/1\">Lawrence O. Hall</a>",
          "description": "Prediction of Overall Survival (OS) of brain cancer patients from multi-modal\nMRI is a challenging field of research. Most of the existing literature on\nsurvival prediction is based on Radiomic features, which does not consider\neither non-biological factors or the functional neurological status of the\npatient(s). Besides, the selection of an appropriate cut-off for survival and\nthe presence of censored data create further problems. Application of deep\nlearning models for OS prediction is also limited due to the lack of large\nannotated publicly available datasets. In this scenario we analyse the\npotential of two novel neuroimaging feature families, extracted from brain\nparcellation atlases and spatial habitats, along with classical radiomic and\ngeometric features; to study their combined predictive power for analysing\noverall survival. A cross validation strategy with grid search is proposed to\nsimultaneously select and evaluate the most predictive feature subset based on\nits predictive power. A Cox Proportional Hazard (CoxPH) model is employed for\nunivariate feature selection, followed by the prediction of patient-specific\nsurvival functions by three multivariate parsimonious models viz. Coxnet,\nRandom survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI\ndata used for this research was taken from two open-access collections TCGA-GBM\nand TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding\nsurvival data for each patient was downloaded from The Cancer Genome Atlas\n(TCGA). A high cross validation $C-index$ score of $0.82\\pm.10$ was achieved\nusing RSF with the best $24$ selected features. Age was found to be the most\nimportant biological predictor. There were $9$, $6$, $6$ and $2$ features\nselected from the parcellation, habitat, radiomic and region-based feature\ngroups respectively.",
          "link": "http://arxiv.org/abs/2109.02785",
          "publishedOn": "2021-09-08T07:20:11.047Z",
          "wordCount": 726,
          "title": "Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guan-Nan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Kinship verification is a long-standing research challenge in computer\nvision. The visual differences presented to the face have a significant effect\non the recognition capabilities of the kinship systems. We argue that\naggregating multiple visual knowledge can better describe the characteristics\nof the subject for precise kinship identification. Typically, the age-invariant\nfeatures can represent more natural facial details. Such age-related\ntransformations are essential for face recognition due to the biological\neffects of aging. However, the existing methods mainly focus on employing the\nsingle-view image features for kinship identification, while more meaningful\nvisual properties such as race and age are directly ignored in the feature\nlearning step. To this end, we propose a novel deep collaborative multi-modal\nlearning (DCML) to integrate the underlying information presented in facial\nproperties in an adaptive manner to strengthen the facial details for effective\nunsupervised kinship verification. Specifically, we construct a well-designed\nadaptive feature fusion mechanism, which can jointly leverage the complementary\nproperties from different visual perspectives to produce composite features and\ndraw greater attention to the most informative components of spatial feature\nmaps. Particularly, an adaptive weighting strategy is developed based on a\nnovel attention mechanism, which can enhance the dependencies between different\nproperties by decreasing the information redundancy in channels in a\nself-adaptive manner. To validate the effectiveness of the proposed method,\nextensive experimental evaluations conducted on four widely-used datasets show\nthat our DCML method is always superior to some state-of-the-art kinship\nverification methods.",
          "link": "http://arxiv.org/abs/2109.02804",
          "publishedOn": "2021-09-08T07:20:11.027Z",
          "wordCount": 683,
          "title": "Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation. (arXiv:2109.02804v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veshki_F/0/1/0/all/0/1\">Farshad G. Veshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobyov_S/0/1/0/all/0/1\">Sergiy A. Vorobyov</a>",
          "description": "Convolutional sparse coding improves on the standard sparse approximation by\nincorporating a global shift-invariant model. The most efficient convolutional\nsparse coding methods are based on the alternating direction method of\nmultipliers and the convolution theorem. The only major difference between\nthese methods is how they approach a convolutional least-squares fitting\nsubproblem. This letter presents a solution to this subproblem, which improves\nthe efficiency of the state-of-the-art algorithms. We also use the same\napproach for developing an efficient convolutional dictionary learning method.\nFurthermore, we propose a novel algorithm for convolutional sparse coding with\na constraint on the approximation error.",
          "link": "http://arxiv.org/abs/2109.02969",
          "publishedOn": "2021-09-08T07:20:11.021Z",
          "wordCount": 541,
          "title": "Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gwilliam_M/0/1/0/all/0/1\">Matthew Gwilliam</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_S/0/1/0/all/0/1\">Srinidhi Hegde</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Tinubu_L/0/1/0/all/0/1\">Lade Tinubu</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hanson_A/0/1/0/all/0/1\">Alex Hanson</a> (1) ((1) University of Maryland, (2) University of Chicago)",
          "description": "Many existing works have made great strides towards reducing racial bias in\nface recognition. However, most of these methods attempt to rectify bias that\nmanifests in models during training instead of directly addressing a major\nsource of the bias, the dataset itself. Exceptions to this are\nBUPT-Balancedface/RFW and Fairface, but these works assume that primarily\ntraining on a single race or not racially balancing the dataset are inherently\ndisadvantageous. We demonstrate that these assumptions are not necessarily\nvalid. In our experiments, training on only African faces induced less bias\nthan training on a balanced distribution of faces and distributions skewed to\ninclude more African faces produced more equitable models. We additionally\nnotice that adding more images of existing identities to a dataset in place of\nadding new identities can lead to accuracy boosts across racial categories. Our\ncode is available at\nhttps://github.com/j-alex-hanson/rethinking-race-face-datasets",
          "link": "http://arxiv.org/abs/2109.03229",
          "publishedOn": "2021-09-08T07:20:11.015Z",
          "wordCount": 633,
          "title": "Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.14580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passananti_J/0/1/0/all/0/1\">Josephine Passananti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1\">Arjun Bhagoji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuanshun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>",
          "description": "Backdoor attacks embed hidden malicious behaviors into deep learning models,\nwhich only activate and cause misclassifications on model inputs containing a\nspecific trigger. Existing works on backdoor attacks and defenses, however,\nmostly focus on digital attacks that use digitally generated patterns as\ntriggers. A critical question remains unanswered: can backdoor attacks succeed\nusing physical objects as triggers, thus making them a credible threat against\ndeep learning systems in the real world? We conduct a detailed empirical study\nto explore this question for facial recognition, a critical deep learning task.\nUsing seven physical objects as triggers, we collect a custom dataset of 3205\nimages of ten volunteers and use it to study the feasibility of physical\nbackdoor attacks under a variety of real-world conditions. Our study reveals\ntwo key findings. First, physical backdoor attacks can be highly successful if\nthey are carefully configured to overcome the constraints imposed by physical\nobjects. In particular, the placement of successful triggers is largely\nconstrained by the target model's dependence on key facial features. Second,\nfour of today's state-of-the-art defenses against (digital) backdoors are\nineffective against physical backdoors, because the use of physical objects\nbreaks core assumptions used to construct these defenses. Our study confirms\nthat (physical) backdoor attacks are not a hypothetical phenomenon but rather\npose a serious real-world threat to critical classification tasks. We need new\nand more robust defenses against backdoors in the physical world.",
          "link": "http://arxiv.org/abs/2006.14580",
          "publishedOn": "2021-09-08T07:20:10.812Z",
          "wordCount": 759,
          "title": "Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.13858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuchen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yibing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Sheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhuo Xu</a>",
          "description": "Nowadays, target recognition technique plays an important role in many\nfields. However, the current target image information based methods suffer from\nthe influence of image quality and the time cost of image reconstruction. In\nthis paper, we propose a novel imaging-free target recognition method combining\nghost imaging (GI) and generative adversarial networks (GAN). Based on the\nmechanism of GI, a set of random speckles sequence is employed to illuminate\ntarget, and a bucket detector without resolution is utilized to receive echo\nsignal. The bucket signal sequence formed after continuous detections is\nconstructed into a bucket signal array, which is regarded as the sample of GAN.\nThen, conditional GAN is used to map bucket signal array and target category.\nIn practical application, the speckles sequence in training step is employed to\nilluminate target, and the bucket signal array is input GAN for recognition.\nThe proposed method can improve the problems caused by conventional recognition\nmethods that based on target image information, and provide a certain\nturbulence-free ability. Extensive experiments show that the proposed method\nachieves promising performance.",
          "link": "http://arxiv.org/abs/2103.13858",
          "publishedOn": "2021-09-08T07:20:10.805Z",
          "wordCount": 652,
          "title": "Generative-Adversarial-Networks-based Ghost Recognition. (arXiv:2103.13858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuning Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1\">Ruoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Cheng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaoguang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>",
          "description": "Optical Character Recognition (OCR) systems have been widely used in various\nof application scenarios. Designing an OCR system is still a challenging task.\nIn previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)\nto balance the accuracy against the efficiency. In order to improve the\naccuracy of PP-OCR and keep high efficiency, in this paper, we propose a more\nrobust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better\ntext detector and a better text recognizer, which include Collaborative Mutual\nLearning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual\nLearning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the\nprecision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.\nIt is also comparable to the server models of the PP-OCR which uses ResNet\nseries as backbones. All of the above mentioned models are open-sourced and the\ncode is available in the GitHub repository PaddleOCR which is powered by\nPaddlePaddle.",
          "link": "http://arxiv.org/abs/2109.03144",
          "publishedOn": "2021-09-08T07:20:10.778Z",
          "wordCount": 629,
          "title": "PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_K/0/1/0/all/0/1\">Katsuyuki Nakamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohashi_H/0/1/0/all/0/1\">Hiroki Ohashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1\">Mitsuhiro Okada</a>",
          "description": "Automatically describing video, or video captioning, has been widely studied\nin the multimedia field. This paper proposes a new task of sensor-augmented\negocentric-video captioning, a newly constructed dataset for it called MMAC\nCaptions, and a method for the newly proposed task that effectively utilizes\nmulti-modal data of video and motion sensors, or inertial measurement units\n(IMUs). While conventional video captioning tasks have difficulty in dealing\nwith detailed descriptions of human activities due to the limited view of a\nfixed camera, egocentric vision has greater potential to be used for generating\nthe finer-grained descriptions of human activities on the basis of a much\ncloser view. In addition, we utilize wearable-sensor data as auxiliary\ninformation to mitigate the inherent problems in egocentric vision: motion\nblur, self-occlusion, and out-of-camera-range activities. We propose a method\nfor effectively utilizing the sensor data in combination with the video data on\nthe basis of an attention mechanism that dynamically determines the modality\nthat requires more attention, taking the contextual information into account.\nWe compared the proposed sensor-fusion method with strong baselines on the MMAC\nCaptions dataset and found that using sensor data as supplementary information\nto the egocentric-video data was beneficial, and that our proposed method\noutperformed the strong baselines, demonstrating the effectiveness of the\nproposed method.",
          "link": "http://arxiv.org/abs/2109.02955",
          "publishedOn": "2021-09-08T07:20:10.772Z",
          "wordCount": 659,
          "title": "Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention. (arXiv:2109.02955v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "Significant advancements made in the generation of deepfakes have caused\nsecurity and privacy issues. Attackers can easily impersonate a person's\nidentity in an image by replacing his face with the target person's face.\nMoreover, a new domain of cloning human voices using deep-learning technologies\nis also emerging. Now, an attacker can generate realistic cloned voices of\nhumans using only a few seconds of audio of the target person. With the\nemerging threat of potential harm deepfakes can cause, researchers have\nproposed deepfake detection methods. However, they only focus on detecting a\nsingle modality, i.e., either video or audio. On the other hand, to develop a\ngood deepfake detector that can cope with the recent advancements in deepfake\ngeneration, we need to have a detector that can detect deepfakes of multiple\nmodalities, i.e., videos and audios. To build such a detector, we need a\ndataset that contains video and respective audio deepfakes. We were able to\nfind a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection\nDataset (FakeAVCeleb), that contains not only deepfake videos but synthesized\nfake audios as well. We used this multimodal deepfake dataset and performed\ndetailed baseline experiments using state-of-the-art unimodal, ensemble-based,\nand multimodal detection methods to evaluate it. We conclude through detailed\nexperimentation that unimodals, addressing only a single modality, video or\naudio, do not perform well compared to ensemble-based methods. Whereas purely\nmultimodal-based baselines provide the worst performance.",
          "link": "http://arxiv.org/abs/2109.02993",
          "publishedOn": "2021-09-08T07:20:10.762Z",
          "wordCount": 738,
          "title": "Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors. (arXiv:2109.02993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged as one of the key ingredients to reduce\nthe size of neural networks for their deployments to resource-limited devices.\nIn order to overcome the nature of transforming continuous activations and\nweights to discrete ones, recent study called Relaxed Quantization (RQ)\n[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that\nallows this transformation with efficient gradient-based optimization. However,\nRQ with this Gumbel-Softmax relaxation still suffers from bias-variance\ntrade-off depending on the temperature parameter of Gumbel-Softmax. To resolve\nthe issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses\nmulti-class straight-through estimator to effectively reduce the bias and\nvariance, along with a new regularization technique, DropBits that replaces\ndropout regularization to randomly drop the bits instead of neurons to further\nreduce the bias of the multi-class straight-through estimator in SRQ. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer\nusing DropBits. We experimentally validate our method on various benchmark\ndatasets and network architectures, and also support the quantized lottery\nticket hypothesis: learning heterogeneous quantization levels outperforms the\ncase using the same but fixed quantization levels from scratch.",
          "link": "http://arxiv.org/abs/1911.12990",
          "publishedOn": "2021-09-08T07:20:10.756Z",
          "wordCount": 719,
          "title": "Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Huy Q.Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_V/0/1/0/all/0/1\">Vi C.Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_A/0/1/0/all/0/1\">An T.Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D.Tran</a>",
          "description": "This paper contributes a new high-quality dataset for hand gesture\nrecognition in hand hygiene systems, named \"MFH\". Generally, current datasets\nare not focused on: (i) fine-grained actions; and (ii) data mismatch between\ndifferent viewpoints, which are available under realistic settings. To address\nthe aforementioned issues, the MFH dataset is proposed to contain a total of\n731147 samples obtained by different camera views in 6 non-overlapping\nlocations. Additionally, each sample belongs to one of seven steps introduced\nby the World Health Organization (WHO). As a minor contribution, inspired by\nadvances in fine-grained image recognition and distribution adaptation, this\npaper recommends using the self-supervised learning method to handle these\npreceding problems. The extensive experiments on the benchmarking MFH dataset\nshow that the introduced method yields competitive performance in both the\nAccuracy and the Macro F1-score. The code and the MFH dataset are available at\nhttps://github.com/willogy-team/hand-gesture-recognition-smc2021.",
          "link": "http://arxiv.org/abs/2109.02917",
          "publishedOn": "2021-09-08T07:20:10.748Z",
          "wordCount": 605,
          "title": "Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene. (arXiv:2109.02917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>",
          "description": "Knowledge distillation (KD) is an effective framework that aims to transfer\nmeaningful information from a large teacher to a smaller student. Generally, KD\noften involves how to define and transfer knowledge. Previous KD methods often\nfocus on mining various forms of knowledge, for example, feature maps and\nrefined information. However, the knowledge is derived from the primary\nsupervised task and thus is highly task-specific. Motivated by the recent\nsuccess of self-supervised representation learning, we propose an auxiliary\nself-supervision augmented task to guide networks to learn more meaningful\nfeatures. Therefore, we can derive soft self-supervision augmented\ndistributions as richer dark knowledge from this task for KD. Unlike previous\nknowledge, this distribution encodes joint knowledge from supervised and\nself-supervised feature learning. Beyond knowledge exploration, another crucial\naspect is how to learn and distill our proposed knowledge effectively. To fully\ntake advantage of hierarchical feature maps, we propose to append several\nauxiliary branches at various hidden layers. Each auxiliary branch is guided to\nlearn self-supervision augmented task and distill this distribution from\nteacher to student. Thus we call our KD method as Hierarchical Self-Supervision\nAugmented Knowledge Distillation (HSSAKD). Experiments on standard image\nclassification show that both offline and online HSSAKD achieves\nstate-of-the-art performance in the field of KD. Further transfer experiments\non object detection further verify that HSSAKD can guide the network to learn\nbetter features, which can be attributed to learn and distill an auxiliary\nself-supervision augmented task effectively.",
          "link": "http://arxiv.org/abs/2109.03075",
          "publishedOn": "2021-09-08T07:20:10.729Z",
          "wordCount": 694,
          "title": "Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.02412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qiuping Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichuan Liu</a>",
          "description": "We present a simple yet effective progressive self-guided loss function to\nfacilitate deep learning-based salient object detection (SOD) in images. The\nsaliency maps produced by the most relevant works still suffer from incomplete\npredictions due to the internal complexity of salient objects. Our proposed\nprogressive self-guided loss simulates a morphological closing operation on the\nmodel predictions for progressively creating auxiliary training supervisions to\nstep-wisely guide the training process. We demonstrate that this new loss\nfunction can guide the SOD model to highlight more complete salient objects\nstep-by-step and meanwhile help to uncover the spatial dependencies of the\nsalient object pixels in a region growing manner. Moreover, a new feature\naggregation module is proposed to capture multi-scale features and aggregate\nthem adaptively by a branch-wise attention mechanism. Benefiting from this\nmodule, our SOD framework takes advantage of adaptively aggregated multi-scale\nfeatures to locate and detect salient objects effectively. Experimental results\non several benchmark datasets show that our loss function not only advances the\nperformance of existing SOD models without architecture modification but also\nhelps our proposed framework to achieve state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2101.02412",
          "publishedOn": "2021-09-08T07:20:10.718Z",
          "wordCount": 657,
          "title": "Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccirilli_M/0/1/0/all/0/1\">Marco Piccirilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>",
          "description": "We consider the task of estimating 3D human pose and shape from videos. While\nexisting frame-based approaches have made significant progress, these methods\nare independently applied to each image, thereby often leading to inconsistent\npredictions. In this work, we present a video-based learning algorithm for 3D\nhuman pose and shape estimation. The key insights of our method are two-fold.\nFirst, to address the inconsistent temporal prediction issue, we exploit\ntemporal information in videos and propose a self-attention module that jointly\nconsiders short-range and long-range dependencies across frames, resulting in\ntemporally coherent estimations. Second, we model human motion with a\nforecasting module that allows the transition between adjacent frames to be\nsmooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M\ndatasets. Extensive experimental results show that our algorithm performs\nfavorably against the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2103.14182",
          "publishedOn": "2021-09-08T07:20:10.711Z",
          "wordCount": 623,
          "title": "Self-Attentive 3D Human Pose and Shape Estimation from Videos. (arXiv:2103.14182v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02643",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Hui Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhuang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_L/0/1/0/all/0/1\">Lianfa Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1\">Jun Lu</a>",
          "description": "Hyperspectral images (HSIs) can provide rich spatial and spectral information\nwith extensive application prospects. Recently, several methods using\nconvolutional neural networks (CNNs) to reconstruct HSIs have been developed.\nHowever, most deep learning methods fit a brute-force mapping relationship\nbetween the compressive and standard HSIs. Thus, the learned mapping would be\ninvalid when the observation data deviate from the training data. To recover\nthe three-dimensional HSIs from two-dimensional compressive images, we present\ndual-camera equipment with a physics-informed self-supervising CNN method based\non a coded aperture snapshot spectral imaging system. Our method effectively\nexploits the spatial-spectral relativization from the coded spectral\ninformation and forms a self-supervising system based on the camera quantum\neffect model. The experimental results show that our method can be adapted to a\nwide imaging environment with good performance. In addition, compared with most\nof the network-based methods, our system does not require a dedicated dataset\nfor pre-training. Therefore, it has greater scenario adaptability and better\ngeneralization ability. Meanwhile, our system can be constantly fine-tuned and\nself-improved in real-life scenarios.",
          "link": "http://arxiv.org/abs/2109.02643",
          "publishedOn": "2021-09-08T07:20:10.704Z",
          "wordCount": 638,
          "title": "End to end hyperspectral imaging system with coded compression imaging process. (arXiv:2109.02643v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>",
          "description": "Learning robust models that generalize well under changes in the data\ndistribution is critical for real-world applications. To this end, there has\nbeen a growing surge of interest to learn simultaneously from multiple training\ndomains - while enforcing different types of invariance across those domains.\nYet, all existing approaches fail to show systematic benefits under fair\nevaluation protocols. In this paper, we propose a new learning scheme to\nenforce domain invariance in the space of the gradients of the loss function:\nspecifically, we introduce a regularization term that matches the domain-level\nvariances of gradients across training domains. Critically, our strategy, named\nFishr, exhibits close relations with the Fisher Information and the Hessian of\nthe loss. We show that forcing domain-level gradient covariances to be similar\nduring the learning procedure eventually aligns the domain-level loss\nlandscapes locally around the final weights. Extensive experiments demonstrate\nthe effectiveness of Fishr for out-of-distribution generalization. In\nparticular, Fishr improves the state of the art on the DomainBed benchmark and\nperforms significantly better than Empirical Risk Minimization. The code is\nreleased at https://github.com/alexrame/fishr.",
          "link": "http://arxiv.org/abs/2109.02934",
          "publishedOn": "2021-09-08T07:20:10.696Z",
          "wordCount": 631,
          "title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00596",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1\">Karl Rohe</a>",
          "description": "Previous versions of sparse principal component analysis (PCA) have presumed\nthat the eigen-basis (a $p \\times k$ matrix) is approximately sparse. We\npropose a method that presumes the $p \\times k$ matrix becomes approximately\nsparse after a $k \\times k$ rotation. The simplest version of the algorithm\ninitializes with the leading $k$ principal components. Then, the principal\ncomponents are rotated with an $k \\times k$ orthogonal rotation to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nprincipal components. This approach differs from prior approaches because it\nuses an orthogonal rotation to approximate a sparse basis. One consequence is\nthat a sparse component need not to be a leading eigenvector, but rather a\nmixture of them. In this way, we propose a new (rotated) basis for sparse PCA.\nIn addition, our approach avoids \"deflation\" and multiple tuning parameters\nrequired for that. Our sparse PCA framework is versatile; for example, it\nextends naturally to a two-way analysis of a data matrix for simultaneous\ndimensionality reduction of rows and columns. We provide evidence showing that\nfor the same level of sparsity, the proposed sparse PCA method is more stable\nand can explain more variance compared to alternative methods. Through three\napplications -- sparse coding of images, analysis of transcriptome sequencing\ndata, and large-scale clustering of social networks, we demonstrate the modern\nusefulness of sparse PCA in exploring multivariate data.",
          "link": "http://arxiv.org/abs/2007.00596",
          "publishedOn": "2021-09-08T07:20:10.678Z",
          "wordCount": 702,
          "title": "A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1\">Suncheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yuzhuo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_M/0/1/0/all/0/1\">Mengyuan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>",
          "description": "Employing clustering strategy to assign unlabeled target images with pseudo\nlabels has become a trend for person re-identification (re-ID) algorithms in\ndomain adaptation. A potential limitation of these clustering-based methods is\nthat they always tend to introduce noisy labels, which will undoubtedly hamper\nthe performance of our re-ID system. To handle this limitation, an intuitive\nsolution is to utilize collaborative training to purify the pseudo label\nquality. However, there exists a challenge that the complementarity of two\nnetworks, which inevitably share a high similarity, becomes weakened gradually\nas training process goes on; worse still, these approaches typically ignore to\nconsider the self-discrepancy of intra-class relations. To address this issue,\nin this paper, we propose a multiple co-teaching framework for domain adaptive\nperson re-ID, opening up a promising direction about self-discrepancy problem\nunder unsupervised condition. On top of that, a mean-teaching mechanism is\nleveraged to enlarge the difference and discover more complementary features.\nComprehensive experiments conducted on several large-scale datasets show that\nour method achieves competitive performance compared with the\nstate-of-the-arts.",
          "link": "http://arxiv.org/abs/2104.02265",
          "publishedOn": "2021-09-08T07:20:10.671Z",
          "wordCount": 682,
          "title": "Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1\">Jomy John</a>",
          "description": "Handwriting recognition has been one of the most fascinating and challenging\nresearch areas in field of image processing and pattern recognition. It\ncontributes enormously to the improvement of automation process. In this paper,\na system for recognition of unconstrained handwritten Malayalam characters is\nproposed. A database of 10,000 character samples of 44 basic Malayalam\ncharacters is used in this work. A discriminate feature set of 64 local and 4\nglobal features are used to train and test SVM classifier and achieved 92.24%\naccuracy",
          "link": "http://arxiv.org/abs/2109.03081",
          "publishedOn": "2021-09-08T07:20:10.662Z",
          "wordCount": 530,
          "title": "Support Vector Machine for Handwritten Character Recognition. (arXiv:2109.03081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Caihua Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Longgang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yufeng Li</a>",
          "description": "Deep learning-based single image deraining (SID) with unpaired information is\nof immense importance, as relying on paired synthetic data often limits their\ngenerality and scalability in real-world applications. However, we noticed that\ndirect employ of unpaired adversarial learning and cycle-consistency\nconstraints in the SID task is insufficient to learn the underlying\nrelationship from rainy input to clean outputs, since the domain knowledge\nbetween rainy and rain-free images is asymmetrical. To address such limitation,\nwe develop an effective unpaired SID method which explores mutual properties of\nthe unpaired exemplars by a contrastive learning manner in a GAN framework,\nnamed as CDR-GAN. The proposed method mainly consists of two cooperative\nbranches: Bidirectional Translation Branch (BTB) and Contrastive Guidance\nBranch (CGB). Specifically, BTB takes full advantage of the circulatory\narchitecture of adversarial consistency to exploit latent feature distributions\nand guide transfer ability between two domains by equipping it with\nbidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings\nof different exemplars in rain space by encouraging the similar feature\ndistributions closer while pushing the dissimilar further away, in order to\nbetter help rain removal and image restoration. During training, we explore\nseveral loss functions to further constrain the proposed CDR-GAN. Extensive\nexperiments show that our method performs favorably against existing unpaired\nderaining approaches on both synthetic and real-world datasets, even\noutperforms several fully-supervised or semi-supervised models.",
          "link": "http://arxiv.org/abs/2109.02973",
          "publishedOn": "2021-09-08T07:20:10.654Z",
          "wordCount": 684,
          "title": "Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiansen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>",
          "description": "Transformers, the default model of choices in natural language processing,\nhave drawn scant attention from the medical imaging community. Given the\nability to exploit long-term dependencies, transformers are promising to help\natypical convolutional neural networks (convnets) to overcome its inherent\nshortcomings of spatial inductive bias. However, most of recently proposed\ntransformer-based segmentation approaches simply treated transformers as\nassisted modules to help encode global context into convolutional\nrepresentations without investigating how to optimally combine self-attention\n(i.e., the core of transformers) with convolution. To address this issue, in\nthis paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful\nsegmentation model with an interleaved architecture based on empirical\ncombination of self-attention and convolution. In practice, nnFormer learns\nvolumetric representations from 3D local volumes. Compared to the naive\nvoxel-level self-attention implementation, such volume-based operations help to\nreduce the computational complexity by approximate 98% and 99.5% on Synapse and\nACDC datasets, respectively. In comparison to prior-art network configurations,\nnnFormer achieves tremendous improvements over previous transformer-based\nmethods on two commonly used datasets Synapse and ACDC. For instance, nnFormer\noutperforms Swin-UNet by over 7 percents on Synapse. Even when compared to\nnnUNet, currently the best performing fully-convolutional medical segmentation\nnetwork, nnFormer still provides slightly better performance on Synapse and\nACDC.",
          "link": "http://arxiv.org/abs/2109.03201",
          "publishedOn": "2021-09-08T07:20:10.647Z",
          "wordCount": 657,
          "title": "nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jungkyoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jinyoung Moon</a>",
          "description": "Temporal moment localization aims to retrieve the best video segment matching\na moment specified by a query. The existing methods generate the visual and\nsemantic embeddings independently and fuse them without full consideration of\nthe long-term temporal relationship between them. To address these\nshortcomings, we introduce a novel recurrent unit, cross-modal long short-term\nmemory (CM-LSTM), by mimicking the human cognitive process of localizing\ntemporal moments that focuses on the part of a video segment related to the\npart of a query, and accumulates the contextual information across the entire\nvideo recurrently. In addition, we devise a two-stream attention mechanism for\nboth attended and unattended video features by the input query to prevent\nnecessary visual information from being neglected. To obtain more precise\nboundaries, we propose a two-stream attentive cross-modal interaction network\n(TACI) that generates two 2D proposal maps obtained globally from the\nintegrated contextual features, which are generated by using CM-LSTM, and\nlocally from boundary score sequences and then combines them into a final 2D\nmap in an end-to-end manner. On the TML benchmark dataset,\nActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1\nof 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we\nshow that the revised state-of-the-arts methods by replacing the original LSTM\nwith our CM-LSTM achieve performance gains.",
          "link": "http://arxiv.org/abs/2109.02925",
          "publishedOn": "2021-09-08T07:20:10.641Z",
          "wordCount": 662,
          "title": "Learning to Combine the Modalities of Language and Video for Temporal Moment Localization. (arXiv:2109.02925v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1\">Suman Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_M/0/1/0/all/0/1\">Manish Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keleras_L/0/1/0/all/0/1\">Laurynas Keleras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotwal_P/0/1/0/all/0/1\">Pranav Kotwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>",
          "description": "In this paper we present our solution to extract albedo of branded labels for\ne-commerce products. To this end, we generate a large-scale photo-realistic\nsynthetic data set for albedo extraction followed by training a generative\nmodel to translate images with diverse lighting conditions to albedo. We\nperformed an extensive evaluation to test the generalisation of our method to\nin-the-wild images. From the experimental results, we observe that our solution\ngeneralises well compared to the existing method both in the unseen rendered\nimages as well as in the wild image.",
          "link": "http://arxiv.org/abs/2109.02929",
          "publishedOn": "2021-09-08T07:20:10.622Z",
          "wordCount": 551,
          "title": "Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03082",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Ren Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>",
          "description": "This paper proposes a Perceptual Learned Video Compression (PLVC) approach\nwith recurrent conditional generative adversarial network. In our approach, the\nrecurrent auto-encoder-based generator learns to fully explore the temporal\ncorrelation for compressing video. More importantly, we propose a recurrent\nconditional discriminator, which judges raw and compressed video conditioned on\nboth spatial and temporal information, including the latent representation,\ntemporal motion and hidden states in recurrent cells. This way, in the\nadversarial training, it pushes the generated video to be not only spatially\nphoto-realistic but also temporally consistent with groundtruth and coherent\namong video frames. Therefore, the proposed PLVC model learns to compress video\ntowards good perceptual quality at low bit-rate. The experimental results show\nthat our PLVC approach outperforms the previous traditional and learned\napproaches on several perceptual quality metrics. The user study further\nvalidates the outstanding perceptual performance of PLVC in comparison with the\nlatest learned video compression approaches and the official HEVC test model\n(HM 16.20). The codes will be released at https://github.com/RenYang-home/PLVC.",
          "link": "http://arxiv.org/abs/2109.03082",
          "publishedOn": "2021-09-08T07:20:10.614Z",
          "wordCount": 617,
          "title": "Perceptual Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1812.01946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nimbhorkar_T/0/1/0/all/0/1\">Tushar Nimbhorkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baslamisli_A/0/1/0/all/0/1\">Anil S. Baslamisli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_S/0/1/0/all/0/1\">Sezer Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gevers_T/0/1/0/all/0/1\">Theo Gevers</a>",
          "description": "There hardly exists any large-scale datasets with dense optical flow of\nnon-rigid motion from real-world imagery as of today. The reason lies mainly in\nthe required setup to derive ground truth optical flows: a series of images\nwith known camera poses along its trajectory, and an accurate 3D model from a\ntextured scene. Human annotation is not only too tedious for large databases,\nit can simply hardly contribute to accurate optical flow. To circumvent the\nneed for manual annotation, we propose a framework to automatically generate\noptical flow from real-world videos. The method extracts and matches objects\nfrom video frames to compute initial constraints, and applies a deformation\nover the objects of interest to obtain dense optical flow fields. We propose\nseveral ways to augment the optical flow variations. Extensive experimental\nresults show that training on our automatically generated optical flow\noutperforms methods that are trained on rigid synthetic data using FlowNet-S,\nLiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow\ngeneration framework are released at https://github.com/lhoangan/arap_flow",
          "link": "http://arxiv.org/abs/1812.01946",
          "publishedOn": "2021-09-08T07:20:10.606Z",
          "wordCount": 692,
          "title": "Automatic Generation of Dense Non-rigid Optical Flow. (arXiv:1812.01946v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guan-Nan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Kinship verification from facial images has been recognized as an emerging\nyet challenging technique in many potential computer vision applications. In\nthis paper, we propose a novel cross-generation feature interaction learning\n(CFIL) framework for robust kinship verification. Particularly, an effective\ncollaborative weighting strategy is constructed to explore the characteristics\nof cross-generation relations by corporately extracting features of both\nparents and children image pairs. Specifically, we take parents and children as\na whole to extract the expressive local and non-local features. Different from\nthe traditional works measuring similarity by distance, we interpolate the\nsimilarity calculations as the interior auxiliary weights into the deep CNN\narchitecture to learn the whole and natural features. These similarity weights\nnot only involve corresponding single points but also excavate the multiple\nrelationships cross points, where local and non-local features are calculated\nby using these two kinds of distance measurements. Importantly, instead of\nseparately conducting similarity computation and feature extraction, we\nintegrate similarity learning and feature extraction into one unified learning\nprocess. The integrated representations deduced from local and non-local\nfeatures can comprehensively express the informative semantics embedded in\nimages and preserve abundant correlation knowledge from image pairs. Extensive\nexperiments demonstrate the efficiency and superiority of the proposed model\ncompared to some state-of-the-art kinship verification methods.",
          "link": "http://arxiv.org/abs/2109.02809",
          "publishedOn": "2021-09-08T07:20:10.598Z",
          "wordCount": 652,
          "title": "Kinship Verification Based on Cross-Generation Feature Interaction Learning. (arXiv:2109.02809v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristians Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_B/0/1/0/all/0/1\">Barbara Seeliger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1\">Pietro Mascagni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1\">Didier Mutter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marescaux_J/0/1/0/all/0/1\">Jacques Marescaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>",
          "description": "Out of all existing frameworks for surgical workflow analysis in endoscopic\nvideos, action triplet recognition stands out as the only one aiming to provide\ntruly fine-grained and comprehensive information on surgical activities. This\ninformation, presented as <instrument, verb, target> combinations, is highly\nchallenging to be accurately identified. Triplet components can be difficult to\nrecognize individually; in this task, it requires not only performing\nrecognition simultaneously for all three triplet components, but also correctly\nestablishing the data association between them. To achieve this task, we\nintroduce our new model, the Rendezvous (RDV), which recognizes triplets\ndirectly from surgical videos by leveraging attention at two different levels.\nWe first introduce a new form of spatial attention to capture individual action\ntriplet components in a scene; called the Class Activation Guided Attention\nMechanism (CAGAM). This technique focuses on the recognition of verbs and\ntargets using activations resulting from instruments. To solve the association\nproblem, our RDV model adds a new form of semantic attention inspired by\nTransformer networks. Using multiple heads of cross and self attentions, RDV is\nable to effectively capture relationships between instruments, verbs, and\ntargets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in\nwhich every frame has been annotated with labels from 100 triplet classes. Our\nproposed RDV model significantly improves the triplet prediction mAP by over 9%\ncompared to the state-of-the-art methods on this dataset.",
          "link": "http://arxiv.org/abs/2109.03223",
          "publishedOn": "2021-09-08T07:20:10.563Z",
          "wordCount": 715,
          "title": "Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>",
          "description": "We study the few-shot learning (FSL) problem, where a model learns to\nrecognize new objects with extremely few labeled training data per category.\nMost of previous FSL approaches resort to the meta-learning paradigm, where the\nmodel accumulates inductive bias through learning many training tasks so as to\nsolve a new unseen few-shot task. In contrast, we propose a simple approach to\nexploit unlabeled data accompanying the few-shot task for improving few-shot\nperformance. Firstly, we propose a Dependency Maximization method based on the\nHilbert-Schmidt norm of the cross-covariance operator, which maximizes the\nstatistical dependency between the embedded feature of those unlabeled data and\ntheir label predictions, together with the supervised loss over the support\nset. We then use the obtained model to infer the pseudo-labels for those\nunlabeled data. Furthermore, we propose anInstance Discriminant Analysis to\nevaluate the credibility of each pseudo-labeled example and select the most\nfaithful ones into an augmented support set to retrain the model as in the\nfirst step. We iterate the above process until the pseudo-labels for the\nunlabeled data becomes stable. Following the standard transductive and\nsemi-supervised FSL setting, our experiments show that the proposed method\nout-performs previous state-of-the-art methods on four widely used benchmarks,\nincluding mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.",
          "link": "http://arxiv.org/abs/2109.02820",
          "publishedOn": "2021-09-08T07:20:10.556Z",
          "wordCount": 656,
          "title": "Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03027",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Taheri_M/0/1/0/all/0/1\">Mohsen Taheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schulz_J/0/1/0/all/0/1\">J&#xf6;rn Schulz</a>",
          "description": "The alignment of shapes has been a crucial step in statistical shape\nanalysis, for example, in calculating mean shape, detecting locational\ndifferences between two shape populations, and classification. Procrustes\nalignment is the most commonly used method and state of the art. In this work,\nwe uncover that alignment might seriously affect the statistical analysis. For\nexample, alignment can induce false shape differences and lead to misleading\nresults and interpretations. We propose a novel hierarchical shape\nparameterization based on local coordinate systems. The local parameterized\nshapes are translation and rotation invariant. Thus, the inherent alignment\nproblems from the commonly used global coordinate system for shape\nrepresentation can be avoided using this parameterization. The new\nparameterization is also superior for shape deformation and simulation. The\nmethod's power is demonstrated on the hypothesis testing of simulated data as\nwell as the left hippocampi of patients with Parkinson's disease and controls.",
          "link": "http://arxiv.org/abs/2109.03027",
          "publishedOn": "2021-09-08T07:20:10.549Z",
          "wordCount": 597,
          "title": "Statistical analysis of locally parameterized shapes. (arXiv:2109.03027v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02865",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuewen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaman_S/0/1/0/all/0/1\">Svebor Karaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tetreault_J/0/1/0/all/0/1\">Joel Tetreault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaimes_A/0/1/0/all/0/1\">Alex Jaimes</a>",
          "description": "The task of news article image captioning aims to generate descriptive and\ninformative captions for news article images. Unlike conventional image\ncaptions that simply describe the content of the image in general terms, news\nimage captions follow journalistic guidelines and rely heavily on named\nentities to describe the image content, often drawing context from the whole\narticle they are associated with. In this work, we propose a new approach to\nthis task, motivated by caption guidelines that journalists follow. Our\napproach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),\nleverages the structure of captions to improve the generation quality and guide\nour representation design. Experimental results, including detailed ablation\nstudies, on two large-scale publicly available datasets show that JoGANIC\nsubstantially outperforms state-of-the-art methods both on caption generation\nand named entity related metrics.",
          "link": "http://arxiv.org/abs/2109.02865",
          "publishedOn": "2021-09-08T07:20:10.542Z",
          "wordCount": 575,
          "title": "Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hanming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaoyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Lewei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "Transformer, as a strong and flexible architecture for modelling long-range\nrelations, has been widely explored in vision tasks. However, when used in\nvideo inpainting that requires fine-grained representation, existed method\nstill suffers from yielding blurry edges in detail due to the hard patch\nsplitting. Here we aim to tackle this problem by proposing FuseFormer, a\nTransformer model designed for video inpainting via fine-grained feature fusion\nbased on novel Soft Split and Soft Composition operations. The soft split\ndivides feature map into many patches with given overlapping interval. On the\ncontrary, the soft composition operates by stitching different patches into a\nwhole feature map where pixels in overlapping regions are summed up. These two\nmodules are first used in tokenization before Transformer layers and\nde-tokenization after Transformer layers, for effective mapping between tokens\nand features. Therefore, sub-patch level information interaction is enabled for\nmore effective feature propagation between neighboring patches, resulting in\nsynthesizing vivid content for hole regions in videos. Moreover, in FuseFormer,\nwe elaborately insert the soft composition and soft split into the feed-forward\nnetwork, enabling the 1D linear layers to have the capability of modelling 2D\nstructure. And, the sub-patch level feature fusion ability is further enhanced.\nIn both quantitative and qualitative evaluations, our proposed FuseFormer\nsurpasses state-of-the-art methods. We also conduct detailed analysis to\nexamine its superiority.",
          "link": "http://arxiv.org/abs/2109.02974",
          "publishedOn": "2021-09-08T07:20:10.508Z",
          "wordCount": 682,
          "title": "FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting. (arXiv:2109.02974v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02736",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jiangpeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Luotao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zeman Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eicher_Miller_H/0/1/0/all/0/1\">Heather A. Eicher-Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Image-based dietary assessment refers to the process of determining what\nsomeone eats and how much energy and nutrients are consumed from visual data.\nFood classification is the first and most crucial step. Existing methods focus\non improving accuracy measured by the rate of correct classification based on\nvisual information alone, which is very challenging due to the high complexity\nand inter-class similarity of foods. Further, accuracy in food classification\nis conceptual as description of a food can always be improved. In this work, we\nintroduce a new food classification framework to improve the quality of\npredictions by integrating the information from multiple domains while\nmaintaining the classification accuracy. We apply a multi-task network based on\na hierarchical structure that uses both visual and nutrition domain specific\ninformation to cluster similar foods. Our method is validated on the modified\nVIPER-FoodNet (VFN) food image dataset by including associated energy and\nnutrient information. We achieve comparable classification accuracy with\nexisting methods that use visual information only, but with less error in terms\nof energy and nutrient values for the wrong predictions.",
          "link": "http://arxiv.org/abs/2109.02736",
          "publishedOn": "2021-09-08T07:20:10.418Z",
          "wordCount": 628,
          "title": "Improving Dietary Assessment Via Integrated Hierarchy Food Classification. (arXiv:2109.02736v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03115",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>",
          "description": "The study of functional brain connectivity (FC) is important for\nunderstanding the underlying mechanisms of many psychiatric disorders. Many\nrecent analyses adopt graph convolutional networks, to study non-linear\ninteractions between functionally-correlated states. However, although patterns\nof brain activation are known to be hierarchically organised in both space and\ntime, many methods have failed to extract powerful spatio-temporal features. To\novercome those challenges, and improve understanding of long-range functional\ndynamics, we translate an approach, from the domain of skeleton-based action\nrecognition, designed to model interactions across space and time. We evaluate\nthis approach using the Human Connectome Project (HCP) dataset on sex\nclassification and fluid intelligence prediction. To account for subject\ntopographic variability of functional organisation, we modelled functional\nconnectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.\nResults show a prediction accuracy of 94.4% for sex classification (an increase\nof 6.2% compared to other methods), and an improvement of correlation with\nfluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes\nspace and time separately. Results suggest that explicit encoding of\nspatio-temporal dynamics of brain functional activity may improve the precision\nwith which behavioural and cognitive phenotypes may be predicted in the future.",
          "link": "http://arxiv.org/abs/2109.03115",
          "publishedOn": "2021-09-08T07:20:10.347Z",
          "wordCount": 663,
          "title": "Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mahabubul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Satwik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_R/0/1/0/all/0/1\">Rasit Onur Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Swaroop Ghosh</a>",
          "description": "Image classification is a major application domain for conventional deep\nlearning (DL). Quantum machine learning (QML) has the potential to\nrevolutionize image classification. In any typical DL-based image\nclassification, we use convolutional neural network (CNN) to extract features\nfrom the image and multi-layer perceptron network (MLP) to create the actual\ndecision boundaries. On one hand, QML models can be useful in both of these\ntasks. Convolution with parameterized quantum circuits (Quanvolution) can\nextract rich features from the images. On the other hand, quantum neural\nnetwork (QNN) models can create complex decision boundaries. Therefore,\nQuanvolution and QNN can be used to create an end-to-end QML model for image\nclassification. Alternatively, we can extract image features separately using\nclassical dimension reduction techniques such as, Principal Components Analysis\n(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to\ntrain a QNN. We review two proposals on quantum-classical hybrid ML models for\nimage classification namely, Quanvolutional Neural Network and dimension\nreduction using a classical algorithm followed by QNN. Particularly, we make a\ncase for trainable filters in Quanvolution and CAE-based feature extraction for\nimage datasets (instead of dimension reduction using linear transformations\nsuch as, PCA). We discuss various design choices, potential opportunities, and\ndrawbacks of these models. We also release a Python-based framework to create\nand explore these hybrid models with a variety of design choices.",
          "link": "http://arxiv.org/abs/2109.02862",
          "publishedOn": "2021-09-08T07:20:10.341Z",
          "wordCount": 678,
          "title": "ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanji_T/0/1/0/all/0/1\">Tanaka Kanji</a>",
          "description": "Landmark-based robot self-localization has recently garnered interest as a\nhighly-compressive domain-invariant approach for performing visual place\nrecognition (VPR) across domains (e.g., time of day, weather, and season).\nHowever, landmark-based self-localization can be an ill-posed problem for a\npassive observer (e.g., manual robot control), as many viewpoints may not\nprovide an effective landmark view. In this study, we consider an active\nself-localization task by an active observer and present a novel reinforcement\nlearning (RL)-based next-best-view (NBV) planner. Our contributions are as\nfollows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based\ncompact scene description as SIMBAD (similarity-based pattern recognition) and\nfurther present its deep learning extension. (2) VPR-to-NBV knowledge transfer:\nWe address the challenge of RL under uncertainty (i.e., active\nself-localization) by transferring the state recognition ability of VPR to the\nNBV. (3) NNQL-based NBV: We regard the available VPR as the experience database\nby adapting nearest-neighbor approximation of Q-learning (NNQL). The result\nshows an extremely compact data structure that compresses both the VPR and NBV\ninto a single incremental inverted index. Experiments using the public NCLT\ndataset validated the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2109.02786",
          "publishedOn": "2021-09-08T07:20:10.335Z",
          "wordCount": 633,
          "title": "Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor. (arXiv:2109.02786v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>",
          "description": "In a regular open set detection problem, samples of known classes (also\ncalled closed set classes) are used to train a special classifier. In testing,\nthe classifier can (1) classify the test samples of known classes to their\nrespective classes and (2) also detect samples that do not belong to any of the\nknown classes (we say they belong to some unknown or open set classes). This\npaper studies the problem of zero-shot open-set detection, which still performs\nthe same two tasks in testing but has no training except using the given known\nclass names. This paper proposes a novel and yet simple method (called ZO-CLIP)\nto solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot\nclassification through multi-modal representation learning. It first extends\nthe pre-trained multi-modal model CLIP by training a text-based image\ndescription generator on top of CLIP. In testing, it uses the extended model to\ngenerate some candidate unknown class names for each test sample and computes a\nconfidence score based on both the known class names and candidate unknown\nclass names for zero-shot open set detection. Experimental results on 5\nbenchmark datasets for open set detection confirm that ZO-CLIP outperforms the\nbaselines by a large margin.",
          "link": "http://arxiv.org/abs/2109.02748",
          "publishedOn": "2021-09-08T07:20:10.327Z",
          "wordCount": 651,
          "title": "Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zobaed_S/0/1/0/all/0/1\">Sm Zobaed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabby_M/0/1/0/all/0/1\">Md Fazle Rabby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Istiaq Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Sazib Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_A/0/1/0/all/0/1\">Asif Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasib_K/0/1/0/all/0/1\">Khan Md. Hasib</a>",
          "description": "The rapid advancement in deep learning makes the differentiation of authentic\nand manipulated facial images and video clips unprecedentedly harder. The\nunderlying technology of manipulating facial appearances through deep\ngenerative approaches, enunciated as DeepFake that have emerged recently by\npromoting a vast number of malicious face manipulation applications.\nSubsequently, the need of other sort of techniques that can assess the\nintegrity of digital visual content is indisputable to reduce the impact of the\ncreations of DeepFake. A large body of research that are performed on DeepFake\ncreation and detection create a scope of pushing each other beyond the current\nstatus. This study presents challenges, research trends, and directions related\nto DeepFake creation and detection techniques by reviewing the notable research\nin the DeepFake domain to facilitate the development of more robust approaches\nthat could deal with the more advance DeepFake in the future.",
          "link": "http://arxiv.org/abs/2109.02874",
          "publishedOn": "2021-09-08T07:20:10.320Z",
          "wordCount": 609,
          "title": "DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02710",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>",
          "description": "Reducing traffic fatalities and serious injuries is a top priority of the US\nDepartment of Transportation. The computer vision (CV)-based crash anticipation\nin the near-crash phase is receiving growing attention. The ability to perceive\nfatal crash risks earlier is also critical because it will improve the\nreliability of crash anticipation. Yet, annotated image data for training a\nreliable AI model for the early visual perception of crash risks are not\nabundant. The Fatality Analysis Reporting System contains big data of fatal\ncrashes. It is a reliable data source for learning the relationship between\ndriving scene characteristics and fatal crashes to compensate for the\nlimitation of CV. Therefore, this paper develops a data analytics model, named\nscenario-wise, Spatio-temporal attention guidance, from fatal crash report\ndata, which can estimate the relevance of detected objects to fatal crashes\nfrom their environment and context information. First, the paper identifies\nfive sparse variables that allow for decomposing the 5-year fatal crash dataset\nto develop scenario-wise attention guidance. Then, exploratory analysis of\nlocation- and time-related variables of the crash report data suggests reducing\nfatal crashes to spatially defined groups. The group's temporal pattern is an\nindicator of the similarity of fatal crashes in the group. Hierarchical\nclustering and K-means clustering merge the spatially defined groups into six\nclusters according to the similarity of their temporal patterns. After that,\nassociation rule mining discovers the statistical relationship between the\ntemporal information of driving scenes with crash features, for each cluster.\nThe paper shows how the developed attention guidance supports the design and\nimplementation of a preliminary CV model that can identify objects of a\npossibility to involve in fatal crashes from their environment and context\ninformation.",
          "link": "http://arxiv.org/abs/2109.02710",
          "publishedOn": "2021-09-08T07:20:10.302Z",
          "wordCount": 765,
          "title": "Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks. (arXiv:2109.02710v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitas_I/0/1/0/all/0/1\">Ioannis Pitas</a>",
          "description": "Existing road pothole detection approaches can be classified as computer\nvision-based or machine learning-based. The former approaches typically employ\n2-D image analysis/understanding or 3-D point cloud modeling and segmentation\nalgorithms to detect road potholes from vision sensor data. The latter\napproaches generally address road pothole detection using convolutional neural\nnetworks (CNNs) in an end-to-end manner. However, road potholes are not\nnecessarily ubiquitous and it is challenging to prepare a large well-annotated\ndataset for CNN training. In this regard, while computer vision-based methods\nwere the mainstream research trend in the past decade, machine learning-based\nmethods were merely discussed. Recently, we published the first stereo\nvision-based road pothole detection dataset and a novel disparity\ntransformation algorithm, whereby the damaged and undamaged road areas can be\nhighly distinguished. However, there are no benchmarks currently available for\nstate-of-the-art (SoTA) CNNs trained using either disparity images or\ntransformed disparity images. Therefore, in this paper, we first discuss the\nSoTA CNNs designed for semantic segmentation and evaluate their performance for\nroad pothole detection with extensive experiments. Additionally, inspired by\ngraph neural network (GNN), we propose a novel CNN layer, referred to as graph\nattention layer (GAL), which can be easily deployed in any existing CNN to\noptimize image feature representations for semantic segmentation. Our\nexperiments compare GAL-DeepLabv3+, our best-performing implementation, with\nnine SoTA CNNs on three modalities of training data: RGB images, disparity\nimages, and transformed disparity images. The experimental results suggest that\nour proposed GAL-DeepLabv3+ achieves the best overall pothole detection\naccuracy on all training data modalities.",
          "link": "http://arxiv.org/abs/2109.02711",
          "publishedOn": "2021-09-08T07:20:10.295Z",
          "wordCount": 734,
          "title": "Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grewal_M/0/1/0/all/0/1\">Monika Grewal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiersma_J/0/1/0/all/0/1\">Jan Wiersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Westerveld_H/0/1/0/all/0/1\">Henrike Westerveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosman_P/0/1/0/all/0/1\">Peter A. N. Bosman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alderliesten_T/0/1/0/all/0/1\">Tanja Alderliesten</a>",
          "description": "Deformable Image Registration (DIR) can benefit from additional guidance\nusing corresponding landmarks in the images. However, the benefits thereof are\nlargely understudied, especially due to the lack of automatic detection methods\nfor corresponding landmarks in three-dimensional (3D) medical images. In this\nwork, we present a Deep Convolutional Neural Network (DCNN), called DCNN-Match,\nthat learns to predict landmark correspondences in 3D images in a\nself-supervised manner. We explored five variants of DCNN-Match that use\ndifferent loss functions and tested DCNN-Match separately as well as in\ncombination with the open-source registration software Elastix to assess its\nimpact on a common DIR approach. We employed lower-abdominal Computed\nTomography (CT) scans from cervical cancer patients: 121 pelvic CT scan pairs\ncontaining simulated elastic transformations and 11 pairs demonstrating\nclinical deformations. Our results show significant improvement in DIR\nperformance when landmark correspondences predicted by DCNN-Match were used in\ncase of simulated as well as clinical deformations. We also observed that the\nspatial distribution of the automatically identified landmarks and the\nassociated matching errors affect the extent of improvement in DIR. Finally,\nDCNN-Match was found to generalize well to Magnetic Resonance Imaging (MRI)\nscans without requiring retraining, indicating easy applicability to other\ndatasets.",
          "link": "http://arxiv.org/abs/2109.02722",
          "publishedOn": "2021-09-08T07:20:10.282Z",
          "wordCount": 663,
          "title": "Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration. (arXiv:2109.02722v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1\">Omid Poursaeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianxing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">SerNam Lim</a>",
          "description": "While deep neural networks have achieved remarkable success in various\ncomputer vision tasks, they often fail to generalize to new domains and subtle\nvariations of input images. Several defenses have been proposed to improve the\nrobustness against these variations. However, current defenses can only\nwithstand the specific attack used in training, and the models often remain\nvulnerable to other input variations. Moreover, these methods often degrade\nperformance of the model on clean images and do not generalize to out-of-domain\nsamples. In this paper we present Generative Adversarial Training, an approach\nto simultaneously improve the model's generalization to the test set and\nout-of-domain samples as well as its robustness to unseen adversarial attacks.\nInstead of altering a low-level pre-defined aspect of images, we generate a\nspectrum of low-level, mid-level and high-level changes using generative models\nwith a disentangled latent space. Adversarial training with these examples\nenable the model to withstand a wide range of attacks by observing a variety of\ninput alterations during training. We show that our approach not only improves\nperformance of the model on clean images and out-of-domain samples but also\nmakes it robust against unforeseen attacks and outperforms prior work. We\nvalidate effectiveness of our method by demonstrating results on various tasks\nsuch as classification, segmentation and object detection.",
          "link": "http://arxiv.org/abs/2109.02765",
          "publishedOn": "2021-09-08T07:20:10.275Z",
          "wordCount": 676,
          "title": "Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02840",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1\">Shuai Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan-Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bao-Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yicong Zhou</a>",
          "description": "Few-shot classification (FSC) is one of the most concerned hot issues in\nrecent years. The general setting consists of two phases: (1) Pre-train a\nfeature extraction model (FEM) with base data (has large amounts of labeled\nsamples). (2) Use the FEM to extract the features of novel data (with few\nlabeled samples and totally different categories from base data), then classify\nthem with the to-be-designed classifier. The adaptability of pre-trained FEM to\nnovel data determines the accuracy of novel features, thereby affecting the\nfinal classification performances. To this end, how to appraise the pre-trained\nFEM is the most crucial focus in the FSC community. It sounds like traditional\nClass Activate Mapping (CAM) based methods can achieve this by overlaying\nweighted feature maps. However, due to the particularity of FSC (e.g., there is\nno backpropagation when using the pre-trained FEM to extract novel features),\nwe cannot activate the feature map with the novel classes. To address this\nchallenge, we propose a simple, flexible method, dubbed as Class-Irrelevant\nMapping (CIM). Specifically, first, we introduce dictionary learning theory and\nview the channels of the feature map as the bases in a dictionary. Then we\nutilize the feature map to fit the feature vector of an image to achieve the\ncorresponding channel weights. Finally, we overlap the weighted feature map for\nvisualization to appraise the ability of pre-trained FEM on novel data. For\nfair use of CIM in evaluating different models, we propose a new measurement\nindex, called Feature Localization Accuracy (FLA). In experiments, we first\ncompare our CIM with CAM in regular tasks and achieve outstanding performances.\nNext, we use our CIM to appraise several classical FSC frameworks without\nconsidering the classification results and discuss them.",
          "link": "http://arxiv.org/abs/2109.02840",
          "publishedOn": "2021-09-08T07:20:10.244Z",
          "wordCount": 727,
          "title": "CIM: Class-Irrelevant Mapping for Few-Shot Classification. (arXiv:2109.02840v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+G_V/0/1/0/all/0/1\">Vijay Kumar B G</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1\">Jeyasri Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chordia_V/0/1/0/all/0/1\">Varnith Chordia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bart_E/0/1/0/all/0/1\">Eugene Bart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1\">Shaobo Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_K/0/1/0/all/0/1\">Kelly Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_R/0/1/0/all/0/1\">Raja Bala</a>",
          "description": "We propose replacing scene text in videos using deep style transfer and\nlearned photometric transformations.Building on recent progress on still image\ntext replacement,we present extensions that alter text while preserving the\nappearance and motion characteristics of the original video.Compared to the\nproblem of still image text replacement,our method addresses additional\nchallenges introduced by video, namely effects induced by changing lighting,\nmotion blur, diverse variations in camera-object pose over time,and\npreservation of temporal consistency. We parse the problem into three steps.\nFirst, the text in all frames is normalized to a frontal pose using a\nspatio-temporal trans-former network. Second, the text is replaced in a single\nreference frame using a state-of-art still-image text replacement method.\nFinally, the new text is transferred from the reference to remaining frames\nusing a novel learned image transformation network that captures lighting and\nblur effects in a temporally consistent manner. Results on synthetic and\nchallenging real videos show realistic text trans-fer, competitive quantitative\nand qualitative performance,and superior inference speed relative to\nalternatives. We introduce new synthetic and real-world datasets with paired\ntext objects. To the best of our knowledge this is the first attempt at deep\nvideo text replacement.",
          "link": "http://arxiv.org/abs/2109.02762",
          "publishedOn": "2021-09-08T07:20:10.238Z",
          "wordCount": 653,
          "title": "STRIVE: Scene Text Replacement In Videos. (arXiv:2109.02762v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02740",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mane_T/0/1/0/all/0/1\">Tejas Mane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayramova_A/0/1/0/all/0/1\">Aylar Bayramova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordohai_P/0/1/0/all/0/1\">Philippos Mordohai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardis_E/0/1/0/all/0/1\">Elena Bernardis</a>",
          "description": "We address the problem of estimating the shape of a person's head, defined as\nthe geometry of the complete head surface, from a video taken with a single\nmoving camera, and determining the alignment of the fitted 3D head for all\nvideo frames, irrespective of the person's pose. 3D head reconstructions\ncommonly tend to focus on perfecting the face reconstruction, leaving the scalp\nto a statistical approximation. Our goal is to reconstruct the head model of\neach person to enable future mixed reality applications. To do this, we recover\na dense 3D reconstruction and camera information via structure-from-motion and\nmulti-view stereo. These are then used in a new two-stage fitting process to\nrecover the 3D head shape by iteratively fitting a 3D morphable model of the\nhead with the dense reconstruction in canonical space and fitting it to each\nperson's head, using both traditional facial landmarks and scalp features\nextracted from the head's segmentation mask. Our approach recovers consistent\ngeometry for varying head shapes, from videos taken by different people, with\ndifferent smartphones, and in a variety of environments from living rooms to\noutdoor spaces.",
          "link": "http://arxiv.org/abs/2109.02740",
          "publishedOn": "2021-09-08T07:20:10.232Z",
          "wordCount": 634,
          "title": "Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1\">Georgios Albanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drakoulis_P/0/1/0/all/0/1\">Petros Drakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkitsas_V/0/1/0/all/0/1\">Vasileios Gkitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterzentsenko_V/0/1/0/all/0/1\">Vladimiros Sterzentsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_F/0/1/0/all/0/1\">Federico Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>",
          "description": "Pano3D is a new benchmark for depth estimation from spherical panoramas. It\naims to assess performance across all depth estimation traits, the primary\ndirect depth estimation performance targeting precision and accuracy, and also\nthe secondary traits, boundary preservation, and smoothness. Moreover, Pano3D\nmoves beyond typical intra-dataset evaluation to inter-dataset performance\nassessment. By disentangling the capacity to generalize to unseen data into\ndifferent test splits, Pano3D represents a holistic benchmark for $360^o$ depth\nestimation. We use it as a basis for an extended analysis seeking to offer\ninsights into classical choices for depth estimation. This results in a solid\nbaseline for panoramic depth that follow-up works can build upon to steer\nfuture progress.",
          "link": "http://arxiv.org/abs/2109.02749",
          "publishedOn": "2021-09-08T07:20:10.219Z",
          "wordCount": 597,
          "title": "Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reedha_R/0/1/0/all/0/1\">Reenul Reedha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dericquebourg_E/0/1/0/all/0/1\">Eric Dericquebourg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canals_R/0/1/0/all/0/1\">Raphael Canals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiane_A/0/1/0/all/0/1\">Adel Hafiane</a>",
          "description": "Crop and weed monitoring is an important challenge for agriculture and food\nproduction nowadays. Thanks to recent advances in data acquisition and\ncomputation technologies, agriculture is evolving to a more smart and precision\nfarming to meet with the high yield and high quality crop production.\nClassification and recognition in Unmanned Aerial Vehicles (UAV) images are\nimportant phases for crop monitoring. Advances in deep learning models relying\non Convolutional Neural Network (CNN) have achieved high performances in image\nclassification in the agricultural domain. Despite the success of this\narchitecture, CNN still faces many challenges such as high computation cost,\nthe need of large labelled datasets, ... Natural language processing's\ntransformer architecture can be an alternative approach to deal with CNN's\nlimitations. Making use of the self-attention paradigm, Vision Transformer\n(ViT) models can achieve competitive or better results without applying any\nconvolution operations. In this paper, we adopt the self-attention mechanism\nvia the ViT models for plant classification of weeds and crops: red beet,\noff-type beet (green leaves), parsley and spinach. Our experiments show that\nwith small set of labelled training data, ViT models perform better compared to\nstate-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy\nof 99.8\\% achieved by the ViT model.",
          "link": "http://arxiv.org/abs/2109.02716",
          "publishedOn": "2021-09-08T07:20:10.166Z",
          "wordCount": 655,
          "title": "Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruwen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Min Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_B/0/1/0/all/0/1\">Bo Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengfa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Junxing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Miao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Degang Sun</a>",
          "description": "Graph convolutional networks (GCNs) achieve promising performance for\nskeleton-based action recognition. However, in most GCN-based methods, the\nspatial-temporal graph convolution is strictly restricted by the graph topology\nwhile only captures the short-term temporal context, thus lacking the\nflexibility of feature extraction. In this work, we present a novel\narchitecture, named Graph Convolutional skeleton Transformer (GCsT), which\naddresses limitations in GCNs by introducing Transformer. Our GCsT employs all\nthe benefits of Transformer (i.e. dynamical attention and global context) while\nkeeps the advantages of GCNs (i.e. hierarchy and local topology structure). In\nGCsT, the spatial-temporal GCN forces the capture of local dependencies while\nTransformer dynamically extracts global spatial-temporal relationships.\nFurthermore, the proposed GCsT shows stronger expressive capability by adding\nadditional information present in skeleton sequences. Incorporating the\nTransformer allows that information to be introduced into the model almost\neffortlessly. We validate the proposed GCsT by conducting extensive\nexperiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU\nRGB+D 120 and Northwestern-UCLA datasets.",
          "link": "http://arxiv.org/abs/2109.02860",
          "publishedOn": "2021-09-08T07:20:10.129Z",
          "wordCount": 620,
          "title": "GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_A/0/1/0/all/0/1\">Arun Balajee Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Jiri Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Humans can robustly recognize and localize objects by using visual and/or\nauditory cues. While machines are able to do the same with visual data already,\nless work has been done with sounds. This work develops an approach for scene\nunderstanding purely based on binaural sounds. The considered tasks include\npredicting the semantic masks of sound-making objects, the motion of\nsound-making objects, and the depth map of the scene. To this aim, we propose a\nnovel sensor setup and record a new audio-visual dataset of street scenes with\neight professional binaural microphones and a 360-degree camera. The\nco-existence of visual and audio cues is leveraged for supervision transfer. In\nparticular, we employ a cross-modal distillation framework that consists of\nmultiple vision teacher methods and a sound student method -- the student\nmethod is trained to generate the same results as the teacher methods do. This\nway, the auditory system can be trained without using human annotations. To\nfurther boost the performance, we propose another novel auxiliary task, coined\nSpatial Sound Super-Resolution, to increase the directional resolution of\nsounds. We then formulate the four tasks into one end-to-end trainable\nmulti-tasking network aiming to boost the overall performance. Experimental\nresults show that 1) our method achieves good results for all four tasks, 2)\nthe four tasks are mutually beneficial -- training them together achieves the\nbest performance, 3) the number and orientation of microphones are both\nimportant, and 4) features learned from the standard spectrogram and features\nobtained by the classic signal processing pipeline are complementary for\nauditory perception tasks. The data and code are released.",
          "link": "http://arxiv.org/abs/2109.02763",
          "publishedOn": "2021-09-08T07:20:10.079Z",
          "wordCount": 738,
          "title": "Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ignat_O/0/1/0/all/0/1\">Oana Ignat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castro_S/0/1/0/all/0/1\">Santiago Castro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_H/0/1/0/all/0/1\">Hanwen Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>",
          "description": "We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.",
          "link": "http://arxiv.org/abs/2109.02747",
          "publishedOn": "2021-09-08T07:20:10.072Z",
          "wordCount": 528,
          "title": "WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>",
          "description": "The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.",
          "link": "http://arxiv.org/abs/2106.13883",
          "publishedOn": "2021-09-07T20:22:05.164Z",
          "wordCount": 661,
          "title": "Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mostafa Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.",
          "link": "http://arxiv.org/abs/2106.13920",
          "publishedOn": "2021-09-07T20:22:05.155Z",
          "wordCount": 691,
          "title": "CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1\">Linhua Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengfu Wang</a>",
          "description": "We propose a simple yet reliable bottom-up approach with a good trade-off\nbetween accuracy and efficiency for the problem of multi-person pose\nestimation. Given an image, we employ an Hourglass Network to infer all the\nkeypoints from different persons indiscriminately as well as the guiding\noffsets connecting the adjacent keypoints belonging to the same persons. Then,\nwe greedily group the candidate keypoints into multiple human poses (if any),\nutilizing the predicted guiding offsets. And we refer to this process as greedy\noffset-guided keypoint grouping (GOG). Moreover, we revisit the\nencoding-decoding method for the multi-person keypoint coordinates and reveal\nsome important facts affecting accuracy. Experiments have demonstrated the\nobvious performance improvements brought by the introduced components. Our\napproach is comparable to the state of the art on the challenging COCO dataset\nunder fair conditions. The source code and our pre-trained model are publicly\navailable online.",
          "link": "http://arxiv.org/abs/2107.03098",
          "publishedOn": "2021-09-07T20:22:05.146Z",
          "wordCount": 627,
          "title": "Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bidur Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.",
          "link": "http://arxiv.org/abs/2106.15475",
          "publishedOn": "2021-09-07T20:22:05.138Z",
          "wordCount": 633,
          "title": "How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_Y/0/1/0/all/0/1\">Ygor C. N. Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassani_H/0/1/0/all/0/1\">Hansenclever F. Bassani</a>",
          "description": "Many works in the recent literature introduce semantic mapping methods that\nuse CNNs (Convolutional Neural Networks) to recognize semantic properties in\nimages. The types of properties (eg.: room size, place category, and objects)\nand their classes (eg.: kitchen and bathroom, for place category) are usually\npredefined and restricted to a specific task. Thus, all the visual data\nacquired and processed during the construction of the maps are lost and only\nthe recognized semantic properties remain on the maps. In contrast, this work\nintroduces a topological semantic mapping method that uses deep visual features\nextracted by a CNN (GoogLeNet), from 2D images captured in multiple views of\nthe environment as the robot operates, to create, through averages,\nconsolidated representations of the visual features acquired in the regions\ncovered by each topological node. These representations allow flexible\nrecognition of semantic properties of the regions and use in other visual\ntasks. Experiments with a real-world indoor dataset showed that the method is\nable to consolidate the visual features of regions and use them to recognize\nobjects and place categories as semantic properties, and to indicate the\ntopological location of images, with very promising results.",
          "link": "http://arxiv.org/abs/2106.12709",
          "publishedOn": "2021-09-07T20:22:05.122Z",
          "wordCount": 675,
          "title": "Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_S/0/1/0/all/0/1\">Sandeep Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>",
          "description": "Data augmentation is a widely used technique in classification to increase\ndata used in training. It improves generalization and reduces amount of\nannotated human activity data needed for training which reduces labour and time\nneeded with the dataset. Sensor time-series data, unlike images, cannot be\naugmented by computationally simple transformation algorithms. State of the art\nmodels like Recurrent Generative Adversarial Networks (RGAN) are used to\ngenerate realistic synthetic data. In this paper, transformer based generative\nadversarial networks which have global attention on data, are compared on\nPAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer\napproach provides improvements in time and savings in computational resources\nneeded for data augmentation than previous approach.",
          "link": "http://arxiv.org/abs/2109.01081",
          "publishedOn": "2021-09-07T20:22:05.111Z",
          "wordCount": 586,
          "title": "Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fast-growing subject, maturing in the\nfull reference (FR) case, yet challenging in the exploding no reference (NR)\ncase. We investigate variants of the popular VMAF video quality assessment\nalgorithm for the FR case, using both support vector regression and feedforward\nneural networks. We extend it to the NR case, using some different features but\nsimilar learning, to develop a partially unified framework for VQA. When fully\ntrained, FR algorithms such as VMAF perform well on test datasets, with 90%+\nmatch in PCC and SRCC; but for predicting performance in the wild, we\ntrain/test from scratch for each database. With an 80/20 train/test split, we\nstill achieve 90%+ performance on average in both PCC and SRCC, with 8-9% gains\nover VMAF. Moreover, we even get decent performance (~75%) if we ignore the\nreference, treating FR as NR, partly justifying our attempts at unification. In\nthe true NR case, we reduce complexity vs. leading recent algorithms VIDEVAL,\nRAPIQUE, yet achieve a stunning 90% in SRCC (~12% gain), while roughly matching\nin PCC (78% vs. 79.6%). At lower complexities, we can still achieve 87% in\nSRCC, 70% in PCC. In short, we find encouraging improvements in trainability in\nboth FR and NR, while also constraining computational complexity against\nleading methods",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-09-07T20:22:04.962Z",
          "wordCount": 737,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v6 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to describe certain\nmathematical results concerning the probability distribution of image pixel\nintensities characterized as generic $2D$ integer arrays. The size of the\nsmallest bounded region within an image is estimated for segmenting an image,\nfrom which, the equilibrium distribution of intensities can be recovered. From\nthe estimated bounded regions, properties of the sub-optimal and equilibrium\ndistributions of intensities are derived, which leads to an image compression\nmethodology whereby only slightly more than half of all pixels are required for\na worst-case reconstruction of the original image. A custom deep belief network\nand heuristic allows for the unsupervised segmentation, detection and\nlocalization of objects in an image. An example illustrates the mathematical\nresults.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-09-07T20:22:04.949Z",
          "wordCount": 697,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v12 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Dietary studies showed that dietary-related problem such as obesity is\nassociated with other chronic diseases like hypertension, irregular blood sugar\nlevels, and increased risk of heart attacks. The primary cause of these\nproblems is poor lifestyle choices and unhealthy dietary habits, which are\nmanageable using interactive mHealth apps. However, traditional dietary\nmonitoring systems using manual food logging suffer from imprecision,\nunderreporting, time consumption, and low adherence. Recent dietary monitoring\nsystems tackle these challenges by automatic assessment of dietary intake\nthrough machine learning methods. This survey discusses the most performing\nmethodologies that have been developed so far for automatic food recognition\nand volume estimation. First, we will present the rationale of visual-based\nmethods for food recognition. The core of the paper is the presentation,\ndiscussion and evaluation of these methods on popular food image databases.\nFollowing that, we discussed the mobile applications that are implementing\nthese methods. The survey ends with a discussion of research gaps and open\nissues in this area.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-09-07T20:22:04.921Z",
          "wordCount": 657,
          "title": "A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment. (arXiv:2106.11776v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-09-07T20:22:04.907Z",
          "wordCount": 782,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper, we tackle two important problems in low-rank learning, which\nare partial singular value decomposition and numerical rank estimation of huge\nmatrices. By using the concepts of Krylov subspaces such as Golub-Kahan\nbidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose\ntwo methods for solving these problems in a fast and accurate way. Our\nexperiments show the advantages of the proposed methods compared to the\ntraditional and randomized singular value decomposition methods. The proposed\nmethods are appropriate for applications involving huge matrices where the\naccuracy of the desired singular values and also all of their corresponding\nsingular vectors are essential. As a real application, we evaluate the\nperformance of our methods on the problem of Riemannian similarity learning\nbetween two various image datasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-09-07T20:22:04.883Z",
          "wordCount": 614,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_Y/0/1/0/all/0/1\">Ye-Ji Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Autonomous vehicles must reason about spatial occlusions in urban\nenvironments to ensure safety without being overly cautious. Prior work\nexplored occlusion inference from observed social behaviors of road agents.\nInferring occupancy from agent behaviors is an inherently multimodal problem; a\ndriver may behave in the same manner for different occupancy patterns ahead of\nthem (e.g., a driver may move at constant speed in traffic or on an open road).\nPast work, however, does not account for this multimodality, thus neglecting to\nmodel this source of aleatoric uncertainty in the relationship between driver\nbehaviors and their environment. We propose an occlusion inference method that\ncharacterizes observed behaviors of human agents as sensor measurements, and\nfuses them with those from a standard sensor suite. To capture the aleatoric\nuncertainty, we train a conditional variational autoencoder with a discrete\nlatent space to learn a multimodal mapping from observed driver trajectories to\nan occupancy grid representation of the view ahead of the driver. Our method\nhandles multi-agent scenarios, combining measurements from multiple observed\ndrivers using evidential theory to solve the sensor fusion problem. Our\napproach is validated on a real-world dataset, outperforming baselines and\ndemonstrating real-time capable performance. Our code is available at\nhttps://github.com/sisl/MultiAgentVariationalOcclusionInference .",
          "link": "http://arxiv.org/abs/2109.02173",
          "publishedOn": "2021-09-07T07:20:14.125Z",
          "wordCount": null,
          "title": "Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01770",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piao_Y/0/1/0/all/0/1\">Yongri Piao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengxuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>",
          "description": "Weakly-supervised salient object detection (WSOD) aims to develop saliency\nmodels using image-level annotations. Despite of the success of previous works,\nexplorations on an effective training strategy for the saliency network and\naccurate matches between image-level annotations and salient objects are still\ninadequate. In this work, 1) we propose a self-calibrated training strategy by\nexplicitly establishing a mutual calibration loop between pseudo labels and\nnetwork predictions, liberating the saliency network from error-prone\npropagation caused by pseudo labels. 2) we prove that even a much smaller\ndataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate\nmodels to achieve better performance as well as generalizability. This sheds\nnew light on the development of WSOD and encourages more contributions to the\ncommunity. Comprehensive experiments demonstrate that our method outperforms\nall the existing WSOD methods by adopting the self-calibrated strategy only.\nSteady improvements are further achieved by training on the proposed dataset.\nAdditionally, our method achieves 94.7% of the performance of fully-supervised\nmethods on average. And what is more, the fully supervised models adopting our\npredicted results as \"ground truths\" achieve successful results (95.6% for\nBASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling\ntime for pixel-level annotation.",
          "link": "http://arxiv.org/abs/2109.01770",
          "publishedOn": "2021-09-07T07:20:14.124Z",
          "wordCount": null,
          "title": "To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection. (arXiv:2109.01770v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-09-07T07:20:14.122Z",
          "wordCount": null,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01854",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1\">Yiran Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yonghao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Price_S/0/1/0/all/0/1\">Stephen J. Price</a>",
          "description": "Glioma is a common malignant brain tumor that shows distinct survival among\npatients. The isocitrate dehydrogenase (IDH) gene mutation status provides\ncritical diagnostic and prognostic value for glioma and is now accepted as the\nstandard of care. A non-invasive prediction of IDH mutation based on the\npre-treatment MRI has crucial clinical significance. Machine learning and deep\nlearning models show reasonable performance in predicting IDH mutation status.\nHowever, most models neglect the systematic brain alterations caused by tumor\ninvasion, where the infiltration along white matter tracts throughout the brain\nis identified as a hallmark of glioma. Structural brain network provides an\neffective tool to characterise brain organisation, which could be captured by\nthe graph neural networks (GNN) for a more accurate prediction of IDH mutation\nstatus.\n\nHere we propose a method to predict the IDH mutation using GNN, based on the\nstructural brain network of patients. Specifically, we firstly construct a\nnetwork template of healthy subjects, which consists of atlases of edges (white\nmatter tracts) and nodes (cortical and subcortical brain regions) to provide\nregions of interest (ROI). Next, we employ autoencoders to extract the latent\nmulti-modal MRI features from the ROIs of the edge and node in patients. These\nfeatures of edge and node of brain networks are used to train a GNN\narchitecture in predicting IDH mutation status. The results show that the\nproposed method outperforms the baseline models using 3D-CNN and 3D-DenseNet.\nIn addition, the model interpretation suggests its ability to identify the\ntracts infiltrated by tumor and corresponds to clinical prior knowledge. In\nconclusion, integrating brain networks with GNN offers a new avenue to study\nbrain lesions using computational neuroscience and computer vision approaches.",
          "link": "http://arxiv.org/abs/2109.01854",
          "publishedOn": "2021-09-07T07:20:14.119Z",
          "wordCount": null,
          "title": "Predicting isocitrate dehydrogenase mutationstatus in glioma using structural brain networksand graph neural networks. (arXiv:2109.01854v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fagbohungbe_O/0/1/0/all/0/1\">Omobayode Fagbohungbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_S/0/1/0/all/0/1\">Sheikh Rufsan Reza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>",
          "description": "In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.",
          "link": "http://arxiv.org/abs/2005.04563",
          "publishedOn": "2021-09-07T07:20:14.114Z",
          "wordCount": null,
          "title": "Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>",
          "description": "Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.",
          "link": "http://arxiv.org/abs/2109.02040",
          "publishedOn": "2021-09-07T07:20:14.102Z",
          "wordCount": null,
          "title": "Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1106.0107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jomy_J/0/1/0/all/0/1\">John Jomy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pramod_K/0/1/0/all/0/1\">K. V. Pramod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_B/0/1/0/all/0/1\">Balakrishnan Kannan</a>",
          "description": "Handwritten character recognition is always a frontier area of research in\nthe field of pattern recognition and image processing and there is a large\ndemand for OCR on hand written documents. Even though, sufficient studies have\nperformed in foreign scripts like Chinese, Japanese and Arabic characters, only\na very few work can be traced for handwritten character recognition of Indian\nscripts especially for the South Indian scripts. This paper provides an\noverview of offline handwritten character recognition in South Indian Scripts,\nnamely Malayalam, Tamil, Kannada and Telungu.",
          "link": "http://arxiv.org/abs/1106.0107",
          "publishedOn": "2021-09-07T07:20:14.100Z",
          "wordCount": null,
          "title": "Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>",
          "description": "Segmenting each moving object instance in a scene is essential for many\napplications. But like many other computer vision tasks, this task performs\nwell in optimal weather, but then adverse weather tends to fail. To be robust\nin weather conditions, the usual way is to train network in data of given\nweather pattern or to fuse multiple sensors. We focus on a new possibility,\nthat is, to improve its resilience to weather interference through the\nnetwork's structural design. First, we propose a novel FPN structure called\nRiWFPN with a progressive top-down interaction and attention refinement module.\nRiWFPN can directly replace other FPN structures to improve the robustness of\nthe network in non-optimal weather conditions. Then we extend SOLOV2 to capture\ntemporal information in video to learn motion information, and propose a moving\nobject instance segmentation network with RiWFPN called RiWNet. Finally, in\norder to verify the effect of moving instance segmentation in different weather\ndisturbances, we propose a VKTTI-moving dataset which is a moving instance\nsegmentation dataset based on the VKTTI dataset, taking into account different\nweather scenes such as rain, fog, sunset, morning as well as overcast. The\nexperiment proves how RiWFPN improves the network's resilience to adverse\nweather effects compared to other FPN structures. We compare RiWNet to several\nother state-of-the-art methods in some challenging datasets, and RiWNet shows\nbetter performance especially under adverse weather conditions.",
          "link": "http://arxiv.org/abs/2109.01820",
          "publishedOn": "2021-09-07T07:20:14.099Z",
          "wordCount": null,
          "title": "RiWNet: A moving object instance segmentation Network being Robust in adverse Weather conditions. (arXiv:2109.01820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1\">Hasam Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1\">Shahroz Tariq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1\">Simon S. Woo</a>",
          "description": "While significant advancements have been made in the generation of deepfakes\nusing deep learning technologies, its misuse is a well-known issue now.\nDeepfakes can cause severe security and privacy issues as they can be used to\nimpersonate a person's identity in a video by replacing his/her face with\nanother person's face. Recently, a new problem of generating synthesized human\nvoice of a person is emerging, where AI-based deep learning models can\nsynthesize any person's voice requiring just a few seconds of audio. With the\nemerging threat of impersonation attacks using deepfake audios and videos, a\nnew generation of deepfake detectors is needed to focus on both video and audio\ncollectively. A large amount of good quality datasets is typically required to\ncapture the real-world scenarios to develop a competent deepfake detector.\nExisting deepfake datasets either contain deepfake videos or audios, which are\nracially biased as well. Hence, there is a crucial need for creating a good\nvideo as well as an audio deepfake dataset, which can be used to detect audio\nand video deepfake simultaneously. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset (FakeAVCeleb) that contains not only deepfake\nvideos but also respective synthesized lip-synced fake audios. We generate this\ndataset using the current most popular deepfake generation methods. We selected\nreal YouTube videos of celebrities with four racial backgrounds (Caucasian,\nBlack, East Asian, and South Asian) to develop a more realistic multimodal\ndataset that addresses racial bias and further help develop multimodal deepfake\ndetectors. We performed several experiments using state-of-the-art detection\nmethods to evaluate our deepfake dataset and demonstrate the challenges and\nusefulness of our multimodal Audio-Video deepfake dataset.",
          "link": "http://arxiv.org/abs/2108.05080",
          "publishedOn": "2021-09-07T07:20:14.099Z",
          "wordCount": null,
          "title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>",
          "description": "Detection of objects is extremely important in various aerial vision-based\napplications. Over the last few years, the methods based on convolution neural\nnetworks have made substantial progress. However, because of the large variety\nof object scales, densities, and arbitrary orientations, the current detectors\nstruggle with the extraction of semantically strong features for small-scale\nobjects by a predefined convolution kernel. To address this problem, we propose\nthe rotation equivariant feature image pyramid network (REFIPN), an image\npyramid network based on rotation equivariance convolution. The proposed model\nadopts single-shot detector in parallel with a lightweight image pyramid module\nto extract representative features and generate regions of interest in an\noptimization approach. The proposed network extracts feature in a wide range of\nscales and orientations by using novel convolution filters. These features are\nused to generate vector fields and determine the weight and angle of the\nhighest-scoring orientation for all spatial locations on an image. By this\napproach, the performance for small-sized object detection is enhanced without\nsacrificing the performance for large-sized object detection. The performance\nof the proposed model is validated on two commonly used aerial benchmarks and\nthe results show our proposed model can achieve state-of-the-art performance\nwith satisfactory efficiency.",
          "link": "http://arxiv.org/abs/2106.00880",
          "publishedOn": "2021-09-07T07:20:14.098Z",
          "wordCount": null,
          "title": "Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery. (arXiv:2106.00880v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_H/0/1/0/all/0/1\">Hyunsung Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunjoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>",
          "description": "Single image camera calibration is the task of estimating the camera\nparameters from a single input image, such as the vanishing points, focal\nlength, and horizon line. In this work, we propose Camera calibration\nTRansformer with Line-Classification (CTRL-C), an end-to-end neural\nnetwork-based approach to single image camera calibration, which directly\nestimates the camera parameters from an image and a set of line segments. Our\nnetwork adopts the transformer architecture to capture the global structure of\nan image with multi-modal inputs in an end-to-end manner. We also propose an\nauxiliary task of line classification to train the network to extract the\nglobal geometric information from lines effectively. Our experiments\ndemonstrate that CTRL-C outperforms the previous state-of-the-art methods on\nthe Google Street View and SUN360 benchmark datasets.",
          "link": "http://arxiv.org/abs/2109.02259",
          "publishedOn": "2021-09-07T07:20:14.097Z",
          "wordCount": null,
          "title": "CTRL-C: Camera calibration TRansformer with Line-Classification. (arXiv:2109.02259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02322",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Marques_R/0/1/0/all/0/1\">Rita Marques</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jesus_D/0/1/0/all/0/1\">Danilo Andrade De Jesus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breda_J/0/1/0/all/0/1\">Jo&#xe3;o Barbosa Breda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eijgen_J/0/1/0/all/0/1\">Jan Van Eijgen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stalmans_I/0/1/0/all/0/1\">Ingeborg Stalmans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walsum_T/0/1/0/all/0/1\">Theo van Walsum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1\">Stefan Klein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vaz_P/0/1/0/all/0/1\">Pedro G. Vaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brea_L/0/1/0/all/0/1\">Luisa S&#xe1;nchez Brea</a>",
          "description": "The optic nerve head represents the intraocular section of the optic nerve\n(ONH), which is prone to damage by intraocular pressure. The advent of optical\ncoherence tomography (OCT) has enabled the evaluation of novel optic nerve head\nparameters, namely the depth and curvature of the lamina cribrosa (LC).\nTogether with the Bruch's membrane opening minimum-rim-width, these seem to be\npromising optic nerve head parameters for diagnosis and monitoring of retinal\ndiseases such as glaucoma. Nonetheless, these optical coherence tomography\nderived biomarkers are mostly extracted through manual segmentation, which is\ntime-consuming and prone to bias, thus limiting their usability in clinical\npractice. The automatic segmentation of optic nerve head in OCT scans could\nfurther improve the current clinical management of glaucoma and other diseases.\n\nThis review summarizes the current state-of-the-art in automatic segmentation\nof the ONH in OCT. PubMed and Scopus were used to perform a systematic review.\nAdditional works from other databases (IEEE, Google Scholar and ARVO IOVS) were\nalso included, resulting in a total of 27 reviewed studies.\n\nFor each algorithm, the methods, the size and type of dataset used for\nvalidation, and the respective results were carefully analyzed. The results\nshow that deep learning-based algorithms provide the highest accuracy,\nsensitivity and specificity for segmenting the different structures of the ONH\nincluding the LC. However, a lack of consensus regarding the definition of\nsegmented regions, extracted parameters and validation approaches has been\nobserved, highlighting the importance and need of standardized methodologies\nfor ONH segmentation.",
          "link": "http://arxiv.org/abs/2109.02322",
          "publishedOn": "2021-09-07T07:20:14.092Z",
          "wordCount": null,
          "title": "Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review. (arXiv:2109.02322v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02171",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_W/0/1/0/all/0/1\">Wangbin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_L/0/1/0/all/0/1\">Liqun Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>",
          "description": "Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is\na crucial step for cardiac morphology and function analysis. However, automatic\nRV segmentation from MRI is still challenging, mainly due to the heterogeneous\nintensity, the complex variable shapes, and the unclear RV boundary. Moreover,\ncurrent methods for the RV segmentation tend to suffer from performance\ndegradation at the basal and apical slices of MRI. In this work, we propose an\nautomatic RV segmentation framework, where the information from long-axis (LA)\nviews is utilized to assist the segmentation of short-axis (SA) views via\ninformation transition. Specifically, we employed the transformed segmentation\nfrom LA views as a prior information, to extract the ROI from SA views for\nbetter segmentation. The information transition aims to remove the surrounding\nambiguous regions in the SA views. %, such as the tricuspid valve regions. We\ntested our model on a public dataset with 360 multi-center, multi-vendor and\nmulti-disease subjects that consist of both LA and SA MRIs. Our experimental\nresults show that including LA views can be effective to improve the accuracy\nof the SA segmentation. Our model is publicly available at\nhttps://github.com/NanYoMy/MMs-2.",
          "link": "http://arxiv.org/abs/2109.02171",
          "publishedOn": "2021-09-07T07:20:14.088Z",
          "wordCount": null,
          "title": "Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition. (arXiv:2109.02171v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.05248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_G/0/1/0/all/0/1\">Gregory Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frome_A/0/1/0/all/0/1\">Andrea Frome</a>",
          "description": "Deep neural network pruning and quantization techniques have demonstrated it\nis possible to achieve high levels of compression with surprisingly little\ndegradation to test set accuracy. However, this measure of performance conceals\nsignificant differences in how different classes and images are impacted by\nmodel compression techniques. We find that models with radically different\nnumbers of weights have comparable top-line performance metrics but diverge\nconsiderably in behavior on a narrow subset of the dataset. This small subset\nof data points, which we term Pruning Identified Exemplars (PIEs) are\nsystematically more impacted by the introduction of sparsity. Compression\ndisproportionately impacts model performance on the underrepresented long-tail\nof the data distribution. PIEs over-index on atypical or noisy images that are\nfar more challenging for both humans and algorithms to classify. Our work\nprovides intuition into the role of capacity in deep neural networks and the\ntrade-offs incurred by compression. An understanding of this disparate impact\nis critical given the widespread deployment of compressed models in the wild.",
          "link": "http://arxiv.org/abs/1911.05248",
          "publishedOn": "2021-09-07T07:20:14.076Z",
          "wordCount": null,
          "title": "What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>",
          "description": "Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR",
          "link": "http://arxiv.org/abs/2109.01949",
          "publishedOn": "2021-09-07T07:20:14.069Z",
          "wordCount": null,
          "title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01835",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Untracht_G/0/1/0/all/0/1\">Gavrielle R. Untracht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matos_R/0/1/0/all/0/1\">Rolando Matos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dikaios_N/0/1/0/all/0/1\">Nikolaos Dikaios</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bapir_M/0/1/0/all/0/1\">Mariam Bapir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Durrani_A/0/1/0/all/0/1\">Abdullah K. Durrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Butsabong_T/0/1/0/all/0/1\">Teemapron Butsabong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Campagnolo_P/0/1/0/all/0/1\">Paola Campagnolo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampson_D/0/1/0/all/0/1\">David D. Sampson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiss_C/0/1/0/all/0/1\">Christian Heiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampson_D/0/1/0/all/0/1\">Danuta M. Sampson</a>",
          "description": "Optical coherence tomography angiography (OCTA) performs non-invasive\nvisualization and characterization of microvasculature in research and clinical\napplications mainly in ophthalmology and dermatology. A wide variety of\ninstruments, imaging protocols, processing methods and metrics have been used\nto describe the microvasculature, such that comparing different study outcomes\nis currently not feasible. With the goal of contributing to standardization of\nOCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA\n(OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and\nquantitative analysis of en face OCTA maximum intensity projection images in a\nstandardized workflow. We present each analysis step, including optimization of\nfiltering and choice of segmentation algorithm, and definition of metrics. We\nperform quantitative analysis of OCTA images from different commercial and\nnon-commercial instruments and samples and show OCTAVA can accurately and\nreproducibly determine metrics for characterization of microvasculature. Wide\nadoption could enable studies and aggregation of data on a scale sufficient to\ndevelop reliable microvascular biomarkers for early detection, and to guide\ntreatment, of microvascular disease.",
          "link": "http://arxiv.org/abs/2109.01835",
          "publishedOn": "2021-09-07T07:20:14.054Z",
          "wordCount": null,
          "title": "OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images. (arXiv:2109.01835v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01132",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Krishnaswamy_D/0/1/0/all/0/1\">Deepa Krishnaswamy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hareendranathan_A/0/1/0/all/0/1\">Abhilash R. Hareendranathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suwatanaviroj_T/0/1/0/all/0/1\">Tan Suwatanaviroj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boulanger_P/0/1/0/all/0/1\">Pierre Boulanger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Becher_H/0/1/0/all/0/1\">Harald Becher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noga_M/0/1/0/all/0/1\">Michelle Noga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Punithakumar_K/0/1/0/all/0/1\">Kumaradevan Punithakumar</a>",
          "description": "Purpose: Echocardiography is commonly used as a non-invasive imaging tool in\nclinical practice for the assessment of cardiac function. However, delineation\nof the left ventricle is challenging due to the inherent properties of\nultrasound imaging, such as the presence of speckle noise and the low\nsignal-to-noise ratio. Methods: We propose a semi-automated segmentation\nalgorithm for the delineation of the left ventricle in temporal 3D\nechocardiography sequences. The method requires minimal user interaction and\nrelies on a diffeomorphic registration approach. Advantages of the method\ninclude no dependence on prior geometrical information, training data, or\nregistration from an atlas. Results: The method was evaluated using\nthree-dimensional ultrasound scan sequences from 18 patients from the\nMazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual\ndelineations provided by an expert cardiologist and four other registration\nalgorithms. The segmentation approach yielded the following results over the\ncardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff\ndistance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02).\nConclusions: The method performed well compared to the four other registration\nalgorithms.",
          "link": "http://arxiv.org/abs/2109.01132",
          "publishedOn": "2021-09-07T07:20:13.898Z",
          "wordCount": null,
          "title": "A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences. (arXiv:2109.01132v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nitsch_J/0/1/0/all/0/1\">Julia Nitsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senanayake_R/0/1/0/all/0/1\">Ransalu Senanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_J/0/1/0/all/0/1\">Juan Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Max Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_C/0/1/0/all/0/1\">Cesar Cadena</a>",
          "description": "Neural networks (NNs) are widely used for object classification in autonomous\ndriving. However, NNs can fail on input data not well represented by the\ntraining dataset, known as out-of-distribution (OOD) data. A mechanism to\ndetect OOD samples is important for safety-critical applications, such as\nautomotive perception, to trigger a safe fallback mode. NNs often rely on\nsoftmax normalization for confidence estimation, which can lead to high\nconfidences being assigned to OOD samples, thus hindering the detection of\nfailures. This paper presents a method for determining whether inputs are OOD,\nwhich does not require OOD data during training and does not increase the\ncomputational cost of inference. The latter property is especially important in\nautomotive applications with limited computational resources and real-time\nconstraints. Our proposed approach outperforms state-of-the-art methods on\nreal-world automotive datasets.",
          "link": "http://arxiv.org/abs/2011.01413",
          "publishedOn": "2021-09-07T07:20:13.876Z",
          "wordCount": null,
          "title": "Out-of-Distribution Detection for Automotive Perception. (arXiv:2011.01413v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tengfei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "We present a novel approach to reference-based super-resolution (RefSR) with\nthe focus on dual-camera super-resolution (DCSR), which utilizes reference\nimages for high-quality and high-fidelity results. Our proposed method\ngeneralizes the standard patch-based feature matching with spatial alignment\noperations. We further explore the dual-camera super-resolution that is one\npromising application of RefSR, and build a dataset that consists of 146 image\npairs from the main and telephoto cameras in a smartphone. To bridge the domain\ngaps between real-world images and the training images, we propose a\nself-supervised domain adaptation strategy for real-world images. Extensive\nexperiments on our dataset and a public benchmark demonstrate clear improvement\nachieved by our method over state of the art in both quantitative evaluation\nand visual comparisons.",
          "link": "http://arxiv.org/abs/2109.01349",
          "publishedOn": "2021-09-07T07:20:13.859Z",
          "wordCount": null,
          "title": "Dual-Camera Super-Resolution with Aligned Attention Modules. (arXiv:2109.01349v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_Sadiya_S/0/1/0/all/0/1\">Sari Saba-Sadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.",
          "link": "http://arxiv.org/abs/2109.01401",
          "publishedOn": "2021-09-07T07:20:13.848Z",
          "wordCount": null,
          "title": "CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-09-07T07:20:13.806Z",
          "wordCount": null,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_W/0/1/0/all/0/1\">William Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>",
          "description": "Targets are essential in problems such as object tracking in cluttered or\ntextureless environments, camera (and multi-sensor) calibration tasks, and\nsimultaneous localization and mapping (SLAM). Target shapes for these tasks\ntypically are symmetric (square, rectangular, or circular) and work well for\nstructured, dense sensor data such as pixel arrays (i.e., image). However,\nsymmetric shapes lead to pose ambiguity when using sparse sensor data such as\nLiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.\nThis paper introduces the concept of optimizing target shape to remove pose\nambiguity for LiDAR point clouds. A target is designed to induce large\ngradients at edge points under rotation and translation relative to the LiDAR\nto ameliorate the quantization uncertainty associated with point cloud\nsparseness. Moreover, given a target shape, we present a means that leverages\nthe target's geometry to estimate the target's vertices while globally\nestimating the pose. Both the simulation and the experimental results (verified\nby a motion capture system) confirm that by using the optimal shape and the\nglobal solver, we achieve centimeter error in translation and a few degrees in\nrotation even when a partially illuminated target is placed 30 meters away. All\nthe implementations and datasets are available at\nhttps://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.",
          "link": "http://arxiv.org/abs/2109.01181",
          "publishedOn": "2021-09-07T07:20:13.803Z",
          "wordCount": null,
          "title": "Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1\">Laura Elena Cu&#xe9; La Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sothe_C/0/1/0/all/0/1\">Camile Sothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feitosa_R/0/1/0/all/0/1\">Raul Queiroz Feitosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_C/0/1/0/all/0/1\">Cl&#xe1;udia Maria de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schimalski_M/0/1/0/all/0/1\">Marcos Benedito Schimalski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Dario Augusto Borges Oliveira</a>",
          "description": "This work proposes a multi-task fully convolutional architecture for tree\nspecies mapping in dense forests from sparse and scarce polygon-level\nannotations using hyperspectral UAV-borne data. Our model implements a partial\nloss function that enables dense tree semantic labeling outcomes from non-dense\ntraining samples, and a distance regression complementary task that enforces\ntree crown boundary constraints and substantially improves the model\nperformance. Our multi-task architecture uses a shared backbone network that\nlearns common representations for both tasks and two task-specific decoders,\none for the semantic segmentation output and one for the distance map\nregression. We report that introducing the complementary task boosts the\nsemantic segmentation performance compared to the single-task counterpart in up\nto 11% reaching an average user's accuracy of 88.63% and an average producer's\naccuracy of 88.59%, achieving state-of-art performance for tree species\nclassification in tropical forests.",
          "link": "http://arxiv.org/abs/2106.00799",
          "publishedOn": "2021-09-07T07:20:13.614Z",
          "wordCount": null,
          "title": "Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data. (arXiv:2106.00799v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.10837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shibo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LI_X/0/1/0/all/0/1\">Xiaohua LI</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Sanjeev T. Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_A/0/1/0/all/0/1\">Arindam Sanyal</a>",
          "description": "Spiking neural network (SNN) is interesting both theoretically and\npractically because of its strong bio-inspiration nature and potentially\noutstanding energy efficiency. Unfortunately, its development has fallen far\nbehind the conventional deep neural network (DNN), mainly because of difficult\ntraining and lack of widely accepted hardware experiment platforms. In this\npaper, we show that a deep temporal-coded SNN can be trained easily and\ndirectly over the benchmark datasets CIFAR10 and ImageNet, with testing\naccuracy within 1% of the DNN of equivalent size and architecture. Training\nbecomes similar to DNN thanks to the closed-form solution to the spiking\nwaveform dynamics. Considering that SNNs should be implemented in practical\nneuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2\nbits and with weights perturbed by random noise to demonstrate its robustness\nin practical applications. In addition, we develop a phase-domain signal\nprocessing circuit schematic to implement our spiking neuron with 90% gain of\nenergy efficiency over existing work. This paper demonstrates that the\ntemporal-coded deep SNN is feasible for applications with high performance and\nhigh energy efficient.",
          "link": "http://arxiv.org/abs/1909.10837",
          "publishedOn": "2021-09-07T07:20:13.611Z",
          "wordCount": null,
          "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>",
          "description": "We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. At the time of writing, our method\nranks first on the CARLA leaderboard, attaining a 25% higher driving score\nwhile using 40 times less data. Our method is also an order of magnitude more\nsample-efficient than state-of-the-art model-free reinforcement learning\ntechniques on navigational tasks in the ProcGen benchmark.",
          "link": "http://arxiv.org/abs/2105.00636",
          "publishedOn": "2021-09-07T07:20:13.609Z",
          "wordCount": null,
          "title": "Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shalmani_S/0/1/0/all/0/1\">Shervin Manzuri Shalmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_F/0/1/0/all/0/1\">Fei Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Rong Zheng</a>",
          "description": "Modern neural networks are powerful predictive models. However, when it comes\nto recognizing that they may be wrong about their predictions, they perform\npoorly. For example, for one of the most common activation functions, the ReLU\nand its variants, even a well-calibrated model can produce incorrect but high\nconfidence predictions. In the related task of action recognition, most current\nclassification methods are based on clip-level classifiers that densely sample\na given video for non-overlapping, same-sized clips and aggregate the results\nusing an aggregation function - typically averaging - to achieve video level\npredictions. While this approach has shown to be effective, it is sub-optimal\nin recognition accuracy and has a high computational overhead. To mitigate both\nthese issues, we propose the confidence distillation framework to teach a\nrepresentation of uncertainty of the teacher to the student sampler and divide\nthe task of full video prediction between the student and the teacher models.\nWe conduct extensive experiments on three action recognition datasets and\ndemonstrate that our framework achieves significant improvements in action\nrecognition accuracy (up to 20%) and computational efficiency (more than 40%).",
          "link": "http://arxiv.org/abs/2109.02137",
          "publishedOn": "2021-09-07T07:20:13.591Z",
          "wordCount": null,
          "title": "Efficient Action Recognition Using Confidence Distillation. (arXiv:2109.02137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1\">Fuwen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiangbo Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1\">Vicente Ordonez</a>",
          "description": "Instance-level image retrieval is the task of searching in a large database\nfor images that match an object in a query image. To address this task, systems\nusually rely on a retrieval step that uses global image descriptors, and a\nsubsequent step that performs domain-specific refinements or reranking by\nleveraging operations such as geometric verification based on local features.\nIn this work, we propose Reranking Transformers (RRTs) as a general model to\nincorporate both local and global features to rerank the matching images in a\nsupervised fashion and thus replace the relatively expensive process of\ngeometric verification. RRTs are lightweight and can be easily parallelized so\nthat reranking a set of top matching results can be performed in a single\nforward-pass. We perform extensive experiments on the Revisited Oxford and\nParis datasets, and the Google Landmarks v2 dataset, showing that RRTs\noutperform previous reranking approaches while using much fewer local\ndescriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs\ncan be optimized jointly with the feature extractor, which can lead to feature\nrepresentations tailored to downstream tasks and further accuracy improvements.\nThe code and trained models are publicly available at\nhttps://github.com/uvavision/RerankingTransformer.",
          "link": "http://arxiv.org/abs/2103.12236",
          "publishedOn": "2021-09-07T07:20:13.566Z",
          "wordCount": null,
          "title": "Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1\">Shiva Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadhawan_R/0/1/0/all/0/1\">Rohan Wadhawan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1\">Tapan K. Gandhi</a>",
          "description": "In the recent decade, high-throughput plant phenotyping techniques, which\ncombine non-invasive image analysis and machine learning, have been\nsuccessfully applied to identify and quantify plant health and diseases.\nHowever, these techniques usually do not consider the progressive nature of\nplant stress and often require images showing severe signs of stress to ensure\nhigh confidence detection, thereby reducing the feasibility for early detection\nand recovery of plants under stress. To overcome the problem mentioned above,\nwe propose a deep learning pipeline for the temporal analysis of the visual\nchanges induced in the plant due to stress and apply it to the specific water\nstress identification case in Chickpea plant shoot images. For this, we have\nconsidered an image dataset of two chickpea varieties JG-62 and Pusa-372, under\nthree water stress conditions; control, young seedling, and before flowering,\ncaptured over five months. We have employed a variant of Convolutional Neural\nNetwork - Long Short Term Memory (CNN-LSTM) network to learn spatio-temporal\npatterns from the chickpea plant dataset and use them for water stress\nclassification. Our model has achieved ceiling level classification performance\nof 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has\noutperformed the best reported time-invariant technique by at least 14% for\nboth JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our\nCNN-LSTM model has demonstrated robustness to noisy input, with a less than\n2.5% dip in average model accuracy and a small standard deviation about the\nmean for both species. Lastly, we have performed an ablation study to analyze\nthe performance of the CNN-LSTM model by decreasing the number of temporal\nsession data used for training.",
          "link": "http://arxiv.org/abs/2104.07911",
          "publishedOn": "2021-09-07T07:20:13.553Z",
          "wordCount": null,
          "title": "Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02244",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1\">Young Kyun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_N/0/1/0/all/0/1\">Nam Ik Cho</a>",
          "description": "Supervised deep learning-based hash and vector quantization are enabling fast\nand large-scale image retrieval systems. By fully exploiting label annotations,\nthey are achieving outstanding retrieval performances compared to the\nconventional methods. However, it is painstaking to assign labels precisely for\na vast amount of training data, and also, the annotation process is\nerror-prone. To tackle these issues, we propose the first deep unsupervised\nimage retrieval method dubbed Self-supervised Product Quantization (SPQ)\nnetwork, which is label-free and trained in a self-supervised manner. We design\na Cross Quantized Contrastive learning strategy that jointly learns codewords\nand deep visual descriptors by comparing individually transformed images\n(views). Our method analyzes the image contents to extract descriptive\nfeatures, allowing us to understand image representations for accurate\nretrieval. By conducting extensive experiments on benchmarks, we demonstrate\nthat the proposed method yields state-of-the-art results even without\nsupervised pretraining.",
          "link": "http://arxiv.org/abs/2109.02244",
          "publishedOn": "2021-09-07T07:20:13.526Z",
          "wordCount": null,
          "title": "Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. (arXiv:2109.02244v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fumagalli_G/0/1/0/all/0/1\">Gianmario Fumagalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_Y/0/1/0/all/0/1\">Yannick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymczyk_M/0/1/0/all/0/1\">Marcin Dymczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1\">Roland Siegwart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dube_R/0/1/0/all/0/1\">Renaud Dub&#xe9;</a>",
          "description": "Camera anomalies like rain or dust can severelydegrade image quality and its\nrelated tasks, such as localizationand segmentation. In this work we address\nthis importantissue by implementing a pre-processing step that can\neffectivelymitigate such artifacts in a real-time fashion, thus supportingthe\ndeployment of autonomous systems with limited computecapabilities. We propose a\nshallow generator with aggregation,trained in an adversarial setting to solve\nthe ill-posed problemof reconstructing the occluded regions. We add an enhancer\ntofurther preserve high-frequency details and image colorization.We also\nproduce one of the largest publicly available datasets1to train our\narchitecture and use realistic synthetic raindrops toobtain an improved\ninitialization of the model. We benchmarkour framework on existing datasets and\non our own imagesobtaining state-of-the-art results while enabling real-time\nper-formance, with up to 40x faster inference time than existingapproaches.",
          "link": "http://arxiv.org/abs/2109.01889",
          "publishedOn": "2021-09-07T07:20:13.501Z",
          "wordCount": null,
          "title": "Fast Image-Anomaly Mitigation for Autonomous Mobile Robots. (arXiv:2109.01889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1\">Dan Kondratyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiansen_E/0/1/0/all/0/1\">Eric Christiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movshovitz_Attias_Y/0/1/0/all/0/1\">Yair Movshovitz-Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1\">Elad Eban</a>",
          "description": "Committee-based models, i.e., model ensembles or cascades, are underexplored\nin recent work on developing efficient models. While committee-based models\nthemselves are not new, there lacks a systematic understanding of their\nefficiency in comparison with single models. To fill this gap, we conduct a\ncomprehensive analysis of the efficiency of committee-based models. We find\nthat committee-based models provide a complementary paradigm to achieve\nsuperior efficiency without tuning the architecture: even the most simplistic\nmethod for building ensembles or cascades from existing pre-trained networks\ncan attain a significant speedup and higher accuracy over state-of-the-art\nsingle models, and also outperforms sophisticated neural architecture search\nmethods (e.g., BigNAS). The superior efficiency of committee-based models holds\ntrue for several tasks, including image classification, video classification,\nand semantic segmentation, and various architecture families, such as\nEfficientNet, ResNet, MobileNetV2, and X3D.",
          "link": "http://arxiv.org/abs/2012.01988",
          "publishedOn": "2021-09-07T07:20:13.486Z",
          "wordCount": null,
          "title": "On the Surprising Efficiency of Committee-based Models. (arXiv:2012.01988v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged for their deployments to resource-limited\ndevices. Although recent studies have successfully discretized a full-precision\nnetwork, they still incur large quantization errors after training, thus giving\nrise to a significant performance gap between a full-precision network and its\nquantized counterpart. In this work, we propose a novel quantization method for\nneural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal\nquantization grids while naturally encouraging the underlying full-precision\nweights to gather around those quantization grids cohesively during training.\nThis property of CPQ is thanks to our two main ingredients that enable\ndifferentiable quantization: i) the use of the categorical distribution\ndesigned by a specific probabilistic parametrization in the forward pass and\nii) our proposed multi-class straight-through estimator (STE) in the backward\npass. Since our second component, multi-class STE, is intrinsically biased, we\nadditionally propose a new bit-drop technique, DropBits, that revises the\nstandard dropout regularization to randomly drop bits instead of neurons. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer by\nimposing an additional regularization on DropBits. We experimentally validate\nour method on various benchmark datasets and network architectures, and also\nsupport a new hypothesis for quantization: learning heterogeneous quantization\nlevels outperforms the case using the same but fixed quantization levels from\nscratch.",
          "link": "http://arxiv.org/abs/2109.02100",
          "publishedOn": "2021-09-07T07:20:13.482Z",
          "wordCount": null,
          "title": "Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pesso_U/0/1/0/all/0/1\">Uriya Pesso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibas_K/0/1/0/all/0/1\">Koby Bibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1\">Meir Feder</a>",
          "description": "Adversarial attacks have been shown to be highly effective at degrading the\nperformance of deep neural networks (DNNs). The most prominent defense is\nadversarial training, a method for learning a robust model. Nevertheless,\nadversarial training does not make DNNs immune to adversarial perturbations. We\npropose a novel solution by adopting the recently suggested Predictive\nNormalized Maximum Likelihood. Specifically, our defense performs adversarial\ntargeted attacks according to different hypotheses, where each hypothesis\nassumes a specific label for the test sample. Then, by comparing the hypothesis\nprobabilities, we predict the label. Our refinement process corresponds to\nrecent findings of the adversarial subspace properties. We extensively evaluate\nour approach on 16 adversarial attack benchmarks using ResNet-50,\nWideResNet-28, and a2-layer ConvNet trained with ImageNet, CIFAR10, and MNIST,\nshowing a significant improvement of up to 5.7%, 3.7%, and 0.6% respectively.",
          "link": "http://arxiv.org/abs/2109.01945",
          "publishedOn": "2021-09-07T07:20:13.471Z",
          "wordCount": null,
          "title": "Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness. (arXiv:2109.01945v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honglei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuanzhouhan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>",
          "description": "Data hiding is the procedure of encoding desired information into the cover\nimage to resist potential noises while ensuring the embedded image has few\nperceptual perturbations from the original one. Recently, with the tremendous\nsuccesses gained by deep neural networks in various fields, the researches of\ndata hiding with deep learning models have attracted an increasing number of\nattentions. In the data hiding task, each pixel of cover images should be\ntreated differently since they have divergent tolerabilities. The neglect of\nconsidering the sensitivity of each pixel will inevitably affect the model\nrobustness for information hiding. Targeting this problem, we propose a novel\ndeep data hiding scheme with Inverse Gradient Attention (IGA), combing the\nideas of adversarial learning and attention mechanism to endow different\nsensitivities for different pixels. With the proposed component, the model can\nspotlight pixels with more robustness for data hiding. Empirically, extensive\nexperiments show that the proposed model outperforms the state-of-the-art\nmethods on two prevalent datasets under multiple evaluations. Besides, we\nfurther identify and discuss the connections between the proposed inverse\ngradient attention and high-frequency regions within images.",
          "link": "http://arxiv.org/abs/2011.10850",
          "publishedOn": "2021-09-07T07:20:13.453Z",
          "wordCount": null,
          "title": "Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1\">Anindya Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukhmali Das</a>",
          "description": "Moving object detection is a crucial task in computer vision. Event-based\ncameras are bio-inspired cameras that work by mimicking the working of the\nhuman eye. These cameras have multiple advantages over conventional frame-based\ncameras, like reduced latency, HDR, reduced motion blur during high motion, low\npower consumption, etc. However, these advantages come at a high cost, as\nevent-based cameras are noise sensitive and have low resolution. Moreover, the\ntask of moving object detection in these cameras is difficult, as event-based\nsensors capture only the binary changes in brightness of a scene, lacking\nuseful visual features like texture and color. In this paper, we investigate\nthe application of the k-means clustering technique in detecting moving objects\nin event-based data. Experimental results in publicly available datasets using\nk-means show significant improvement in performance over the state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2109.01879",
          "publishedOn": "2021-09-07T07:20:13.436Z",
          "wordCount": null,
          "title": "Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelkarim_S/0/1/0/all/0/1\">Sherif Abdelkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aniket Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "Several approaches have been proposed in recent literature to alleviate the\nlong-tail problem, mainly in object classification tasks. In this paper, we\nmake the first large-scale study concerning the task of Long-Tail Visual\nRelationship Recognition (LTVRR). LTVRR aims at improving the learning of\nstructured visual relationships that come from the long-tail (e.g., \"rabbit\ngrazing on grass\"). In this setup, the subject, relation, and object classes\neach follow a long-tail distribution. To begin our study and make a future\nbenchmark for the community, we introduce two LTVRR-related benchmarks, dubbed\nVG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.\nWe use these benchmarks to study the performance of several state-of-the-art\nlong-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic\nhubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR\nsetup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top\nof existing models and despite being simple, our results show that they can\nremarkably improve the performance, especially on tail classes. Benchmarks,\ncode, and models have been made available at:\nhttps://github.com/Vision-CAIR/LTVRR.",
          "link": "http://arxiv.org/abs/2004.00436",
          "publishedOn": "2021-09-07T07:20:13.420Z",
          "wordCount": null,
          "title": "Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.10187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiahao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Duanshun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_D/0/1/0/all/0/1\">Dong Tian</a>",
          "description": "Topology matters. Despite the recent success of point cloud processing with\ngeometric deep learning, it remains arduous to capture the complex topologies\nof point cloud data with a learning model. Given a point cloud dataset\ncontaining objects with various genera, or scenes with multiple objects, we\npropose an autoencoder, TearingNet, which tackles the challenging task of\nrepresenting the point clouds using a fixed-length descriptor. Unlike existing\nworks directly deforming predefined primitives of genus zero (e.g., a 2D square\npatch) to an object-level point cloud, our TearingNet is characterized by a\nproposed Tearing network module and a Folding network module interacting with\neach other iteratively. Particularly, the Tearing network module learns the\npoint cloud topology explicitly. By breaking the edges of a primitive graph, it\ntears the graph into patches or with holes to emulate the topology of a target\npoint cloud, leading to faithful reconstructions. Experimentation shows the\nsuperiority of our proposal in terms of reconstructing point clouds as well as\ngenerating more topology-friendly representations than benchmarks.",
          "link": "http://arxiv.org/abs/2006.10187",
          "publishedOn": "2021-09-07T07:20:13.417Z",
          "wordCount": null,
          "title": "TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations. (arXiv:2006.10187v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on Spark AR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on the enhanced data sets and showing that they outperform\nequivalent systems trained on the original data sets (containing faces without\nmasks), when the test benchmark contains masked faces.",
          "link": "http://arxiv.org/abs/2109.01745",
          "publishedOn": "2021-09-07T07:20:13.416Z",
          "wordCount": null,
          "title": "A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02227",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiwu Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>",
          "description": "Learning from image-text data has demonstrated recent success for many\nrecognition tasks, yet is currently limited to visual features or individual\nvisual concepts such as objects. In this paper, we propose one of the first\nmethods that learn from image-sentence pairs to extract a graphical\nrepresentation of localized objects and their relationships within an image,\nknown as scene graph. To bridge the gap between images and texts, we leverage\nan off-the-shelf object detector to identify and localize object instances,\nmatch labels of detected regions to concepts parsed from captions, and thus\ncreate \"pseudo\" labels for learning scene graph. Further, we design a\nTransformer-based model to predict these \"pseudo\" labels via a masked token\nprediction task. Learning from only image-sentence pairs, our model achieves\n30% relative gain over a latest method trained with human-annotated unlocalized\nscene graphs. Our model also shows strong results for weakly and fully\nsupervised scene graph generation. In addition, we explore an open-vocabulary\nsetting for detecting scene graphs, and present the first result for open-set\nscene graph generation. Our code is available at\nhttps://github.com/YiwuZhong/SGG_from_NLS.",
          "link": "http://arxiv.org/abs/2109.02227",
          "publishedOn": "2021-09-07T07:20:13.368Z",
          "wordCount": null,
          "title": "Learning to Generate Scene Graph from Natural Language Supervision. (arXiv:2109.02227v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Adversarial training (AT) has been demonstrated as one of the most promising\ndefense methods against various adversarial attacks. To our knowledge, existing\nAT-based methods usually train with the locally most adversarial perturbed\npoints and treat all the perturbed points equally, which may lead to\nconsiderably weaker adversarial robust generalization on test data. In this\nwork, we introduce a new adversarial training framework that considers the\ndiversity as well as characteristics of the perturbed points in the vicinity of\nbenign samples. To realize the framework, we propose a Regional Adversarial\nTraining (RAT) defense method that first utilizes the attack path generated by\nthe typical iterative attack method of projected gradient descent (PGD), and\nconstructs an adversarial region based on the attack path. Then, RAT samples\ndiverse perturbed training points efficiently inside this region, and utilizes\na distance-aware label smoothing mechanism to capture our intuition that\nperturbed points at different locations should have different impact on the\nmodel performance. Extensive experiments on several benchmark datasets show\nthat RAT consistently makes significant improvement on standard adversarial\ntraining (SAT), and exhibits better robust generalization.",
          "link": "http://arxiv.org/abs/2109.00678",
          "publishedOn": "2021-09-07T07:20:13.329Z",
          "wordCount": null,
          "title": "Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1\">Zhihao Gu</a>",
          "description": "The rapid development of facial manipulation techniques has aroused public\nconcerns in recent years. Following the success of deep learning, existing\nmethods always formulate DeepFake video detection as a binary classification\nproblem and develop frame-based and video-based solutions. However, little\nattention has been paid to capturing the spatial-temporal inconsistency in\nforged videos. To address this issue, we term this task as a Spatial-Temporal\nInconsistency Learning (STIL) process and instantiate it into a novel STIL\nblock, which consists of a Spatial Inconsistency Module (SIM), a Temporal\nInconsistency Module (TIM), and an Information Supplement Module (ISM).\nSpecifically, we present a novel temporal modeling paradigm in TIM by\nexploiting the temporal difference over adjacent frames along with both\nhorizontal and vertical directions. And the ISM simultaneously utilizes the\nspatial information from SIM and temporal information from TIM to establish a\nmore comprehensive spatial-temporal representation. Moreover, our STIL block is\nflexible and could be plugged into existing 2D CNNs. Extensive experiments and\nvisualizations are presented to demonstrate the effectiveness of our method\nagainst the state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2109.01860",
          "publishedOn": "2021-09-07T07:20:13.311Z",
          "wordCount": null,
          "title": "Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiahao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_D/0/1/0/all/0/1\">Dong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetro_A/0/1/0/all/0/1\">Anthony Vetro</a>",
          "description": "Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D\npoint clouds, and 4D dynamic point clouds, have found a wide range of\napplications including immersive telepresence, autonomous driving,\nsurveillance, etc. Due to irregular sampling patterns of most geometric data,\ntraditional image/video processing methodologies are limited, while Graph\nSignal Processing (GSP) -- a fast-developing field in the signal processing\ncommunity -- enables processing signals that reside on irregular domains and\nplays a critical role in numerous applications of geometric data from low-level\nprocessing to high-level analysis. To further advance the research in this\nfield, we provide the first timely and comprehensive overview of GSP\nmethodologies for geometric data in a unified manner by bridging the\nconnections between geometric data and graphs, among the various geometric data\nmodalities, and with spectral/nodal graph filtering techniques. We also discuss\nthe recently developed Graph Neural Networks (GNNs) and interpret the operation\nof these networks from the perspective of GSP. We conclude with a brief\ndiscussion of open problems and challenges.",
          "link": "http://arxiv.org/abs/2008.01918",
          "publishedOn": "2021-09-07T07:20:13.287Z",
          "wordCount": null,
          "title": "Graph Signal Processing for Geometric Data and Beyond: Theory and Applications. (arXiv:2008.01918v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhengcong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat https://github.com/lizekang/DSTC10-MOD.",
          "link": "http://arxiv.org/abs/2109.01839",
          "publishedOn": "2021-09-07T07:20:13.187Z",
          "wordCount": null,
          "title": "Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>",
          "description": "Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.",
          "link": "http://arxiv.org/abs/2104.08773",
          "publishedOn": "2021-09-07T07:20:13.185Z",
          "wordCount": null,
          "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1\">Julia Lust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>",
          "description": "Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.",
          "link": "http://arxiv.org/abs/2008.09381",
          "publishedOn": "2021-09-07T07:20:13.184Z",
          "wordCount": null,
          "title": "A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuqi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yu Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jinwei Ye</a>",
          "description": "Underwater 3D reconstruction is challenging due to the refraction of light at\nthe water-air interface (most electronic devices cannot be directly submerged\nin water). In this paper, we present an underwater 3D reconstruction solution\nusing light field cameras. We first develop a light field camera calibration\nalgorithm that simultaneously estimates the camera parameters and the geometry\nof the water-air interface. We then design a novel depth estimation algorithm\nfor 3D reconstruction. Specifically, we match correspondences on curved\nepipolar lines caused by water refraction. We also observe that the\nview-dependent specular reflection is very weak in the underwater environment,\nresulting the angularly sampled rays in light field has uniform intensity. We\ntherefore propose an angular uniformity constraint for depth optimization. We\nalso develop a fast algorithm for locating the angular patches in presence of\nnon-linear light paths. Extensive synthetic and real experiments demonstrate\nthat our method can perform underwater 3D reconstruction with high accuracy.",
          "link": "http://arxiv.org/abs/2109.02116",
          "publishedOn": "2021-09-07T07:20:13.179Z",
          "wordCount": 594,
          "title": "Underwater 3D Reconstruction Using Light Fields. (arXiv:2109.02116v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.04097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>",
          "description": "Navigation is one of the fundamental features of a autonomous robot. And the\nability of long-term navigation with semantic instruction is a `holy grail`\ngoals of intelligent robots. The development of 3D simulation technology\nprovide a large scale of data to simulate the real-world environment. The deep\nlearning proves its ability to robustly learn various embodied navigation\ntasks. However, deep learning on embodied navigation is still in its infancy\ndue to the unique challenges faced by the navigation exploration and learning\nfrom partial observed visual input. Recently, deep learning in embodied\nnavigation has become even thriving, with numerous methods have been proposed\nto tackle different challenges in this area. To give a promising direction for\nfuture research, in this paper, we present a comprehensive review of embodied\nnavigation tasks and the recent progress in deep learning based methods. It\nincludes two major tasks: target-oriented navigation and the\ninstruction-oriented navigation.",
          "link": "http://arxiv.org/abs/2108.04097",
          "publishedOn": "2021-09-07T07:20:13.169Z",
          "wordCount": null,
          "title": "Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.03459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wanling Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jianfeng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xu Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Chuanxin Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chunjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zihan Jiang</a>",
          "description": "Modern real-world application scenarios like Internet services consist of a\ndiversity of AI and non-AI modules with huge code sizes and long and\ncomplicated execution paths, which raises serious benchmarking or evaluating\nchallenges. Using AI components or micro benchmarks alone can lead to\nerror-prone conclusions. This paper presents a methodology to attack the above\nchallenge. We formalize a real-world application scenario as a Directed Acyclic\nGraph-based model and propose the rules to distill it into a permutation of\nessential AI and non-AI tasks, which we call a scenario benchmark. Together\nwith seventeen industry partners, we extract nine typical scenario benchmarks.\nWe design and implement an extensible, configurable, and flexible benchmark\nframework. We implement two Internet service AI scenario benchmarks based on\nthe framework as proxies to two real-world application scenarios. We consider\nscenario, component, and micro benchmarks as three indispensable parts for\nevaluating. Our evaluation shows the advantage of our methodology against using\ncomponent or micro AI benchmarks alone. The specifications, source code,\ntestbed, and results are publicly available from\n\\url{https://www.benchcouncil.org/aibench/scenario/}.",
          "link": "http://arxiv.org/abs/2005.03459",
          "publishedOn": "2021-09-07T07:20:13.165Z",
          "wordCount": 690,
          "title": "AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v4 [cs.PF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>",
          "description": "Traffic accident anticipation aims to accurately and promptly predict the\noccurrence of a future accident from dashcam videos, which is vital for a\nsafety-guaranteed self-driving system. To encourage an early and accurate\ndecision, existing approaches typically focus on capturing the cues of spatial\nand temporal context before a future accident occurs. However, their\ndecision-making lacks visual explanation and ignores the dynamic interaction\nwith the environment. In this paper, we propose Deep ReInforced accident\nanticipation with Visual Explanation, named DRIVE. The method simulates both\nthe bottom-up and top-down visual attention mechanism in a dashcam observation\nenvironment so that the decision from the proposed stochastic multi-task agent\ncan be visually explained by attentive regions. Moreover, the proposed dense\nanticipation reward and sparse fixation reward are effective in training the\nDRIVE model with our improved reinforcement learning algorithm. Experimental\nresults show that the DRIVE model achieves state-of-the-art performance on\nmultiple real-world traffic accident datasets. Code and pre-trained model are\navailable at \\url{https://www.rit.edu/actionlab/drive}.",
          "link": "http://arxiv.org/abs/2107.10189",
          "publishedOn": "2021-09-07T07:20:13.157Z",
          "wordCount": null,
          "title": "DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_F/0/1/0/all/0/1\">Farid Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Multi-view representation learning captures comprehensive information from\nmultiple views of a shared context. Recent works intuitively apply contrastive\nlearning (CL) to learn representations, regarded as a pairwise manner, which is\nstill scalable: view-specific noise is not filtered in learning view-shared\nrepresentations; the fake negative pairs, where the negative terms are actually\nwithin the same class as the positive, and the real negative pairs are\ncoequally treated; and evenly measuring the similarities between terms might\ninterfere with optimization. Importantly, few works research the theoretical\nframework of generalized self-supervised multi-view learning, especially for\nmore than two views. To this end, we rethink the existing multi-view learning\nparadigm from the information theoretical perspective and then propose a novel\ninformation theoretical framework for generalized multi-view learning. Guided\nby it, we build a multi-view coding method with a three-tier progressive\narchitecture, namely Information theory-guided heuristic Progressive Multi-view\nCoding (IPMC). In the distribution-tier, IPMC aligns the distribution between\nviews to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted\npools for contrasting, which utilizes a view filter to adaptively modify the\npools. Lastly, in the instance-tier, we adopt a designed unified loss to learn\ndiscriminative representations and reduce the gradient interference.\nTheoretically and empirically, we demonstrate the superiority of IPMC over\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.02344",
          "publishedOn": "2021-09-07T07:20:13.150Z",
          "wordCount": 664,
          "title": "Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sajid_U/0/1/0/all/0/1\">Usman Sajid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajid_H/0/1/0/all/0/1\">Hasan Sajid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Crowd estimation is a very challenging problem. The most recent study tries\nto exploit auditory information to aid the visual models, however, the\nperformance is limited due to the lack of an effective approach for feature\nextraction and integration. The paper proposes a new audiovisual multi-task\nnetwork to address the critical challenges in crowd counting by effectively\nutilizing both visual and audio inputs for better modalities association and\nproductive feature extraction. The proposed network introduces the notion of\nauxiliary and explicit image patch-importance ranking (PIR) and patch-wise\ncrowd estimate (PCE) information to produce a third (run-time) modality. These\nmodalities (audio, visual, run-time) undergo a transformer-inspired\ncross-modality co-attention mechanism to finally output the crowd estimate. To\nacquire rich visual features, we propose a multi-branch structure with\ntransformer-style fusion in-between. Extensive experimental evaluations show\nthat the proposed scheme outperforms the state-of-the-art networks under all\nevaluation settings with up to 33.8% improvement. We also analyze and compare\nthe vision-only variant of our network and empirically demonstrate its\nsuperiority over previous approaches.",
          "link": "http://arxiv.org/abs/2109.01926",
          "publishedOn": "2021-09-07T07:20:13.126Z",
          "wordCount": 608,
          "title": "Audio-Visual Transformer Based Crowd Counting. (arXiv:2109.01926v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+John_J/0/1/0/all/0/1\">Jomy John</a>",
          "description": "Handwritten character recognition is an active research challenge,especially\nfor Indian scripts. This paper deals with handwritten Malayalam, with a\ncomplete set of basic characters, vowel and consonant signs and compound\ncharacters that may be present in the script. Spatial domain features suitable\nfor recognition are chosen in this work. For classification, k-NN, SVM and ELM\nare employed",
          "link": "http://arxiv.org/abs/2109.02153",
          "publishedOn": "2021-09-07T07:20:13.119Z",
          "wordCount": 523,
          "title": "Spatial Domain Feature Extraction Methods for Unconstrained Handwritten Malayalam Character Recognition. (arXiv:2109.02153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05509",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karvounas_G/0/1/0/all/0/1\">Giorgos Karvounas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyriazis_N/0/1/0/all/0/1\">Nikolaos Kyriazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomidis_I/0/1/0/all/0/1\">Iason Oikonomidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoli_A/0/1/0/all/0/1\">Aggeliki Tsoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyros_A/0/1/0/all/0/1\">Antonis A. Argyros</a>",
          "description": "The amount and quality of datasets and tools available in the research field\nof hand pose and shape estimation act as evidence to the significant progress\nthat has been made.However, even the datasets of the highest quality, reported\nto date, have shortcomings in annotation. We propose a refinement approach,\nbased on differentiable ray tracing,and demonstrate how a high-quality publicly\navailable, multi-camera dataset of hands(InterHand2.6M) can become an even\nbetter dataset, with respect to annotation quality. Differentiable ray tracing\nhas not been employed so far to relevant problems and is hereby shown to be\nsuperior to the approximative alternatives that have been employed in the past.\nTo tackle the lack of reliable ground truth, as far as quantitative evaluation\nis concerned, we resort to realistic synthetic data, to show that the\nimprovement we induce is indeed significant. The same becomes evident in real\ndata through visual evaluation.",
          "link": "http://arxiv.org/abs/2107.05509",
          "publishedOn": "2021-09-07T07:20:13.092Z",
          "wordCount": null,
          "title": "Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_E/0/1/0/all/0/1\">Eric-Tuan L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1\">Radomir Mech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubekeur_T/0/1/0/all/0/1\">Tamy Boubekeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>",
          "description": "Representing human-made objects as a collection of base primitives has a long\nhistory in computer vision and reverse engineering. In the case of\nhigh-resolution point cloud scans, the challenge is to be able to detect both\nlarge primitives as well as those explaining the detailed parts. While the\nclassical RANSAC approach requires case-specific parameter tuning,\nstate-of-the-art networks are limited by memory consumption of their backbone\nmodules such as PointNet++, and hence fail to detect the fine-scale primitives.\nWe present Cascaded Primitive Fitting Networks (CPFN) that relies on an\nadaptive patch sampling network to assemble detection results of global and\nlocal primitive detection networks. As a key enabler, we present a merging\nformulation that dynamically aggregates the primitives across global and local\nscales. Our evaluation demonstrates that CPFN improves the state-of-the-art\nSPFN performance by 13-14% on high-resolution point cloud datasets and\nspecifically improves the detection of fine-scale primitives by 20-22%.",
          "link": "http://arxiv.org/abs/2109.00113",
          "publishedOn": "2021-09-07T07:20:13.069Z",
          "wordCount": null,
          "title": "CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01999",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1\">Khawar Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dang_L/0/1/0/all/0/1\">L. Minh Dang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1\">Sujin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonjoon Moon</a>",
          "description": "Image compression is a method to remove spatial redundancy between adjacent\npixels and reconstruct a high-quality image. In the past few years, deep\nlearning has gained huge attention from the research community and produced\npromising image reconstruction results. Therefore, recent methods focused on\ndeveloping deeper and more complex networks, which significantly increased\nnetwork complexity. In this paper, two effective novel blocks are developed:\nanalysis and synthesis block that employs the convolution layer and Generalized\nDivisive Normalization (GDN) in the variable-rate encoder and decoder side. Our\nnetwork utilizes a pixel RNN approach for quantization. Furthermore, to improve\nthe whole network, we encode a residual image using LSTM cells to reduce\nunnecessary information. Experimental results demonstrated that the proposed\nvariable-rate framework with novel blocks outperforms existing methods and\nstandard image codecs, such as George's ~\\cite{002} and JPEG in terms of image\nsimilarity. The project page along with code and models are available at\nhttps://khawar512.github.io/cvpr/",
          "link": "http://arxiv.org/abs/2109.01999",
          "publishedOn": "2021-09-07T07:20:13.063Z",
          "wordCount": 625,
          "title": "Image Compression with Recurrent Neural Network and Generalized Divisive Normalization. (arXiv:2109.01999v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09384",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Challa_A/0/1/0/all/0/1\">Aditya Challa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danda_S/0/1/0/all/0/1\">Sravan Danda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagar_B/0/1/0/all/0/1\">B.S.Daya Sagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1\">Laurent Najman</a>",
          "description": "Hyperspectral images (HSI) consist of rich spatial and spectral information,\nwhich can potentially be used for several applications. However, noise, band\ncorrelations and high dimensionality restrict the applicability of such data.\nThis is recently addressed using creative deep learning network architectures\nsuch as ResNet, SSRN, and A2S2K. However, the last layer, i.e the\nclassification layer, remains unchanged and is taken to be the softmax\nclassifier. In this article, we propose to use a watershed classifier.\nWatershed classifier extends the watershed operator from Mathematical\nMorphology for classification. In its vanilla form, the watershed classifier\ndoes not have any trainable parameters. In this article, we propose a novel\napproach to train deep learning networks to obtain representations suitable for\nthe watershed classifier. The watershed classifier exploits the connectivity\npatterns, a characteristic of HSI datasets, for better inference. We show that\nexploiting such characteristics allows the Triplet-Watershed to achieve\nstate-of-art results in supervised and semi-supervised contexts. These results\nare validated on Indianpines (IP), University of Pavia (UP), Kennedy Space\nCenter (KSC) and University of Houston (UH) datasets, relying on simple convnet\narchitecture using a quarter of parameters compared to previous\nstate-of-the-art networks. The source code for reproducing the experiments and\nsupplementary material (high resolution images) is available at\nhttps://github.com/ac20/TripletWatershed Code.",
          "link": "http://arxiv.org/abs/2103.09384",
          "publishedOn": "2021-09-07T07:20:13.026Z",
          "wordCount": null,
          "title": "Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.05750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fawakherji_M/0/1/0/all/0/1\">Mulham Fawakherji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potena_C/0/1/0/all/0/1\">Ciro Potena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1\">Alberto Pretto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloisi_D/0/1/0/all/0/1\">Domenico D. Bloisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1\">Daniele Nardi</a>",
          "description": "An effective perception system is a fundamental component for farming robots,\nas it enables them to properly perceive the surrounding environment and to\ncarry out targeted operations. The most recent methods make use of\nstate-of-the-art machine learning techniques to learn a valid model for the\ntarget task. However, those techniques need a large amount of labeled data for\ntraining. A recent approach to deal with this issue is data augmentation\nthrough Generative Adversarial Networks (GANs), where entire synthetic scenes\nare added to the training data, thus enlarging and diversifying their\ninformative content. In this work, we propose an alternative solution with\nrespect to the common data augmentation methods, applying it to the fundamental\nproblem of crop/weed segmentation in precision farming. Starting from real\nimages, we create semi-artificial samples by replacing the most relevant object\nclasses (i.e., crop and weeds) with their synthesized counterparts. To do that,\nwe employ a conditional GAN (cGAN), where the generative model is trained by\nconditioning the shape of the generated object. Moreover, in addition to RGB\ndata, we take into account also near-infrared (NIR) information, generating\nfour channel multi-spectral synthetic images. Quantitative experiments, carried\nout on three publicly available datasets, show that (i) our model is capable of\ngenerating realistic multi-spectral images of plants and (ii) the usage of such\nsynthetic images in the training process improves the segmentation performance\nof state-of-the-art semantic segmentation convolutional networks.",
          "link": "http://arxiv.org/abs/2009.05750",
          "publishedOn": "2021-09-07T07:20:13.025Z",
          "wordCount": null,
          "title": "Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming. (arXiv:2009.05750v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brynte_L/0/1/0/all/0/1\">Lucas Brynte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1\">Viktor Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1\">Jos&#xe9; Pedro Iglesias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_C/0/1/0/all/0/1\">Carl Olsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1\">Fredrik Kahl</a>",
          "description": "Why is it that semidefinite relaxations have been so successful in numerous\napplications in computer vision and robotics for solving non-convex\noptimization problems involving rotations? In studying the empirical\nperformance we note that there are few failure cases reported in the\nliterature, in particular for estimation problems with a single rotation,\nmotivating us to gain further theoretical understanding.\n\nA general framework based on tools from algebraic geometry is introduced for\nanalyzing the power of semidefinite relaxations of problems with quadratic\nobjective functions and rotational constraints. Applications include\nregistration, hand-eye calibration and rotation averaging. We characterize the\nextreme points, and show that there exist failure cases for which the\nrelaxation is not tight, even in the case of a single rotation. We also show\nthat some problem classes are always tight given an appropriate\nparametrization. Our theoretical findings are accompanied with numerical\nsimulations, providing further evidence and understanding of the results.",
          "link": "http://arxiv.org/abs/2101.02099",
          "publishedOn": "2021-09-07T07:20:13.022Z",
          "wordCount": null,
          "title": "On the Tightness of Semidefinite Relaxations for Rotation Estimation. (arXiv:2101.02099v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianxiong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_F/0/1/0/all/0/1\">Francesc Moreno</a>",
          "description": "Neural Radiance Fields (NeRF) has become a popular framework for learning\nimplicit 3D representations and addressing different tasks such as novel-view\nsynthesis or depth-map estimation. However, in downstream applications where\ndecisions need to be made based on automatic predictions, it is critical to\nleverage the confidence associated with the model estimations. Whereas\nuncertainty quantification is a long-standing problem in Machine Learning, it\nhas been largely overlooked in the recent NeRF literature. In this context, we\npropose Stochastic Neural Radiance Fields (S-NeRF), a generalization of\nstandard NeRF that learns a probability distribution over all the possible\nradiance fields modeling the scene. This distribution allows to quantify the\nuncertainty associated with the scene information provided by the model. S-NeRF\noptimization is posed as a Bayesian learning problem which is efficiently\naddressed using the Variational Inference framework. Exhaustive experiments\nover benchmark datasets demonstrate that S-NeRF is able to provide more\nreliable predictions and confidence values than generic approaches previously\nproposed for uncertainty estimation in other domains.",
          "link": "http://arxiv.org/abs/2109.02123",
          "publishedOn": "2021-09-07T07:20:13.020Z",
          "wordCount": null,
          "title": "Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhixuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zhenning Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Le Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuehu Liu</a>",
          "description": "In this manuscript, we introduce a semi-automatic scene graph annotation tool\nfor images, the GeneAnnotator. This software allows human annotators to\ndescribe the existing relationships between participators in the visual scene\nin the form of directed graphs, hence enabling the learning and reasoning on\nvisual relationships, e.g., image captioning, VQA and scene graph generation,\netc. The annotations for certain image datasets could either be merged in a\nsingle VG150 data-format file to support most existing models for scene graph\nlearning or transformed into a separated annotation file for each single image\nto build customized datasets. Moreover, GeneAnnotator provides a rule-based\nrelationship recommending algorithm to reduce the heavy annotation workload.\nWith GeneAnnotator, we propose Traffic Genome, a comprehensive scene graph\ndataset with 1000 diverse traffic images, which in return validates the\neffectiveness of the proposed software for scene graph annotation. The project\nsource code, with usage examples and sample data is available at\nhttps://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph,\nunder the Apache open-source license.",
          "link": "http://arxiv.org/abs/2109.02226",
          "publishedOn": "2021-09-07T07:20:13.020Z",
          "wordCount": null,
          "title": "GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. (arXiv:2109.02226v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingjian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jie Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>",
          "description": "Compared with image scene parsing, video scene parsing introduces temporal\ninformation, which can effectively improve the consistency and accuracy of\nprediction. In this paper, we propose a Spatial-Temporal Semantic Consistency\nmethod to capture class-exclusive context information. Specifically, we design\na spatial-temporal consistency loss to constrain the semantic consistency in\nspatial and temporal dimensions. In addition, we adopt an pseudo-labeling\nstrategy to enrich the training dataset. We obtain the scores of 59.84% and\n58.85% mIoU on development (test part 1) and testing set of VSPW, respectively.\nAnd our method wins the 1st place on VSPW challenge at ICCV2021.",
          "link": "http://arxiv.org/abs/2109.02281",
          "publishedOn": "2021-09-07T07:20:13.020Z",
          "wordCount": null,
          "title": "Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing. (arXiv:2109.02281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jihwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1\">David Quispe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>",
          "description": "Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.",
          "link": "http://arxiv.org/abs/2101.10423",
          "publishedOn": "2021-09-07T07:20:13.016Z",
          "wordCount": 787,
          "title": "Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02084",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saini_S/0/1/0/all/0/1\">Shreshth Saini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agrawal_G/0/1/0/all/0/1\">Geetika Agrawal</a>",
          "description": "Segmentation plays a crucial role in diagnosis. Studying the retinal\nvasculatures from fundus images help identify early signs of many crucial\nillnesses such as diabetic retinopathy. Due to the varying shape, size, and\npatterns of retinal vessels, along with artefacts and noises in fundus images,\nno one-stage method can accurately segment retinal vessels. In this work, we\npropose a multi-scale, multi-level attention embedded CNN architecture\n((M)SLAe-Net) to address the issue of multi-stage processing for robust and\nprecise segmentation of retinal vessels. We do this by extracting features at\nmultiple scales and multiple levels of the network, enabling our model to\nholistically extracts the local and global features. Multi-scale features are\nextracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We\nalso aggregate the features from all the network levels. These effectively\nresolved the issues of varying shapes and artefacts and hence the need for\nmultiple stages. To assist in better pixel-level classification, we use the\nSqueeze and Attention(SA) module, a smartly adapted version of the Squeeze and\nExcitation(SE) module for segmentation tasks in our network to facilitate\npixel-group attention. Our unique network design and novel D-DPP module with\nefficient task-specific loss function for thin vessels enabled our model for\nbetter cross data performance. Exhaustive experimental results on DRIVE, STARE,\nHRF, and CHASE-DB1 show the superiority of our method.",
          "link": "http://arxiv.org/abs/2109.02084",
          "publishedOn": "2021-09-07T07:20:12.994Z",
          "wordCount": 699,
          "title": "(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>",
          "description": "Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.",
          "link": "http://arxiv.org/abs/2012.01338",
          "publishedOn": "2021-09-07T07:20:12.979Z",
          "wordCount": 684,
          "title": "Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrell_S/0/1/0/all/0/1\">Steven Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>",
          "description": "The use of mobiles phones when driving have been a major factor when it comes\nto road traffic incidents and the process of capturing such violations can be a\nlaborious task. Advancements in both modern object detection frameworks and\nhigh-performance hardware has paved the way for a more automated approach when\nit comes to video surveillance. In this work, we propose a custom-trained\nstate-of-the-art object detector to work with roadside cameras to capture\ndriver phone usage without the need for human intervention. The proposed\napproach also addresses the issues caused by windscreen glare and introduces\nthe steps required to remedy this. Twelve pre-trained models are fine-tuned\nwith our custom dataset using four popular object detection methods: YOLO, SSD,\nFaster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO\nyields the highest accuracy levels of up to 96% (AP10) and frame rates of up to\n~30 FPS. DeepSort object tracking algorithm is also integrated into the\nbest-performing model to collect records of only the unique violations, and\nenable the proposed approach to count the number of vehicles. The proposed\nautomated system will collect the output images of the identified violations,\ntimestamps of each violation, and total vehicle count. Data can be accessed via\na purpose-built user interface.",
          "link": "http://arxiv.org/abs/2109.02119",
          "publishedOn": "2021-09-07T07:20:12.970Z",
          "wordCount": 669,
          "title": "Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuerkaixi_A/0/1/0/all/0/1\">Abudukelimu Wuerkaixi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianjiang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "In this paper, we investigate the problem of facial kinship verification by\nlearning hierarchical reasoning graph networks. Conventional methods usually\nfocus on learning discriminative features for each facial image of a paired\nsample and neglect how to fuse the obtained two facial image features and\nreason about the relations between them. To address this, we propose a\nStar-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a\nstar-shaped graph where each surrounding node encodes the information of\ncomparisons in a feature dimension and the central node is employed as the\nbridge for the interaction of surrounding nodes. Then we perform relational\nreasoning on this star graph with iterative message passing. The proposed S-RGN\nuses only one central node to analyze and process information from all\nsurrounding nodes, which limits its reasoning capacity. We further develop a\nHierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and\nflexible capacity. More specifically, our H-RGN introduces a set of latent\nreasoning nodes and constructs a hierarchical graph with them. Then bottom-up\ncomparative information abstraction and top-down comprehensive signal\npropagation are iteratively performed on the hierarchical graph to update the\nnode features. Extensive experimental results on four widely used kinship\ndatabases show that the proposed methods achieve very competitive results.",
          "link": "http://arxiv.org/abs/2109.02219",
          "publishedOn": "2021-09-07T07:20:12.950Z",
          "wordCount": 680,
          "title": "Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical. (arXiv:2109.02219v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanaparthy_S/0/1/0/all/0/1\">Samhita Kanaparthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1\">Manisha Padala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1\">Sankarshan Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1\">Sujit Gujar</a>",
          "description": "We consider the problem of achieving fair classification in Federated\nLearning (FL) under data heterogeneity. Most of the approaches proposed for\nfair classification require diverse data that represent the different\ndemographic groups involved. In contrast, it is common for each client to own\ndata that represents only a single demographic group. Hence the existing\napproaches cannot be adopted for fair classification models at the client\nlevel. To resolve this challenge, we propose several aggregation techniques. We\nempirically validate these techniques by comparing the resulting fairness\nmetrics and accuracy on CelebA, UTK, and FairFace datasets.",
          "link": "http://arxiv.org/abs/2109.02351",
          "publishedOn": "2021-09-07T07:20:12.941Z",
          "wordCount": 542,
          "title": "Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sixian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xinhang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yubing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weijie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1\">Yakui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuqiang Jiang</a>",
          "description": "The goal of object navigation is to reach the expected objects according to\nvisual information in the unseen environments. Previous works usually implement\ndeep models to train an agent to predict actions in real-time. However, in the\nunseen environment, when the target object is not in egocentric view, the agent\nmay not be able to make wise decisions due to the lack of guidance. In this\npaper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent\nin a coarse-to-fine manner, and an online-learning mechanism is also proposed\nto update HOZ according to the real-time observation in new environments. In\nparticular, the HOZ graph is composed of scene nodes, zone nodes and object\nnodes. With the pre-learned HOZ graph, the real-time observation and the target\ngoal, the agent can constantly plan an optimal path from zone to zone. In the\nestimated path, the next potential zone is regarded as sub-goal, which is also\nfed into the deep reinforcement learning model for action prediction. Our\nmethods are evaluated on the AI2-Thor simulator. In addition to widely used\nevaluation metrics SR and SPL, we also propose a new evaluation metric of SAE\nthat focuses on the effective action rate. Experimental results demonstrate the\neffectiveness and efficiency of our proposed method.",
          "link": "http://arxiv.org/abs/2109.02066",
          "publishedOn": "2021-09-07T07:20:12.933Z",
          "wordCount": 658,
          "title": "Hierarchical Object-to-Zone Graph for Object Navigation. (arXiv:2109.02066v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mallat_K/0/1/0/all/0/1\">Khawla Mallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_Riera_F/0/1/0/all/0/1\">Fabiola Becerra-Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_Gonzalez_A/0/1/0/all/0/1\">Annette Morales-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_H/0/1/0/all/0/1\">Heydi M&#xe9;ndez-V&#xe1;zquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugelay_J/0/1/0/all/0/1\">Jean-Luc Dugelay</a>",
          "description": "In this paper, we explore whether automatic face recognition can help in\nverifying widespread misinformation on social media, particularly conspiracy\ntheories that are based on the existence of body doubles. The conspiracy theory\naddressed in this paper is the case of the Melania Trump body double. We\nemployed four different state-of-the-art descriptors for face recognition to\nverify the integrity of the claim of the studied conspiracy theory. In\naddition, we assessed the impact of different image quality metrics on the\nvariation of face recognition results. Two sets of image quality metrics were\nconsidered: acquisition-related metrics and subject-related metrics.",
          "link": "http://arxiv.org/abs/2109.02283",
          "publishedOn": "2021-09-07T07:20:12.926Z",
          "wordCount": 560,
          "title": "Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02342",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1\">Seung Su Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Preuhs_E/0/1/0/all/0/1\">Elisabeth Preuhs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Michaela Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forman_C/0/1/0/all/0/1\">Christoph Forman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chitiboi_T/0/1/0/all/0/1\">Teodora Chitiboi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Puneet Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliano Lara Fernandes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tillmanns_C/0/1/0/all/0/1\">Christoph Tillmanns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wetzl_J/0/1/0/all/0/1\">Jens Wetzl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>",
          "description": "Purpose: Static cardiac imaging such as late gadolinium enhancement, mapping,\nor 3-D coronary angiography require prior information, e.g., the phase during a\ncardiac cycle with least motion, called resting phase (RP). The purpose of this\nwork is to propose a fully automated framework that allows the detection of the\nright coronary artery (RCA) RP within CINE series. Methods: The proposed\nprototype system consists of three main steps. First, the localization of the\nregions of interest (ROI) is performed. Second, as CINE series are\ntime-resolved, the cropped ROI series over all time points are taken for\ntracking motions quantitatively. Third, the output motion values are used to\nclassify RPs. In this work, we focused on the detection of the area with the\nouter edge of the cross-section of the RCA as our target. The proposed\nframework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The\nautomatically classified RPs were compared with the ground truth RPs annotated\nmanually by a medical expert for testing the robustness and feasibility of the\nframework. Results: The predicted RCA RPs showed high agreement with the\nexperts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0%\nspecificity for the unseen study dataset. The mean absolute difference of the\nstart and end RP was 13.6 ${\\pm}$ 18.6 ms for the validation study dataset\n(n=102). Conclusion: In this work, automated RP detection has been introduced\nby the proposed framework and demonstrated feasibility, robustness, and\napplicability for diverse static imaging acquisitions.",
          "link": "http://arxiv.org/abs/2109.02342",
          "publishedOn": "2021-09-07T07:20:12.918Z",
          "wordCount": 729,
          "title": "Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery. (arXiv:2109.02342v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Huan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jianchao Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>",
          "description": "Model compression techniques are recently gaining explosive attention for\nobtaining efficient AI models for various real-time applications. Channel\npruning is one important compression strategy and is widely used in slimming\nvarious DNNs. Previous gate-based or importance-based pruning methods aim to\nremove channels whose importance is smallest. However, it remains unclear what\ncriteria the channel importance should be measured on, leading to various\nchannel selection heuristics. Some other sampling-based pruning methods deploy\nsampling strategies to train sub-nets, which often causes the training\ninstability and the compressed model's degraded performance. In view of the\nresearch gaps, we present a new module named Gates with Differentiable\nPolarization (GDP), inspired by principled optimization ideas. GDP can be\nplugged before convolutional layers without bells and whistles, to control the\non-and-off of each channel or whole layer block. During the training process,\nthe polarization effect will drive a subset of gates to smoothly decrease to\nexact zero, while other gates gradually stay away from zero by a large margin.\nWhen training terminates, those zero-gated channels can be painlessly removed,\nwhile other non-zero gates can be absorbed into the succeeding convolution\nkernel, causing completely no interruption to training nor damage to the\ntrained model. Experiments conducted over CIFAR-10 and ImageNet datasets show\nthat the proposed GDP algorithm achieves the state-of-the-art performance on\nvarious benchmark DNNs at a broad range of pruning ratios. We also apply GDP to\nDeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose\ntest performance sees no drop (even slightly improved) with over 60% FLOPs\nsaving.",
          "link": "http://arxiv.org/abs/2109.02220",
          "publishedOn": "2021-09-07T07:20:12.910Z",
          "wordCount": 710,
          "title": "GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sha_T/0/1/0/all/0/1\">Tong Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoujun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>",
          "description": "Deep person generation has attracted extensive research attention due to its\nwide applications in virtual agents, video conferencing, online shopping and\nart/movie production. With the advancement of deep learning, visual appearances\n(face, pose, cloth) of a person image can be easily generated or manipulated on\ndemand. In this survey, we first summarize the scope of person generation, and\nthen systematically review recent progress and technical trends in deep person\ngeneration, covering three major tasks: talking-head generation (face),\npose-guided person generation (pose) and garment-oriented person generation\n(cloth). More than two hundred papers are covered for a thorough overview, and\nthe milestone works are highlighted to witness the major technical\nbreakthrough. Based on these fundamental tasks, a number of applications are\ninvestigated, e.g., virtual fitting, digital human, generative data\naugmentation. We hope this survey could shed some light on the future prospects\nof deep person generation, and provide a helpful foundation for full\napplications towards digital human.",
          "link": "http://arxiv.org/abs/2109.02081",
          "publishedOn": "2021-09-07T07:20:12.903Z",
          "wordCount": 613,
          "title": "Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis. (arXiv:2109.02081v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1909.10819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Risheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1\">Pan Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>",
          "description": "Alternating Direction Method of Multiplier (ADMM) has been a popular\nalgorithmic framework for separable optimization problems with linear\nconstraints. For numerical ADMM fail to exploit the particular structure of the\nproblem at hand nor the input data information, leveraging task-specific\nmodules (e.g., neural networks and other data-driven architectures) to extend\nADMM is a significant but challenging task. This work focuses on designing a\nflexible algorithmic framework to incorporate various task-specific modules\n(with no additional constraints) to improve the performance of ADMM in\nreal-world applications. Specifically, we propose Guidance from Optimality\n(GO), a new customization strategy, to embed task-specific modules into ADMM\n(GO-ADMM). By introducing an optimality-based criterion to guide the\npropagation, GO-ADMM establishes an updating scheme agnostic to the choice of\nadditional modules. The existing task-specific methods just plug their\ntask-specific modules into the numerical iterations in a straightforward\nmanner. Even with some restrictive constraints on the plug-in modules, they can\nonly obtain some relatively weaker convergence properties for the resulted ADMM\niterations. Fortunately, without any restrictions on the embedded modules, we\nprove the convergence of GO-ADMM regarding objective values and constraint\nviolations, and derive the worst-case convergence rate measured by iteration\ncomplexity. Extensive experiments are conducted to verify the theoretical\nresults and demonstrate the efficiency of GO-ADMM.",
          "link": "http://arxiv.org/abs/1909.10819",
          "publishedOn": "2021-09-07T07:20:12.895Z",
          "wordCount": 690,
          "title": "Investigating Customization Strategies and Convergence Behaviors of Task-specific ADMM. (arXiv:1909.10819v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D\nspatial-temporal representation of a dynamic scene from a set of RGB images.\nKey to our approach is the use of a neural implicit representation that learns\nto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing\nconsistency across different modalities, our representation enables multi-view\nrendering in diverse dynamic scenes, including water pouring, robotic\ninteraction, and real images, outperforming state-of-the-art methods for\nspatial-temporal view synthesis. Our approach works even when inputs images are\ncaptured with only one camera. We further demonstrate that the learned\nrepresentation can serve as an implicit scene prior, enabling video processing\ntasks such as image super-resolution and de-noising without any additional\nsupervision.",
          "link": "http://arxiv.org/abs/2012.09790",
          "publishedOn": "2021-09-07T07:20:12.855Z",
          "wordCount": 616,
          "title": "Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hongwei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "In this paper we focus on landscape animation, which aims to generate\ntime-lapse videos from a single landscape image. Motion is crucial for\nlandscape animation as it determines how objects move in videos. Existing\nmethods are able to generate appealing videos by learning motion from real\ntime-lapse videos. However, current methods suffer from inaccurate motion\ngeneration, which leads to unrealistic video results. To tackle this problem,\nwe propose a model named FGLA to generate high-quality and realistic videos by\nlearning Fine-Grained motion embedding for Landscape Animation. Our model\nconsists of two parts: (1) a motion encoder which embeds time-lapse motion in a\nfine-grained way. (2) a motion generator which generates realistic motion to\nanimate input images. To train and evaluate on diverse time-lapse videos, we\nbuild the largest high-resolution Time-lapse video dataset with Diverse scenes,\nnamely Time-lapse-D, which includes 16,874 video clips with over 10 million\nframes. Quantitative and qualitative experimental results demonstrate the\nsuperiority of our method. In particular, our method achieves relative\nimprovements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art\nmethods on our dataset. A user study carried out with 700 human subjects shows\nthat our approach visually outperforms existing methods by a large margin.",
          "link": "http://arxiv.org/abs/2109.02216",
          "publishedOn": "2021-09-07T07:20:12.820Z",
          "wordCount": 648,
          "title": "Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoorick_B/0/1/0/all/0/1\">Basile Van Hoorick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>",
          "description": "The elementary operation of cropping underpins nearly every computer vision\nsystem, ranging from data augmentation and translation invariance to\ncomputational photography and representation learning. This paper investigates\nthe subtle traces introduced by this operation. For example, despite\nrefinements to camera optics, lenses will leave behind certain clues, notably\nchromatic aberration and vignetting. Photographers also leave behind other\nclues relating to image aesthetics and scene composition. We study how to\ndetect these traces, and investigate the impact that cropping has on the image\ndistribution. While our aim is to dissect the fundamental impact of spatial\ncrops, there are also a number of practical implications to our work, such as\nrevealing faulty photojournalism and equipping neural network researchers with\na better understanding of shortcut learning. Code is available at\nhttps://github.com/basilevh/dissecting-image-crops.",
          "link": "http://arxiv.org/abs/2011.11831",
          "publishedOn": "2021-09-07T07:20:12.804Z",
          "wordCount": 614,
          "title": "Dissecting Image Crops. (arXiv:2011.11831v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "Mixture of Experts (MoE) with sparse conditional computation has been proved\nan effective architecture for scaling attention-based models to more parameters\nwith comparable computation cost. In this paper, we propose Sparse-MLP, scaling\nthe recent MLP-Mixer model with sparse MoE layers, to achieve a more\ncomputation-efficient architecture. We replace a subset of dense MLP blocks in\nthe MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two\nstages of MoE layers: one with MLP experts mixing information within channels\nalong image patch dimension, one with MLP experts mixing information within\npatches along the channel dimension. Besides, to reduce computational cost in\nrouting and improve experts capacity, we design Re-represent layers in each\nSparse block. These layers are to re-scale image representations by two simple\nbut effective linear transformations. By pre-training on ImageNet-1k with MoCo\nv3 algorithm, our models can outperform dense MLP models with comparable\nparameters and less computational cost on several downstream image\nclassification tasks.",
          "link": "http://arxiv.org/abs/2109.02008",
          "publishedOn": "2021-09-07T07:20:12.769Z",
          "wordCount": 605,
          "title": "Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1710.03113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>",
          "description": "The rapid emergence of high-dimensional data in various areas has brought new\nchallenges to current ensemble clustering research. To deal with the curse of\ndimensionality, recently considerable efforts in ensemble clustering have been\nmade by means of different subspace-based techniques. However, besides the\nemphasis on subspaces, rather limited attention has been paid to the potential\ndiversity in similarity/dissimilarity metrics. It remains a surprisingly open\nproblem in ensemble clustering how to create and aggregate a large population\nof diversified metrics, and furthermore, how to jointly investigate the\nmulti-level diversity in the large populations of metrics, subspaces, and\nclusters in a unified framework. To tackle this problem, this paper proposes a\nnovel multidiversified ensemble clustering approach. In particular, we create a\nlarge number of diversified metrics by randomizing a scaled exponential\nsimilarity kernel, which are then coupled with random subspaces to form a large\nset of metric-subspace pairs. Based on the similarity matrices derived from\nthese metric-subspace pairs, an ensemble of diversified base clusterings can\nthereby be constructed. Further, an entropy-based criterion is utilized to\nexplore the cluster-wise diversity in ensembles, based on which three specific\nensemble clustering algorithms are presented by incorporating three types of\nconsensus functions. Extensive experiments are conducted on 30 high-dimensional\ndatasets, including 18 cancer gene expression datasets and 12 image/speech\ndatasets, which demonstrate the superiority of our algorithms over the\nstate-of-the-art. The source code is available at\nhttps://github.com/huangdonghere/MDEC.",
          "link": "http://arxiv.org/abs/1710.03113",
          "publishedOn": "2021-09-07T07:20:12.748Z",
          "wordCount": 764,
          "title": "Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_D/0/1/0/all/0/1\">David Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_C/0/1/0/all/0/1\">Christiane Sommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usenko_V/0/1/0/all/0/1\">Vladyslav Usenko</a>",
          "description": "In this paper we propose a novel square root sliding-window bundle adjustment\nsuitable for real-time odometry applications. The square root formulation\npervades three major aspects of our optimization-based sliding-window\nestimator: for bundle adjustment we eliminate landmark variables with nullspace\nprojection; to store the marginalization prior we employ a matrix square root\nof the Hessian; and when marginalizing old poses we avoid forming normal\nequations and update the square root prior directly with a specialized QR\ndecomposition. We show that the proposed square root marginalization is\nalgebraically equivalent to the conventional use of Schur complement (SC) on\nthe Hessian. Moreover, it elegantly deals with rank-deficient Jacobians\nproducing a prior equivalent to SC with Moore-Penrose inverse. Our evaluation\nof visual and visual-inertial odometry on real-world datasets demonstrates that\nthe proposed estimator is 36% faster than the baseline. It furthermore shows\nthat in single precision, conventional Hessian-based marginalization leads to\nnumeric failures and reduced accuracy. We analyse numeric properties of the\nmarginalization prior to explain why our square root form does not suffer from\nthe same effect and therefore entails superior performance.",
          "link": "http://arxiv.org/abs/2109.02182",
          "publishedOn": "2021-09-07T07:20:12.732Z",
          "wordCount": 634,
          "title": "Square Root Marginalization for Sliding-Window Bundle Adjustment. (arXiv:2109.02182v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghodke_V/0/1/0/all/0/1\">Vedant Ghodke</a>",
          "description": "The shortage of workforce and increasing cost of maintenance has forced many\nfarm industrialists to shift towards automated and mechanized approaches. The\nkey component for autonomous systems is the path planning techniques used.\nCoverage path planning (CPP) algorithm is used for navigating over farmlands to\nperform various agricultural operations such as seeding, ploughing, or spraying\npesticides and fertilizers. This report paper compares novel algorithms for\nautonomous navigation of farmlands. For reduction of navigational constraints,\na high-resolution grid map representation is taken into consideration specific\nto Indian environments. The free space is covered by distinguishing the grid\ncells as covered, unexplored, partially explored and presence of an obstacle.\nThe performance of the compared algorithms is evaluated with metrics such as\ntime efficiency, space efficiency, accuracy, and robustness to changes in the\nenvironment. Robotic Operating System (ROS), Dassault Systemes Experience\nPlatform (3DS Experience), MATLAB along Python were used for the simulation of\nthe compared algorithms. The results proved the applicability of the algorithms\nfor autonomous field navigation and feasibility with robotic path planning.",
          "link": "http://arxiv.org/abs/2109.02015",
          "publishedOn": "2021-09-07T07:20:12.686Z",
          "wordCount": 612,
          "title": "Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiumei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>",
          "description": "Visual emotion analysis (VEA) has attracted great attention recently, due to\nthe increasing tendency of expressing and understanding emotions through images\non social networks. Different from traditional vision tasks, VEA is inherently\nmore challenging since it involves a much higher level of complexity and\nambiguity in human cognitive process. Most of the existing methods adopt deep\nlearning techniques to extract general features from the whole image,\ndisregarding the specific features evoked by various emotional stimuli.\nInspired by the \\textit{Stimuli-Organism-Response (S-O-R)} emotion model in\npsychological theory, we proposed a stimuli-aware VEA method consisting of\nthree stages, namely stimuli selection (S), feature extraction (O) and emotion\nprediction (R). First, specific emotional stimuli (i.e., color, object, face)\nare selected from images by employing the off-the-shelf tools. To the best of\nour knowledge, it is the first time to introduce stimuli selection process into\nVEA in an end-to-end network. Then, we design three specific networks, i.e.,\nGlobal-Net, Semantic-Net and Expression-Net, to extract distinct emotional\nfeatures from different stimuli simultaneously. Finally, benefiting from the\ninherent structure of Mikel's wheel, we design a novel hierarchical\ncross-entropy loss to distinguish hard false examples from easy ones in an\nemotion-specific manner. Experiments demonstrate that the proposed method\nconsistently outperforms the state-of-the-art approaches on four public visual\nemotion datasets. Ablation study and visualizations further prove the validity\nand interpretability of our method.",
          "link": "http://arxiv.org/abs/2109.01812",
          "publishedOn": "2021-09-07T07:20:12.677Z",
          "wordCount": 685,
          "title": "Stimuli-Aware Visual Emotion Analysis. (arXiv:2109.01812v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01668",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sanner_A/0/1/0/all/0/1\">Antoine Sanner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "The recent achievements of Deep Learning rely on the test data being similar\nin distribution to the training data. In an ideal case, Deep Learning models\nwould achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make\npredictions on out-of-distribution data. Yet in practice, models usually fail\nto generalize well when facing a shift in distribution. Several methods were\nthereby designed to improve the robustness of the features learned by a model\nthrough Regularization- or Domain-Prediction-based schemes. Segmenting medical\nimages such as MRIs of the hippocampus is essential for the diagnosis and\ntreatment of neuropsychiatric disorders. But these brain images often suffer\nfrom distribution shift due to the patient's age and various pathologies\naffecting the shape of the organ. In this work, we evaluate OoD Generalization\nsolutions for the problem of hippocampus segmentation in MR data using both\nfully- and semi-supervised training. We find that no method performs reliably\nin all experiments. Only the V-REx loss stands out as it remains easy to tune,\nwhile it outperforms a standard U-Net in most cases.",
          "link": "http://arxiv.org/abs/2109.01668",
          "publishedOn": "2021-09-07T07:20:12.669Z",
          "wordCount": 629,
          "title": "How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02103",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Md Gulzar Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shiren_Y/0/1/0/all/0/1\">Ye Shiren</a>",
          "description": "Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the\nChest (CXR) as a complementary screening technique to RT-PCR testing grows to\nits clinical use for respiratory complaints. Many new deep learning approaches\nhave developed as a consequence. The goal of this research is to assess the\nconvolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray\nimages of chest. The performance of CNN with one, three, and four convolution\nlayers has been evaluated in this research. A dataset of 13,808 CXR photographs\nare used in this research. When evaluated on X-ray images with three splits of\nthe dataset, our preliminary experimental results show that the CNN model with\nthree convolution layers can reliably detect with 96 percent accuracy\n(precision being 96 percent). This fact indicates the commitment of our\nsuggested model for reliable screening of COVID-19.",
          "link": "http://arxiv.org/abs/2109.02103",
          "publishedOn": "2021-09-07T07:20:12.662Z",
          "wordCount": 656,
          "title": "Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN. (arXiv:2109.02103v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Debnath_B/0/1/0/all/0/1\">Biplob Debnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_W/0/1/0/all/0/1\">Wang-Pin Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drolia_U/0/1/0/all/0/1\">Utsav Drolia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat Chakradhar</a>",
          "description": "Identification of people with elevated body temperature can reduce or\ndramatically slow down the spread of infectious diseases like COVID-19. We\npresent a novel fever-screening system, F3S, that uses edge machine learning\ntechniques to accurately measure core body temperatures of multiple individuals\nin a free-flow setting. F3S performs real-time sensor fusion of visual camera\nwith thermal camera data streams to detect elevated body temperature, and it\nhas several unique features: (a) visual and thermal streams represent very\ndifferent modalities, and we dynamically associate semantically-equivalent\nregions across visual and thermal frames by using a new, dynamic alignment\ntechnique that analyzes content and context in real-time, (b) we track people\nthrough occlusions, identify the eye (inner canthus), forehead, face and head\nregions where possible, and provide an accurate temperature reading by using a\nprioritized refinement algorithm, and (c) we robustly detect elevated body\ntemperature even in the presence of personal protective equipment like masks,\nor sunglasses or hats, all of which can be affected by hot weather and lead to\nspurious temperature readings. F3S has been deployed at over a dozen large\ncommercial establishments, providing contact-less, free-flow, real-time fever\nscreening for thousands of employees and customers in indoors and outdoor\nsettings.",
          "link": "http://arxiv.org/abs/2109.01733",
          "publishedOn": "2021-09-07T07:20:12.521Z",
          "wordCount": null,
          "title": "F3S: Free Flow Fever Screening. (arXiv:2109.01733v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chae_Y/0/1/0/all/0/1\">Yujeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>",
          "description": "Event cameras are novel sensors that perceive the per-pixel intensity changes\nand output asynchronous event streams with high dynamic range and less motion\nblur. It has been shown that events alone can be used for end-task learning,\n\\eg, semantic segmentation, based on encoder-decoder-like networks. However, as\nevents are sparse and mostly reflect edge information, it is difficult to\nrecover original details merely relying on the decoder. Moreover, most methods\nresort to pixel-wise loss alone for supervision, which might be insufficient to\nfully exploit the visual details from sparse events, thus leading to less\noptimal performance. In this paper, we propose a simple yet flexible two-stream\nframework named Dual Transfer Learning (DTL) to effectively enhance the\nperformance on the end-tasks without adding extra inference cost. The proposed\napproach consists of three parts: event to end-task learning (EEL) branch,\nevent to image translation (EIT) branch, and transfer learning (TL) module that\nsimultaneously explores the feature-level affinity information and pixel-level\nknowledge from the EIT branch to improve the EEL branch. This simple yet novel\nmethod leads to strong representation learning from events and is evidenced by\nthe significant performance boost on the end-tasks such as semantic\nsegmentation and depth estimation.",
          "link": "http://arxiv.org/abs/2109.01801",
          "publishedOn": "2021-09-07T07:20:12.519Z",
          "wordCount": null,
          "title": "Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation. (arXiv:2109.01801v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01824",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_X/0/1/0/all/0/1\">Xiaojun Ning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yuanlai He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Ronghao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehman_L/0/1/0/all/0/1\">Li-wei H. Lehman</a>",
          "description": "Sleep stage classification is essential for sleep assessment and disease\ndiagnosis. Although previous attempts to classify sleep stages have achieved\nhigh classification performance, several challenges remain open: 1) How to\neffectively utilize time-varying spatial and temporal features from\nmulti-channel brain signals remains challenging. Prior works have not been able\nto fully utilize the spatial topological information among brain regions. 2)\nDue to the many differences found in individual biological signals, how to\novercome the differences of subjects and improve the generalization of deep\nneural networks is important. 3) Most deep learning methods ignore the\ninterpretability of the model to the brain. To address the above challenges, we\npropose a multi-view spatial-temporal graph convolutional networks (MSTGCN)\nwith domain generalization for sleep stage classification. Specifically, we\nconstruct two brain view graphs for MSTGCN based on the functional connectivity\nand physical distance proximity of the brain regions. The MSTGCN consists of\ngraph convolutions for extracting spatial features and temporal convolutions\nfor capturing the transition rules among sleep stages. In addition, attention\nmechanism is employed for capturing the most relevant spatial-temporal\ninformation for sleep stage classification. Finally, domain generalization and\nMSTGCN are integrated into a unified framework to extract subject-invariant\nsleep features. Experiments on two public datasets demonstrate that the\nproposed model outperforms the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2109.01824",
          "publishedOn": "2021-09-07T07:20:12.511Z",
          "wordCount": null,
          "title": "Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>",
          "description": "3D hand-mesh reconstruction from RGB images facilitates many applications,\nincluding augmented reality (AR). However, this requires not only real-time\nspeed and accurate hand pose and shape but also plausible mesh-image alignment.\nWhile existing works already achieve promising results, meeting all three\nrequirements is very challenging. This paper presents a novel pipeline by\ndecoupling the hand-mesh reconstruction task into three stages: a joint stage\nto predict hand joints and segmentation; a mesh stage to predict a rough hand\nmesh; and a refine stage to fine-tune it with an offset mesh for mesh-image\nalignment. With careful design in the network structure and in the loss\nfunctions, we can promote high-quality finger-level mesh-image alignment and\ndrive the models together to deliver real-time predictions. Extensive\nquantitative and qualitative results on benchmark datasets demonstrate that the\nquality of our results outperforms the state-of-the-art methods on\nhand-mesh/pose precision and hand-image alignment. In the end, we also showcase\nseveral real-time AR scenarios.",
          "link": "http://arxiv.org/abs/2109.01723",
          "publishedOn": "2021-09-07T07:20:12.503Z",
          "wordCount": null,
          "title": "Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. (arXiv:2109.01723v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonbong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1\">Lourdes Agapito</a>",
          "description": "CodeNeRF is an implicit 3D neural representation that learns the variation of\nobject shapes and textures across a category and can be trained, from a set of\nposed images, to synthesize novel views of unseen objects. Unlike the original\nNeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture\nby learning separate embeddings. At test time, given a single unposed image of\nan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and\nappearance codes via optimization. Unseen objects can be reconstructed from a\nsingle image, and then rendered from new viewpoints or their shape and texture\nedited by varying the latent codes. We conduct experiments on the SRN\nbenchmark, which show that CodeNeRF generalises well to unseen objects and\nachieves on-par performance with methods that require known camera pose at test\ntime. Our results on real-world images demonstrate that CodeNeRF can bridge the\nsim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}",
          "link": "http://arxiv.org/abs/2109.01750",
          "publishedOn": "2021-09-07T07:20:12.502Z",
          "wordCount": null,
          "title": "CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenfeng Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gui Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhipeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>",
          "description": "In a complex urban scene, observation from a single sensor unavoidably leads\nto voids in observations, failing to describe urban objects in a comprehensive\nmanner. In this paper, we propose a spatio-temporal-spectral-angular\nobservation model to integrate observations from UAV and mobile mapping vehicle\nplatform, realizing a joint, coordinated observation operation from both air\nand ground. We develop a multi-source remote sensing data acquisition system to\neffectively acquire multi-angle data of complex urban scenes. Multi-source data\nfusion solves the missing data problem caused by occlusion and achieves\naccurate, rapid, and complete collection of holographic spatial and temporal\ninformation in complex urban scenes. We carried out an experiment on Baisha\nTown, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and\nmobile mapping vehicle. We first extracted the point cloud from UAV and then\nintegrated the UAV and mobile mapping vehicle point cloud. The integrated\nresults combined both the characteristic of UAV and mobile mapping vehicle\npoint cloud, confirming the practicability of the proposed joint data\nacquisition platform and the effectiveness of spatio-temporal-spectral-angular\nobservation model. Compared with the observation from UAV or mobile mapping\nvehicle alone, the integrated system provides an effective data acquisition\nsolution towards comprehensive urban monitoring.",
          "link": "http://arxiv.org/abs/2109.00900",
          "publishedOn": "2021-09-07T07:20:12.426Z",
          "wordCount": null,
          "title": "Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping. (arXiv:2109.00900v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nataraj_L/0/1/0/all/0/1\">Lakshmanan Nataraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gudavalli_C/0/1/0/all/0/1\">Chandrakanth Gudavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_T/0/1/0/all/0/1\">Tajuddin Manhar Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Shivkumar Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>",
          "description": "Seam carving is a method to resize an image in a content aware fashion.\nHowever, this method can also be used to carve out objects from images. In this\npaper, we propose a two-step method to detect and localize seam carved images.\nFirst, we build a detector to detect small patches in an image that has been\nseam carved. Next, we compute a heatmap on an image based on the patch\ndetector's output. Using these heatmaps, we build another detector to detect if\na whole image is seam carved or not. Our experimental results show that our\napproach is effective in detecting and localizing seam carved images.",
          "link": "http://arxiv.org/abs/2109.01764",
          "publishedOn": "2021-09-07T07:20:12.423Z",
          "wordCount": null,
          "title": "Seam Carving Detection and Localization using Two-Stage Deep Neural Networks. (arXiv:2109.01764v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "Catastrophic forgetting of previously learned knowledge while learning new\ntasks is a widely observed limitation of contemporary neural networks. Although\nmany continual learning methods are proposed to mitigate this drawback, the\nmain question remains unanswered: what is the root cause of catastrophic\nforgetting? In this work, we aim at answering this question by posing and\nvalidating a set of research hypotheses related to the specificity of\nrepresentations built internally by neural models. More specifically, we design\na set of empirical evaluations that compare the robustness of representations\nin discriminative and generative models against catastrophic forgetting. We\nobserve that representations learned by discriminative models are more prone to\ncatastrophic forgetting than their generative counterparts, which sheds new\nlight on the advantages of developing generative models for continual learning.\nFinally, our work opens new research pathways and possibilities to adopt\ngenerative models in continual learning beyond mere replay mechanisms.",
          "link": "http://arxiv.org/abs/2109.01844",
          "publishedOn": "2021-09-07T07:20:12.414Z",
          "wordCount": null,
          "title": "On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01799",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runnan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Penghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>",
          "description": "Personalized video highlight detection aims to shorten a long video to\ninteresting moments according to a user's preference, which has recently raised\nthe community's attention. Current methods regard the user's history as\nholistic information to predict the user's preference but negating the inherent\ndiversity of the user's interests, resulting in vague preference\nrepresentation. In this paper, we propose a simple yet efficient preference\nreasoning framework (PR-Net) to explicitly take the diverse interests into\naccount for frame-level highlight prediction. Specifically, distinct\nuser-specific preferences for each input query frame are produced, presented as\nthe similarity weighted sum of history highlights to the corresponding query\nframe. Next, distinct comprehensive preferences are formed by the user-specific\npreferences and a learnable generic preference for more overall highlight\nmeasurement. Lastly, the degree of highlight and non-highlight for each query\nframe is calculated as semantic similarity to its comprehensive and\nnon-highlight preferences, respectively. Besides, to alleviate the ambiguity\ndue to the incomplete annotation, a new bi-directional contrastive loss is\nproposed to ensure a compact and differentiable metric space. In this way, our\nmethod significantly outperforms state-of-the-art methods with a relative\nimprovement of 12% in mean accuracy precision.",
          "link": "http://arxiv.org/abs/2109.01799",
          "publishedOn": "2021-09-07T07:20:12.389Z",
          "wordCount": null,
          "title": "PR-Net: Preference Reasoning for Personalized Video Highlight Detection. (arXiv:2109.01799v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01071",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saboo_A/0/1/0/all/0/1\">Aakash Saboo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramachandran_S/0/1/0/all/0/1\">Sai Niranjan Ramachandran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dierkes_K/0/1/0/all/0/1\">Kai Dierkes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>",
          "description": "Disease-aware image editing by means of generative adversarial networks\n(GANs) constitutes a promising avenue for advancing the use of AI in the\nhealthcare sector. Here, we present a proof of concept of this idea. While\nGAN-based techniques have been successful in generating and manipulating\nnatural images, their application to the medical domain, however, is still in\nits infancy. Working with the CheXpert data set, we show that StyleGAN can be\ntrained to generate realistic chest X-rays. Inspired by the Cyclic Reverse\nGenerator (CRG) framework, we train an encoder that allows for faithfully\ninverting the generator on synthetic X-rays and provides organ-level\nreconstructions of real ones. Employing a guided manipulation of latent codes,\nwe confer the medical condition of cardiomegaly (increased heart size) onto\nreal X-rays from healthy patients. This work was presented in the Medical\nImaging meets Neurips Workshop 2020, which was held as part of the 34th\nConference on Neural Information Processing Systems (NeurIPS 2020) in\nVancouver, Canada",
          "link": "http://arxiv.org/abs/2109.01071",
          "publishedOn": "2021-09-07T07:20:12.273Z",
          "wordCount": null,
          "title": "Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.13978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng-Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1\">Feng Mao</a>",
          "description": "Semantic segmentation models trained on public datasets have achieved great\nsuccess in recent years. However, these models didn't consider the\npersonalization issue of segmentation though it is important in practice. In\nthis paper, we address the problem of personalized image segmentation. The\nobjective is to generate more accurate segmentation results on unlabeled\npersonalized images by investigating the data's personalized traits. To open up\nfuture research in this area, we collect a large dataset containing various\nusers' personalized images called PIS (Personalized Image Semantic\nSegmentation). We also survey some recent researches related to this problem\nand report their performance on our dataset. Furthermore, by observing the\ncorrelation among a user's personalized images, we propose a baseline method\nthat incorporates the inter-image context when segmenting certain images.\nExtensive experiments show that our method outperforms the existing methods on\nthe proposed dataset. The code and the PIS dataset will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2107.13978",
          "publishedOn": "2021-09-07T07:20:12.252Z",
          "wordCount": null,
          "title": "Personalized Image Semantic Segmentation. (arXiv:2107.13978v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1\">Leon Amadeus Varga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>",
          "description": "Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging\ntask. The recordings are mostly sparse and contain only small objects. In this\nwork, we propose a simple tiling method that improves the detection capability\nin the remote sensing case without modifying the model itself. By reducing the\nbackground bias and enabling the usage of higher image resolutions during\ntraining, our method can improve the performance of models substantially. The\nprocedure was validated on three different data sets and outperformed similar\napproaches in performance and speed.",
          "link": "http://arxiv.org/abs/2106.02288",
          "publishedOn": "2021-09-07T07:20:12.246Z",
          "wordCount": null,
          "title": "Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.02093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1\">Prakhar Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yao Chen</a>",
          "description": "We unveil a long-standing problem in the prevailing co-saliency detection\nsystems: there is indeed inconsistency between training and testing.\nConstructing a high-quality co-saliency detection dataset involves\ntime-consuming and labor-intensive pixel-level labeling, which has forced most\nrecent works to rely instead on semantic segmentation or saliency detection\ndatasets for training. However, the lack of proper co-saliency and the absence\nof multiple foreground objects in these datasets can lead to spurious\nvariations and inherent biases learned by models. To tackle this, we introduce\nthe idea of counterfactual training through context adjustment, and propose a\n\"cost-free\" group-cut-paste (GCP) procedure to leverage images from\noff-the-shelf saliency detection datasets and synthesize new samples. Following\nGCP, we collect a novel dataset called Context Adjustment Training (CAT). CAT\nconsists of 33,500 images, making it four times larger than the current\nco-saliency detection datasets. All images are automatically annotated with\nhigh-quality mask annotations, object categories, and edge maps. Extensive\nexperiments with state-of-the-art models are conducted to demonstrate the\nsuperiority of our dataset. We hope that the scale, diversity, and quality of\nour dataset can benefit researchers in this area and beyond. The dataset and\nbenchmark toolkit will be publicly accessible through our project page.",
          "link": "http://arxiv.org/abs/2108.02093",
          "publishedOn": "2021-09-07T07:20:12.236Z",
          "wordCount": null,
          "title": "Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02252",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_J/0/1/0/all/0/1\">Junhua Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bermejo_I/0/1/0/all/0/1\">Inigo Bermejo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Dekker_A/0/1/0/all/0/1\">Andre Dekker</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wee_L/0/1/0/all/0/1\">Leonard Wee</a>",
          "description": "Radiomics is an active area of research focusing on high throughput feature\nextraction from medical images with a wide array of applications in clinical\npractice, such as clinical decision support in oncology. However, noise in low\ndose computed tomography (CT) scans can impair the accurate extraction of\nradiomic features. In this article, we investigate the possibility of using\ndeep learning generative models to improve the performance of radiomics from\nlow dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics\nand LIDC-IDRI - as test datasets for two tasks - pre-treatment survival\nprediction and lung cancer diagnosis. We used encoder-decoder networks and\nconditional generative adversarial networks (CGANs) trained in a previous study\nas generative models to transform low dose CT images into full dose CT images.\nRadiomic features extracted from the original and improved CT scans were used\nto build two classifiers - a support vector machine (SVM) and a deep attention\nbased multiple instance learning model - for survival prediction and lung\ncancer diagnosis respectively. Finally, we compared the performance of the\nmodels derived from the original and improved CT scans. Encoder-decoder\nnetworks and CGANs improved the area under the curve (AUC) of survival\nprediction from 0.52 to 0.57 (p-value<0.01). On the other hand, Encoder-decoder\nnetwork and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88\nand 0.89 respectively (p-value<0.01). Moreover, there are no statistically\nsignificant differences in improving AUC by using encoder-decoder network and\nCGAN (p-value=0.34) when networks trained at 75 and 100 epochs. Generative\nmodels can improve the performance of low dose CT-based radiomics in different\ntasks. Hence, denoising using generative models seems to be a necessary\npre-processing step for calculating radiomic features from low dose CTs.",
          "link": "http://arxiv.org/abs/2109.02252",
          "publishedOn": "2021-09-07T07:20:12.235Z",
          "wordCount": null,
          "title": "Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study. (arXiv:2109.02252v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01880",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yexing Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lafci_B/0/1/0/all/0/1\">Berkan Lafci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luzgin_A/0/1/0/all/0/1\">Artur Luzgin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klohs_J/0/1/0/all/0/1\">Jan Klohs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dean_Ben_X/0/1/0/all/0/1\">Xose Luis Dean-Ben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_R/0/1/0/all/0/1\">Ruiqing Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razansky_D/0/1/0/all/0/1\">Daniel Razansky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wuwei Ren</a>",
          "description": "Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging\nmethod providing multiplex molecular and functional information from the rodent\nbrain. It can be greatly augmented by magnetic resonance imaging (MRI) that\noffers excellent soft-tissue contrast and high-resolution brain anatomy.\nNevertheless, registration of multi-modal images remains challenging, chiefly\ndue to the entirely different image contrast rendered by these modalities.\nPreviously reported registration algorithms mostly relied on manual\nuser-dependent brain segmentation, which compromised data interpretation and\naccurate quantification. Here we propose a fully automated registration method\nfor MSOT-MRI multimodal imaging empowered by deep learning. The automated\nworkflow includes neural network-based image segmentation to generate suitable\nmasks, which are subsequently registered using an additional neural network.\nPerformance of the algorithm is showcased with datasets acquired by\ncross-sectional MSOT and high-field MRI preclinical scanners. The automated\nregistration method is further validated with manual and half-automated\nregistration, demonstrating its robustness and accuracy.",
          "link": "http://arxiv.org/abs/2109.01880",
          "publishedOn": "2021-09-07T07:20:12.233Z",
          "wordCount": null,
          "title": "Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02711",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sarasaen_C/0/1/0/all/0/1\">Chompunuch Sarasaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breitkopf_M/0/1/0/all/0/1\">Mario Breitkopf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>",
          "description": "Dynamic imaging is a beneficial tool for interventions to assess\nphysiological changes. Nonetheless during dynamic MRI, while achieving a high\ntemporal resolution, the spatial resolution is compromised. To overcome this\nspatio-temporal trade-off, this research presents a super-resolution (SR) MRI\nreconstruction with prior knowledge based fine-tuning to maximise spatial\ninformation while reducing the required scan-time for dynamic MRIs. An U-Net\nbased network with perceptual loss is trained on a benchmark dataset and\nfine-tuned using one subject-specific static high resolution MRI as prior\nknowledge to obtain high resolution dynamic images during the inference stage.\n3D dynamic data for three subjects were acquired with different parameters to\ntest the generalisation capabilities of the network. The method was tested for\ndifferent levels of in-plane undersampling for dynamic MRI. The reconstructed\ndynamic SR results after fine-tuning showed higher similarity with the high\nresolution ground-truth, while quantitatively achieving statistically\nsignificant improvement. The average SSIM of the lowest resolution experimented\nduring this research (6.25~\\% of the k-space) before and after fine-tuning were\n0.939 $\\pm$ 0.008 and 0.957 $\\pm$ 0.006 respectively. This could theoretically\nresult in an acceleration factor of 16, which can potentially be acquired in\nless than half a second. The proposed approach shows that the super-resolution\nMRI reconstruction with prior-information can alleviate the spatio-temporal\ntrade-off in dynamic MRI, even for high acceleration factors.",
          "link": "http://arxiv.org/abs/2102.02711",
          "publishedOn": "2021-09-07T07:20:12.232Z",
          "wordCount": null,
          "title": "Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1\">Pedro H. T. Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Jos&#xe9; Marcato Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Semantic segmentation is a classic computer vision task with multiple\napplications, which includes medical and remote sensing image analysis. Despite\nrecent advances with deep-based approaches, labeling samples (pixels) for\ntraining models is laborious and, in some cases, unfeasible. In this paper, we\npresent two novel meta learning methods, named WeaSeL and ProtoSeg, for the\nfew-shot semantic segmentation task with sparse annotations. We conducted\nextensive evaluation of the proposed methods in different applications (12\ndatasets) in medical imaging and agricultural remote sensing, which are very\ndistinct fields of knowledge and usually subject to data scarcity. The results\ndemonstrated the potential of our method, achieving suitable results for\nsegmenting both coffee/orange crops and anatomical parts of the human body in\ncomparison with full dense annotation.",
          "link": "http://arxiv.org/abs/2109.01693",
          "publishedOn": "2021-09-07T07:20:12.228Z",
          "wordCount": null,
          "title": "Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.10310",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoxiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weijian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Consider a scenario where we are supplied with a number of ready-to-use\nmodels trained on a certain source domain and hope to directly apply the most\nappropriate ones to different target domains based on the models' relative\nperformance. Ideally we should annotate a validation set for model performance\nassessment on each new target environment, but such annotations are often very\nexpensive. Under this circumstance, we introduce the problem of ranking models\nin unlabeled new environments. For this problem, we propose to adopt a proxy\ndataset that 1) is fully labeled and 2) well reflects the true model rankings\nin a given target environment, and use the performance rankings on the proxy\nsets as surrogates. We first select labeled datasets as the proxy.\nSpecifically, datasets that are more similar to the unlabeled target domain are\nfound to better preserve the relative performance rankings. Motivated by this,\nwe further propose to search the proxy set by sampling images from various\ndatasets that have similar distributions as the target. We analyze the problem\nand its solutions on the person re-identification (re-ID) task, for which\nsufficient datasets are publicly available, and show that a carefully\nconstructed proxy set effectively captures relative performance ranking in new\nenvironments. Code is available at \\url{https://github.com/sxzrt/Proxy-Set}.",
          "link": "http://arxiv.org/abs/2108.10310",
          "publishedOn": "2021-09-07T07:20:12.227Z",
          "wordCount": null,
          "title": "Ranking Models in Unlabeled New Environments. (arXiv:2108.10310v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Letunovskiy_A/0/1/0/all/0/1\">Alexey Letunovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korviakov_V/0/1/0/all/0/1\">Vladimir Korviakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polovnikov_V/0/1/0/all/0/1\">Vladimir Polovnikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kargapoltseva_A/0/1/0/all/0/1\">Anastasiia Kargapoltseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurenko_I/0/1/0/all/0/1\">Ivan Mazurenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yepan Xiong</a>",
          "description": "In recent years Deep Learning reached significant results in many practical\nproblems, such as computer vision, natural language processing, speech\nrecognition and many others. For many years the main goal of the research was\nto improve the quality of models, even if the complexity was impractically\nhigh. However, for the production solutions, which often require real-time\nwork, the latency of the model plays a very important role. Current\nstate-of-the-art architectures are found with neural architecture search (NAS)\ntaking model complexity into account. However, designing of the search space\nsuitable for specific hardware is still a challenging task. To address this\nproblem we propose a measure of hardware efficiency of neural architecture\nsearch space - matrix efficiency measure (MEM); a search space comprising of\nhardware-efficient operations; a latency-aware scaling method; and ISyNet - a\nset of architectures designed to be fast on the specialized neural processing\nunit (NPU) hardware and accurate at the same time. We show the advantage of the\ndesigned architectures for the NPU devices on ImageNet and the generalization\nability for the downstream classification and detection tasks.",
          "link": "http://arxiv.org/abs/2109.01932",
          "publishedOn": "2021-09-07T07:20:12.226Z",
          "wordCount": null,
          "title": "ISyNet: Convolutional Neural Networks design for AI accelerator. (arXiv:2109.01932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1\">Jingyuan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Huansheng Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yaping Wan</a>",
          "description": "Human activity recognition plays an increasingly important role not only in\nour daily lives, but also in the medical and rehabilitation fields. The\ndevelopment of deep learning has also contributed to the advancement of human\nactivity recognition, but the large amount of data annotation work required to\ntrain deep learning models is a major obstacle to the development of human\nactivity recognition. Contrastive learning has started to be used in the field\nof sensor-based human activity recognition due to its ability to avoid the cost\nof labeling large datasets and its ability to better distinguish between sample\nrepresentations of different instances. Among them, data augmentation, an\nimportant part of contrast learning, has a significant impact on model\neffectiveness, but current data augmentation methods do not perform too\nsuccessfully in contrast learning frameworks for wearable sensor-based activity\nrecognition. To optimize the effect of contrast learning models, in this paper,\nwe investigate the sampling frequency of sensors and propose a resampling data\naugmentation method. In addition, we also propose a contrast learning framework\nbased on human activity recognition and apply the resampling augmentation\nmethod to the data augmentation phase of contrast learning. The experimental\nresults show that the resampling augmentation method outperforms supervised\nlearning by 9.88% on UCI HAR and 7.69% on Motion Sensor in the fine-tuning\nevaluation of contrast learning with a small amount of labeled data, and also\nreveal that not all data augmentation methods will have positive effects in the\ncontrast learning framework. Finally, we explored the influence of the\ncombination of different augmentation methods on contrastive learning, and the\nexperimental results showed that the effect of most combination augmentation\nmethods was better than that of single augmentation.",
          "link": "http://arxiv.org/abs/2109.02054",
          "publishedOn": "2021-09-07T07:20:12.224Z",
          "wordCount": null,
          "title": "Sensor Data Augmentation with Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2103.06911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qiaojun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1\">Sai Jadhav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1\">Nikolay Atanasov</a>",
          "description": "This paper considers online object-level mapping using partial point-cloud\nobservations obtained online in an unknown environment. We develop and approach\nfor fully Convolutional Object Retrieval and Symmetry-AIded Registration\n(CORSAIR). Our model extends the Fully Convolutional Geometric Features model\nto learn a global object-shape embedding in addition to local point-wise\nfeatures from the point-cloud observations. The global feature is used to\nretrieve a similar object from a category database, and the local features are\nused for robust pose registration between the observed and the retrieved\nobject. Our formulation also leverages symmetries, present in the object\nshapes, to obtain promising local-feature pairs from different symmetry classes\nfor matching. We present results from synthetic and real-world datasets with\ndifferent object categories to verify the robustness of our method.",
          "link": "http://arxiv.org/abs/2103.06911",
          "publishedOn": "2021-09-07T07:20:12.213Z",
          "wordCount": null,
          "title": "CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>",
          "description": "GAN-based techniques that generate and synthesize realistic faces have caused\nsevere social concerns and security problems. Existing methods for detecting\nGAN-generated faces can perform well on limited public datasets. However,\nimages from existing public datasets do not represent real-world scenarios well\nenough in terms of view variations and data distributions (where real faces\nlargely outnumber synthetic faces). The state-of-the-art methods do not\ngeneralize well in real-world problems and lack the interpretability of\ndetection results. Performance of existing GAN-face detection models degrades\nsignificantly when facing imbalanced data distributions. To address these\nshortcomings, we propose a robust, attentive, end-to-end network that can spot\nGAN-generated faces by analyzing their eye inconsistencies. Specifically, our\nmodel learns to identify inconsistent eye components by localizing and\ncomparing the iris artifacts between the two eyes automatically. Our deep\nnetwork addresses the imbalance learning issues by considering the AUC loss and\nthe traditional cross-entropy loss jointly. Comprehensive evaluations of the\nFFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the\nsuperiority of the proposed method.",
          "link": "http://arxiv.org/abs/2109.02167",
          "publishedOn": "2021-09-07T07:20:12.208Z",
          "wordCount": null,
          "title": "Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramaiyer_R/0/1/0/all/0/1\">Raghav B. Venkataramaiyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Abhishek Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Saisha Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Hatching is a common method used by artists to accentuate the third dimension\nof a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete\nwith a human at hatching generic three-dimensional (3D) shapes, and also tries\nto assist her in a form exploration exercise. The novelty of our approach lies\nin the fact that we make no assumptions about the input other than that it\nrepresents a 3D shape, and yet, given a contextual information of illumination\nand texture, we synthesise an accurate hatch pattern over the sketch, without\naccess to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet\neffective method to synthesise a sufficiently large high fidelity dataset,\npertinent to task; b) creating a pipeline with conditional generative\nadversarial network (CGAN); and c) creating an interactive utility with GIMP,\nthat is a tool for artists to engage with automated hatching or a\nform-exploration exercise. User evaluation of the tool suggests that the model\nperformance does generalise satisfactorily over diverse input, both in terms of\nstyle as well as shape. A simple comparison of inception scores suggest that\nthe generated distribution is as diverse as the ground truth.",
          "link": "http://arxiv.org/abs/2011.06822",
          "publishedOn": "2021-09-07T07:20:12.109Z",
          "wordCount": null,
          "title": "SHAD3S: A model to Sketch, Shade and Shadow. (arXiv:2011.06822v3 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_K/0/1/0/all/0/1\">Kevin Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>",
          "description": "Multimodal self-supervised learning is getting more and more attention as it\nallows not only to train large networks without human supervision but also to\nsearch and retrieve data across various modalities. In this context, this paper\nproposes a self-supervised training framework that learns a common multimodal\nembedding space that, in addition to sharing representations across different\nmodalities, enforces a grouping of semantically similar instances. To this end,\nwe extend the concept of instance-level contrastive learning with a multimodal\nclustering step in the training pipeline to capture semantic similarities\nacross modalities. The resulting embedding space enables retrieval of samples\nacross all modalities, even from unseen datasets and different domains. To\nevaluate our approach, we train our model on the HowTo100M dataset and evaluate\nits zero-shot retrieval capabilities in two challenging domains, namely\ntext-to-video retrieval, and temporal action localization, showing\nstate-of-the-art results on four different datasets.",
          "link": "http://arxiv.org/abs/2104.12671",
          "publishedOn": "2021-09-07T07:20:12.106Z",
          "wordCount": null,
          "title": "Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos. (arXiv:2104.12671v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01841",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Iida_K/0/1/0/all/0/1\">Kenta Iida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>",
          "description": "In this paper, we propose a privacy-preserving image-retrieval scheme using a\ncodebook generated by using a plain-image dataset. Encryption-then-compression\n(EtC) images, which were proposed for EtC systems, have been used in\nconventional privacy-preserving image-retrieval schemes, in which a codebook is\ngenerated from EtC images uploaded by image owners, and extended SIMPLE\ndescriptors are then calculated as image descriptors by using the codebook. In\ncontrast, in the proposed scheme, a codebook is generated from a dataset\nindependent of uploaded images. The use of an independent dataset enables us\nnot only to use a codebook that does not require recalculation but also to\nconstantly provide a high retrieval accuracy. In an experiment, the proposed\nscheme is demonstrated to maintain a high retrieval performance, even if\ncodebooks are generated from a plain image dataset independent of image owners'\nencrypted images.",
          "link": "http://arxiv.org/abs/2109.01841",
          "publishedOn": "2021-09-07T07:20:12.092Z",
          "wordCount": null,
          "title": "A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset. (arXiv:2109.01841v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asao_Y/0/1/0/all/0/1\">Yasuhiko Asao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagase_J/0/1/0/all/0/1\">Jumpei Nagase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakamoto_R/0/1/0/all/0/1\">Ryotaro Sakamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1\">Shiro Takagi</a>",
          "description": "Extracting informative features from images has been of capital importance in\ncomputer vision. In this paper, we propose a way to extract such features from\nimages by a method based on algebraic topology. To that end, we construct a\nweighted graph from an image, which extracts local information of an image. By\nconsidering this weighted graph as a pseudo-metric space, we construct a\nVietoris-Rips complex with a parameter $\\varepsilon$ by a well-known process of\nalgebraic topology. We can extract information of complexity of the image and\ncan detect a sub-image with a relatively high concentration of information from\nthis Vietoris-Rips complex. The parameter $\\varepsilon$ of the Vietoris-Rips\ncomplex produces robustness to noise. We empirically show that the extracted\nfeature captures well images' characteristics.",
          "link": "http://arxiv.org/abs/2109.02231",
          "publishedOn": "2021-09-07T07:20:12.092Z",
          "wordCount": null,
          "title": "Image recognition via Vietoris-Rips complex. (arXiv:2109.02231v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1\">Long-Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1\">Anh Tuan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_Q/0/1/0/all/0/1\">Quynh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoai_M/0/1/0/all/0/1\">Minh Hoai</a>",
          "description": "Recovering the 3D structure of an object from a single image is a challenging\ntask due to its ill-posed nature. One approach is to utilize the plentiful\nphotos of the same object category to learn a strong 3D shape prior for the\nobject. This approach has successfully been demonstrated by a recent work of Wu\net al. (2020), which obtained impressive 3D reconstruction networks with\nunsupervised learning. However, their algorithm is only applicable to symmetric\nobjects. In this paper, we eliminate the symmetry requirement with a novel\nunsupervised algorithm that can learn a 3D reconstruction network from a\nmulti-image dataset. Our algorithm is more general and covers the\nsymmetry-required scenario as a special case. Besides, we employ a novel albedo\nloss that improves the reconstructed details and realisticity. Our method\nsurpasses the previous work in both quality and robustness, as shown in\nexperiments on datasets of various structures, including single-view,\nmulti-view, image-collection, and video sets.",
          "link": "http://arxiv.org/abs/2109.02288",
          "publishedOn": "2021-09-07T07:20:12.062Z",
          "wordCount": null,
          "title": "Toward Realistic Single-View 3D Object Reconstructionwith Unsupervised Learning from Multiple Images. (arXiv:2109.02288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hujun Yin</a>",
          "description": "The spatial attention mechanism captures long-range dependencies by\naggregating global contextual information to each query location, which is\nbeneficial for semantic segmentation. In this paper, we present a sparse\nspatial attention network (SSANet) to improve the efficiency of the spatial\nattention mechanism without sacrificing the performance. Specifically, a sparse\nnon-local (SNL) block is proposed to sample a subset of key and value elements\nfor each query element to capture long-range relations adaptively and generate\na sparse affinity matrix to aggregate contextual information efficiently.\nExperimental results show that the proposed approach outperforms other context\naggregation methods and achieves state-of-the-art performance on the\nCityscapes, PASCAL Context and ADE20K datasets.",
          "link": "http://arxiv.org/abs/2109.01915",
          "publishedOn": "2021-09-07T07:20:12.046Z",
          "wordCount": null,
          "title": "Sparse Spatial Attention Network for Semantic Segmentation. (arXiv:2109.01915v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_R/0/1/0/all/0/1\">Rujiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhibo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongpan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper tackles the problem of table structure parsing (TSP) from images\nin the wild. In contrast to existing studies that mainly focus on parsing\nwell-aligned tabular images with simple layouts from scanned PDF documents, we\naim to establish a practical table structure parsing system for real-world\nscenarios where tabular input images are taken or scanned with severe\ndeformation, bending or occlusions. For designing such a system, we propose an\napproach named Cycle-CenterNet on the top of CenterNet with a novel\ncycle-pairing module to simultaneously detect and group tabular cells into\nstructured tables. In the cycle-pairing module, a new pairing loss function is\nproposed for the network training. Alongside with our Cycle-CenterNet, we also\npresent a large-scale dataset, named Wired Table in the Wild (WTW), which\nincludes well-annotated structure parsing of multiple style tables in several\nscenes like the photo, scanning files, web pages, \\emph{etc.}. In experiments,\nwe demonstrate that our Cycle-CenterNet consistently achieves the best accuracy\nof table structure parsing on the new WTW dataset by 24.6\\% absolute\nimprovement evaluated by the TEDS metric. A more comprehensive experimental\nanalysis also validates the advantages of our proposed methods for the TSP\ntask.",
          "link": "http://arxiv.org/abs/2109.02199",
          "publishedOn": "2021-09-07T07:20:12.030Z",
          "wordCount": null,
          "title": "Parsing Table Structures in the Wild. (arXiv:2109.02199v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vogel_R/0/1/0/all/0/1\">Robin Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1\">Pierre Laforgue</a>",
          "description": "In practice, and more especially when training deep neural networks, visual\nrecognition rules are often learned based on various sources of information. On\nthe other hand, the recent deployment of facial recognition systems with uneven\npredictive performances on different population segments highlights the\nrepresentativeness issues possibly induced by a naive aggregation of image\ndatasets. Indeed, sampling bias does not vanish simply by considering larger\ndatasets, and ignoring its impact may completely jeopardize the generalization\ncapacity of the learned prediction rules. In this paper, we show how biasing\nmodels, originally introduced for nonparametric estimation in (Gill et al.,\n1988), and recently revisited from the perspective of statistical learning\ntheory in (Laforgue and Cl\\'emen\\c{c}on, 2019), can be applied to remedy these\nproblems in the context of visual recognition. Based on the (approximate)\nknowledge of the biasing mechanisms at work, our approach consists in\nreweighting the observations, so as to form a nearly debiased estimator of the\ntarget distribution. One key condition for our method to be theoretically valid\nis that the supports of the distributions generating the biased datasets at\ndisposal must overlap, and cover the support of the target distribution. In\norder to meet this requirement in practice, we propose to use a low dimensional\nimage representation, shared across the image databases. Finally, we provide\nnumerical experiments highlighting the relevance of our approach whenever the\nbiasing functions are appropriately chosen.",
          "link": "http://arxiv.org/abs/2109.02357",
          "publishedOn": "2021-09-07T07:20:11.967Z",
          "wordCount": null,
          "title": "Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pirnay_J/0/1/0/all/0/1\">Jonathan Pirnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_K/0/1/0/all/0/1\">Keng Chai</a>",
          "description": "Anomaly detection in computer vision is the task of identifying images which\ndeviate from a set of normal images. A common approach is to train deep\nconvolutional autoencoders to inpaint covered parts of an image and compare the\noutput with the original image. By training on anomaly-free samples only, the\nmodel is assumed to not being able to reconstruct anomalous regions properly.\nFor anomaly detection by inpainting we suggest it to be beneficial to\nincorporate information from potentially distant regions. In particular we pose\nanomaly detection as a patch-inpainting problem and propose to solve it with a\npurely self-attention based approach discarding convolutions. The proposed\nInpainting Transformer (InTra) is trained to inpaint covered patches in a large\nsequence of image patches, thereby integrating information across large regions\nof the input image. When training from scratch, in comparison to other methods\nnot using extra training data, InTra achieves results on par with the current\nstate-of-the-art on the MVTec AD dataset for detection and surpassing them on\nsegmentation.",
          "link": "http://arxiv.org/abs/2104.13897",
          "publishedOn": "2021-09-07T07:20:11.955Z",
          "wordCount": null,
          "title": "Inpainting Transformer for Anomaly Detection. (arXiv:2104.13897v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnici_R/0/1/0/all/0/1\">Russell Sammut Bonnici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benning_M/0/1/0/all/0/1\">Martin Benning</a>",
          "description": "This research project investigates the application of deep learning to timbre\ntransfer, where the timbre of a source audio can be converted to the timbre of\na target audio with minimal loss in quality. The adopted approach combines\nVariational Autoencoders with Generative Adversarial Networks to construct\nmeaningful representations of the source audio and produce realistic\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\nfor transferring the vocal timbre between speakers and the URMP dataset for\ntransferring the musical timbre between instruments. Furthermore, variations of\nthe adopted approach are trained, and generalised performance is compared using\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\'et Audio\nDistance). It was found that a many-to-many approach supersedes a one-to-one\napproach in terms of reconstructive capabilities, and that the adoption of a\nbasic over a bottleneck residual block design is more suitable for enriching\ncontent information about a latent space. It was also found that the decision\non whether cyclic loss takes on a variational autoencoder or vanilla\nautoencoder approach does not have a significant impact on reconstructive and\nadversarial translation aspects of the model.",
          "link": "http://arxiv.org/abs/2109.02096",
          "publishedOn": "2021-09-07T07:20:11.952Z",
          "wordCount": null,
          "title": "Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jin-Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Zhu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liang-Jian Deng</a>",
          "description": "Hyperspectral image has become increasingly crucial due to its abundant\nspectral information. However, It has poor spatial resolution with the\nlimitation of the current imaging mechanism. Nowadays, many convolutional\nneural networks have been proposed for the hyperspectral image super-resolution\nproblem. However, convolutional neural network (CNN) based methods only\nconsider the local information instead of the global one with the limited\nkernel size of receptive field in the convolution operation. In this paper, we\ndesign a network based on the transformer for fusing the low-resolution\nhyperspectral images and high-resolution multispectral images to obtain the\nhigh-resolution hyperspectral images. Thanks to the representing ability of the\ntransformer, our approach is able to explore the intrinsic relationships of\nfeatures globally. Furthermore, considering the LR-HSIs hold the main spectral\nstructure, the network focuses on the spatial detail estimation releasing from\nthe burden of reconstructing the whole data. It reduces the mapping space of\nthe proposed network, which enhances the final performance. Various experiments\nand quality indexes show our approach's superiority compared with other\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.02079",
          "publishedOn": "2021-09-07T07:20:11.945Z",
          "wordCount": null,
          "title": "Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. (arXiv:2109.02079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02303",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Ziniu Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Maoqing Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Shuai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>",
          "description": "3D human shape and pose estimation is the essential task for human motion\nanalysis, which is widely used in many 3D applications. However, existing\nmethods cannot simultaneously capture the relations at multiple levels,\nincluding spatial-temporal level and human joint level. Therefore they fail to\nmake accurate predictions in some hard scenarios when there is cluttered\nbackground, occlusion, or extreme pose. To this end, we propose Multi-level\nAttention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder\n(STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in\na unified framework. STE consists of a series of cascaded blocks based on\nMulti-Head Self-Attention, and each block uses two parallel branches to learn\nspatial and temporal attention respectively. Meanwhile, KTD aims at modeling\nthe joint level attention. It regards pose estimation as a top-down\nhierarchical process similar to SMPL kinematic tree. With the training set of\n3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4\nmm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and\nHuman3.6M respectively. Our code is available at\nhttps://github.com/ziniuwan/maed.",
          "link": "http://arxiv.org/abs/2109.02303",
          "publishedOn": "2021-09-07T07:20:11.945Z",
          "wordCount": null,
          "title": "Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation. (arXiv:2109.02303v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12721",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaeyoung Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Inseop Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_G/0/1/0/all/0/1\">Geonseok Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "In multi-object detection using neural networks, the fundamental problem is,\n\"How should the network learn a variable number of bounding boxes in different\ninput images?\". Previous methods train a multi-object detection network through\na procedure that directly assigns the ground truth bounding boxes to the\nspecific locations of the network's output. However, this procedure makes the\ntraining of a multi-object detection network too heuristic and complicated. In\nthis paper, we reformulate the multi-object detection task as a problem of\ndensity estimation of bounding boxes. Instead of assigning each ground truth to\nspecific locations of network's output, we train a network by estimating the\nprobability density of bounding boxes in an input image using a mixture model.\nFor this purpose, we propose a novel network for object detection called\nMixture Density Object Detector (MDOD), and the corresponding objective\nfunction for the density-estimation-based training. We applied MDOD to MS COCO\ndataset. Our proposed method not only deals with multi-object detection\nproblems in a new approach, but also improves detection performances through\nMDOD. The code is available: https://github.com/yoojy31/MDOD.",
          "link": "http://arxiv.org/abs/1911.12721",
          "publishedOn": "2021-09-07T07:20:11.940Z",
          "wordCount": null,
          "title": "Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image. (arXiv:1911.12721v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Large pre-trained models such as CLIP offer consistent accuracy across a\nrange of data distributions when performing zero-shot inference (i.e., without\nfine-tuning on a specific dataset). Although existing fine-tuning approaches\nsubstantially improve accuracy in-distribution, they also reduce\nout-of-distribution robustness. We address this tension by introducing a simple\nand effective method for improving robustness: ensembling the weights of the\nzero-shot and fine-tuned models. Compared to standard fine-tuning, the\nresulting weight-space ensembles provide large accuracy improvements\nout-of-distribution, while matching or improving in-distribution accuracy. On\nImageNet and five derived distribution shifts, weight-space ensembles improve\nout-of-distribution accuracy by 2 to 10 percentage points while increasing\nin-distribution accuracy by nearly 1 percentage point relative to standard\nfine-tuning. These improvements come at no additional computational cost during\nfine-tuning or inference.",
          "link": "http://arxiv.org/abs/2109.01903",
          "publishedOn": "2021-09-07T07:20:11.899Z",
          "wordCount": null,
          "title": "Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rui_C/0/1/0/all/0/1\">Chen Rui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youwei_G/0/1/0/all/0/1\">Guo Youwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huafei_Z/0/1/0/all/0/1\">Zheng Huafei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongyu_J/0/1/0/all/0/1\">Jiang Hongyu</a>",
          "description": "Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role\nin UAV defense systems. Deep learning is widely adopted for UAV object\ndetection whereas researches on this topic are limited by the amount of dataset\nand small scale of UAV. To tackle these problems, a novel comprehensive\napproach that combines transfer learning based on simulation data and adaptive\nfusion is proposed. Firstly, the open-source plugin AirSim proposed by\nMicrosoft is used to generate mass realistic simulation data. Secondly,\ntransfer learning is applied to obtain a pre-trained YOLOv5 model on the\nsimulated dataset and fine-tuned model on the real-world dataset. Finally, an\nadaptive fusion mechanism is proposed to further improve small object detection\nperformance. Experiment results demonstrate the effectiveness of\nsimulation-based transfer learning which leads to a 2.7% performance increase\non UAV object detection. Furthermore, with transfer learning and adaptive\nfusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5\nmodel.",
          "link": "http://arxiv.org/abs/2109.01800",
          "publishedOn": "2021-09-07T07:20:11.884Z",
          "wordCount": null,
          "title": "A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion. (arXiv:2109.01800v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1\">Yossi Gandelsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David E. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlhoff_K/0/1/0/all/0/1\">Kai Kohlhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1\">Yael Pritch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>",
          "description": "Using only a model that was trained to predict where people look at images,\nand no additional training data, we can produce a range of powerful editing\neffects for reducing distraction in images. Given an image and a mask\nspecifying the region to edit, we backpropagate through a state-of-the-art\nsaliency model to parameterize a differentiable editing operator, such that the\nsaliency within the masked region is reduced. We demonstrate several operators,\nincluding: a recoloring operator, which learns to apply a color transform that\ncamouflages and blends distractors into their surroundings; a warping operator,\nwhich warps less salient image regions to cover distractors, gradually\ncollapsing objects into themselves and effectively removing them (an effect\nakin to inpainting); a GAN operator, which uses a semantic prior to fully\nreplace image regions with plausible, less salient alternatives. The resulting\neffects are consistent with cognitive research on the human visual system\n(e.g., since color mismatch is salient, the recoloring operator learns to\nharmonize objects' colors with their surrounding to reduce their saliency),\nand, importantly, are all achieved solely through the guidance of the\npretrained saliency model, with no additional supervision. We present results\non a variety of natural images and conduct a perceptual study to evaluate and\nvalidate the changes in viewers' eye-gaze between the original images and our\nedited results.",
          "link": "http://arxiv.org/abs/2109.01980",
          "publishedOn": "2021-09-07T07:20:11.873Z",
          "wordCount": null,
          "title": "Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01878",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_G/0/1/0/all/0/1\">Gauthier Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dedieu_J/0/1/0/all/0/1\">Jules Dedieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertrand_C/0/1/0/all/0/1\">Capucine Bertrand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshayedi_A/0/1/0/all/0/1\">Alireza Moshayedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mammadov_A/0/1/0/all/0/1\">Ali Mammadov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petit_S/0/1/0/all/0/1\">St&#xe9;phanie Petit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadj_S/0/1/0/all/0/1\">Saima Ben Hadj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fick_R/0/1/0/all/0/1\">Rutger H.J. Fick</a> (Tribvn Healthcare)",
          "description": "For the MIDOG mitosis detection challenge, we created a cascade algorithm\nconsisting of a Mask-RCNN detector, followed by a classification ensemble\nconsisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.\nThe MIDOG training data consists of 200 frames originating from four scanners,\nthree of which are annotated for mitotic instances with centroid annotations.\nOur main algorithmic choices are as follows: first, to enhance the\ngeneralizability of our detector and classification networks, we use a\nstate-of-the-art residual Cycle-GAN to transform each scanner domain to every\nother scanner domain. During training, we then randomly load, for each image,\none of the four domains. In this way, our networks can learn from the fourth\nnon-annotated scanner domain even if we don't have annotations for it. Second,\nfor training the detector network, rather than using centroid-based fixed-size\nbounding boxes, we create mitosis-specific bounding boxes. We do this by\nmanually annotating a small selection of mitoses, training a Mask-RCNN on this\nsmall dataset, and applying it to the rest of the data to obtain full\nannotations. We trained the follow-up classification ensemble using only the\nchallenge-provided positive and hard-negative examples. On the preliminary test\nset, the algorithm scores an F1 score of 0.7578, putting us as the second-place\nteam on the leaderboard.",
          "link": "http://arxiv.org/abs/2109.01878",
          "publishedOn": "2021-09-07T07:20:11.824Z",
          "wordCount": null,
          "title": "Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>",
          "description": "We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications.\n\nOur algorithm produces primal solutions and dual lower bounds that estimate\nthe distance to optimum. We implement our algorithm on GPUs and show resulting\none to two order-of-magnitudes improvements in execution speed without\nsacrificing solution quality compared to traditional serial algorithms that run\non CPUs. We can solve very large scale benchmark problems with up to\n$\\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We\nmake our code available at https://github.com/pawelswoboda/RAMA.",
          "link": "http://arxiv.org/abs/2109.01838",
          "publishedOn": "2021-09-07T07:20:11.800Z",
          "wordCount": null,
          "title": "RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Charles Germain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baco_J/0/1/0/all/0/1\">Joshua Ortiz Baco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salter_S/0/1/0/all/0/1\">Sarah H. Salter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_J/0/1/0/all/0/1\">Jim Casey</a>",
          "description": "This paper presents a computational method of analysis that draws from\nmachine learning, library science, and literary studies to map the visual\nlayouts of multi-ethnic newspapers from the late 19th and early 20th century\nUnited States. This work departs from prior approaches to newspapers that focus\non individual pieces of textual and visual content. Our method combines\nChronicling America's MARC data and the Newspaper Navigator machine learning\ndataset to identify the visual patterns of newspaper page layouts. By analyzing\nhigh-dimensional visual similarity, we aim to better understand how editors\nspoke and protested through the layout of their papers.",
          "link": "http://arxiv.org/abs/2109.01732",
          "publishedOn": "2021-09-07T07:20:11.799Z",
          "wordCount": null,
          "title": "Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01664",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chengliang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Super-resolving the Magnetic Resonance (MR) image of a target contrast under\nthe guidance of the corresponding auxiliary contrast, which provides additional\nanatomical information, is a new and effective solution for fast MR imaging.\nHowever, current multi-contrast super-resolution (SR) methods tend to\nconcatenate different contrasts directly, ignoring their relationships in\ndifferent clues, \\eg, in the foreground and background. In this paper, we\npropose a separable attention network (comprising a foreground priority\nattention and background separation attention), named SANet. Our method can\nexplore the foreground and background areas in the forward and reverse\ndirections with the help of the auxiliary contrast, enabling it to learn\nclearer anatomical structures and edge information for the SR of a\ntarget-contrast MR image. SANet provides three appealing benefits: (1) It is\nthe first model to explore a separable attention mechanism that uses the\nauxiliary contrast to predict the foreground and background regions, diverting\nmore attention to refining any uncertain details between these regions and\ncorrecting the fine areas in the reconstructed results. (2) A multi-stage\nintegration module is proposed to learn the response of multi-contrast fusion\nat different stages, obtain the dependency between the fused features, and\nimprove their representation ability. (3) Extensive experiments with various\nstate-of-the-art multi-contrast SR methods on fastMRI and clinical \\textit{in\nvivo} datasets demonstrate the superiority of our model.",
          "link": "http://arxiv.org/abs/2109.01664",
          "publishedOn": "2021-09-07T07:20:11.775Z",
          "wordCount": null,
          "title": "Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>",
          "description": "A recent work from Bello shows that training and scaling strategies may be\nmore significant than model architectures for visual recognition. This short\nnote studies effective training and scaling strategies for video recognition\nmodels. We propose a simple scaling strategy for 3D ResNets, in combination\nwith improved training strategies and minor architectural changes. The\nresulting models, termed 3D ResNet-RS, attain competitive performance of 81.0\non Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained\non a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on\nKinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated\nin a self-supervised setup using contrastive learning, demonstrating improved\nperformance. Code is available at:\nhttps://github.com/tensorflow/models/tree/master/official.",
          "link": "http://arxiv.org/abs/2109.01696",
          "publishedOn": "2021-09-07T07:20:11.774Z",
          "wordCount": null,
          "title": "Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kiyokawa_T/0/1/0/all/0/1\">Takuya Kiyokawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katayama_H/0/1/0/all/0/1\">Hiroki Katayama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatsuta_Y/0/1/0/all/0/1\">Yuya Tatsuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamatsu_J/0/1/0/all/0/1\">Jun Takamatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogasawara_T/0/1/0/all/0/1\">Tsukasa Ogasawara</a>",
          "description": "Owing to human labor shortages, the automation of labor-intensive manual\nwaste-sorting is needed. The goal of automating waste-sorting is to replace the\nhuman role of robust detection and agile manipulation of waste items with\nrobots. To achieve this, we propose three methods. First, we provide a combined\nmanipulation method using graspless push-and-drop and pick-and-release\nmanipulation. Second, we provide a robotic system that can automatically\ncollect object images to quickly train a deep neural-network model. Third, we\nprovide a method to mitigate the differences in the appearance of target\nobjects from two scenes: one for dataset collection and the other for waste\nsorting in a recycling factory. If differences exist, the performance of a\ntrained waste detector may decrease. We address differences in illumination and\nbackground by applying object scaling, histogram matching with histogram\nequalization, and background synthesis to the source target-object images. Via\nexperiments in an indoor experimental workplace for waste-sorting, we confirm\nthat the proposed methods enable quick collection of the training image sets\nfor three classes of waste items (i.e., aluminum can, glass bottle, and plastic\nbottle) and detection with higher performance than the methods that do not\nconsider the differences. We also confirm that the proposed method enables the\nrobot quickly manipulate the objects.",
          "link": "http://arxiv.org/abs/2104.01260",
          "publishedOn": "2021-09-07T07:20:11.511Z",
          "wordCount": null,
          "title": "Robotic Waste Sorter with Agile Manipulation and Quickly Trainable Detector. (arXiv:2104.01260v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1\">Alireza Ahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halstead_M/0/1/0/all/0/1\">Michael Halstead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCool_C/0/1/0/all/0/1\">Chris McCool</a>",
          "description": "This paper explores the potential for performing temporal semantic\nsegmentation in the context of agricultural robotics without temporally\nlabelled data. We achieve this by proposing to generate virtual temporal\nsamples from labelled still images. By exploiting the relatively static scene\nand assuming that the robot (camera) moves we are able to generate virtually\nlabelled temporal sequences with no extra annotation effort. Normally, to train\na recurrent neural network (RNN), labelled samples from a video (temporal)\nsequence are required which is laborious and has stymied work in this\ndirection. By generating virtual temporal samples, we demonstrate that it is\npossible to train a lightweight RNN to perform semantic segmentation on two\nchallenging agricultural datasets. Our results show that by training a temporal\nsemantic segmenter using virtual samples we can increase the performance by an\nabsolute amount of $4.6$ and $4.9$ on sweet pepper and sugar beet datasets,\nrespectively. This indicates that our virtual data augmentation technique is\nable to accurately classify agricultural images temporally without the use of\ncomplicated synthetic data generation techniques nor with the overhead of\nlabelling large amounts of temporal sequences.",
          "link": "http://arxiv.org/abs/2106.10118",
          "publishedOn": "2021-09-07T07:20:11.488Z",
          "wordCount": null,
          "title": "Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.",
          "link": "http://arxiv.org/abs/2109.01934",
          "publishedOn": "2021-09-07T07:20:11.482Z",
          "wordCount": null,
          "title": "Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-09-07T07:20:11.481Z",
          "wordCount": null,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bangbang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yijin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Han Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>",
          "description": "Implicit neural rendering techniques have shown promising results for novel\nview synthesis. However, existing methods usually encode the entire scene as a\nwhole, which is generally not aware of the object identity and limits the\nability to the high-level editing tasks such as moving or adding furniture. In\nthis paper, we present a novel neural scene rendering system, which learns an\nobject-compositional neural radiance field and produces realistic rendering\nwith editing capability for a clustered and real-world scene. Specifically, we\ndesign a novel two-pathway architecture, in which the scene branch encodes the\nscene geometry and appearance, and the object branch encodes each standalone\nobject conditioned on learnable object activation codes. To survive the\ntraining in heavily cluttered scenes, we propose a scene-guided training\nstrategy to solve the 3D space ambiguity in the occluded regions and learn\nsharp boundaries for each object. Extensive experiments demonstrate that our\nsystem not only achieves competitive performance for static scene novel-view\nsynthesis, but also produces realistic rendering for object-level editing.",
          "link": "http://arxiv.org/abs/2109.01847",
          "publishedOn": "2021-09-07T07:20:11.423Z",
          "wordCount": null,
          "title": "Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering. (arXiv:2109.01847v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hong-Gyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sin-Han Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hee-Dong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dong-Ok Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "To understand the black-box characteristics of deep networks, counterfactual\nexplanation that deduces not only the important features of an input space but\nalso how those features should be modified to classify input as a target class\nhas gained an increasing interest. The patterns that deep networks have learned\nfrom a training dataset can be grasped by observing the feature variation among\nvarious classes. However, current approaches perform the feature modification\nto increase the classification probability for the target class irrespective of\nthe internal characteristics of deep networks. This often leads to unclear\nexplanations that deviate from real-world data distributions. To address this\nproblem, we propose a counterfactual explanation method that exploits the\nstatistics learned from a training dataset. Especially, we gradually construct\nan explanation by iterating over masking and composition steps. The masking\nstep aims to select an important feature from the input data to be classified\nas a target class. Meanwhile, the composition step aims to optimize the\npreviously selected feature by ensuring that its output score is close to the\nlogit space of the training data that are classified as the target class.\nExperimental results show that our method produces human-friendly\ninterpretations on various classification datasets and verify that such\ninterpretations can be achieved with fewer feature modification.",
          "link": "http://arxiv.org/abs/2008.01897",
          "publishedOn": "2021-09-07T07:20:11.375Z",
          "wordCount": null,
          "title": "Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>",
          "description": "Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.",
          "link": "http://arxiv.org/abs/2104.01111",
          "publishedOn": "2021-09-07T07:20:11.163Z",
          "wordCount": null,
          "title": "Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>",
          "description": "In this work, we introduce pixel wise tensor normalization, which is inserted\nafter rectifier linear units and, together with batch normalization, provides a\nsignificant improvement in the accuracy of modern deep neural networks. In\naddition, this work deals with the robustness of networks. We show that the\nfactorized superposition of images from the training set and the reformulation\nof the multi class problem into a multi-label problem yields significantly more\nrobust networks. The reformulation and the adjustment of the multi class log\nloss also improves the results compared to the overlay with only one class as\nlabel.\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FTNandFDT&mode=list",
          "link": "http://arxiv.org/abs/2109.02345",
          "publishedOn": "2021-09-07T07:20:11.133Z",
          "wordCount": null,
          "title": "Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gilles_T/0/1/0/all/0/1\">Thomas Gilles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabatini_S/0/1/0/all/0/1\">Stefano Sabatini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1\">Dzmitry Tsishkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanciulescu_B/0/1/0/all/0/1\">Bogdan Stanciulescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>",
          "description": "In this paper, we propose GOHOME, a method leveraging graph representations\nof the High Definition Map and sparse projections to generate a heatmap output\nrepresenting the future position probability distribution for a given agent in\na traffic scene. This heatmap output yields an unconstrained 2D grid\nrepresentation of agent future possible locations, allowing inherent\nmultimodality and a measure of the uncertainty of the prediction. Our\ngraph-oriented model avoids the high computation burden of representing the\nsurrounding context as squared images and processing it with classical CNNs,\nbut focuses instead only on the most probable lanes where the agent could end\nup in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion\nForecasting Benchmark on the MissRate$_6$ metric while achieving significant\nspeed-up and memory burden diminution compared to 1$^{st}$ place method HOME.\nWe also highlight that heatmap output enables multimodal ensembling and improve\n1$^{st}$ place MissRate$_6$ by more than 15$\\%$ with our best ensemble.",
          "link": "http://arxiv.org/abs/2109.01827",
          "publishedOn": "2021-09-07T07:20:10.947Z",
          "wordCount": null,
          "title": "GOHOME: Graph-Oriented Heatmap Output forfuture Motion Estimation. (arXiv:2109.01827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maletzky_A/0/1/0/all/0/1\">Alexander Maletzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thumfart_S/0/1/0/all/0/1\">Stefan Thumfart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wruss_C/0/1/0/all/0/1\">Christoph Wru&#xdf;</a>",
          "description": "We compare the machine readability of pictograms found on Austrian and German\ntraffic signs. To that end, we train classification models on synthetic data\nsets and evaluate their classification accuracy in a controlled setting. In\nparticular, we focus on differences between currently deployed pictograms in\nthe two countries, and a set of new pictograms designed to increase human\nreadability. Besides other results, we find that machine-learning models\ngeneralize poorly to data sets with pictogram designs they have not been\ntrained on. We conclude that manufacturers of advanced driver-assistance\nsystems (ADAS) must take special care to properly address small visual\ndifferences between current and newly designed traffic sign pictograms, as well\nas between pictograms from different countries.",
          "link": "http://arxiv.org/abs/2109.02362",
          "publishedOn": "2021-09-07T07:20:10.672Z",
          "wordCount": null,
          "title": "Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2109.01262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">T. Patrick Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feinberg_B/0/1/0/all/0/1\">Ben Feinberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_C/0/1/0/all/0/1\">Christopher H. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_V/0/1/0/all/0/1\">Venkatraman Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1\">Prashant Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1\">Vineet Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sapan Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinella_M/0/1/0/all/0/1\">Matthew J. Marinella</a>",
          "description": "Specialized accelerators have recently garnered attention as a method to\nreduce the power consumption of neural network inference. A promising category\nof accelerators utilizes nonvolatile memory arrays to both store weights and\nperform $\\textit{in situ}$ analog computation inside the array. While prior\nwork has explored the design space of analog accelerators to optimize\nperformance and energy efficiency, there is seldom a rigorous evaluation of the\naccuracy of these accelerators. This work shows how architectural design\ndecisions, particularly in mapping neural network parameters to analog memory\ncells, influence inference accuracy. When evaluated using ResNet50 on ImageNet,\nthe resilience of the system to analog non-idealities - cell programming\nerrors, analog-to-digital converter resolution, and array parasitic resistances\n- all improve when analog quantities in the hardware are made proportional to\nthe weights in the network. Moreover, contrary to the assumptions of prior\nwork, nearly equivalent resilience to cell imprecision can be achieved by fully\nstoring weights as analog quantities, rather than spreading weight bits across\nmultiple devices, often referred to as bit slicing. By exploiting\nproportionality, analog system designers have the freedom to match the\nprecision of the hardware to the needs of the algorithm, rather than attempting\nto guarantee the same level of precision in the intermediate results as an\nequivalent digital accelerator. This ultimately results in an analog\naccelerator that is more accurate, more robust to analog errors, and more\nenergy-efficient.",
          "link": "http://arxiv.org/abs/2109.01262",
          "publishedOn": "2021-09-14T07:20:16.132Z",
          "wordCount": 729,
          "title": "On the Accuracy of Analog Neural Network Inference Accelerators. (arXiv:2109.01262v2 [cs.AR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>",
          "description": "Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.",
          "link": "http://arxiv.org/abs/2012.14541",
          "publishedOn": "2021-09-14T07:20:16.090Z",
          "wordCount": 621,
          "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.14209",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "Langevin Monte Carlo (LMC) is a popular Bayesian sampling method. For the\nlog-concave distribution function, the method converges exponentially fast, up\nto a controllable discretization error. However, the method requires the\nevaluation of a full gradient in each iteration, and for a problem on\n$\\mathbb{R}^d$, this amounts to $d$ times partial derivative evaluations per\niteration. The cost is high when $d\\gg1$. In this paper, we investigate how to\nenhance computational efficiency through the application of RCD (random\ncoordinate descent) on LMC. There are two sides of the theory:\n\n1 By blindly applying RCD to LMC, one surrogates the full gradient by a\nrandomly selected directional derivative per iteration. Although the cost is\nreduced per iteration, the total number of iteration is increased to achieve a\npreset error tolerance. Ultimately there is no computational gain;\n\n2 We then incorporate variance reduction techniques, such as SAGA (stochastic\naverage gradient) and SVRG (stochastic variance reduced gradient), into\nRCD-LMC. It will be proved that the cost is reduced compared with the classical\nLMC, and in the underdamped case, convergence is achieved with the same number\nof iterations, while each iteration requires merely one-directional derivative.\nThis means we obtain the best possible computational cost in the\nunderdamped-LMC framework.",
          "link": "http://arxiv.org/abs/2007.14209",
          "publishedOn": "2021-09-14T07:20:16.053Z",
          "wordCount": 705,
          "title": "Langevin Monte Carlo: random coordinate descent and variance reduction. (arXiv:2007.14209v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05267",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvi_S/0/1/0/all/0/1\">Sheeraz A. Alvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_S/0/1/0/all/0/1\">Salman Durrani</a>",
          "description": "Federated learning (FL) allows predictive model training on the sensed data\nin a wireless Internet of things (IoT) network evading data collection cost in\nterms of energy, time, and privacy. In this paper, for a FL setting, we model\nthe learning gain achieved by an IoT device against its participation cost as\nits utility. The local model quality and the associated cost differs from\ndevice to device due to the device-heterogeneity which could be time-varying.\nWe identify that this results in utility unfairness because the same global\nmodel is shared among the devices. In the vanilla FL setting, the master is\nunaware of devices' local model computation and transmission costs, thus it is\nunable to address the utility unfairness problem. In addition, a device may\nexploit this lack of knowledge at the master to intentionally reduce its\nexpenditure and thereby boost its utility. We propose to control the quality of\nthe global model shared with the devices, in each round, based on their\ncontribution and expenditure. This is achieved by employing differential\nprivacy to curtail global model divulgence based on the learning contribution.\nFurthermore, we devise adaptive computation and transmission policies for each\ndevice to control its expenditure in order to mitigate utility unfairness. Our\nresults show that the proposed scheme reduces the standard deviation of the\nenergy cost of devices by 99% in comparison to the benchmark scheme, while the\nstandard deviation of the training loss of devices varies around 0.103.",
          "link": "http://arxiv.org/abs/2109.05267",
          "publishedOn": "2021-09-14T07:20:16.045Z",
          "wordCount": 680,
          "title": "Utility Fairness for the Differentially Private Federated Learning. (arXiv:2109.05267v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07407",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Laine_E/0/1/0/all/0/1\">Elodie Laine</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Eismann_S/0/1/0/all/0/1\">Stephan Eismann</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elofsson_A/0/1/0/all/0/1\">Arne Elofsson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Grudinin_S/0/1/0/all/0/1\">Sergei Grudinin</a>",
          "description": "The potential of deep learning has been recognized in the protein structure\nprediction community for some time, and became indisputable after CASP13. In\nCASP14, deep learning has boosted the field to unanticipated levels reaching\nnear-experimental accuracy. This success comes from advances transferred from\nother machine learning areas, as well as methods specifically designed to deal\nwith protein sequences and structures, and their abstractions. Novel emerging\napproaches include (i) geometric learning, i.e. learning on representations\nsuch as graphs, 3D Voronoi tessellations, and point clouds; (ii) pre-trained\nprotein language models leveraging attention; (iii) equivariant architectures\npreserving the symmetry of 3D space; (iv) use of large meta-genome databases;\n(v) combinations of protein representations; (vi) and finally truly end-to-end\narchitectures, i.e. differentiable models starting from a sequence and\nreturning a 3D structure. Here, we provide an overview and our opinion of the\nnovel deep learning approaches developed in the last two years and widely used\nin CASP14.",
          "link": "http://arxiv.org/abs/2105.07407",
          "publishedOn": "2021-09-14T07:20:16.026Z",
          "wordCount": 611,
          "title": "Protein sequence-to-structure learning: Is this the end(-to-end revolution)?. (arXiv:2105.07407v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1\">Muhammad Fahad Saleem</a>",
          "description": "Machine learning algorithms have enabled computers to predict things by\nlearning from previous data. The data storage and processing power are\nincreasing rapidly, thus increasing machine learning and Artificial\nintelligence applications. Much of the work is done to improve the accuracy of\nthe models built in the past, with little research done to determine the\ncomputational costs of machine learning acquisitions. In this paper, I will\nproceed with this later research work and will make a performance comparison of\nmulti-threaded machine learning clustering algorithms. I will be working on\nLinear Regression, Random Forest, and K-Nearest Neighbors to determine the\nperformance characteristics of the algorithms as well as the computation costs\nto the obtained results. I will be benchmarking system hardware performance by\nrunning these multi-threaded algorithms to train and test the models on a\ndataset to note the differences in performance matrices of the algorithms. In\nthe end, I will state the best performing algorithms concerning the performance\nefficiency of these algorithms on my system.",
          "link": "http://arxiv.org/abs/2109.05276",
          "publishedOn": "2021-09-14T07:20:15.970Z",
          "wordCount": 630,
          "title": "Benchmarking Processor Performance by Multi-Threaded Machine Learning Algorithms. (arXiv:2109.05276v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jahani_M/0/1/0/all/0/1\">Majid Jahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rusakov_S/0/1/0/all/0/1\">Sergey Rusakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1\">Peter Richt&#xe1;rik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1\">Martin Tak&#xe1;&#x10d;</a>",
          "description": "We present a novel adaptive optimization algorithm for large-scale machine\nlearning problems. Equipped with a low-cost estimate of local curvature and\nLipschitz smoothness, our method dynamically adapts the search direction and\nstep-size. The search direction contains gradient information preconditioned by\na well-scaled diagonal preconditioning matrix that captures the local curvature\ninformation. Our methodology does not require the tedious task of learning rate\ntuning, as the learning rate is updated automatically without adding an extra\nhyperparameter. We provide convergence guarantees on a comprehensive collection\nof optimization problems, including convex, strongly convex, and nonconvex\nproblems, in both deterministic and stochastic regimes. We also conduct an\nextensive empirical evaluation on standard machine learning problems,\njustifying our algorithm's versatility and demonstrating its strong performance\ncompared to other start-of-the-art first-order and second-order methods.",
          "link": "http://arxiv.org/abs/2109.05198",
          "publishedOn": "2021-09-14T07:20:15.894Z",
          "wordCount": 582,
          "title": "Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information. (arXiv:2109.05198v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.11096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karthik_S/0/1/0/all/0/1\">Shyamgopal Karthik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revaud_J/0/1/0/all/0/1\">J&#xe9;rome Revaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1\">Boris Chidlovskii</a>",
          "description": "Class imbalance and noisy labels are the norm rather than the exception in\nmany large-scale classification datasets. Nevertheless, most works in machine\nlearning typically assume balanced and clean data. There have been some recent\nattempts to tackle, on one side, the problem of learning from noisy labels and,\non the other side, learning from long-tailed data. Each group of methods make\nsimplifying assumptions about the other. Due to this separation, the proposed\nsolutions often underperform when both assumptions are violated. In this work,\nwe present a simple two-stage approach based on recent advances in\nself-supervised learning to treat both challenges simultaneously. It consists\nof, first, task-agnostic self-supervised pre-training, followed by\ntask-specific fine-tuning using an appropriate loss. Most significantly, we\nfind that self-supervised learning approaches are effectively able to cope with\nsevere class imbalance. In addition, the resulting learned representations are\nalso remarkably robust to label noise, when fine-tuned with an imbalance- and\nnoise-resistant loss function. We validate our claims with experiments on\nCIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as\nthe large-scale inherently noisy Clothing-1M dataset.",
          "link": "http://arxiv.org/abs/2108.11096",
          "publishedOn": "2021-09-14T07:20:15.862Z",
          "wordCount": 651,
          "title": "Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhihan Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Juanyong Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianmeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Congrui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yunhai Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bixiong Xu</a>",
          "description": "This paper presents TS2Vec, a universal framework for learning\nrepresentations of time series in an arbitrary semantic level. Unlike existing\nmethods, TS2Vec performs contrastive learning in a hierarchical way over\naugmented context views, which enables a robust contextual representation for\neach timestamp. Furthermore, to obtain the representation of an arbitrary\nsub-sequence in the time series, we can apply a simple aggregation over the\nrepresentations of corresponding timestamps. We conduct extensive experiments\non time series classification tasks to evaluate the quality of time series\nrepresentations. As a result, TS2Vec achieves significant improvement over\nexisting SOTAs of unsupervised time series representation on 125 UCR datasets\nand 29 UEA datasets. The learned timestamp-level representations also achieve\nsuperior results in time series forecasting and anomaly detection tasks. A\nlinear regression trained on top of the learned representations outperforms\nprevious SOTAs of time series forecasting. Furthermore, we present a simple way\nto apply the learned representations for unsupervised anomaly detection, which\nestablishes SOTA results in the literature. The source code is publicly\navailable at https://github.com/yuezhihan/ts2vec.",
          "link": "http://arxiv.org/abs/2106.10466",
          "publishedOn": "2021-09-14T07:20:14.410Z",
          "wordCount": 652,
          "title": "TS2Vec: Towards Universal Representation of Time Series. (arXiv:2106.10466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "In recent years, distributed optimization is proven to be an effective\napproach to accelerate training of large scale machine learning models such as\ndeep neural networks. With the increasing computation power of GPUs, the\nbottleneck of training speed in distributed training is gradually shifting from\ncomputation to communication. Meanwhile, in the hope of training machine\nlearning models on mobile devices, a new distributed training paradigm called\n``federated learning'' has become popular. The communication time in federated\nlearning is especially important due to the low bandwidth of mobile devices.\nWhile various approaches to improve the communication efficiency have been\nproposed for federated learning, most of them are designed with SGD as the\nprototype training algorithm. While adaptive gradient methods have been proven\neffective for training neural nets, the study of adaptive gradient methods in\nfederated learning is scarce. In this paper, we propose an adaptive gradient\nmethod that can guarantee both the convergence and the communication efficiency\nfor federated learning.",
          "link": "http://arxiv.org/abs/2109.05109",
          "publishedOn": "2021-09-14T07:20:14.395Z",
          "wordCount": 604,
          "title": "Toward Communication Efficient Adaptive Gradient Method. (arXiv:2109.05109v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongjie Zhang</a>",
          "description": "Feature extraction is an efficient approach for alleviating the issue of\ndimensionality in high-dimensional data. As a popular self-supervised learning\nmethod, contrastive learning has recently garnered considerable attention. In\nthis study, we proposed a unified framework based on a new perspective of\ncontrastive learning (CL) that is suitable for both unsupervised and supervised\nfeature extraction. The proposed framework first constructed two CL graph for\nuniquely defining the positive and negative pairs. Subsequently, the projection\nmatrix was determined by minimizing the contrastive loss function. In addition,\nthe proposed framework considered both similar and dissimilar samples to unify\nunsupervised and supervised feature extraction. Moreover, we propose the three\nspecific methods: unsupervised contrastive learning method, supervised\ncontrastive learning method 1 ,and supervised contrastive learning method 2.\nFinally, the numerical experiments on five real datasets demonstrated the\nsuperior performance of the proposed framework in comparison to the existing\nmethods.",
          "link": "http://arxiv.org/abs/2101.11703",
          "publishedOn": "2021-09-14T07:20:14.370Z",
          "wordCount": 605,
          "title": "Unified Framework for Feature Extraction based on Contrastive Learning. (arXiv:2101.11703v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shixian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1\">Amanda Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>",
          "description": "Deep neural networks can be fooled by adversarial attacks: adding carefully\ncomputed small adversarial perturbations to clean inputs can cause\nmisclassification on state-of-the-art machine learning models. The reason is\nthat neural networks fail to accommodate the distribution drift of the input\ndata caused by adversarial perturbations. Here, we present a new solution -\nBeneficial Perturbation Network (BPN) - to defend against adversarial attacks\nby fixing the distribution drift. During training, BPN generates and leverages\nbeneficial perturbations (somewhat opposite to well-known adversarial\nperturbations) by adding new, out-of-network biasing units. Biasing units\ninfluence the parameter space of the network, to preempt and neutralize future\nadversarial perturbations on input data samples. To achieve this, BPN creates\nreverse adversarial attacks during training, with very little cost, by\nrecycling the training gradients already computed. Reverse attacks are captured\nby the biasing units, and the biases can in turn effectively defend against\nfuture adversarial examples. Reverse attacks are a shortcut, i.e., they affect\nthe network's parameters without requiring instantiation of adversarial\nexamples that could assist training. We provide comprehensive empirical\nevidence showing that 1) BPN is robust to adversarial examples and is much more\nrunning memory and computationally efficient compared to classical adversarial\ntraining. 2) BPN can defend against adversarial examples with negligible\nadditional computation and parameter costs compared to training only on clean\nexamples; 3) BPN hurts the accuracy on clean examples much less than classic\nadversarial training; 4) BPN can improve the generalization of the network 5)\nBPN trained only with Fast Gradient Sign Attack can generalize to defend PGD\nattacks.",
          "link": "http://arxiv.org/abs/2009.12724",
          "publishedOn": "2021-09-14T07:20:14.359Z",
          "wordCount": 748,
          "title": "Beneficial Perturbations Network for Defending Adversarial Examples. (arXiv:2009.12724v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.05089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu Dinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>",
          "description": "Recently, graph neural networks (GNNs) have become an important and active\nresearch direction in deep learning. It is worth noting that most of the\nexisting GNN-based methods learn graph representations within the Euclidean\nvector space. Beyond the Euclidean space, learning representation and\nembeddings in hyper-complex space have also shown to be a promising and\neffective approach. To this end, we propose Quaternion Graph Neural Networks\n(QGNN) and Gated Quaternion Graph Neural Networks (GQGNN) to learn graph\nrepresentations within the Quaternion space. As demonstrated, the Quaternion\nspace, a hyper-complex vector space, provides highly meaningful computations\nand analogical calculus through Hamilton product compared to the Euclidean and\ncomplex vector spaces. Our QGNN obtains state-of-the-art results on a range of\nbenchmark datasets for graph classification and node classification. Besides,\nregarding knowledge graphs, our QGNN-based knowledge graph embedding method\ngets state-of-the-art results on three new and challenging benchmark datasets\nfor knowledge graph completion. Furthermore, regarding text graphs, our\nGQGNN-based text classification method works better than state-of-the-art\nmethods on benchmark datasets for inductive text classification. Our code is\navailable at: \\url{https://github.com/daiquocnguyen/QGNN}.",
          "link": "http://arxiv.org/abs/2008.05089",
          "publishedOn": "2021-09-14T07:20:14.353Z",
          "wordCount": 694,
          "title": "Quaternion Graph Neural Networks. (arXiv:2008.05089v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhaolong Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiuyong Li</a>",
          "description": "Causal Learner is a toolbox for learning causal structure and Markov blanket\n(MB) from data. It integrates functions for generating simulated Bayesian\nnetwork data, a set of state-of-the-art global causal structure learning\nalgorithms, a set of state-of-the-art local causal structure learning\nalgorithms, a set of state-of-the-art MB learning algorithms, and functions for\nevaluating algorithms. The data generation part of Causal Learner is written in\nR, and the rest of Causal Learner is written in MATLAB. Causal Learner aims to\nprovide researchers and practitioners with an open-source platform for causal\nlearning from data and for the development and evaluation of new causal\nlearning algorithms. The Causal Learner project is available at\nthis http URL",
          "link": "http://arxiv.org/abs/2103.06544",
          "publishedOn": "2021-09-14T07:20:14.343Z",
          "wordCount": 593,
          "title": "Causal Learner: A Toolbox for Causal Structure and Markov Blanket Learning. (arXiv:2103.06544v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.02320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hogan_A/0/1/0/all/0/1\">Aidan Hogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blomqvist_E/0/1/0/all/0/1\">Eva Blomqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1\">Michael Cochez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dAmato_C/0/1/0/all/0/1\">Claudia d&#x27;Amato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_C/0/1/0/all/0/1\">Claudio Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gayo_J/0/1/0/all/0/1\">Jos&#xe9; Emilio Labra Gayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirrane_S/0/1/0/all/0/1\">Sabrina Kirrane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumaier_S/0/1/0/all/0/1\">Sebastian Neumaier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polleres_A/0/1/0/all/0/1\">Axel Polleres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navigli_R/0/1/0/all/0/1\">Roberto Navigli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_S/0/1/0/all/0/1\">Sabbir M. Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rula_A/0/1/0/all/0/1\">Anisa Rula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmelzeisen_L/0/1/0/all/0/1\">Lukas Schmelzeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeda_J/0/1/0/all/0/1\">Juan Sequeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1\">Steffen Staab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_A/0/1/0/all/0/1\">Antoine Zimmermann</a>",
          "description": "In this paper we provide a comprehensive introduction to knowledge graphs,\nwhich have recently garnered significant attention from both industry and\nacademia in scenarios that require exploiting diverse, dynamic, large-scale\ncollections of data. After some opening remarks, we motivate and contrast\nvarious graph-based data models and query languages that are used for knowledge\ngraphs. We discuss the roles of schema, identity, and context in knowledge\ngraphs. We explain how knowledge can be represented and extracted using a\ncombination of deductive and inductive techniques. We summarise methods for the\ncreation, enrichment, quality assessment, refinement, and publication of\nknowledge graphs. We provide an overview of prominent open knowledge graphs and\nenterprise knowledge graphs, their applications, and how they use the\naforementioned techniques. We conclude with high-level future research\ndirections for knowledge graphs.",
          "link": "http://arxiv.org/abs/2003.02320",
          "publishedOn": "2021-09-14T07:20:14.300Z",
          "wordCount": 691,
          "title": "Knowledge Graphs. (arXiv:2003.02320v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panda_J/0/1/0/all/0/1\">J P Panda</a>",
          "description": "Machine Learning (ML) based algorithms have found significant impact in many\nfields of engineering and sciences, where datasets are available from\nexperiments and high fidelity numerical simulations. Those datasets are\ngenerally utilized in a machine learning model to extract information about the\nunderlying physics and derive functional relationships mapping input variables\nto target quantities of interest. Commonplace machine learning algorithms\nutilized in Scientific Machine Learning (SciML) include neural networks,\nregression trees, random forests, support vector machines, etc. The focus of\nthis article is to review the applications of ML in naval architecture, ocean,\nand marine engineering problems; and identify priority directions of research.\nWe discuss the applications of machine learning algorithms for different\nproblems such as wave height prediction, calculation of wind loads on ships,\ndamage detection of offshore platforms, calculation of ship added resistance,\nand various other applications in coastal and marine environments. The details\nof the data sets including the source of data-sets utilized in the ML model\ndevelopment are included. The features used as the inputs to the ML models are\npresented in detail and finally, the methods employed in optimization of the ML\nmodels were also discussed. Based on this comprehensive analysis we point out\nfuture directions of research that may be fruitful for the application of ML to\nthe ocean and marine engineering problems.",
          "link": "http://arxiv.org/abs/2109.05574",
          "publishedOn": "2021-09-14T07:20:14.279Z",
          "wordCount": 671,
          "title": "Machine Learning for Naval Architecture, Ocean and Marine Engineering. (arXiv:2109.05574v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n\nBy combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.",
          "link": "http://arxiv.org/abs/2104.07155",
          "publishedOn": "2021-09-14T07:20:14.264Z",
          "wordCount": 656,
          "title": "Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+San_Roman_R/0/1/0/all/0/1\">Robin San-Roman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Generative diffusion models have emerged as leading models in speech and\nimage generation. However, in order to perform well with a small number of\ndenoising steps, a costly tuning of the set of noise parameters is needed. In\nthis work, we present a simple and versatile learning scheme that can\nstep-by-step adjust those noise parameters, for any given number of steps,\nwhile the previous work needs to retune for each number separately.\nFurthermore, without modifying the weights of the diffusion model, we are able\nto significantly improve the synthesis results, for a small number of steps.\nOur approach comes at a negligible computation cost.",
          "link": "http://arxiv.org/abs/2104.02600",
          "publishedOn": "2021-09-14T07:20:14.257Z",
          "wordCount": 570,
          "title": "Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Battash_B/0/1/0/all/0/1\">Barak Battash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazan_T/0/1/0/all/0/1\">Tamir Hazan</a>",
          "description": "The cross entropy loss is widely used due to its effectiveness and solid\ntheoretical grounding. However, as training progresses, the loss tends to focus\non hard to classify samples, which may prevent the network from obtaining gains\nin performance. While most work in the field suggest ways to classify hard\nnegatives, we suggest to strategically leave hard negatives behind, in order to\nfocus on misclassified samples with higher probabilities. We show that adding\nto the optimization goal the expectation loss, which is a better approximation\nof the zero-one loss, helps the network to achieve better accuracy. We,\ntherefore, propose to shift between the two losses during training, focusing\nmore on the expectation loss gradually during the later stages of training. Our\nexperiments show that the new training protocol improves performance across a\ndiverse set of classification domains, including computer vision, natural\nlanguage processing, tabular data, and sequences. Our code and scripts are\navailable at supplementary.",
          "link": "http://arxiv.org/abs/2109.05635",
          "publishedOn": "2021-09-14T07:20:14.249Z",
          "wordCount": 600,
          "title": "Mixing between the Cross Entropy and the Expectation Loss Terms. (arXiv:2109.05635v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenavarmasouleh_F/0/1/0/all/0/1\">Farzan Shenavarmasouleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arabnia_H/0/1/0/all/0/1\">Hamid R. Arabnia</a>",
          "description": "The explosion of advancements in artificial intelligence, sensor\ntechnologies, and wireless communication activates ubiquitous sensing through\ndistributed sensors. These sensors are various domains of networks that lead us\nto smart systems in healthcare, transportation, environment, and other relevant\nbranches/networks. Having collaborative interaction among the smart systems\nconnects end-user devices to each other which enables achieving a new\nintegrated entity called Smart Cities. The goal of this study is to provide a\ncomprehensive survey of data analytics in smart cities. In this paper, we aim\nto focus on one of the smart cities important branches, namely Smart Mobility,\nand its positive ample impact on the smart cities decision-making process.\nIntelligent decision-making systems in smart mobility offer many advantages\nsuch as saving energy, relaying city traffic, and more importantly, reducing\nair pollution by offering real-time useful information and imperative\nknowledge. Making a decision in smart cities in time is challenging due to\nvarious and high dimensional factors and parameters, which are not frequently\ncollected. In this paper, we first address current challenges in smart cities\nand provide an overview of potential solutions to these challenges. Then, we\noffer a framework of these solutions, called universal smart cities decision\nmaking, with three main sections of data capturing, data analysis, and decision\nmaking to optimize the smart mobility within smart cities. With this framework,\nwe elaborate on fundamental concepts of big data, machine learning, and deep\nleaning algorithms that have been applied to smart cities and discuss the role\nof these algorithms in decision making for smart mobility in smart cities.",
          "link": "http://arxiv.org/abs/2109.05581",
          "publishedOn": "2021-09-14T07:20:14.230Z",
          "wordCount": 704,
          "title": "Data Analytics for Smart cities: Challenges and Promises. (arXiv:2109.05581v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Gang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>",
          "description": "Federated learning (FL) is a popular technique to train machine learning (ML)\nmodels with decentralized data. Extensive works have studied the performance of\nthe global model; however, it is still unclear how the training process affects\nthe final test accuracy. Exacerbating this problem is the fact that FL\nexecutions differ significantly from traditional ML with heterogeneous data\ncharacteristics across clients, involving more hyperparameters. In this work,\nwe show that the final test accuracy of FL is dramatically affected by the\nearly phase of the training process, i.e., FL exhibits critical learning\nperiods, in which small gradient errors can have irrecoverable impact on the\nfinal test accuracy. To further explain this phenomenon, we generalize the\ntrace of the Fisher Information Matrix (FIM) to FL and define a new notion\ncalled FedFIM, a quantity reflecting the local curvature of each clients from\nthe beginning of the training in FL. Our findings suggest that the {\\em initial\nlearning phase} plays a critical role in understanding the FL performance. This\nis in contrast to many existing works which generally do not connect the final\naccuracy of FL to the early phase training. Finally, seizing critical learning\nperiods in FL is of independent interest and could be useful for other problems\nsuch as the choices of hyperparameters such as the number of client selected\nper round, batch size, and more, so as to improve the performance of FL\ntraining and testing.",
          "link": "http://arxiv.org/abs/2109.05613",
          "publishedOn": "2021-09-14T07:20:14.222Z",
          "wordCount": 674,
          "title": "Critical Learning Periods in Federated Learning. (arXiv:2109.05613v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin &#x15e;im&#x15f;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ged_F/0/1/0/all/0/1\">Fran&#xe7;ois Ged</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spadaro_F/0/1/0/all/0/1\">Francesco Spadaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1\">Wulfram Gerstner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brea_J/0/1/0/all/0/1\">Johanni Brea</a>",
          "description": "We study how permutation symmetries in overparameterized multi-layer neural\nnetworks generate `symmetry-induced' critical points. Assuming a network with $\nL $ layers of minimal widths $ r_1^*, \\ldots, r_{L-1}^* $ reaches a zero-loss\nminimum at $ r_1^*! \\cdots r_{L-1}^*! $ isolated points that are permutations\nof one another, we show that adding one extra neuron to each layer is\nsufficient to connect all these previously discrete minima into a single\nmanifold. For a two-layer overparameterized network of width $ r^*+ h =: m $ we\nexplicitly describe the manifold of global minima: it consists of $ T(r^*, m) $\naffine subspaces of dimension at least $ h $ that are connected to one another.\nFor a network of width $m$, we identify the number $G(r,m)$ of affine subspaces\ncontaining only symmetry-induced critical points that are related to the\ncritical points of a smaller network of width $r<r^*$. Via a combinatorial\nanalysis, we derive closed-form formulas for $ T $ and $ G $ and show that the\nnumber of symmetry-induced critical subspaces dominates the number of affine\nsubspaces forming the global minima manifold in the mildly overparameterized\nregime (small $ h $) and vice versa in the vastly overparameterized regime ($h\n\\gg r^*$). Our results provide new insights into the minimization of the\nnon-convex loss function of overparameterized neural networks.",
          "link": "http://arxiv.org/abs/2105.12221",
          "publishedOn": "2021-09-14T07:20:14.214Z",
          "wordCount": 710,
          "title": "Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances. (arXiv:2105.12221v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokar_G/0/1/0/all/0/1\">Ghada Sokar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tim van der Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_E/0/1/0/all/0/1\">Elena Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1\">Raymond Veldhuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>",
          "description": "Major complications arise from the recent increase in the amount of\nhigh-dimensional data, including high computational costs and memory\nrequirements. Feature selection, which identifies the most relevant and\ninformative attributes of a dataset, has been introduced as a solution to this\nproblem. Most of the existing feature selection methods are computationally\ninefficient; inefficient algorithms lead to high energy consumption, which is\nnot desirable for devices with limited computational and energy resources. In\nthis paper, a novel and flexible method for unsupervised feature selection is\nproposed. This method, named QuickSelection, introduces the strength of the\nneuron in sparse neural networks as a criterion to measure the feature\nimportance. This criterion, blended with sparsely connected denoising\nautoencoders trained with the sparse evolutionary training procedure, derives\nthe importance of all input features simultaneously. We implement\nQuickSelection in a purely sparse manner as opposed to the typical approach of\nusing a binary mask over connections to simulate sparsity. It results in a\nconsiderable speed increase and memory reduction. When tested on several\nbenchmark datasets, including five low-dimensional and three high-dimensional\ndatasets, the proposed method is able to achieve the best trade-off of\nclassification and clustering accuracy, running time, and maximum memory usage,\namong widely used approaches for feature selection. Besides, our proposed\nmethod requires the least amount of energy among the state-of-the-art\nautoencoder-based feature selection methods.",
          "link": "http://arxiv.org/abs/2012.00560",
          "publishedOn": "2021-09-14T07:20:14.206Z",
          "wordCount": 720,
          "title": "Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders. (arXiv:2012.00560v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05536",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhongyuan Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Verma_G/0/1/0/all/0/1\">Gunjan Verma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rao_C/0/1/0/all/0/1\">Chirag Rao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Swami_A/0/1/0/all/0/1\">Ananthram Swami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Segarra_S/0/1/0/all/0/1\">Santiago Segarra</a>",
          "description": "Efficient scheduling of transmissions is a key problem in wireless networks.\nThe main challenge stems from the fact that optimal link scheduling involves\nsolving a maximum weighted independent set (MWIS) problem, which is known to be\nNP-hard. For practical link scheduling schemes, centralized and distributed\ngreedy heuristics are commonly used to approximate the solution to the MWIS\nproblem. However, these greedy schemes mostly ignore important topological\ninformation of the wireless network. To overcome this limitation, we propose\nfast heuristics based on graph convolutional networks (GCNs) that can be\nimplemented in centralized and distributed manners. Our centralized MWIS solver\nis based on tree search guided by a trainable GCN module and 1-step rollout. In\nour distributed MWIS solver, a trainable GCN module learns topology-aware node\nembeddings that are combined with the network weights before calling a\ndistributed greedy solver. Test results on medium-sized wireless networks show\nthat a GCN-based centralized MWIS solver can reach a near-optimal solution\nquickly. Moreover, we demonstrate that a shallow GCN-based distributed MWIS\nscheduler can reduce by nearly half the suboptimality gap of the distributed\ngreedy solver with minimal increase in complexity. The proposed scheduling\nsolutions also exhibit good generalizability across graph and weight\ndistributions.",
          "link": "http://arxiv.org/abs/2109.05536",
          "publishedOn": "2021-09-14T07:20:14.199Z",
          "wordCount": 671,
          "title": "Link Scheduling using Graph Neural Networks. (arXiv:2109.05536v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05409",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Macar_U/0/1/0/all/0/1\">Uzay Macar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karthik_E/0/1/0/all/0/1\">Enamundram Naga Karthik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gros_C/0/1/0/all/0/1\">Charley Gros</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andr&#xe9;anne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_Adad_J/0/1/0/all/0/1\">Julien Cohen-Adad</a>",
          "description": "This paper gives a detailed description of the pipelines used for the 2nd\nedition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.\nAn overview of the data preprocessing steps applied is provided along with a\nbrief description of the pipelines used, in terms of the architecture and the\nhyperparameters. Our code for this work can be found at:\nhttps://github.com/ivadomed/ms-challenge-2021.",
          "link": "http://arxiv.org/abs/2109.05409",
          "publishedOn": "2021-09-14T07:20:14.180Z",
          "wordCount": 557,
          "title": "Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04505",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>",
          "description": "Mobile devices such as smartphones and autonomous vehicles increasingly rely\non deep neural networks (DNNs) to execute complex inference tasks such as image\nclassification and speech recognition, among others. However, continuously\nexecuting the entire DNN on the mobile device can quickly deplete its battery.\nAlthough task offloading to edge servers may decrease the mobile device's\ncomputational burden, erratic patterns in channel quality, network and edge\nserver load can lead to a significant delay in task execution.\nRecently,approaches based on split computing (SC) have been proposed, where the\nDNN is split into a head and a tail model, executed respectively on the mobile\ndevice and on the edge server. Ultimately, this may reduce bandwidth usage as\nwell as energy consumption. Another approach, called early exiting (EE), trains\nmodels to present multiple \"exits\" earlier in the architecture, each providing\nincreasingly higher target accuracy. Therefore, the trade-off between accuracy\nand delay can be tuned according to the current conditions or application\ndemands. In this paper, we provide a comprehensive survey of the state of the\nart in SC and EE strategies, by presenting a comparison of the most relevant\napproaches. We conclude the paper by providing a set of compelling research\nchallenges.",
          "link": "http://arxiv.org/abs/2103.04505",
          "publishedOn": "2021-09-14T07:20:14.173Z",
          "wordCount": 671,
          "title": "Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges. (arXiv:2103.04505v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05539",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_E/0/1/0/all/0/1\">Erfan Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kheradpisheh_S/0/1/0/all/0/1\">Saeed Reza Kheradpisheh</a>",
          "description": "Recent studies have shown that convolutional neural networks (CNNs) are not\nthe only feasible solution for image classification. Furthermore, weight\nsharing and backpropagation used in CNNs do not correspond to the mechanisms\npresent in the primate visual system. To propose a more biologically plausible\nsolution, we designed a locally connected spiking neural network (SNN) trained\nusing spike-timing-dependent plasticity (STDP) and its reward-modulated variant\n(R-STDP) learning rules. The use of spiking neurons and local connections along\nwith reinforcement learning (RL) led us to the nomenclature BioLCNet for our\nproposed architecture. Our network consists of a rate-coded input layer\nfollowed by a locally connected hidden layer and a decoding output layer. A\nspike population-based voting scheme is adopted for decoding in the output\nlayer. We used the MNIST dataset to obtain image classification accuracy and to\nassess the robustness of our rewarding system to varying target responses.",
          "link": "http://arxiv.org/abs/2109.05539",
          "publishedOn": "2021-09-14T07:20:14.156Z",
          "wordCount": 612,
          "title": "BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05023",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Baum_Z/0/1/0/all/0/1\">Zachary M C Baum</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yipeng Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1\">Dean C Barratt</a>",
          "description": "We present Free Point Transformer (FPT) - a deep neural network architecture\nfor non-rigid point-set registration. Consisting of two modules, a global\nfeature extraction module and a point transformation module, FPT does not\nassume explicit constraints based on point vicinity, thereby overcoming a\ncommon requirement of previous learning-based point-set registration methods.\nFPT is designed to accept unordered and unstructured point-sets with a variable\nnumber of points and uses a \"model-free\" approach without heuristic\nconstraints. Training FPT is flexible and involves minimizing an intuitive\nunsupervised loss function, but supervised, semi-supervised, and partially- or\nweakly-supervised training are also supported. This flexibility makes FPT\namenable to multimodal image registration problems where the ground-truth\ndeformations are difficult or impossible to measure. In this paper, we\ndemonstrate the application of FPT to non-rigid registration of prostate\nmagnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound\n(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete\nTRUS imaging and sparsely-sampled TRUS imaging, respectively. The results\nindicate superior accuracy to the alternative rigid and non-rigid registration\nalgorithms tested and substantially lower computation time. The rapid inference\npossible with FPT makes it particularly suitable for applications where\nreal-time registration is beneficial.",
          "link": "http://arxiv.org/abs/2109.05023",
          "publishedOn": "2021-09-14T07:20:14.123Z",
          "wordCount": 661,
          "title": "Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05075",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1\">Felipe Dennis de Resende Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batista_E/0/1/0/all/0/1\">Eduardo Luiz Ortiz Batista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seara_R/0/1/0/all/0/1\">Rui Seara</a>",
          "description": "Despite the growing availability of high-capacity computational platforms,\nimplementation complexity still has been a great concern for the real-world\ndeployment of neural networks. This concern is not exclusively due to the huge\ncosts of state-of-the-art network architectures, but also due to the recent\npush towards edge intelligence and the use of neural networks in embedded\napplications. In this context, network compression techniques have been gaining\ninterest due to their ability for reducing deployment costs while keeping\ninference accuracy at satisfactory levels. The present paper is dedicated to\nthe development of a novel compression scheme for neural networks. To this end,\na new $\\ell_0$-norm-based regularization approach is firstly developed, which\nis capable of inducing strong sparseness in the network during training. Then,\ntargeting the smaller weights of the trained network with pruning techniques,\nsmaller yet highly effective networks can be obtained. The proposed compression\nscheme also involves the use of $\\ell_2$-norm regularization to avoid\noverfitting as well as fine tuning to improve the performance of the pruned\nnetwork. Experimental results are presented aiming to show the effectiveness of\nthe proposed scheme as well as to make comparisons with competing approaches.",
          "link": "http://arxiv.org/abs/2109.05075",
          "publishedOn": "2021-09-14T07:20:14.102Z",
          "wordCount": 658,
          "title": "On the Compression of Neural Networks Using $\\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minfang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fengyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1\">Shuai Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangrong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>",
          "description": "Black-box optimization (BBO) algorithms are concerned with finding the best\nsolutions for problems with missing analytical details. Most classical methods\nfor such problems are based on strong and fixed a priori assumptions, such as\nGaussianity. However, the complex real-world problems, especially when the\nglobal optimum is desired, could be very far from the a priori assumptions\nbecause of their diversities, bringing some unexpected obstacles to these\nmethods. In this paper, we present a generative adversarial nets-based\noptimizer (OPT-GAN) to adapt to diverse black-box problems via estimating the\ndistribution of optima. The method learns the extensive distribution of the\noptimal region dominated by selective and randomly moving candidates, balancing\nthe exploration and exploitation. Experiments conducted on Black-box\nOptimization Benchmarking (BBOB) problems and several other benchmarks with\ndiversified distributions exhibit that, the OPT-GAN outperforms many\ntraditional and neural net-based BBO algorithms.",
          "link": "http://arxiv.org/abs/2102.03888",
          "publishedOn": "2021-09-14T07:20:14.036Z",
          "wordCount": 629,
          "title": "Black-Box Optimization via Generative Adversarial Nets. (arXiv:2102.03888v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03782",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carpenter_B/0/1/0/all/0/1\">Bob Carpenter</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gelman_A/0/1/0/all/0/1\">Andrew Gelman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>",
          "description": "We introduce Pathfinder, a variational method for approximately sampling from\ndifferentiable log densities. Starting from a random initialization, Pathfinder\nlocates normal approximations to the target density along a quasi-Newton\noptimization path, with local covariance estimated using the inverse Hessian\nestimates produced by the optimizer. Pathfinder returns draws from the\napproximation with the lowest estimated Kullback-Leibler (KL) divergence to the\ntrue posterior. We evaluate Pathfinder on a wide range of posterior\ndistributions, demonstrating that its approximate draws are better than those\nfrom automatic differentiation variational inference (ADVI) and comparable to\nthose produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as\nmeasured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC\nruns, Pathfinder requires one to two orders of magnitude fewer log density and\ngradient evaluations, with greater reductions for more challenging posteriors.\nImportance resampling over multiple runs of Pathfinder improves the diversity\nof approximate draws, reducing 1-Wasserstein distance further and providing a\nmeasure of robustness to optimization failures on plateaus, saddle points, or\nin minor modes. The Monte Carlo KL-divergence estimates are embarrassingly\nparallelizable in the core Pathfinder algorithm, as are multiple runs in the\nresampling version, further increasing Pathfinder's speed advantage with\nmultiple cores.",
          "link": "http://arxiv.org/abs/2108.03782",
          "publishedOn": "2021-09-14T07:20:14.029Z",
          "wordCount": null,
          "title": "Pathfinder: Parallel quasi-Newton variational inference. (arXiv:2108.03782v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1\">Dishant Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_R/0/1/0/all/0/1\">Ragesh Jaiswal</a>",
          "description": "In this work, we study the socially fair $k$-median/$k$-means problem. We are\ngiven a set of points $P$ in a metric space $\\mathcal{X}$ with a distance\nfunction $d(.,.)$. There are $\\ell$ groups: $P_1,\\dotsc,P_{\\ell} \\subseteq P$.\nWe are also given a set $F$ of feasible centers in $\\mathcal{X}$. The goal in\nthe socially fair $k$-median problem is to find a set $C \\subseteq F$ of $k$\ncenters that minimizes the maximum average cost over all the groups. That is,\nfind $C$ that minimizes the objective function $\\Phi(C,P) \\equiv \\max_{j}\n\\Big\\{ \\sum_{x \\in P_j} d(C,x)/|P_j| \\Big\\}$, where $d(C,x)$ is the distance of\n$x$ to the closest center in $C$. The socially fair $k$-means problem is\ndefined similarly by using squared distances, i.e., $d^{2}(.,.)$ instead of\n$d(.,.)$. The current best approximation guarantee for both the problems is\n$O\\left( \\frac{\\log \\ell}{\\log \\log \\ell} \\right)$ due to Makarychev and\nVakilian [COLT 2021]. In this work, we study the fixed parameter tractability\nof the problems with respect to parameter $k$. We design $(3+\\varepsilon)$ and\n$(9 + \\varepsilon)$ approximation algorithms for the socially fair $k$-median\nand $k$-means problems, respectively, in FPT (fixed parameter tractable) time\n$f(k,\\varepsilon) \\cdot n^{O(1)}$, where $f(k,\\varepsilon) =\n(k/\\varepsilon)^{{O}(k)}$ and $n = |P \\cup F|$. Furthermore, we show that if\nGap-ETH holds, then better approximation guarantees are not possible in FPT\ntime.",
          "link": "http://arxiv.org/abs/2106.06755",
          "publishedOn": "2021-09-14T07:20:14.021Z",
          "wordCount": 728,
          "title": "Tight FPT Approximation for Socially Fair Clustering. (arXiv:2106.06755v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jingjun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Gaojuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almpanidis_G/0/1/0/all/0/1\">George Almpanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garcia</a>",
          "description": "In predictive tasks, real-world datasets often present different degrees of\nimbalanced (i.e., long-tailed or skewed) distributions. While the majority (the\nhead) classes have sufficient samples, the minority (the tail) classes can be\nunder-represented by a rather limited number of samples. Data pre-processing\nhas been shown to be very effective in dealing with such problems. On one hand,\ndata re-sampling is a common approach to tackling class imbalance. On the other\nhand, dimension reduction, which reduces the feature space, is a conventional\ntechnique for reducing noise and inconsistencies in a dataset. However, the\npossible synergy between feature selection and data re-sampling for\nhigh-performance imbalance classification has rarely been investigated before.\nTo address this issue, we carry out a comprehensive empirical study on the\njoint influence of feature selection and re-sampling on two-class imbalance\nclassification. Specifically, we study the performance of two opposite\npipelines for imbalance classification by applying feature selection before or\nafter data re-sampling. We conduct a large number of experiments, with a total\nof 9225 tests, on 52 publicly available datasets, using 9 feature selection\nmethods, 6 re-sampling approaches for class imbalance learning, and 3\nwell-known classification algorithms. Experimental results show that there is\nno constant winner between the two pipelines; thus both of them should be\nconsidered to derive the best performing model for imbalance classification. We\nfind that the performance of an imbalance classification model not only depends\non the classifier adopted and the ratio between the number of majority and\nminority samples, but also depends on the ratio between the number of samples\nand features. Overall, this study should provide new reference value for\nresearchers and practitioners in imbalance learning.",
          "link": "http://arxiv.org/abs/2109.00201",
          "publishedOn": "2021-09-14T07:20:14.003Z",
          "wordCount": 767,
          "title": "An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Rohan Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>",
          "description": "Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic, which has demonstrated competitive performance in many\nNatural Language Processing (NLP) tasks, including sentiment analysis, text\nclassification, and Word Sense Disambiguation. To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the performance and interpretability of TM. We\nachieve this by extracting semantically related words from pre-trained word\nrepresentations as input features to the TM. Our experiments show that the\naccuracy of the proposed approach is significantly higher than the previous\nBOW-based TM, reaching the level of DNN-based models.",
          "link": "http://arxiv.org/abs/2104.06901",
          "publishedOn": "2021-09-14T07:20:13.992Z",
          "wordCount": null,
          "title": "Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khritankov_A/0/1/0/all/0/1\">Anton S. Khritankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilkevich_A/0/1/0/all/0/1\">Anton A. Pilkevich</a>",
          "description": "We explore a hidden feedback loops effect in online recommender systems.\nFeedback loops result in degradation of online multi-armed bandit (MAB)\nrecommendations to a small subset and loss of coverage and novelty. We study\nhow uncertainty and noise in user interests influence the existence of feedback\nloops. First, we show that an unbiased additive random noise in user interests\ndoes not prevent a feedback loop. Second, we demonstrate that a non-zero\nprobability of resetting user interests is sufficient to limit the feedback\nloop and estimate the size of the effect. Our experiments confirm the\ntheoretical findings in a simulated environment for four bandit algorithms.",
          "link": "http://arxiv.org/abs/2109.05278",
          "publishedOn": "2021-09-14T07:20:13.985Z",
          "wordCount": null,
          "title": "Existence conditions for hidden feedback loops in online recommender systems. (arXiv:2109.05278v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05180",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>",
          "description": "In machine learning, the performance of a classifier depends on both the\nclassifier model and the separability/complexity of datasets. To quantitatively\nmeasure the separability of datasets, we create an intrinsic measure -- the\nDistance-based Separability Index (DSI), which is independent of the classifier\nmodel. We consider the situation in which different classes of data are mixed\nin the same distribution to be the most difficult for classifiers to separate.\nWe then formally show that the DSI can indicate whether the distributions of\ndatasets are identical for any dimensionality. And we verify the DSI to be an\neffective separability measure by comparing to several state-of-the-art\nseparability/complexity measures using synthetic and real datasets. Having\ndemonstrated the DSI's ability to compare distributions of samples, we also\ndiscuss some of its other promising applications, such as measuring the\nperformance of generative adversarial networks (GANs) and evaluating the\nresults of clustering methods.",
          "link": "http://arxiv.org/abs/2109.05180",
          "publishedOn": "2021-09-14T07:20:13.984Z",
          "wordCount": null,
          "title": "A Novel Intrinsic Measure of Data Separability. (arXiv:2109.05180v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Coursey_A/0/1/0/all/0/1\">Austin Coursey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nath_G/0/1/0/all/0/1\">Gopal Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhu_S/0/1/0/all/0/1\">Srikanth Prabhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Saptarshi Sengupta</a>",
          "description": "Physical and cloud storage services are well-served by functioning and\nreliable high-volume storage systems. Recent observations point to hard disk\nreliability as one of the most pressing reliability issues in data centers\ncontaining massive volumes of storage devices such as HDDs. In this regard,\nearly detection of impending failure at the disk level aids in reducing system\ndowntime and reduces operational loss making proactive health monitoring a\npriority for AIOps in such settings. In this work, we introduce methods of\nextracting meaningful attributes associated with operational failure and of\npre-processing the highly imbalanced health statistics data for subsequent\nprediction tasks using data-driven approaches. We use a Bidirectional LSTM with\na multi-day look back period to learn the temporal progression of health\nindicators and baseline them against vanilla LSTM and Random Forest models to\ncome up with several key metrics that establish the usefulness of and\nsuperiority of our model under some tightly defined operational constraints.\nFor example, using a 15 day look back period, our approach can predict the\noccurrence of disk failure with an accuracy of 96.4% considering test data 60\ndays before failure. This helps to alert operations maintenance well in-advance\nabout potential mitigation needs. In addition, our model reports a mean\nabsolute error of 0.12 for predicting failure up to 60 days in advance, placing\nit among the state-of-the-art in recent literature.",
          "link": "http://arxiv.org/abs/2109.05351",
          "publishedOn": "2021-09-14T07:20:13.983Z",
          "wordCount": null,
          "title": "Remaining Useful Life Estimation of Hard Disk Drives using Bidirectional LSTM Networks. (arXiv:2109.05351v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianwei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1\">Leandros Tassiulas</a>",
          "description": "Federated learning (FL) is a distributed learning paradigm that enables a\nlarge number of mobile devices to collaboratively learn a model under the\ncoordination of a central server without sharing their raw data. Despite its\npractical efficiency and effectiveness, the iterative on-device learning\nprocess (e.g., local computations and global communications with the server)\nincurs a considerable cost in terms of learning time and energy consumption,\nwhich depends crucially on the number of selected clients and the number of\nlocal iterations in each training round. In this paper, we analyze how to\ndesign adaptive FL in mobile edge networks that optimally chooses these\nessential control variables to minimize the total cost while ensuring\nconvergence. We establish the analytical relationship between the total cost\nand the control variables with the convergence upper bound. To efficiently\nsolve the cost minimization problem, we develop a low-cost sampling-based\nalgorithm to learn the convergence related unknown parameters. We derive\nimportant solution properties that effectively identify the design principles\nfor different optimization metrics. Practically, we evaluate our theoretical\nresults both in a simulated environment and on a hardware prototype.\nExperimental evidence verifies our derived properties and demonstrates that our\nproposed solution achieves near-optimal performance for different optimization\nmetrics for various datasets and heterogeneous system and statistical settings.",
          "link": "http://arxiv.org/abs/2109.05411",
          "publishedOn": "2021-09-14T07:20:13.983Z",
          "wordCount": null,
          "title": "Cost-Effective Federated Learning in Mobile Edge Networks. (arXiv:2109.05411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yandong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1\">Bhiksha Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rita Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>",
          "description": "This paper addresses the deep face recognition problem under an open-set\nprotocol, where ideal face features are expected to have smaller maximal\nintra-class distance than minimal inter-class distance under a suitably chosen\nmetric space. To this end, hyperspherical face recognition, as a promising line\nof research, has attracted increasing attention and gradually become a major\nfocus in face recognition research. As one of the earliest works in\nhyperspherical face recognition, SphereFace explicitly proposed to learn face\nembeddings with large inter-class angular margin. However, SphereFace still\nsuffers from severe training instability which limits its application in\npractice. In order to address this problem, we introduce a unified framework to\nunderstand large angular margin in hyperspherical face recognition. Under this\nframework, we extend the study of SphereFace and propose an improved variant\nwith substantially better training stability -- SphereFace-R. Specifically, we\npropose two novel ways to implement the multiplicative margin, and study\nSphereFace-R under three different feature normalization schemes (no feature\nnormalization, hard feature normalization and soft feature normalization). We\nalso propose an implementation strategy -- \"characteristic gradient detachment\"\n-- to stabilize training. Extensive experiments on SphereFace-R show that it is\nconsistently better than or competitive with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05565",
          "publishedOn": "2021-09-14T07:20:13.982Z",
          "wordCount": null,
          "title": "SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sajedi_A/0/1/0/all/0/1\">Ahmad Sajedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1\">Konstantinos N. Plataniotis</a>",
          "description": "This work introduces a novel knowledge distillation framework for\nclassification tasks where information on existing subclasses is available and\ntaken into consideration. In classification tasks with a small number of\nclasses or binary detection (two classes) the amount of information transferred\nfrom the teacher to the student network is restricted, thus limiting the\nutility of knowledge distillation. Performance can be improved by leveraging\ninformation about possible subclasses within the available classes in the\nclassification task. To that end, we propose the so-called Subclass Knowledge\nDistillation (SKD) framework, which is the process of transferring the\nsubclasses' prediction knowledge from a large teacher model into a smaller\nstudent one. Through SKD, additional meaningful information which is not in the\nteacher's class logits but exists in subclasses (e.g., similarities inside\nclasses) will be conveyed to the student and boost its performance.\nMathematically, we measure how many extra information bits the teacher can\nprovide for the student via SKD framework. The framework developed is evaluated\nin clinical application, namely colorectal polyp binary classification. In this\napplication, clinician-provided annotations are used to define subclasses based\non the annotation label's variability in a curriculum style of learning. A\nlightweight, low complexity student trained with the proposed framework\nachieves an F1-score of 85.05%, an improvement of 2.14% and 1.49% gain over the\nstudent that trains without and with conventional knowledge distillation,\nrespectively. These results show that the extra subclasses' knowledge (i.e.,\n0.4656 label bits per training sample in our experiment) can provide more\ninformation about the teacher generalization, and therefore SKD can benefit\nfrom using more information to increase the student performance.",
          "link": "http://arxiv.org/abs/2109.05587",
          "publishedOn": "2021-09-14T07:20:13.981Z",
          "wordCount": 705,
          "title": "On the Efficiency of Subclass Knowledge Distillation in Classification Tasks. (arXiv:2109.05587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05095",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_K/0/1/0/all/0/1\">Kaushik Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_D/0/1/0/all/0/1\">Devesh Upadhyay</a>",
          "description": "Dynamical systems are ubiquitous and are often modeled using a non-linear\nsystem of governing equations. Numerical solution procedures for many dynamical\nsystems have existed for several decades, but can be slow due to\nhigh-dimensional state space of the dynamical system. Thus, deep learning-based\nreduced order models (ROMs) are of interest and one such family of algorithms\nalong these lines are based on the Koopman theory. This paper extends a\nrecently developed adversarial Koopman model (Balakrishnan \\& Upadhyay,\narXiv:2006.05547) to stochastic space, where the Koopman operator applies on\nthe probability distribution of the latent encoding of an encoder.\nSpecifically, the latent encoding of the system is modeled as a Gaussian, and\nis advanced in time by using an auxiliary neural network that outputs two\nKoopman matrices $K_{\\mu}$ and $K_{\\sigma}$. Adversarial and gradient losses\nare used and this is found to lower the prediction errors. A reduced Koopman\nformulation is also undertaken where the Koopman matrices are assumed to have a\ntridiagonal structure, and this yields predictions comparable to the baseline\nmodel with full Koopman matrices. The efficacy of the stochastic Koopman model\nis demonstrated on different test problems in chaos, fluid dynamics,\ncombustion, and reaction-diffusion models. The proposed model is also applied\nin a setting where the Koopman matrices are conditioned on other input\nparameters for generalization and this is applied to simulate the state of a\nLithium-ion battery in time. The Koopman models discussed in this study are\nvery promising for the wide range of problems considered.",
          "link": "http://arxiv.org/abs/2109.05095",
          "publishedOn": "2021-09-14T07:20:13.973Z",
          "wordCount": null,
          "title": "Stochastic Adversarial Koopman Model for Dynamical Systems. (arXiv:2109.05095v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rego_D/0/1/0/all/0/1\">David Martinez Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>",
          "description": "Universal Adversarial Perturbations (UAPs) are input perturbations that can\nfool a neural network on large sets of data. They are a class of attacks that\nrepresents a significant threat as they facilitate realistic, practical, and\nlow-cost attacks on neural networks. In this work, we derive upper bounds for\nthe effectiveness of UAPs based on norms of data-dependent Jacobians. We\nempirically verify that Jacobian regularization greatly increases model\nrobustness to UAPs by up to four times whilst maintaining clean performance.\nOur theoretical analysis also allows us to formulate a metric for the strength\nof shared adversarial perturbations between pairs of inputs. We apply this\nmetric to benchmark datasets and show that it is highly correlated with the\nactual observed robustness. This suggests that realistic and practical\nuniversal attacks can be reliably mitigated without sacrificing clean accuracy,\nwhich shows promise for the robustness of machine learning systems.",
          "link": "http://arxiv.org/abs/2104.10459",
          "publishedOn": "2021-09-14T07:20:13.969Z",
          "wordCount": null,
          "title": "Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Ming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cailian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "In federated learning (FL), model training is distributed over clients and\nlocal models are aggregated by a central server. The performance of uploaded\nmodels in such situations can vary widely due to imbalanced data distributions,\npotential demands on privacy protections, and quality of transmissions. In this\npaper, we aim to minimize FL training delay over wireless channels, constrained\nby overall training performance as well as each client's differential privacy\n(DP) requirement. We solve this problem in the framework of multi-agent\nmulti-armed bandit (MAMAB) to deal with the situation where there are multiple\nclients confornting different unknown transmission environments, e.g., channel\nfading and interferences. Specifically, we first transform the long-term\nconstraints on both training performance and each client's DP into a virtual\nqueue based on the Lyapunov drift technique. Then, we convert the MAMAB to a\nmax-min bipartite matching problem at each communication round, by estimating\nrewards with the upper confidence bound (UCB) approach. More importantly, we\npropose two efficient solutions to this matching problem, i.e., modified\nHungarian algorithm and greedy matching with a better alternative (GMBA), in\nwhich the first one can achieve the optimal solution with a high complexity\nwhile the second one approaches a better trade-off by enabling a verified\nlow-complexity with little performance loss. In addition, we develop an upper\nbound on the expected regret of this MAMAB based FL framework, which shows a\nlinear growth over the logarithm of communication rounds, justifying its\ntheoretical feasibility. Extensive experimental results are conducted to\nvalidate the effectiveness of our proposed algorithms, and the impacts of\nvarious parameters on the FL performance over wireless edge networks are also\ndiscussed.",
          "link": "http://arxiv.org/abs/2106.13039",
          "publishedOn": "2021-09-14T07:20:13.968Z",
          "wordCount": null,
          "title": "Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chuanpu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Meng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>",
          "description": "Machine learning (ML) based malicious traffic detection is an emerging\nsecurity paradigm, particularly for zero-day attack detection, which is\ncomplementary to existing rule based detection. However, the existing ML based\ndetection has low detection accuracy and low throughput incurred by inefficient\ntraffic features extraction. Thus, they cannot detect attacks in realtime\nespecially in high throughput networks. Particularly, these detection systems\nsimilar to the existing rule based detection can be easily evaded by\nsophisticated attacks. To this end, we propose Whisper, a realtime ML based\nmalicious traffic detection system that achieves both high accuracy and high\nthroughput by utilizing frequency domain features. It utilizes sequential\nfeatures represented by the frequency domain features to achieve bounded\ninformation loss, which ensures high detection accuracy, and meanwhile\nconstrains the scale of features to achieve high detection throughput.\nParticularly, attackers cannot easily interfere with the frequency domain\nfeatures and thus Whisper is robust against various evasion attacks. Our\nexperiments with 42 types of attacks demonstrate that, compared with the\nstate-of-theart systems, Whisper can accurately detect various sophisticated\nand stealthy attacks, achieving at most 18.36% improvement, while achieving two\norders of magnitude throughput. Even under various evasion attacks, Whisper is\nstill able to maintain around 90% detection accuracy.",
          "link": "http://arxiv.org/abs/2106.14707",
          "publishedOn": "2021-09-14T07:20:13.966Z",
          "wordCount": null,
          "title": "Realtime Robust Malicious Traffic Detection via Frequency Domain Analysis. (arXiv:2106.14707v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05598",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Yin_J/0/1/0/all/0/1\">Junqi Yin</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Pei_Z/0/1/0/all/0/1\">Zongrui Pei</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Gao_M/0/1/0/all/0/1\">Michael Gao</a>",
          "description": "Phase transition is one of the most important phenomena in nature and plays a\ncentral role in materials design. All phase transitions are characterized by\nsuitable order parameters, including the order-disorder phase transition.\nHowever, finding a representative order parameter for complex systems is\nnontrivial, such as for high-entropy alloys. Given variational autoencoder's\n(VAE) strength of reducing high dimensional data into few principal components,\nhere we coin a new concept of \"VAE order parameter\". We propose that the\nManhattan distance in the VAE latent space can serve as a generic order\nparameter for order-disorder phase transitions. The physical properties of the\norder parameter are quantitatively interpreted and demonstrated by multiple\nrefractory high-entropy alloys. Assisted by it, a generally applicable alloy\ndesign concept is proposed by mimicking the nature mixing of elements. Our\nphysically interpretable \"VAE order parameter\" lays the foundation for the\nunderstanding of and alloy design by chemical ordering.",
          "link": "http://arxiv.org/abs/2109.05598",
          "publishedOn": "2021-09-14T07:20:13.957Z",
          "wordCount": null,
          "title": "Neural network based order parameter for phase transitions and its applications in high-entropy alloys. (arXiv:2109.05598v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2005.05889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1\">Borja Rodr&#xed;guez-G&#xe1;lvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bassi_G/0/1/0/all/0/1\">Germ&#xe1;n Bassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1\">Mikael Skoglund</a>",
          "description": "In this work, we study the generalization capability of algorithms from an\ninformation-theoretic perspective. It has been shown that the expected\ngeneralization error of an algorithm is bounded from above by a function of the\nrelative entropy between the conditional probability distribution of the\nalgorithm's output hypothesis, given the dataset with which it was trained, and\nits marginal probability distribution. We build upon this fact and introduce a\nmathematical formulation to obtain upper bounds on this relative entropy.\nAssuming that the data is discrete, we then develop a strategy using this\nformulation, based on the method of types and typicality, to find explicit\nupper bounds on the generalization error of stable algorithms, i.e., algorithms\nthat produce similar output hypotheses given similar input datasets. In\nparticular, we show the bounds obtained with this strategy for the case of\n$\\epsilon$-DP and $\\mu$-GDP algorithms.",
          "link": "http://arxiv.org/abs/2005.05889",
          "publishedOn": "2021-09-14T07:20:13.957Z",
          "wordCount": null,
          "title": "Upper Bounds on the Generalization Error of Private Algorithms for Discrete Data. (arXiv:2005.05889v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05490",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhaopeng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>",
          "description": "Discrete-continuous hybrid action space is a natural setting in many\npractical problems, such as robot control and game AI. However, most previous\nReinforcement Learning (RL) works only demonstrate the success in controlling\nwith either discrete or continuous action space, while seldom take into account\nthe hybrid action space. One naive way to address hybrid action RL is to\nconvert the hybrid action space into a unified homogeneous action space by\ndiscretization or continualization, so that conventional RL algorithms can be\napplied. However, this ignores the underlying structure of hybrid action space\nand also induces the scalability issue and additional approximation\ndifficulties, thus leading to degenerated results. In this paper, we propose\nHybrid Action Representation (HyAR) to learn a compact and decodable latent\nrepresentation space for the original hybrid action space. HyAR constructs the\nlatent space and embeds the dependence between discrete action and continuous\nparameter via an embedding table and conditional Variantional Auto-Encoder\n(VAE). To further improve the effectiveness, the action representation is\ntrained to be semantically smooth through unsupervised environmental dynamics\nprediction. Finally, the agent then learns its policy with conventional DRL\nalgorithms in the learned representation space and interacts with the\nenvironment by decoding the hybrid action embeddings to the original action\nspace. We evaluate HyAR in a variety of environments with discrete-continuous\naction space. The results demonstrate the superiority of HyAR when compared\nwith previous baselines, especially for high-dimensional action spaces.",
          "link": "http://arxiv.org/abs/2109.05490",
          "publishedOn": "2021-09-14T07:20:13.956Z",
          "wordCount": null,
          "title": "HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation. (arXiv:2109.05490v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cuong_N/0/1/0/all/0/1\">Nguyen-Tien Cuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1\">Hung Ngoc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_N/0/1/0/all/0/1\">Nhat Minh Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Phuong Hoai Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Synh Viet-Uyen Ha</a>",
          "description": "Background modeling is a promising research area in video analysis with a\nvariety of video surveillance applications. Recent years have witnessed the\nproliferation of deep neural networks via effective learning-based approaches\nin motion analysis. However, these techniques only provide a limited\ndescription of the observed scenes' insufficient properties where a\nsingle-valued mapping is learned to approximate the temporal conditional\naverages of the target background. On the other hand, statistical learning in\nimagery domains has become one of the most prevalent approaches with high\nadaptation to dynamic context transformation, notably Gaussian Mixture Models,\ncombined with a foreground extraction step. In this work, we propose a novel,\ntwo-stage method of change detection with two convolutional neural networks.\nThe first architecture is grounded on the unsupervised Gaussian mixtures\nstatistical learning to describe the scenes' salient features. The second one\nimplements a light-weight pipeline of foreground detection. Our two-stage\nframework contains approximately 3.5K parameters in total but still maintains\nrapid convergence to intricate motion patterns. Our experiments on publicly\navailable datasets show that our proposed networks are not only capable of\ngeneralizing regions of moving objects in unseen cases with promising results\nbut also are competitive in performance efficiency and effectiveness regarding\nforeground segmentation.",
          "link": "http://arxiv.org/abs/2106.03776",
          "publishedOn": "2021-09-14T07:20:13.955Z",
          "wordCount": null,
          "title": "CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05583",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Coors_S/0/1/0/all/0/1\">Stefan Coors</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schalk_D/0/1/0/all/0/1\">Daniel Schalk</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rugamer_D/0/1/0/all/0/1\">David R&#xfc;gamer</a>",
          "description": "In practice, machine learning (ML) workflows require various different steps,\nfrom data preprocessing, missing value imputation, model selection, to model\ntuning as well as model evaluation. Many of these steps rely on human ML\nexperts. AutoML - the field of automating these ML pipelines - tries to help\npractitioners to apply ML off-the-shelf without any expert knowledge. Most\nmodern AutoML systems like auto-sklearn, H20-AutoML or TPOT aim for high\npredictive performance, thereby generating ensembles that consist almost\nexclusively of black-box models. This, in turn, makes the interpretation for\nthe layperson more intricate and adds another layer of opacity for users. We\npropose an AutoML system that constructs an interpretable additive model that\ncan be fitted using a highly scalable componentwise boosting algorithm. Our\nsystem provides tools for easy model interpretation such as visualizing partial\neffects and pairwise interactions, allows for a straightforward calculation of\nfeature importance, and gives insights into the required model complexity to\nfit the given task. We introduce the general framework and outline its\nimplementation autocompboost. To demonstrate the frameworks efficacy, we\ncompare autocompboost to other existing systems based on the OpenML\nAutoML-Benchmark. Despite its restriction to an interpretable model space, our\nsystem is competitive in terms of predictive performance on most data sets\nwhile being more user-friendly and transparent.",
          "link": "http://arxiv.org/abs/2109.05583",
          "publishedOn": "2021-09-14T07:20:13.951Z",
          "wordCount": null,
          "title": "Automatic Componentwise Boosting: An Interpretable AutoML System. (arXiv:2109.05583v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jimeng Sun</a>",
          "description": "Crucial for building trust in deep learning models for critical real-world\napplications is efficient and theoretically sound uncertainty quantification, a\ntask that continues to be challenging. Useful uncertainty information is\nexpected to have two key properties: It should be valid (guaranteeing coverage)\nand discriminative (more uncertain when the expected risk is high). Moreover,\nwhen combined with deep learning (DL) methods, it should be scalable and affect\nthe DL model performance minimally. Most existing Bayesian methods lack\nfrequentist coverage guarantees and usually affect model performance. The few\navailable frequentist methods are rarely discriminative and/or violate coverage\nguarantees due to unrealistic assumptions. Moreover, many methods are expensive\nor require substantial modifications to the base neural network. Building upon\nrecent advances in conformal prediction [13, 31] and leveraging the classical\nidea of kernel regression, we propose Locally Valid and Discriminative\npredictive intervals (LVD), a simple, efficient, and lightweight method to\nconstruct discriminative predictive intervals (PIs) for almost any DL model.\nWith no assumptions on the data distribution, such PIs also offer finite-sample\nlocal coverage guarantees (contrasted to the simpler marginal coverage). We\nempirically verify, using diverse datasets, that besides being the only locally\nvalid method, LVD also exceeds or matches the performance (including coverage\nrate and prediction accuracy) of existing uncertainty quantification methods,\nwhile offering additional benefits in scalability and flexibility.",
          "link": "http://arxiv.org/abs/2106.00225",
          "publishedOn": "2021-09-14T07:20:13.950Z",
          "wordCount": null,
          "title": "Locally Valid and Discriminative Predictive Intervals for Deep Learning Models. (arXiv:2106.00225v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.03348",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Md Shamim Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaki_M/0/1/0/all/0/1\">Mohammed J. Zaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1\">Dharmashankar Subramanian</a>",
          "description": "Transformer neural networks have achieved state-of-the-art results for\nunstructured data such as text and images but their adoption for\ngraph-structured data has been limited. This is partly due to the difficulty of\nincorporating complex structural information in the basic transformer\nframework. We propose a simple yet powerful extension to the transformer -\nresidual edge channels. The resultant framework, which we call Edge-augmented\nGraph Transformer (EGT), can directly accept, process and output structural\ninformation as well as node information. It allows us to use global\nself-attention, the key element of transformers, directly for graphs and comes\nwith the benefit of long-range interaction among nodes. Moreover, the edge\nchannels allow the structural information to evolve from layer to layer, and\nprediction tasks on edges/links can be performed directly from the output\nembeddings of these channels. In addition, we introduce a generalized\npositional encoding scheme for graphs based on Singular Value Decomposition\nwhich can improve the performance of EGT. Our framework, which relies on global\nnode feature aggregation, achieves better performance compared to\nConvolutional/Message-Passing Graph Neural Networks, which rely on local\nfeature aggregation within a neighborhood. We verify the performance of EGT in\na supervised learning setting on a wide range of experiments on benchmark\ndatasets. Our findings indicate that convolutional aggregation is not an\nessential inductive bias for graphs and global self-attention can serve as a\nflexible and adaptive alternative.",
          "link": "http://arxiv.org/abs/2108.03348",
          "publishedOn": "2021-09-14T07:20:13.948Z",
          "wordCount": null,
          "title": "Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs. (arXiv:2108.03348v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.",
          "link": "http://arxiv.org/abs/2109.05125",
          "publishedOn": "2021-09-14T07:20:13.947Z",
          "wordCount": null,
          "title": "MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaehyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hur_Y/0/1/0/all/0/1\">Youngbum Hur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sejun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "While semi-supervised learning (SSL) has proven to be a promising way for\nleveraging unlabeled data when labeled data is scarce, the existing SSL\nalgorithms typically assume that training class distributions are balanced.\nHowever, these SSL algorithms trained under imbalanced class distributions can\nseverely suffer when generalizing to a balanced testing criterion, since they\nutilize biased pseudo-labels of unlabeled data toward majority classes. To\nalleviate this issue, we formulate a convex optimization problem to softly\nrefine the pseudo-labels generated from the biased model, and develop a simple\nalgorithm, named Distribution Aligning Refinery of Pseudo-label (DARP) that\nsolves it provably and efficiently. Under various class-imbalanced\nsemi-supervised scenarios, we demonstrate the effectiveness of DARP and its\ncompatibility with state-of-the-art SSL schemes.",
          "link": "http://arxiv.org/abs/2007.08844",
          "publishedOn": "2021-09-14T07:20:13.945Z",
          "wordCount": null,
          "title": "Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning. (arXiv:2007.08844v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Libing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Min Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "Adaptive traffic signal control plays a significant role in the construction\nof smart cities. This task is challenging because of many essential factors,\nsuch as cooperation among neighboring intersections and dynamic traffic\nscenarios. First, to facilitate cooperation of traffic signals, existing work\nadopts graph neural networks to incorporate the temporal and spatial influences\nof the surrounding intersections into the target intersection, where\nspatial-temporal information is used separately. However, one drawback of these\nmethods is that the spatial-temporal correlations are not adequately exploited\nto obtain a better control scheme. Second, in a dynamic traffic environment,\nthe historical state of the intersection is also critical for predicting future\nsignal switching. Previous work mainly solves this problem using the current\nintersection's state, neglecting the fact that traffic flow is continuously\nchanging both spatially and temporally and does not handle the historical\nstate.\n\nIn this paper, we propose a novel neural network framework named DynSTGAT,\nwhich integrates dynamic historical state into a new spatial-temporal graph\nattention network to address the above two problems. More specifically, our\nDynSTGAT model employs a novel multi-head graph attention mechanism, which aims\nto adequately exploit the joint relations of spatial-temporal information.\nThen, to efficiently utilize the historical state information of the\nintersection, we design a sequence model with the temporal convolutional\nnetwork (TCN) to capture the historical information and further merge it with\nthe spatial information to improve its performance. Extensive experiments\nconducted in the multi-intersection scenario on synthetic data and real-world\ndata confirm that our method can achieve superior performance in travel time\nand throughput against the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05491",
          "publishedOn": "2021-09-14T07:20:13.943Z",
          "wordCount": null,
          "title": "DynSTGAT: Dynamic Spatial-Temporal Graph Attention Network for Traffic Signal Control. (arXiv:2109.05491v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07815",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_S/0/1/0/all/0/1\">Shaowu Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hongping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_R/0/1/0/all/0/1\">Runjie Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jinjun Wang</a>",
          "description": "Three-dimensional particle reconstruction with limited two-dimensional\nprojections is an under-determined inverse problem that the exact solution is\noften difficult to be obtained. In general, approximate solutions can be\nobtained by iterative optimization methods. In the current work, a practical\nparticle reconstruction method based on a convolutional neural network (CNN)\nwith geometry-informed features is proposed. The proposed technique can refine\nthe particle reconstruction from a very coarse initial guess of particle\ndistribution generated by any traditional algebraic reconstruction technique\n(ART) based methods. Compared with available ART-based algorithms, the novel\ntechnique makes significant improvements in terms of reconstruction quality,\n{robustness to noises}, and at least an order of magnitude faster in the\noffline stage.",
          "link": "http://arxiv.org/abs/1909.07815",
          "publishedOn": "2021-09-14T07:20:13.941Z",
          "wordCount": null,
          "title": "Particle reconstruction of volumetric particle image velocimetry with strategy of machine learning. (arXiv:1909.07815v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Deep Neural Networks (DNNs) have been utilized in various applications\nranging from image classification and facial recognition to medical imagery\nanalysis and real-time object detection. As our models become more\nsophisticated and complex, the computational cost of training such models\nbecomes a burden for small companies and individuals; for this reason,\noutsourcing the training process has been the go-to option for such users.\nUnfortunately, outsourcing the training process comes at the cost of\nvulnerability to backdoor attacks. These attacks aim at establishing hidden\nbackdoors in the DNN such that the model performs well on benign samples but\noutputs a particular target label when a trigger is applied to the input.\nCurrent backdoor attacks rely on generating triggers in the image/pixel domain;\nhowever, as we show in this paper, it is not the only domain to exploit and one\nshould always \"check the other doors\". In this work, we propose a complete\npipeline for generating a dynamic, efficient, and invisible backdoor attack in\nthe frequency domain. We show the advantages of utilizing the frequency domain\nfor establishing undetectable and powerful backdoor attacks through extensive\nexperiments on various datasets and network architectures. The backdoored\nmodels are shown to break various state-of-the-art defences. We also show two\npossible defences that succeed against frequency-based backdoor attacks and\npossible ways for the attacker to bypass them. We conclude the work with some\nremarks regarding a network's learning capacity and the capability of embedding\na backdoor attack in the model.",
          "link": "http://arxiv.org/abs/2109.05507",
          "publishedOn": "2021-09-14T07:20:13.938Z",
          "wordCount": null,
          "title": "Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>",
          "description": "Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.",
          "link": "http://arxiv.org/abs/2109.05281",
          "publishedOn": "2021-09-14T07:20:13.930Z",
          "wordCount": null,
          "title": "COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kell_A/0/1/0/all/0/1\">Alexander J. M. Kell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1\">A. Stephen McGough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forshaw_M/0/1/0/all/0/1\">Matthew Forshaw</a>",
          "description": "A lowering in the cost of batteries and solar PV systems has led to a high\nuptake of solar battery home systems. In this work, we use the deep\ndeterministic policy gradient algorithm to optimise the charging and\ndischarging behaviour of a battery within such a system. Our approach outputs a\ncontinuous action space when it charges and discharges the battery, and can\nfunction well in a stochastic environment. We show good performance of this\nalgorithm by lowering the expenditure of a single household on electricity to\nalmost \\$1AUD for large batteries across selected weeks within a year.",
          "link": "http://arxiv.org/abs/2109.05024",
          "publishedOn": "2021-09-14T07:20:13.908Z",
          "wordCount": 558,
          "title": "Optimizing a domestic battery and solar photovoltaic system with deep reinforcement learning. (arXiv:2109.05024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vial_D/0/1/0/all/0/1\">Daniel Vial</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parulekar_A/0/1/0/all/0/1\">Advait Parulekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1\">R. Srikant</a>",
          "description": "For the misspecified linear Markov decision process (MLMDP) model of Jin et\nal. [2020], we propose an algorithm with three desirable properties. (P1) Its\nregret after $K$ episodes scales as $K \\max \\{ \\varepsilon_{\\text{mis}},\n\\varepsilon_{\\text{tol}} \\}$, where $\\varepsilon_{\\text{mis}}$ is the degree of\nmisspecification and $\\varepsilon_{\\text{tol}}$ is a user-specified error\ntolerance. (P2) Its space and per-episode time complexities remain bounded as\n$K \\rightarrow \\infty$. (P3) It does not require $\\varepsilon_{\\text{mis}}$ as\ninput. To our knowledge, this is the first algorithm satisfying all three\nproperties. For concrete choices of $\\varepsilon_{\\text{tol}}$, we also improve\nexisting regret bounds (up to log factors) while achieving either (P2) or (P3)\n(existing algorithms satisfy neither). At a high level, our algorithm\ngeneralizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura\net al. [2021] recently showed satisfies (P3) in the contextual bandit setting.",
          "link": "http://arxiv.org/abs/2109.05546",
          "publishedOn": "2021-09-14T07:20:13.738Z",
          "wordCount": null,
          "title": "Improved Algorithms for Misspecified Linear Markov Decision Processes. (arXiv:2109.05546v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.04795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Shuwei Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Co-training, extended from self-training, is one of the frameworks for\nsemi-supervised learning. Without natural split of features, single-view\nco-training works at the cost of training extra classifiers, where the\nalgorithm should be delicately designed to prevent individual classifiers from\ncollapsing into each other. To remove these obstacles which deter the adoption\nof single-view co-training, we present a simple and efficient algorithm\nMulti-Head Co-Training. By integrating base learners into a multi-head\nstructure, the model is in a minimal amount of extra parameters. Every\nclassification head in the unified model interacts with its peers through a\n\"Weak and Strong Augmentation\" strategy, in which the diversity is naturally\nbrought by the strong data augmentation. Therefore, the proposed method\nfacilitates single-view co-training by 1). promoting diversity implicitly and\n2). only requiring a small extra computational overhead. The effectiveness of\nMulti-Head Co-Training is demonstrated in an empirical study on standard\nsemi-supervised learning benchmarks.",
          "link": "http://arxiv.org/abs/2107.04795",
          "publishedOn": "2021-09-14T07:20:13.735Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Siwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1\">Kukjin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyun-Soo Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "In recent years, proposed studies on time-series anomaly detection (TAD)\nreport high F1 scores on benchmark TAD datasets, giving the impression of clear\nimprovements. However, most studies apply a peculiar evaluation protocol called\npoint adjustment (PA) before scoring. In this paper, we theoretically and\nexperimentally reveal that the PA protocol has a great possibility of\noverestimating the detection performance; that is, even a random anomaly score\ncan easily turn into a state-of-the-art TAD method. Therefore, the comparison\nof TAD methods with F1 scores after the PA protocol can lead to misguided\nrankings. Furthermore, we question the potential of existing TAD methods by\nshowing that an untrained model obtains comparable detection performance to the\nexisting methods even without PA. Based on our findings, we propose a new\nbaseline and an evaluation protocol. We expect that our study will help a\nrigorous evaluation of TAD and lead to further improvement in future\nresearches.",
          "link": "http://arxiv.org/abs/2109.05257",
          "publishedOn": "2021-09-14T07:20:13.734Z",
          "wordCount": null,
          "title": "Towards a Rigorous Evaluation of Time-series Anomaly Detection. (arXiv:2109.05257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1\">Feng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>",
          "description": "Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.",
          "link": "http://arxiv.org/abs/2010.03084",
          "publishedOn": "2021-09-14T07:20:13.733Z",
          "wordCount": null,
          "title": "Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Che_L/0/1/0/all/0/1\">Liwei Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zewei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Houping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>",
          "description": "Federated Learning has shown great potentials for the distributed data\nutilization and privacy protection. Most existing federated learning approaches\nfocus on the supervised setting, which means all the data stored in each client\nhas labels. However, in real-world applications, the client data are impossible\nto be fully labeled. Thus, how to exploit the unlabeled data should be a new\nchallenge for federated learning. Although a few studies are attempting to\novercome this challenge, they may suffer from information leakage or misleading\ninformation usage problems. To tackle these issues, in this paper, we propose a\nnovel federated semi-supervised learning method named FedTriNet, which consists\nof two learning phases. In the first phase, we pre-train FedTriNet using\nlabeled data with FedAvg. In the second phase, we aim to make most of the\nunlabeled data to help model learning. In particular, we propose to use three\nnetworks and a dynamic quality control mechanism to generate high-quality\npseudo labels for unlabeled data, which are added to the training set. Finally,\nFedTriNet uses the new training set to retrain the model. Experimental results\non three publicly available datasets show that the proposed FedTriNet\noutperforms state-of-the-art baselines under both IID and Non-IID settings.",
          "link": "http://arxiv.org/abs/2109.05612",
          "publishedOn": "2021-09-14T07:20:13.732Z",
          "wordCount": null,
          "title": "FedTriNet: A Pseudo Labeling Method with Three Players for Federated Semi-supervised Learning. (arXiv:2109.05612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiwei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>",
          "description": "The monitoring of coastal wetlands is of great importance to the protection\nof marine and terrestrial ecosystems. However, due to the complex environment,\nsevere vegetation mixture, and difficulty of access, it is impossible to\naccurately classify coastal wetlands and identify their species with\ntraditional classifiers. Despite the integration of multisource remote sensing\ndata for performance enhancement, there are still challenges with acquiring and\nexploiting the complementary merits from multisource data. In this paper, the\nDeepwise Feature Interaction Network (DFINet) is proposed for wetland\nclassification. A depthwise cross attention module is designed to extract\nself-correlation and cross-correlation from multisource feature pairs. In this\nway, meaningful complementary information is emphasized for classification.\nDFINet is optimized by coordinating consistency loss, discrimination loss, and\nclassification loss. Accordingly, DFINet reaches the standard solution-space\nunder the regularity of loss functions, while the spatial consistency and\nfeature discrimination are preserved. Comprehensive experimental results on two\nhyperspectral and multispectral wetland datasets demonstrate that the proposed\nDFINet outperforms other competitive methods in terms of overall accuracy.",
          "link": "http://arxiv.org/abs/2106.06896",
          "publishedOn": "2021-09-14T07:20:13.732Z",
          "wordCount": null,
          "title": "Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12084",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>",
          "description": "Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.",
          "link": "http://arxiv.org/abs/2108.12084",
          "publishedOn": "2021-09-14T07:20:13.732Z",
          "wordCount": null,
          "title": "Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07491",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbalau_A/0/1/0/all/0/1\">Antonio Barbalau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Anomaly detection in video is a challenging computer vision problem. Due to\nthe lack of anomalous events at training time, anomaly detection requires the\ndesign of learning methods without full supervision. In this paper, we approach\nanomalous event detection in video through self-supervised and multi-task\nlearning at the object level. We first utilize a pre-trained detector to detect\nobjects. Then, we train a 3D convolutional neural network to produce\ndiscriminative anomaly-specific information by jointly learning multiple proxy\ntasks: three self-supervised and one based on knowledge distillation. The\nself-supervised tasks are: (i) discrimination of forward/backward moving\nobjects (arrow of time), (ii) discrimination of objects in\nconsecutive/intermittent frames (motion irregularity) and (iii) reconstruction\nof object-specific appearance information. The knowledge distillation task\ntakes into account both classification and detection information, generating\nlarge prediction discrepancies between teacher and student models when\nanomalies occur. To the best of our knowledge, we are the first to approach\nanomalous event detection in video as a multi-task learning problem,\nintegrating multiple self-supervised and knowledge distillation proxy tasks in\na single architecture. Our lightweight architecture outperforms the\nstate-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD\nPed2. Additionally, we perform an ablation study demonstrating the importance\nof integrating self-supervised learning and normality-specific distillation in\na multi-task learning setting.",
          "link": "http://arxiv.org/abs/2011.07491",
          "publishedOn": "2021-09-14T07:20:13.731Z",
          "wordCount": null,
          "title": "Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grupen_N/0/1/0/all/0/1\">Niko A. Grupen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selman_B/0/1/0/all/0/1\">Bart Selman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daniel D. Lee</a>",
          "description": "We study fairness through the lens of cooperative multi-agent learning. Our\nwork is motivated by empirical evidence that naive maximization of team reward\nyields unfair outcomes for individual team members. To address fairness in\nmulti-agent contexts, we introduce team fairness, a group-based fairness\nmeasure for multi-agent learning. We then prove that it is possible to enforce\nteam fairness during policy optimization by transforming the team's joint\npolicy into an equivariant map. We refer to our multi-agent learning strategy\nas Fairness through Equivariance (Fair-E) and demonstrate its effectiveness\nempirically. We then introduce Fairness through Equivariance Regularization\n(Fair-ER) as a soft-constraint version of Fair-E and show that it reaches\nhigher levels of utility than Fair-E and fairer outcomes than non-equivariant\npolicies. Finally, we present novel findings regarding the fairness-utility\ntrade-off in multi-agent settings; showing that the magnitude of the trade-off\nis dependent on agent skill level.",
          "link": "http://arxiv.org/abs/2106.05727",
          "publishedOn": "2021-09-14T07:20:13.730Z",
          "wordCount": null,
          "title": "Cooperative Multi-Agent Fairness and Equivariant Policies. (arXiv:2106.05727v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.05252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiacheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Songze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wensi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Bochuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>",
          "description": "We propose OmniLytics, a blockchain-based secure data trading marketplace for\nmachine learning applications. Utilizing OmniLytics, many distributed data\nowners can contribute their private data to collectively train an ML model\nrequested by some model owners, and receive compensation for data contribution.\nOmniLytics enables such model training while simultaneously providing 1) model\nsecurity against curious data owners; 2) data security against the curious\nmodel and data owners; 3) resilience to malicious data owners who provide\nfaulty results to poison model training; and 4) resilience to malicious model\nowners who intend to evade payment. OmniLytics is implemented as a blockchain\nsmart contract to guarantee the atomicity of payment. In OmniLytics, a model\nowner splits its model into the private and public parts and publishes the\npublic part on the contract. Through the execution of the contract, the\nparticipating data owners securely aggregate their locally trained models to\nupdate the model owner's public model and receive reimbursement through the\ncontract. We implement a working prototype of OmniLytics on Ethereum blockchain\nand perform extensive experiments to measure its gas cost, execution time, and\nmodel quality under various parameter combinations. For training a CNN on the\nMNIST dataset, the MO is able to boost its model accuracy from 62% to 83%\nwithin 500ms in blockchain processing time.This demonstrates the effectiveness\nof OmniLytics for practical deployment.",
          "link": "http://arxiv.org/abs/2107.05252",
          "publishedOn": "2021-09-14T07:20:13.729Z",
          "wordCount": null,
          "title": "OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patsantzis_S/0/1/0/all/0/1\">Stassa Patsantzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muggleton_S/0/1/0/all/0/1\">Stephen H. Muggleton</a>",
          "description": "In Meta-Interpretive Learning (MIL) the metarules, second-order datalog\nclauses acting as inductive bias, are manually defined by the user. In this\nwork we show that second-order metarules for MIL can be learned by MIL. We\ndefine a generality ordering of metarules by $\\theta$-subsumption and show that\nuser-defined sort metarules are derivable by specialisation of the most-general\nmatrix metarules in a language class; and that these matrix metarules are in\nturn derivable by specialisation of third-order punch metarules with variables\nthat range over the set of second-order literals and for which only an upper\nbound on their number of literals need be user-defined. We show that the\ncardinality of a metarule language is polynomial in the number of literals in\npunch metarules. We re-frame MIL as metarule specialisation by resolution. We\nmodify the MIL metarule specialisation operator to return new metarules rather\nthan first-order clauses and prove the correctness of the new operator. We\nimplement the new operator as TOIL, a sub-system of the MIL system Louise. Our\nexperiments show that as user-defined sort metarules are progressively replaced\nby sort metarules learned by TOIL, Louise's predictive accuracy is maintained\nat the cost of a small increase in training times. We conclude that\nautomatically derived metarules can replace user-defined metarules.",
          "link": "http://arxiv.org/abs/2106.07464",
          "publishedOn": "2021-09-14T07:20:13.728Z",
          "wordCount": null,
          "title": "Meta-Interpretive Learning as Metarule Specialisation. (arXiv:2106.07464v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kuangqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yanfei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wee Sun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>",
          "description": "A Graph Convolutional Network (GCN) stacks several layers and in each layer\nperforms a PROPagation operation (PROP) and a TRANsformation operation (TRAN)\nfor learning node representations over graph-structured data. Though powerful,\nGCNs tend to suffer performance drop when the model gets deep. Previous works\nfocus on PROPs to study and mitigate this issue, but the role of TRANs is\nbarely investigated. In this work, we study performance degradation of GCNs by\nexperimentally examining how stacking only TRANs or PROPs works. We find that\nTRANs contribute significantly, or even more than PROPs, to declining\nperformance, and moreover that they tend to amplify node-wise feature variance\nin GCNs, causing variance inflammation that we identify as a key factor for\ncausing performance drop. Motivated by such observations, we propose a\nvariance-controlling technique termed Node Normalization (NodeNorm), which\nscales each node's features using its own standard deviation. Experimental\nresults validate the effectiveness of NodeNorm on addressing performance\ndegradation of GCNs. Specifically, it enables deep GCNs to outperform shallow\nones in cases where deep models are needed, and to achieve comparable results\nwith shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and\ncan well generalize to other GNN architectures. Code is publicly available at\nhttps://github.com/miafei/NodeNorm.",
          "link": "http://arxiv.org/abs/2006.07107",
          "publishedOn": "2021-09-14T07:20:13.727Z",
          "wordCount": null,
          "title": "Understanding and Resolving Performance Degradation in Graph Convolutional Networks. (arXiv:2006.07107v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>",
          "description": "Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model's robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models'\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .",
          "link": "http://arxiv.org/abs/2109.00544",
          "publishedOn": "2021-09-14T07:20:13.641Z",
          "wordCount": null,
          "title": "Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.05862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yanqing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1\">Michael Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhongfang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahashweta_D/0/1/0/all/0/1\">Das Mahashweta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mangesh_B/0/1/0/all/0/1\">Bendre Mangesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feifei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff Phillips</a>",
          "description": "Embeddings are one of the fundamental building blocks for data analysis\ntasks. Embeddings are already essential tools for large language models and\nimage analysis, and their use is being extended to many other research domains.\nThe generation of these distributed representations is often a data- and\ncomputation-expensive process; yet the holistic analysis and adjustment of them\nafter they have been created is still a developing area. In this paper, we\nfirst propose a very general quantitatively measure for the presence of\nfeatures in the embedding data based on if it can be learned. We then devise a\nmethod to remove or alleviate undesired features in the embedding while\nretaining the essential structure of the data. We use a Domain Adversarial\nNetwork (DAN) to generate a non-affine transformation, but we add constraints\nto ensure the essential structure of the embedding is preserved. Our empirical\nresults demonstrate that the proposed algorithm significantly outperforms the\nstate-of-art unsupervised algorithm on several data sets, including novel\napplications from the industry.",
          "link": "http://arxiv.org/abs/1910.05862",
          "publishedOn": "2021-09-14T07:20:13.635Z",
          "wordCount": null,
          "title": "Constrained Non-Affine Alignment of Embeddings. (arXiv:1910.05862v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08472",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Alanwar_A/0/1/0/all/0/1\">Amr Alanwar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koch_A/0/1/0/all/0/1\">Anne Koch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Allgower_F/0/1/0/all/0/1\">Frank Allg&#xf6;wer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johansson_K/0/1/0/all/0/1\">Karl Henrik Johansson</a>",
          "description": "In this paper, we propose a data-driven reachability analysis approach for\nunknown system dynamics. Reachability analysis is an essential tool for\nguaranteeing safety properties. However, most current reachability analysis\nheavily relies on the existence of a suitable system model, which is often not\ndirectly available in practice. We instead propose a data-driven reachability\nanalysis approach from noisy data. More specifically, we first provide an\nalgorithm for over-approximating the reachable set of a linear time-invariant\nsystem using matrix zonotopes. Then we introduce an extension for Lipschitz\nnonlinear systems. We provide theoretical guarantees in both cases. Numerical\nexamples show the potential and applicability of the introduced methods.",
          "link": "http://arxiv.org/abs/2011.08472",
          "publishedOn": "2021-09-14T07:20:13.625Z",
          "wordCount": 589,
          "title": "Data-Driven Reachability Analysis Using Matrix Zonotopes. (arXiv:2011.08472v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heaton_C/0/1/0/all/0/1\">Connor Heaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_P/0/1/0/all/0/1\">Prasenjit Mitra</a>",
          "description": "Major League Baseball (MLB) has a storied history of using statistics to\nbetter understand and discuss the game of baseball, with an entire discipline\nof statistics dedicated to the craft, known as sabermetrics. At their core, all\nsabermetrics seek to quantify some aspect of the game, often a specific aspect\nof a player's skill set - such as a batter's ability to drive in runs (RBI) or\na pitcher's ability to keep batters from reaching base (WHIP). While useful,\nsuch statistics are fundamentally limited by the fact that they are derived\nfrom an account of what happened on the field, not how it happened. As a first\nstep towards alleviating this shortcoming, we present a novel, contrastive\nlearning-based framework for describing player form in the MLB. We use form to\nrefer to the way in which a player has impacted the course of play in their\nrecent appearances. Concretely, a player's form is described by a\n72-dimensional vector. By comparing clusters of players resulting from our form\nrepresentations and those resulting from traditional abermetrics, we\ndemonstrate that our form representations contain information about how players\nimpact the course of play, not present in traditional, publicly available\nstatistics. We believe these embeddings could be utilized to predict both\nin-game and game-level events, such as the result of an at-bat or the winner of\na game.",
          "link": "http://arxiv.org/abs/2109.05280",
          "publishedOn": "2021-09-14T07:20:13.618Z",
          "wordCount": 659,
          "title": "Learning To Describe Player Form in The MLB. (arXiv:2109.05280v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>",
          "description": "Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.",
          "link": "http://arxiv.org/abs/2109.05424",
          "publishedOn": "2021-09-14T07:20:13.611Z",
          "wordCount": 609,
          "title": "Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1906.00460",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malyshkin_V/0/1/0/all/0/1\">Vladislav Gennadievich Malyshkin</a>",
          "description": "Problems of interpolation, classification, and clustering are considered. In\nthe tenets of Radon--Nikodym approach $\\langle f(\\mathbf{x})\\psi^2 \\rangle /\n\\langle\\psi^2\\rangle$, where the $\\psi(\\mathbf{x})$ is a linear function on\ninput attributes, all the answers are obtained from a generalized eigenproblem\n$|f|\\psi^{[i]}\\rangle = \\lambda^{[i]} |\\psi^{[i]}\\rangle$. The solution to the\ninterpolation problem is a regular Radon-Nikodym derivative. The solution to\nthe classification problem requires prior and posterior probabilities that are\nobtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian\napproach new observations change only outcome probabilities, in the\nRadon-Nikodym approach not only outcome probabilities but also the probability\nspace $|\\psi^{[i]}\\rangle$ change with new observations. This is a remarkable\nfeature of the approach: both the probabilities and the probability space are\nconstructed from the data. The Lebesgue quadrature technique can be also\napplied to the optimal clustering problem. The problem is solved by\nconstructing a Gaussian quadrature on the Lebesgue measure. A distinguishing\nfeature of the Radon-Nikodym approach is the knowledge of the invariant group:\nall the answers are invariant relatively any non-degenerated linear transform\nof input vector $\\mathbf{x}$ components. A software product implementing the\nalgorithms of interpolation, classification, and optimal clustering is\navailable from the authors.",
          "link": "http://arxiv.org/abs/1906.00460",
          "publishedOn": "2021-09-14T07:20:13.593Z",
          "wordCount": 864,
          "title": "On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Antonio Camara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>",
          "description": "Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.",
          "link": "http://arxiv.org/abs/2104.07814",
          "publishedOn": "2021-09-14T07:20:13.585Z",
          "wordCount": 668,
          "title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stekovic_S/0/1/0/all/0/1\">Sinisa Stekovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mahdi Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraundorfer_F/0/1/0/all/0/1\">Friedrich Fraundorfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>",
          "description": "We propose a novel method for reconstructing floor plans from noisy 3D point\nclouds. Our main contribution is a principled approach that relies on the Monte\nCarlo Tree Search (MCTS) algorithm to maximize a suitable objective function\nefficiently despite the complexity of the problem. Like previous work, we first\nproject the input point cloud to a top view to create a density map and extract\nroom proposals from it. Our method selects and optimizes the polygonal shapes\nof these room proposals jointly to fit the density map and outputs an accurate\nvectorized floor map even for large complex scenes. To do this, we adapted\nMCTS, an algorithm originally designed to learn to play games, to select the\nroom proposals by maximizing an objective function combining the fitness with\nthe density map as predicted by a deep network and regularizing terms on the\nroom shapes. We also introduce a refinement step to MCTS that adjusts the shape\nof the room proposals. For this step, we propose a novel differentiable method\nfor rendering the polygonal shapes of these proposals. We evaluate our method\non the recent and challenging Structured3D and Floor-SP datasets and show a\nsignificant improvement over the state-of-the-art, without imposing any hard\nconstraints nor assumptions on the floor plan configurations.",
          "link": "http://arxiv.org/abs/2103.11161",
          "publishedOn": "2021-09-14T07:20:13.567Z",
          "wordCount": 702,
          "title": "MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01103",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Rao_N/0/1/0/all/0/1\">Nihal Rao</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Liu_K/0/1/0/all/0/1\">Ke Liu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Machaczek_M/0/1/0/all/0/1\">Marc Machaczek</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Pollet_L/0/1/0/all/0/1\">Lode Pollet</a>",
          "description": "We use a recently developed interpretable and unsupervised machine-learning\nmethod, the tensorial kernel support vector machine (TK-SVM), to investigate\nthe low-temperature classical phase diagram of a generalized\nHeisenberg-Kitaev-$\\Gamma$ ($J$-$K$-$\\Gamma$) model on a honeycomb lattice.\nAside from reproducing phases reported by previous quantum and classical\nstudies, our machine finds a hitherto missed nested zigzag-stripy order and\nestablishes the robustness of a recently identified modulated $S_3 \\times Z_3$\nphase, which emerges through the competition between the Kitaev and $\\Gamma$\nspin liquids, against Heisenberg interactions. The results imply that, in the\nrestricted parameter space spanned by the three primary exchange interactions\n-- $J$, $K$, and $\\Gamma$, the representative Kitaev material $\\alpha$-${\\rm\nRuCl}_3$ lies close to the boundaries of several phases, including a simple\nferromagnet, the unconventional $S_3 \\times Z_3$ and nested zigzag-stripy\nmagnets. A zigzag order is stabilized by a finite $\\Gamma^{\\prime}$ and/or\n$J_3$ term, whereas the four magnetic orders may compete in particular if\n$\\Gamma^{\\prime}$ is anti-ferromagnetic.",
          "link": "http://arxiv.org/abs/2102.01103",
          "publishedOn": "2021-09-14T07:20:13.512Z",
          "wordCount": 644,
          "title": "Machine-Learned Phase Diagrams of Generalized Kitaev Honeycomb Magnets. (arXiv:2102.01103v2 [cond-mat.str-el] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Oliver K. Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartol_T/0/1/0/all/0/1\">Tom Bartol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1\">Terrence Sejnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>",
          "description": "We present a machine learning method for model reduction which incorporates\ndomain-specific physics through candidate functions. Our method estimates an\neffective probability distribution and differential equation model from\nstochastic simulations of a reaction network. The close connection between\nreduced and fine scale descriptions allows approximations derived from the\nmaster equation to be introduced into the learning problem. This representation\nis shown to improve generalization and allows a large reduction in network size\nfor a classic model of inositol trisphosphate (IP3) dependent calcium\noscillations in non-excitable cells.",
          "link": "http://arxiv.org/abs/2109.05053",
          "publishedOn": "2021-09-14T07:20:13.502Z",
          "wordCount": 554,
          "title": "Physics-based machine learning for modeling stochastic IP3-dependent calcium dynamics. (arXiv:2109.05053v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.08767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leon_B/0/1/0/all/0/1\">Borja G. Le&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1\">Murray Shanahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belardinelli_F/0/1/0/all/0/1\">Francesco Belardinelli</a>",
          "description": "This work introduces a neuro-symbolic agent that combines deep reinforcement\nlearning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e.,\nnever-seen-before, generalisation of formally specified instructions. In\nparticular, we present a neuro-symbolic framework where a symbolic module\ntransforms TL specifications into a form that helps the training of a DRL agent\ntargeting generalisation, while a neural module learns systematically to solve\nthe given tasks. We study the emergence of systematic learning in different\nsettings and find that the architecture of the convolutional layers is key when\ngeneralising to new instructions. We also provide evidence that systematic\nlearning can emerge with abstract operators such as negation when learning from\na few training examples, which previous research have struggled with.",
          "link": "http://arxiv.org/abs/2006.08767",
          "publishedOn": "2021-09-14T07:20:13.495Z",
          "wordCount": 607,
          "title": "Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning. (arXiv:2006.08767v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Correa_J/0/1/0/all/0/1\">Juan D Correa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghack Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "The Ladder of Causation describes three qualitatively different types of\nactivities an agent may be interested in engaging in, namely, seeing\n(observational), doing (interventional), and imagining (counterfactual) (Pearl\nand Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy\nis that data is collected by an agent observing or intervening in a system\n(layers 1 and 2), while its goal may be to understand what would have happened\nhad it taken a different course of action, contrary to what factually ended up\nhappening (layer 3). While there exists a solid understanding of the conditions\nunder which cross-layer inferences are allowed from observations to\ninterventions, the results are somewhat scarcer when targeting counterfactual\nquantities. In this paper, we study the identification of nested\ncounterfactuals from an arbitrary combination of observations and experiments.\nSpecifically, building on a more explicit definition of nested counterfactuals,\nwe prove the counterfactual unnesting theorem (CUT), which allows one to map\narbitrary nested counterfactuals to unnested ones. For instance, applications\nin mediation and fairness analysis usually evoke notions of direct, indirect,\nand spurious effects, which naturally require nesting. Second, we introduce a\nsufficient and necessary graphical condition for counterfactual identification\nfrom an arbitrary combination of observational and experimental distributions.\nLastly, we develop an efficient and complete algorithm for identifying nested\ncounterfactuals; failure of the algorithm returning an expression for a query\nimplies it is not identifiable.",
          "link": "http://arxiv.org/abs/2107.03190",
          "publishedOn": "2021-09-14T07:20:13.475Z",
          "wordCount": 699,
          "title": "Nested Counterfactual Identification from Arbitrary Surrogate Experiments. (arXiv:2107.03190v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1\">Dezhong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wanning Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yutong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaofeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>",
          "description": "Federated learning enables multiple clients to collaboratively learn a global\nmodel by periodically aggregating the clients' models without transferring the\nlocal data. However, due to the heterogeneity of the system and data, many\napproaches suffer from the \"client-drift\" issue that could significantly slow\ndown the convergence of the global model training. As clients perform local\nupdates on heterogeneous data through heterogeneous systems, their local models\ndrift apart. To tackle this issue, one intuitive idea is to guide the local\nmodel training by the global teachers, i.e., past global models, where each\nclient learns the global knowledge from past global models via adaptive\nknowledge distillation techniques. Coming from these insights, we propose a\nnovel approach for heterogeneous federated learning, namely FedGKD, which fuses\nthe knowledge from historical global models for local training to alleviate the\n\"client-drift\" issue. In this paper, we evaluate FedGKD with extensive\nexperiments on various CV/NLP datasets (i.e., CIFAR-10/100, Tiny-ImageNet, AG\nNews, SST5) and different heterogeneous settings. The proposed method is\nguaranteed to converge under common assumptions, and achieves superior\nempirical accuracy in fewer communication runs than five state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.00051",
          "publishedOn": "2021-09-14T07:20:13.460Z",
          "wordCount": null,
          "title": "Local-Global Knowledge Distillation in Heterogeneous Federated Learning with Non-IID Data. (arXiv:2107.00051v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Junhyung Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmahdy_A/0/1/0/all/0/1\">Adel Elmahdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohajer_S/0/1/0/all/0/1\">Soheil Mohajer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1\">Changho Suh</a>",
          "description": "We study the matrix completion problem that leverages hierarchical similarity\ngraphs as side information in the context of recommender systems. Under a\nhierarchical stochastic block model that well respects practically-relevant\nsocial graphs and a low-rank rating matrix model, we characterize the exact\ninformation-theoretic limit on the number of observed matrix entries (i.e.,\noptimal sample complexity) by proving sharp upper and lower bounds on the\nsample complexity. In the achievability proof, we demonstrate that probability\nof error of the maximum likelihood estimator vanishes for sufficiently large\nnumber of users and items, if all sufficient conditions are satisfied. On the\nother hand, the converse (impossibility) proof is based on the genie-aided\nmaximum likelihood estimator. Under each necessary condition, we present\nexamples of a genie-aided estimator to prove that the probability of error does\nnot vanish for sufficiently large number of users and items. One important\nconsequence of this result is that exploiting the hierarchical structure of\nsocial graphs yields a substantial gain in sample complexity relative to the\none that simply identifies different groups without resorting to the relational\nstructure across them. More specifically, we analyze the optimal sample\ncomplexity and identify different regimes whose characteristics rely on quality\nmetrics of side information of the hierarchical similarity graph. Finally, we\npresent simulation results to corroborate our theoretical findings and show\nthat the characterized information-theoretic limit can be asymptotically\nachieved.",
          "link": "http://arxiv.org/abs/2109.05408",
          "publishedOn": "2021-09-14T07:20:13.459Z",
          "wordCount": 713,
          "title": "On the Fundamental Limits of Matrix Completion: Leveraging Hierarchical Similarity Graphs. (arXiv:2109.05408v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12826",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Montanari_A/0/1/0/all/0/1\">Andrea Montanari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqiao Zhong</a>",
          "description": "Modern neural networks are often operated in a strongly overparametrized\nregime: they comprise so many parameters that they can interpolate the training\nset, even if actual labels are replaced by purely random ones. Despite this,\nthey achieve good prediction error on unseen data: interpolating the training\nset does not lead to a large generalization error. Further, overparametrization\nappears to be beneficial in that it simplifies the optimization landscape. Here\nwe study these phenomena in the context of two-layers neural networks in the\nneural tangent (NT) regime. We consider a simple data model, with isotropic\ncovariates vectors in $d$ dimensions, and $N$ hidden neurons. We assume that\nboth the sample size $n$ and the dimension $d$ are large, and they are\npolynomially related. Our first main result is a characterization of the\neigenstructure of the empirical NT kernel in the overparametrized regime $Nd\\gg\nn$. This characterization implies as a corollary that the minimum eigenvalue of\nthe empirical NT kernel is bounded away from zero as soon as $Nd\\gg n$, and\ntherefore the network can exactly interpolate arbitrary labels in the same\nregime.\n\nOur second main result is a characterization of the generalization error of\nNT ridge regression including, as a special case, min-$\\ell_2$ norm\ninterpolation. We prove that, as soon as $Nd\\gg n$, the test error is well\napproximated by the one of kernel ridge regression with respect to the\ninfinite-width kernel. The latter is in turn well approximated by the error of\npolynomial ridge regression, whereby the regularization parameter is increased\nby a `self-induced' term related to the high-degree components of the\nactivation function. The polynomial degree depends on the sample size and the\ndimension (in particular on $\\log n/\\log d$).",
          "link": "http://arxiv.org/abs/2007.12826",
          "publishedOn": "2021-09-14T07:20:13.440Z",
          "wordCount": 765,
          "title": "The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training. (arXiv:2007.12826v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_F/0/1/0/all/0/1\">Faraz Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_A/0/1/0/all/0/1\">Ali Motie Nasrabadi</a>",
          "description": "A conventional subject-dependent (SD) brain-computer interface (BCI) requires\na complete data-gathering, training, and calibration phase for each user before\nit can be used. In recent years, a number of subject-independent (SI) BCIs have\nbeen developed. However, there are many problems preventing them from being\nused in real-world BCI applications. A weaker performance compared to the\nsubject-dependent (SD) approach, and a relatively large model requiring high\ncomputational power are the most important ones. Therefore, a potential\nreal-world BCI would greatly benefit from a compact low-power\nsubject-independent BCI framework, ready to be used immediately after the user\nputs it on. To move towards this goal, we propose a novel subject-independent\nBCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)\ntrained on the motor imagery (MI) paradigm of a large-scale\nelectroencephalography (EEG) signals database consisting of 21600 trials for 54\nsubjects performing two-class hand-movement MI tasks. The proposed framework\napplies a wavelet kernel convolutional neural network (WKCNN) and a temporal\nconvolutional neural network (TCNN) in order to represent and extract the\ndiverse spectral features of EEG signals. The outputs of the convolutional\nlayers go through a common spatial pattern (CSP) algorithm for spatial feature\nextraction. The number of CSP features is reduced by a dense neural network,\nand the final class label is determined by a linear discriminative analysis\n(LDA) classifier. The CCSPNet framework evaluation results show that it is\npossible to have a low-power compact BCI that achieves both SD and SI\nperformance comparable to complex and computationally expensive models.",
          "link": "http://arxiv.org/abs/2012.13567",
          "publishedOn": "2021-09-14T07:20:13.433Z",
          "wordCount": 769,
          "title": "Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harada_S/0/1/0/all/0/1\">Shonosuke Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Outcome estimation of treatments for target individuals is an important\nfoundation for decision making based on causal relations. Most existing outcome\nestimation methods deal with binary or multiple-choice treatments; however, in\nsome applications, the number of treatments can be significantly large, while\nthe treatments themselves have rich information. In this study, we considered\none important instance of such cases: the outcome estimation problem of\ngraph-structured treatments such as drugs. Owing to the large number of\npossible treatments, the counterfactual nature of observational data that\nappears in conventional treatment effect estimation becomes more of a concern\nfor this problem. Our proposed method, GraphITE (pronounced \"graphite\") learns\nthe representations of graph-structured treatments using graph neural networks\nwhile mitigating observation biases using Hilbert-Schmidt Independence\nCriterion regularization, which increases the independence of the\nrepresentations of the targets and treatments. Experiments on two real-world\ndatasets show that GraphITE outperforms baselines, especially in cases with a\nlarge number of treatments.",
          "link": "http://arxiv.org/abs/2009.14061",
          "publishedOn": "2021-09-14T07:20:13.425Z",
          "wordCount": 628,
          "title": "GraphITE: Estimating Individual Effects of Graph-structured Treatments. (arXiv:2009.14061v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05522",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arjmand_M/0/1/0/all/0/1\">Mehdi Arjmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1\">Mohammad Javad Dousti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>",
          "description": "Multimodal language analysis is a burgeoning field of NLP that aims to\nsimultaneously model a speaker's words, acoustical annotations, and facial\nexpressions. In this area, lexicon features usually outperform other modalities\nbecause they are pre-trained on large corpora via Transformer-based models.\nDespite their strong performance, training a new self-supervised learning (SSL)\nTransformer on any modality is not usually attainable due to insufficient data,\nwhich is the case in multimodal language learning. This work proposes a\nTransformer-Based Speech-Prefixed Language Model called TEASEL to approach the\nmentioned constraints without training a complete Transformer model. TEASEL\nmodel includes speech modality as a dynamic prefix besides the textual modality\ncompared to a conventional language model. This method exploits a conventional\npre-trained language model as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.\nExtensive experiments show that our model outperforms unimodal baseline\nlanguage models by 4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%\nsmaller than the SoTA model.",
          "link": "http://arxiv.org/abs/2109.05522",
          "publishedOn": "2021-09-14T07:20:13.416Z",
          "wordCount": 616,
          "title": "TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05580",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saueressig_C/0/1/0/all/0/1\">Camillo Saueressig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berkley_A/0/1/0/all/0/1\">Adam Berkley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Munbodh_R/0/1/0/all/0/1\">Reshma Munbodh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_R/0/1/0/all/0/1\">Ritambhara Singh</a>",
          "description": "We present a joint graph convolution-image convolution neural network as our\nsubmission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model\neach brain as a graph composed of distinct image regions, which is initially\nsegmented by a graph neural network (GNN). Subsequently, the tumorous volume\nidentified by the GNN is further refined by a simple (voxel) convolutional\nneural network (CNN), which produces the final segmentation. This approach\ncaptures both global brain feature interactions via the graphical\nrepresentation and local image details through the use of convolutional\nfilters. We find that the GNN component by itself can effectively identify and\nsegment the brain tumors. The addition of the CNN further improves the median\nperformance of the model by 2 percent across all metrics evaluated. On the\nvalidation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,\n0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm\non the whole tumor, core tumor, and enhancing tumor, respectively.",
          "link": "http://arxiv.org/abs/2109.05580",
          "publishedOn": "2021-09-14T07:20:13.407Z",
          "wordCount": 647,
          "title": "A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00736",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yu_C/0/1/0/all/0/1\">Christina Lee Yu</a>",
          "description": "Tensor completion exhibits an interesting computational-statistical gap in\nterms of the number of samples needed to perform tensor estimation. While there\nare only $\\Theta(tn)$ degrees of freedom in a $t$-order tensor with $n^t$\nentries, the best known polynomial time algorithm requires $O(n^{t/2})$ samples\nin order to guarantee consistent estimation. In this paper, we show that weak\nside information is sufficient to reduce the sample complexity to $O(n)$. The\nside information consists of a weight vector for each of the modes which is not\northogonal to any of the latent factors along that mode; this is significantly\nweaker than assuming noisy knowledge of the subspaces. We provide an algorithm\nthat utilizes this side information to produce a consistent estimator with\n$O(n^{1+\\kappa})$ samples for any small constant $\\kappa > 0$.",
          "link": "http://arxiv.org/abs/2007.00736",
          "publishedOn": "2021-09-14T07:20:13.400Z",
          "wordCount": 594,
          "title": "Tensor Estimation with Nearly Linear Samples Given Weak Side Information. (arXiv:2007.00736v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yixin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Austin S. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1\">Akshara Rai</a>",
          "description": "Manipulation tasks like loading a dishwasher can be seen as a sequence of\nspatial constraints and relationships between different objects. For example, a\nplate can be placed in a tray only if the tray is open. We aim to discover such\ntask-specific rules from demonstrations. We pose manipulation as a\nclassification problem over a graph, whose nodes represent task relevant\nentities like objects and goals, transform the environment scene into a graph\nand learn a graph neural network (GNN) policy using imitation learning. In our\nexperiments, a single learned GNN policy, trained using 20 expert\ndemonstrations, can solve multiple blockstacking and rearrangement tasks in\nboth simulation and on hardware, without any task description. The policy\nsuccessfully generalizes over the number of objects in the environment, their\npositions, and goal configurations (trained on single stacks, generalizes to\npyramids and multiple stacks). We also apply our approach to a complex\nsimulated dishwasher environment, where a robot learns to load a dishwasher\nfrom only 5 high-level human demonstrations. These experiments show that\nimitation learning on a graphical state and policy is a simple, yet powerful\ntool for solving complex long-horizon manipulation problems, without requiring\ndetailed task descriptions. Videos can be found at:\nhttps://youtu.be/x9hcKBh6K0A.",
          "link": "http://arxiv.org/abs/2102.13177",
          "publishedOn": "2021-09-14T07:20:13.384Z",
          "wordCount": 682,
          "title": "Efficient and Interpretable Robot Manipulation with Graph Neural Networks. (arXiv:2102.13177v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05402",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pournaderi_M/0/1/0/all/0/1\">Mehrdad Pournaderi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>",
          "description": "The knockoff filter, recently developed by Barber and Candes, is an effective\nprocedure to perform variable selection with a controlled false discovery rate\n(FDR). We propose a private version of the knockoff filter by incorporating\nGaussian and Laplace mechanisms, and show that variable selection with\ncontrolled FDR can be achieved. Simulations demonstrate that our setting has\nreasonable statistical power.",
          "link": "http://arxiv.org/abs/2109.05402",
          "publishedOn": "2021-09-14T07:20:13.376Z",
          "wordCount": 528,
          "title": "Differentially Private Variable Selection via the Knockoff Filter. (arXiv:2109.05402v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1804.02969",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kliegr_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Kliegr</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bahnik_S/0/1/0/all/0/1\">&#x160;t&#x11b;p&#xe1;n Bahn&#xed;k</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Furnkranz_J/0/1/0/all/0/1\">Johannes F&#xfc;rnkranz</a>",
          "description": "While the interpretability of machine learning models is often equated with\ntheir mere syntactic comprehensibility, we think that interpretability goes\nbeyond that, and that human interpretability should also be investigated from\nthe point of view of cognitive science. The goal of this paper is to discuss to\nwhat extent cognitive biases may affect human understanding of interpretable\nmachine learning models, in particular of logical rules discovered from data.\nTwenty cognitive biases are covered, as are possible debiasing techniques that\ncan be adopted by designers of machine learning algorithms and software. Our\nreview transfers results obtained in cognitive psychology to the domain of\nmachine learning, aiming to bridge the current gap between these two areas. It\nneeds to be followed by empirical studies specifically focused on the machine\nlearning domain.",
          "link": "http://arxiv.org/abs/1804.02969",
          "publishedOn": "2021-09-14T07:20:13.352Z",
          "wordCount": 660,
          "title": "A review of possible effects of cognitive biases on the interpretation of rule-based machine learning models. (arXiv:1804.02969v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14336",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Feng_B/0/1/0/all/0/1\">Bo Feng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fox_G/0/1/0/all/0/1\">Geoffrey C. Fox</a>",
          "description": "Geoscience and seismology have utilized the most advanced technologies and\nequipment to monitor seismic events globally from the past few decades. With\nthe enormous amount of data, modern GPU-powered deep learning presents a\npromising approach to analyze data and discover patterns. In recent years,\nthere are plenty of successful deep learning models for picking seismic waves.\nHowever, forecasting extreme earthquakes, which can cause disasters, is still\nan underdeveloped topic in history. Relevant research in spatiotemporal\ndynamics mining and forecasting has revealed some successful predictions, a\ncrucial topic in many scientific research fields. Most studies of them have\nmany successful applications of using deep neural networks. In Geology and\nEarth science studies, earthquake prediction is one of the world's most\nchallenging problems, about which cutting-edge deep learning technologies may\nhelp discover some valuable patterns. In this project, we propose a deep\nlearning modeling approach, namely \\tseqpre, to mine spatiotemporal patterns\nfrom data to nowcast extreme earthquakes by discovering visual dynamics in\nregional coarse-grained spatial grids over time. In this modeling approach, we\nuse synthetic deep learning neural networks with domain knowledge in geoscience\nand seismology to exploit earthquake patterns for prediction using\nconvolutional long short-term memory neural networks. Our experiments show a\nstrong correlation between location prediction and magnitude prediction for\nearthquakes in Southern California. Ablation studies and visualization validate\nthe effectiveness of the proposed modeling method.",
          "link": "http://arxiv.org/abs/2012.14336",
          "publishedOn": "2021-09-14T07:20:13.336Z",
          "wordCount": 706,
          "title": "Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosch_J/0/1/0/all/0/1\">Jan Bosch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsson_H/0/1/0/all/0/1\">Helena Holmstr&#xf6;m Olsson</a>",
          "description": "With the development and the increasing interests in ML/DL fields, companies\nare eager to apply Machine Learning/Deep Learning approaches to increase\nservice quality and customer experience. Federated Learning was implemented as\nan effective model training method for distributing and accelerating\ntime-consuming model training while protecting user data privacy. However,\ncommon Federated Learning approaches, on the other hand, use a synchronous\nprotocol to conduct model aggregation, which is inflexible and unable to adapt\nto rapidly changing environments and heterogeneous hardware settings in\nreal-world scenarios. In this paper, we present an approach to real-time\nend-to-end Federated Learning combined with a novel asynchronous model\naggregation protocol. Our method is validated in an industrial use case in the\nautomotive domain, focusing on steering wheel angle prediction for autonomous\ndriving. Our findings show that asynchronous Federated Learning can\nsignificantly improve the prediction performance of local edge models while\nmaintaining the same level of accuracy as centralized machine learning.\nFurthermore, by using a sliding training window, the approach can minimize\ncommunication overhead, accelerate model training speed and consume real-time\nstreaming data, proving high efficiency when deploying ML/DL components to\nheterogeneous real-world embedded systems.",
          "link": "http://arxiv.org/abs/2103.11879",
          "publishedOn": "2021-09-14T07:20:13.315Z",
          "wordCount": null,
          "title": "Real-time End-to-End Federated Learning: An Automotive Case Study. (arXiv:2103.11879v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines,\nboth (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for\nunseen targets. Finally, we perform an exhaustive analysis of the cross-domain\nresults, and we highlight the important factors influencing the model\nperformance.",
          "link": "http://arxiv.org/abs/2104.07467",
          "publishedOn": "2021-09-14T07:20:13.315Z",
          "wordCount": null,
          "title": "Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.02714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xutao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yunming Ye</a>",
          "description": "Heterogeneous domain adaptation (HDA) tackles the learning of cross-domain\nsamples with both different probability distributions and feature\nrepresentations. Most of the existing HDA studies focus on the single-source\nscenario. In reality, however, it is not uncommon to obtain samples from\nmultiple heterogeneous domains. In this article, we study the multisource HDA\nproblem and propose a conditional weighting adversarial network (CWAN) to\naddress it. The proposed CWAN adversarially learns a feature transformer, a\nlabel classifier, and a domain discriminator. To quantify the importance of\ndifferent source domains, CWAN introduces a sophisticated conditional weighting\nscheme to calculate the weights of the source domains according to the\nconditional distribution divergence between the source and target domains.\nDifferent from existing weighting schemes, the proposed conditional weighting\nscheme not only weights the source domains but also implicitly aligns the\nconditional distributions during the optimization process. Experimental results\nclearly demonstrate that the proposed CWAN performs much better than several\nstate-of-the-art methods on four real-world datasets.",
          "link": "http://arxiv.org/abs/2008.02714",
          "publishedOn": "2021-09-14T07:20:13.314Z",
          "wordCount": null,
          "title": "Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network. (arXiv:2008.02714v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>",
          "description": "A hyperbole is an intentional and creative exaggeration not to be taken\nliterally. Despite its ubiquity in daily life, the computational explorations\nof hyperboles are scarce. In this paper, we tackle the under-explored and\nchallenging task: sentence-level hyperbole generation. We start with a\nrepresentative syntactic pattern for intensification and systematically study\nthe semantic (commonsense and counterfactual) relationships between each\ncomponent in such hyperboles. Next, we leverage the COMeT and reverse COMeT\nmodels to do commonsense and counterfactual inference. We then generate\nmultiple hyperbole candidates based on our findings from the pattern, and train\nneural classifiers to rank and select high-quality hyperboles. Automatic and\nhuman evaluations show that our generation method is able to generate\nhyperboles creatively with high success rate and intensity scores.",
          "link": "http://arxiv.org/abs/2109.05097",
          "publishedOn": "2021-09-14T07:20:13.307Z",
          "wordCount": null,
          "title": "HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mallah_R/0/1/0/all/0/1\">Ranwa Al Mallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badu_Marfo_G/0/1/0/all/0/1\">Godwin Badu-Marfo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farooq_B/0/1/0/all/0/1\">Bilal Farooq</a>",
          "description": "In Federated Learning (FL), a group of workers participate to build a global\nmodel under the coordination of one node, the chief. Regarding the\ncybersecurity of FL, some attacks aim at injecting the fabricated local model\nupdates into the system. Some defenses are based on malicious worker detection\nand behavioral pattern analysis. In this context, without timely and dynamic\nmonitoring methods, the chief cannot detect and remove the malicious or\nunreliable workers from the system. Our work emphasize the urgency to prepare\nthe federated learning process for monitoring and eventually behavioral pattern\nanalysis. We study the information inside the learning process in the early\nstages of training, propose a monitoring process and evaluate the monitoring\nperiod required. The aim is to analyse at what time is it appropriate to start\nthe detection algorithm in order to remove the malicious or unreliable workers\nfrom the system and optimise the defense mechanism deployment. We tested our\nstrategy on a behavioral pattern analysis defense applied to the FL process of\ndifferent benchmark systems for text and image classification. Our results show\nthat the monitoring process lowers false positives and false negatives and\nconsequently increases system efficiency by enabling the distributed learning\nsystem to achieve better performance in the early stage of training.",
          "link": "http://arxiv.org/abs/2109.05385",
          "publishedOn": "2021-09-14T07:20:13.300Z",
          "wordCount": null,
          "title": "On the Initial Behavior Monitoring Issues in Federated Learning. (arXiv:2109.05385v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11708",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Alsup_T/0/1/0/all/0/1\">Terrence Alsup</a>, <a href=\"http://arxiv.org/find/math/1/au:+Peherstorfer_B/0/1/0/all/0/1\">Benjamin Peherstorfer</a>",
          "description": "Multi-fidelity methods leverage low-cost surrogate models to speed up\ncomputations and make occasional recourse to expensive high-fidelity models to\nestablish accuracy guarantees. Because surrogate and high-fidelity models are\nused together, poor predictions by surrogate models can be compensated with\nfrequent recourse to high-fidelity models. Thus, there is a trade-off between\ninvesting computational resources to improve the accuracy of surrogate models\nversus simply making more frequent recourse to expensive high-fidelity models;\nhowever, this trade-off is ignored by traditional modeling methods that\nconstruct surrogate models that are meant to replace high-fidelity models\nrather than being used together with high-fidelity models. This work considers\nmulti-fidelity importance sampling and theoretically and computationally trades\noff increasing the fidelity of surrogate models for constructing more accurate\nbiasing densities and the numbers of samples that are required from the\nhigh-fidelity models to compensate poor biasing densities. Numerical examples\ndemonstrate that such context-aware surrogate models for multi-fidelity\nimportance sampling have lower fidelity than what typically is set as tolerance\nin traditional model reduction, leading to runtime speedups of up to one order\nof magnitude in the presented examples.",
          "link": "http://arxiv.org/abs/2010.11708",
          "publishedOn": "2021-09-14T07:20:13.299Z",
          "wordCount": null,
          "title": "Context-aware surrogate modeling for balancing approximation and sampling costs in multi-fidelity importance sampling and Bayesian inverse problems. (arXiv:2010.11708v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jintang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qibiao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>",
          "description": "Recent studies have shown that Graph Convolutional Networks (GCNs) are\nvulnerable to adversarial attacks on the graph structure. Although multiple\nworks have been proposed to improve their robustness against such structural\nadversarial attacks, the reasons for the success of the attacks remain unclear.\nIn this work, we theoretically and empirically demonstrate that structural\nadversarial examples can be attributed to the non-robust aggregation scheme\n(i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage\nof the breakdown point which can quantitatively measure the robustness of\naggregation schemes. The key insight is that weighted mean, as the basic design\nof GCNs, has a low breakdown point and its output can be dramatically changed\nby injecting a single edge. We show that adopting the aggregation scheme with a\nhigh breakdown point (e.g., median or trimmed mean) could significantly enhance\nthe robustness of GCNs against structural attacks. Extensive experiments on\nfour real-world datasets demonstrate that such a simple but effective method\nachieves the best robustness performance compared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2108.06280",
          "publishedOn": "2021-09-14T07:20:13.293Z",
          "wordCount": null,
          "title": "Understanding Structural Vulnerability in Graph Convolutional Networks. (arXiv:2108.06280v2 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1\">Michael C. Hughes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1\">Soha Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>",
          "description": "Recent works leveraging Graph Neural Networks to approach graph matching\ntasks have shown promising results. Recent progress in learning discrete\ndistributions poses new opportunities for learning graph matching models. In\nthis work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),\nto address the graph matching problem. Our model defines a distribution of\nmatchings for a graph pair so the model can explore a wide range of possible\nmatchings. We further introduce a novel multi-step matching procedure, which\nlearns how to refine a graph pair's matching results incrementally. The model\nalso includes dummy nodes so that the model does not have to find matchings for\nnodes without correspondence. We fit this model to data via scalable stochastic\noptimization. We conduct extensive experiments across synthetic graph datasets\nas well as biochemistry and computer vision applications. Across all tasks, our\nresults show that SIGMA can produce significantly improved graph matching\nresults compared to state-of-the-art models. Ablation studies verify that each\nof our components (stochastic training, iterative matching, and dummy nodes)\noffers noticeable improvement.",
          "link": "http://arxiv.org/abs/2106.02206",
          "publishedOn": "2021-09-14T07:20:13.289Z",
          "wordCount": null,
          "title": "Stochastic Iterative Graph Matching. (arXiv:2106.02206v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09134",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1\">Jed Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Siwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>",
          "description": "Federated Learning (FL) is a recent development in the field of machine\nlearning that collaboratively trains models without the training data leaving\nclient devices, to preserve data privacy. In realistic FL settings, the\ntraining set is distributed over clients in a highly non-Independent and\nIdentically Distributed (non-IID) fashion, which has been shown extensively to\nharm FL convergence speed and final model performance. To address this\nchallenge, we propose a novel, generalised approach for incorporating adaptive\noptimisation techniques into FL with the Federated Global Biased Optimiser\n(FedGBO) algorithm. FedGBO accelerates FL by employing a set of global biased\noptimiser values during the client-training phase, which helps to reduce\n`client-drift' from non-IID data, whilst also benefiting from adaptive\noptimisation. We show that the FedGBO update with a generic optimiser can be\nreformulated as centralised training using biased gradients and optimiser\nupdates, and apply this theoretical framework to prove the convergence of\nFedGBO using momentum-Stochastic Gradient Descent (SGDm). We also conduct\nextensive experiments using 4 realistic FL benchmark datasets (CIFAR100,\nSent140, FEMNIST, Shakespeare) and 3 popular adaptive optimisers (RMSProp,\nSGDm, Adam) to compare the performance of state-of-the-art adaptive-FL\nalgorithms. The results demonstrate that FedGBO has highly competitive\nperformance whilst achieving lower communication and computation costs, and\nprovide practical insights into the trade-offs associated with the different\nadaptive-FL algorithms and optimisers for real-world FL deployments.",
          "link": "http://arxiv.org/abs/2108.09134",
          "publishedOn": "2021-09-14T07:20:13.289Z",
          "wordCount": null,
          "title": "Accelerating Federated Learning with a Global Biased Optimiser. (arXiv:2108.09134v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.07110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faez Ahmed</a>",
          "description": "Multi-objective optimization is key to solving many Engineering Design\nproblems, where design parameters are optimized for several performance\nindicators. However, optimization results are highly dependent on how the\ndesigns are parameterized. Researchers have shown that deep generative models\ncan learn compact design representations, providing a new way of parameterizing\ndesigns to achieve faster convergence and improved optimization performance.\nDespite their success in capturing complex distributions, existing generative\nmodels face three challenges when used for design problems: 1) generated\ndesigns have limited design space coverage, 2) the generator ignores design\nperformance, and 3)~the new parameterization is unable to represent designs\nbeyond training data. To address these challenges, we propose MO-PaDGAN, which\nadds a Determinantal Point Processes based loss function to the generative\nadversarial network to simultaneously model diversity and (multi-variate)\nperformance. MO-PaDGAN can thus improve the performances and coverage of\ngenerated designs, and even generate designs with performances exceeding those\nfrom training data. When using MO-PaDGAN as a new parameterization in\nmulti-objective optimization, we can discover much better Pareto fronts even\nthough the training data do not cover those Pareto fronts. In a real-world\nmulti-objective airfoil design example, we demonstrate that MO-PaDGAN achieves,\non average, an over 180\\% improvement in the hypervolume indicator when\ncompared to the vanilla GAN or other state-of-the-art parameterization methods.",
          "link": "http://arxiv.org/abs/2009.07110",
          "publishedOn": "2021-09-14T07:20:13.285Z",
          "wordCount": null,
          "title": "MO-PaDGAN: Reparameterizing Engineering Designs for Augmented Multi-objective Optimization. (arXiv:2009.07110v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kapetanovic_Z/0/1/0/all/0/1\">Zerina Kapetanovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasisht_D/0/1/0/all/0/1\">Deepak Vasisht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tusher Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Joshua R. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Ranveer Chandra</a>",
          "description": "Low power long-range networks like LoRa have become increasingly mainstream\nfor Internet of Things deployments. Given the versatility of applications that\nthese protocols enable, they support many data rates and bandwidths. Yet, for a\ngiven network that supports hundreds of devices over multiple miles, the\nnetwork operator typically needs to specify the same configuration or among a\nsmall subset of configurations for all the client devices to communicate with\nthe gateway. This one-size-fits-all approach is highly inefficient in large\nnetworks. We propose an alternative approach -- we allow network devices to\ntransmit at any data rate they choose. The gateway uses the first few symbols\nin the preamble to classify the correct data rate, switches its configuration,\nand then decodes the data. Our design leverages the inherent asymmetry in\noutdoor IoT deployments where the clients are power-starved and\nresource-constrained, but the gateway is not. Our gateway design, Proteus, runs\na neural network architecture and is backward compatible with existing LoRa\nprotocols. Our experiments reveal that Proteus can identify the correct\nconfiguration with over 97% accuracy in both indoor and outdoor deployments.\nOur network architecture leads to a 3.8 to 11 times increase in throughput for\nour LoRa testbed.",
          "link": "http://arxiv.org/abs/2109.05103",
          "publishedOn": "2021-09-14T07:20:13.280Z",
          "wordCount": null,
          "title": "No Size Fits All: Automated Radio Configuration for LPWANs. (arXiv:2109.05103v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thuerey_N/0/1/0/all/0/1\">Nils Thuerey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holl_P/0/1/0/all/0/1\">Philipp Holl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_M/0/1/0/all/0/1\">Maximilian Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schnell_P/0/1/0/all/0/1\">Patrick Schnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trost_F/0/1/0/all/0/1\">Felix Trost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Um_K/0/1/0/all/0/1\">Kiwon Um</a>",
          "description": "This digital book contains a practical and comprehensive introduction of\neverything related to deep learning in the context of physical simulations. As\nmuch as possible, all topics come with hands-on code examples in the form of\nJupyter notebooks to quickly get started. Beyond standard supervised learning\nfrom data, we'll look at physical loss constraints, more tightly coupled\nlearning algorithms with differentiable simulations, as well as reinforcement\nlearning and uncertainty modeling. We live in exciting times: these methods\nhave a huge potential to fundamentally change what computer simulations can\nachieve.",
          "link": "http://arxiv.org/abs/2109.05237",
          "publishedOn": "2021-09-14T07:20:13.280Z",
          "wordCount": null,
          "title": "Physics-based Deep Learning. (arXiv:2109.05237v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05294",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Alkhalifah_T/0/1/0/all/0/1\">Tariq Alkhalifah</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ovcharenko_O/0/1/0/all/0/1\">Oleg Ovcharenko</a>",
          "description": "Among the biggest challenges we face in utilizing neural networks trained on\nwaveform data (i.e., seismic, electromagnetic, or ultrasound) is its\napplication to real data. The requirement for accurate labels forces us to\ndevelop solutions using synthetic data, where labels are readily available.\nHowever, synthetic data often do not capture the reality of the field/real\nexperiment, and we end up with poor performance of the trained neural network\n(NN) at the inference stage. We describe a novel approach to enhance supervised\ntraining on synthetic data with real data features (domain adaptation).\nSpecifically, for tasks in which the absolute values of the vertical axis (time\nor depth) of the input data are not crucial, like classification, or can be\ncorrected afterward, like velocity model building using a well-log, we suggest\na series of linear operations on the input so the training and application data\nhave similar distributions. This is accomplished by applying two operations on\nthe input data to the NN model: 1) The crosscorrelation of the input data\n(i.e., shot gather, seismic image, etc.) with a fixed reference trace from the\nsame dataset. 2) The convolution of the resulting data with the mean (or a\nrandom sample) of the autocorrelated data from another domain. In the training\nstage, the input data are from the synthetic domain and the auto-correlated\ndata are from the real domain, and random samples from real data are drawn at\nevery training epoch. In the inference/application stage, the input data are\nfrom the real subset domain and the mean of the autocorrelated sections are\nfrom the synthetic data subset domain. Example applications on passive seismic\ndata for microseismic event source location determination and active seismic\ndata for predicting low frequencies are used to demonstrate the power of this\napproach in improving the applicability of trained models to real data.",
          "link": "http://arxiv.org/abs/2109.05294",
          "publishedOn": "2021-09-14T07:20:13.280Z",
          "wordCount": null,
          "title": "MLReal: Bridging the gap between training on synthetic data and real data applications in machine learning. (arXiv:2109.05294v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yinglun Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katz_Samuels_J/0/1/0/all/0/1\">Julian Katz-Samuels</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowak_R/0/1/0/all/0/1\">Robert Nowak</a>",
          "description": "The model selection problem in the pure exploration linear bandit setting is\nintroduced and studied in both the fixed confidence and fixed budget settings.\nThe model selection problem considers a nested sequence of hypothesis classes\nof increasing complexities. Our goal is to automatically adapt to the\ninstance-dependent complexity measure of the smallest hypothesis class\ncontaining the true model, rather than suffering from the complexity measure\nrelated to the largest hypothesis class. We provide evidence showing that a\nstandard doubling trick over dimension fails to achieve the optimal\ninstance-dependent sample complexity. Our algorithms define a new optimization\nproblem based on experimental design that leverages the geometry of the action\nset to efficiently identify a near-optimal hypothesis class. Our fixed budget\nalgorithm uses a novel application of a selection-validation trick in bandits.\nThis provides a new method for the understudied fixed budget setting in linear\nbandits (even without the added challenge of model selection). We further\ngeneralize the model selection problem to the misspecified regime, adapting our\nalgorithms in both fixed confidence and fixed budget settings.",
          "link": "http://arxiv.org/abs/2109.05131",
          "publishedOn": "2021-09-14T07:20:13.119Z",
          "wordCount": 619,
          "title": "Near Instance Optimal Model Selection for Pure Exploration Linear Bandits. (arXiv:2109.05131v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2004.14545",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ras_G/0/1/0/all/0/1\">Gabrielle Ras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_N/0/1/0/all/0/1\">Ning Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerven_M/0/1/0/all/0/1\">Marcel van Gerven</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doran_D/0/1/0/all/0/1\">Derek Doran</a>",
          "description": "Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model's input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN's decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means ``to explain'' the actions of a DNN and to\nevaluate an approach's ``ability to explain''. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.",
          "link": "http://arxiv.org/abs/2004.14545",
          "publishedOn": "2021-09-14T07:20:13.107Z",
          "wordCount": 729,
          "title": "Explainable Deep Learning: A Field Guide for the Uninitiated. (arXiv:2004.14545v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.06366",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1\">Fan Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "We develop methodology for visualization of labeled mixed-featured datasets.\nWe first investigate datasets with continuous features where our Max-Ratio\nProjection (MRP) method utilizes the group information in high dimensions to\nprovide distinctive lower-dimensional projections that are then displayed using\nRadviz3D. Our methodology is extended to datasets with discrete and continuous\nfeatures where a Gaussianized distributional transform is used in conjunction\nwith copula models before applying MRP and visualizing the result using\nRadViz3D. A R package $radviz3d$ implementing our complete methodology is\navailable.",
          "link": "http://arxiv.org/abs/1904.06366",
          "publishedOn": "2021-09-14T07:20:13.006Z",
          "wordCount": 564,
          "title": "Visualization of Labeled Mixed-featured Datasets. (arXiv:1904.06366v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12487",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masoudian_S/0/1/0/all/0/1\">Saeed Masoudian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seldin_Y/0/1/0/all/0/1\">Yevgeny Seldin</a>",
          "description": "We derive improved regret bounds for the Tsallis-INF algorithm of Zimmert and\nSeldin (2021). We show that in adversarial regimes with a $(\\Delta,C,T)$\nself-bounding constraint the algorithm achieves\n$\\mathcal{O}\\left(\\left(\\sum_{i\\neq i^*}\n\\frac{1}{\\Delta_i}\\right)\\log_+\\left(\\frac{(K-1)T}{\\left(\\sum_{i\\neq i^*}\n\\frac{1}{\\Delta_i}\\right)^2}\\right)+\\sqrt{C\\left(\\sum_{i\\neq\ni^*}\\frac{1}{\\Delta_i}\\right)\\log_+\\left(\\frac{(K-1)T}{C\\sum_{i\\neq\ni^*}\\frac{1}{\\Delta_i}}\\right)}\\right)$ regret bound, where $T$ is the time\nhorizon, $K$ is the number of arms, $\\Delta_i$ are the suboptimality gaps,\n$i^*$ is the best arm, $C$ is the corruption magnitude, and $\\log_+(x) =\n\\max\\left(1,\\log x\\right)$. The regime includes stochastic bandits,\nstochastically constrained adversarial bandits, and stochastic bandits with\nadversarial corruptions as special cases. Additionally, we provide a general\nanalysis, which allows to achieve the same kind of improvement for\ngeneralizations of Tsallis-INF to other settings beyond multiarmed bandits.",
          "link": "http://arxiv.org/abs/2103.12487",
          "publishedOn": "2021-09-14T07:20:12.960Z",
          "wordCount": null,
          "title": "Improved Analysis of the Tsallis-INF Algorithm in Stochastically Constrained Adversarial Bandits and Stochastic Bandits with Adversarial Corruptions. (arXiv:2103.12487v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_J/0/1/0/all/0/1\">Jed Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_G/0/1/0/all/0/1\">Geyong Min</a>",
          "description": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative training among geographically distributed and\nheterogeneous users without gathering their data. Extending FL beyond the\nconventional supervised learning paradigm, federated Reinforcement Learning\n(RL) was proposed to handle sequential decision-making problems for various\nprivacy-sensitive applications such as autonomous driving. However, the\nexisting federated RL algorithms directly combine model-free RL with FL, and\nthus generally have high sample complexity and lack theoretical guarantees. To\naddress the above challenges, we propose a new federated RL algorithm that\nincorporates model-based RL and ensemble knowledge distillation into FL.\nSpecifically, we utilise FL and knowledge distillation to create an ensemble of\ndynamics models from clients, and then train the policy by solely using the\nensemble model without interacting with the real environment. Furthermore, we\ntheoretically prove that the monotonic improvement of the proposed algorithm is\nguaranteed. Extensive experimental results demonstrate that our algorithm\nobtains significantly higher sample efficiency compared to federated model-free\nRL algorithms in the challenging continuous control benchmark environments. The\nresults also show the impact of non-IID client data and local update steps on\nthe performance of federated RL, validating the insights obtained from our\ntheoretical analysis.",
          "link": "http://arxiv.org/abs/2109.05549",
          "publishedOn": "2021-09-14T07:20:12.934Z",
          "wordCount": null,
          "title": "Federated Ensemble Model-based Reinforcement Learning. (arXiv:2109.05549v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03416",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>",
          "description": "We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.",
          "link": "http://arxiv.org/abs/2104.03416",
          "publishedOn": "2021-09-14T07:20:12.932Z",
          "wordCount": null,
          "title": "Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>",
          "description": "Learning to model how the world changes as time elapses has proven a\nchallenging problem for the computer vision community. We propose a\nself-supervised solution to this problem using temporal cycle consistency\njointly in vision and language, training on narrated video. Our model learns\nmodality-agnostic functions to predict forward and backward in time, which must\nundo each other when composed. This constraint leads to the discovery of\nhigh-level transitions between moments in time, since such transitions are\neasily inverted and shared across modalities. We justify the design of our\nmodel with an ablation study on different configurations of the cycle\nconsistency problem. We then show qualitatively and quantitatively that our\napproach yields a meaningful, high-level model of the future and past. We apply\nthe learned dynamics model without further training to various tasks, such as\npredicting future action and temporally ordering sets of images. Project page:\nhttps://dave.ml/mmcc",
          "link": "http://arxiv.org/abs/2101.02337",
          "publishedOn": "2021-09-14T07:20:12.931Z",
          "wordCount": null,
          "title": "Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.09780",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lucas O. Teixeira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1\">Rodolfo M. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1\">Diego Bertolini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1\">Luiz S. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1\">George D. C. Cavalcanti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1\">Yandre M. G. Costa</a>",
          "description": "COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging\nexams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,\nand uses less radiation. Here, we demonstrate the impact of lung segmentation\nin COVID-19 identification using CXR images and evaluate which contents of the\nimage influenced the most. Semantic segmentation was performed using a U-Net\nCNN architecture, and the classification using three CNN architectures (VGG,\nResNet, and Inception). Explainable Artificial Intelligence techniques were\nemployed to estimate the impact of segmentation. A three-classes database was\ncomposed: lung opacity (pneumonia), COVID-19, and normal. We assessed the\nimpact of creating a CXR image database from different sources, and the\nCOVID-19 generalization from one source to another. The segmentation achieved a\nJaccard distance of 0.034 and a Dice coefficient of 0.982. The classification\nusing segmented images achieved an F1-Score of 0.88 for the multi-class setup,\nand 0.83 for COVID-19 identification. In the cross-dataset scenario, we\nobtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for\nCOVID-19 identification using segmented images. Experiments support the\nconclusion that even after segmentation, there is a strong bias introduced by\nunderlying factors from different sources.",
          "link": "http://arxiv.org/abs/2009.09780",
          "publishedOn": "2021-09-14T07:20:12.919Z",
          "wordCount": null,
          "title": "Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.03959",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merlis_N/0/1/0/all/0/1\">Nadav Merlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1\">Shie Mannor</a>",
          "description": "We consider the Multi-Armed Bandit (MAB) problem, where an agent sequentially\nchooses actions and observes rewards for the actions it took. While the\nmajority of algorithms try to minimize the regret, i.e., the cumulative\ndifference between the reward of the best action and the agent's action, this\ncriterion might lead to undesirable results. For example, in large problems, or\nwhen the interaction with the environment is brief, finding an optimal arm is\ninfeasible, and regret-minimizing algorithms tend to over-explore. To overcome\nthis issue, algorithms for such settings should instead focus on playing\nnear-optimal arms. To this end, we suggest a new, more lenient, regret\ncriterion that ignores suboptimality gaps smaller than some $\\epsilon$. We then\npresent a variant of the Thompson Sampling (TS) algorithm, called\n$\\epsilon$-TS, and prove its asymptotic optimality in terms of the lenient\nregret. Importantly, we show that when the mean of the optimal arm is high\nenough, the lenient regret of $\\epsilon$-TS is bounded by a constant. Finally,\nwe show that $\\epsilon$-TS can be applied to improve the performance when the\nagent knows a lower bound of the suboptimality gaps.",
          "link": "http://arxiv.org/abs/2008.03959",
          "publishedOn": "2021-09-14T07:20:12.917Z",
          "wordCount": null,
          "title": "Lenient Regret for Multi-Armed Bandits. (arXiv:2008.03959v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05207",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Stevens_G/0/1/0/all/0/1\">Grant Stevens</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fotopoulou_S/0/1/0/all/0/1\">Sotiria Fotopoulou</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bremer_M/0/1/0/all/0/1\">Malcolm N. Bremer</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Ray_O/0/1/0/all/0/1\">Oliver Ray</a>",
          "description": "AstronomicAL is a human-in-the-loop interactive labelling and training\ndashboard that allows users to create reliable datasets and robust classifiers\nusing active learning. This technique prioritises data that offer high\ninformation gain, leading to improved performance using substantially less\ndata. The system allows users to visualise and integrate data from different\nsources and deal with incorrect or missing labels and imbalanced class sizes.\nAstronomicAL enables experts to visualise domain-specific plots and key\ninformation relating both to broader context and details of a point of interest\ndrawn from a variety of data sources, ensuring reliable labels. In addition,\nAstronomicAL provides functionality to explore all aspects of the training\nprocess, including custom models and query strategies. This makes the software\na tool for experimenting with both domain-specific classifications and more\ngeneral-purpose machine learning strategies. We illustrate using the system\nwith an astronomical dataset due to the field's immediate need; however,\nAstronomicAL has been designed for datasets from any discipline. Finally, by\nexporting a simple configuration file, entire layouts, models, and assigned\nlabels can be shared with the community. This allows for complete transparency\nand ensures that the process of reproducing results is effortless",
          "link": "http://arxiv.org/abs/2109.05207",
          "publishedOn": "2021-09-14T07:20:12.911Z",
          "wordCount": null,
          "title": "AstronomicAL: An interactive dashboard for visualisation, integration and classification of data using Active Learning. (arXiv:2109.05207v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/1908.03918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>",
          "description": "Dynamical models estimate and predict the temporal evolution of physical\nsystems. State Space Models (SSMs) in particular represent the system dynamics\nwith many desirable properties, such as being able to model uncertainty in both\nthe model and measurements, and optimal (in the Bayesian sense) recursive\nformulations e.g. the Kalman Filter. However, they require significant domain\nknowledge to derive the parametric form and considerable hand-tuning to\ncorrectly set all the parameters. Data driven techniques e.g. Recurrent Neural\nNetworks have emerged as compelling alternatives to SSMs with wide success\nacross a number of challenging tasks, in part due to their ability to extract\nrelevant features from rich inputs. They however lack interpretability and\nrobustness to unseen conditions. In this work, we present DynaNet, a hybrid\ndeep learning and time-varying state-space model which can be trained\nend-to-end. Our neural Kalman dynamical model allows us to exploit the relative\nmerits of each approach. We demonstrate state-of-the-art estimation and\nprediction on a number of physically challenging tasks, including visual\nodometry, sensor fusion for visual-inertial navigation and pendulum control. In\naddition we show how DynaNet can indicate failures through investigation of\nproperties such as the rate of innovation (Kalman Gain).",
          "link": "http://arxiv.org/abs/1908.03918",
          "publishedOn": "2021-09-14T07:20:12.900Z",
          "wordCount": null,
          "title": "DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zeyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbo Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suttie_M/0/1/0/all/0/1\">Michael Suttie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">J. Alison Noble</a>",
          "description": "Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result\nin a series of cranio-facial anomalies, and behavioral and neurocognitive\nproblems. Current diagnosis of FAS is typically done by identifying a set of\nfacial characteristics, which are often obtained by manual examination.\nAnatomical landmark detection, which provides rich geometric information, is\nimportant to detect the presence of FAS associated facial anomalies. This\nimaging application is characterized by large variations in data appearance and\nlimited availability of labeled data. Current deep learning-based heatmap\nregression methods designed for facial landmark detection in natural images\nassume availability of large datasets and are therefore not wellsuited for this\napplication. To address this restriction, we develop a new regularized transfer\nlearning approach that exploits the knowledge of a network learned on large\nfacial recognition datasets. In contrast to standard transfer learning which\nfocuses on adjusting the pre-trained weights, the proposed learning approach\nregularizes the model behavior. It explicitly reuses the rich visual semantics\nof a domain-similar source model on the target task data as an additional\nsupervisory signal for regularizing landmark detection optimization.\nSpecifically, we develop four regularization constraints for the proposed\ntransfer learning, including constraining the feature outputs from\nclassification and intermediate layers, as well as matching activation\nattention maps in both spatial and channel levels. Experimental evaluation on a\ncollected clinical imaging dataset demonstrate that the proposed approach can\neffectively improve model generalizability under limited training samples, and\nis advantageous to other approaches in the literature.",
          "link": "http://arxiv.org/abs/2109.05485",
          "publishedOn": "2021-09-14T07:20:12.839Z",
          "wordCount": null,
          "title": "Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05633",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korosteleva_M/0/1/0/all/0/1\">Maria Korosteleva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hee Lee</a>",
          "description": "Garments are ubiquitous in both real and many of the virtual worlds. They are\nhighly deformable objects, exhibit an immense variety of designs and shapes,\nand yet, most garments are created from a set of regularly shaped flat pieces.\nExploration of garment structure presents a peculiar case for an object\nstructure estimation task and might prove useful for downstream tasks of neural\n3D garment modeling and reconstruction by providing strong prior on garment\nshapes. To facilitate research in these directions, we propose a method for\ngenerating large synthetic datasets of 3D garment designs and their sewing\npatterns. Our method consists of a flexible description structure for\nspecifying parametric sewing pattern templates and the automatic generation\npipeline to produce garment 3D models with little-to-none manual intervention.\nTo add realism, the pipeline additionally creates corrupted versions of the\nfinal meshes that imitate artifacts of 3D scanning.\n\nWith this pipeline, we created the first large-scale synthetic dataset of 3D\ngarment models with their sewing patterns. The dataset contains more than 20000\ngarment design variations produced from 19 different base types. Seven of these\ngarment types are specifically designed to target evaluation of the\ngeneralization across garment sewing pattern topologies.",
          "link": "http://arxiv.org/abs/2109.05633",
          "publishedOn": "2021-09-14T07:20:12.829Z",
          "wordCount": null,
          "title": "Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05493",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>",
          "description": "The utilization of prior knowledge about anomalies is an essential issue for\nanomaly detections. Recently, the visual attention mechanism has become a\npromising way to improve the performance of CNNs for some computer vision\ntasks. In this paper, we propose a novel model called Layer-wise External\nAttention Network (LEA-Net) for efficient image anomaly detection. The core\nidea relies on the integration of unsupervised and supervised anomaly detectors\nvia the visual attention mechanism. Our strategy is as follows: (i) Prior\nknowledge about anomalies is represented as the anomaly map generated by\nunsupervised learning of normal instances, (ii) The anomaly map is translated\nto an attention map by the external network, (iii) The attention map is then\nincorporated into intermediate layers of the anomaly detection network.\nNotably, this layer-wise external attention can be applied to any CNN model in\nan end-to-end training manner. For a pilot study, we validate LEA-Net on color\nanomaly detection tasks. Through extensive experiments on PlantVillage, MVTec\nAD, and Cloud datasets, we demonstrate that the proposed layer-wise visual\nattention mechanism consistently boosts anomaly detection performances of an\nexisting CNN model, even on imbalanced datasets. Moreover, we show that our\nattention mechanism successfully boosts the performance of several CNN models.",
          "link": "http://arxiv.org/abs/2109.05493",
          "publishedOn": "2021-09-14T07:20:12.825Z",
          "wordCount": null,
          "title": "LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1806.04823",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nekipelov_D/0/1/0/all/0/1\">Denis Nekipelov</a>, <a href=\"http://arxiv.org/find/math/1/au:+Semenova_V/0/1/0/all/0/1\">Vira Semenova</a>, <a href=\"http://arxiv.org/find/math/1/au:+Syrgkanis_V/0/1/0/all/0/1\">Vasilis Syrgkanis</a>",
          "description": "This paper proposes a Lasso-type estimator for a high-dimensional sparse\nparameter identified by a single index conditional moment restriction (CMR). In\naddition to this parameter, the moment function can also depend on a nuisance\nfunction, such as the propensity score or the conditional choice probability,\nwhich we estimate by modern machine learning tools. We first adjust the moment\nfunction so that the gradient of the future loss function is insensitive\n(formally, Neyman-orthogonal) with respect to the first-stage regularization\nbias, preserving the single index property. We then take the loss function to\nbe an indefinite integral of the adjusted moment function with respect to the\nsingle index. The proposed Lasso estimator converges at the oracle rate, where\nthe oracle knows the nuisance function and solves only the parametric problem.\nWe demonstrate our method by estimating the short-term heterogeneous impact of\nConnecticut's Jobs First welfare reform experiment on women's welfare\nparticipation decision.",
          "link": "http://arxiv.org/abs/1806.04823",
          "publishedOn": "2021-09-14T07:20:12.817Z",
          "wordCount": null,
          "title": "Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v8 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04279",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ding_Z/0/1/0/all/0/1\">Zhiyan Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_Q/0/1/0/all/0/1\">Qin Li</a>",
          "description": "The classical Langevin Monte Carlo method looks for samples from a target\ndistribution by descending the samples along the gradient of the target\ndistribution. The method enjoys a fast convergence rate. However, the numerical\ncost is sometimes high because each iteration requires the computation of a\ngradient. One approach to eliminate the gradient computation is to employ the\nconcept of \"ensemble\". A large number of particles are evolved together so the\nneighboring particles provide gradient information to each other. In this\narticle, we discuss two algorithms that integrate the ensemble feature into LMC\nand the associated properties.\n\nIn particular, we find that if one directly surrogates the gradient using the\nensemble approximation, the algorithm, termed Ensemble Langevin Monte Carlo, is\nunstable due to a high variance term. If the gradients are replaced by the\nensemble approximations only in a constrained manner, to protect from the\nunstable points, the algorithm, termed Constrained Ensemble Langevin Monte\nCarlo, resembles the classical LMC up to an ensemble error but removes most of\nthe gradient computation.",
          "link": "http://arxiv.org/abs/2102.04279",
          "publishedOn": "2021-09-14T07:20:12.813Z",
          "wordCount": null,
          "title": "Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Namdar_K/0/1/0/all/0/1\">Khashayar Namdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_M/0/1/0/all/0/1\">Masoom A. Haider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalvati_F/0/1/0/all/0/1\">Farzad Khalvati</a>",
          "description": "Receiver operating characteristic (ROC) curve is an informative tool in\nbinary classification and Area Under ROC Curve (AUC) is a popular metric for\nreporting performance of binary classifiers. In this paper, first we present a\ncomprehensive review of ROC curve and AUC metric. Next, we propose a modified\nversion of AUC that takes confidence of the model into account and at the same\ntime, incorporates AUC into Binary Cross Entropy (BCE) loss used for training a\nConvolutional neural Network for classification tasks. We demonstrate this on\nthree datasets: MNIST, prostate MRI, and brain MRI. Furthermore, we have\npublished GenuineAI, a new python library, which provides the functions for\nconventional AUC and the proposed modified AUC along with metrics including\nsensitivity, specificity, recall, precision, and F1 for each point of the ROC\ncurve.",
          "link": "http://arxiv.org/abs/2006.04836",
          "publishedOn": "2021-09-14T07:20:12.801Z",
          "wordCount": null,
          "title": "A Modified AUC for Training Convolutional Neural Networks: Taking Confidence into Account. (arXiv:2006.04836v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_M/0/1/0/all/0/1\">Muhamamd Ishfaq Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_M/0/1/0/all/0/1\">Muhammad Aasim Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1\">Moongu Jeon</a>",
          "description": "Stereoscopy exposits a natural perception of distance in a scene, and its\nmanifestation in 3D world understanding is an intuitive phenomenon. However, an\ninnate rigid calibration of binocular vision sensors is crucial for accurate\ndepth estimation. Alternatively, a monocular camera alleviates the limitation\nat the expense of accuracy in estimating depth, and the challenge exacerbates\nin harsh environmental conditions. Moreover, an optical sensor often fails to\nacquire vital signals in harsh environments, and radar is used instead, which\ngives coarse but more accurate signals. This work explores the utility of\ncoarse signals from radar when fused with fine-grained data from a monocular\ncamera for depth estimation in harsh environmental conditions. A variant of\nfeature pyramid network (FPN) extensively operates on fine-grained image\nfeatures at multiple scales with a fewer number of parameters. FPN feature maps\nare fused with sparse radar features extracted with a Convolutional neural\nnetwork. The concatenated hierarchical features are used to predict the depth\nwith ordinal regression. We performed experiments on the nuScenes dataset, and\nthe proposed architecture stays on top in quantitative evaluations with reduced\nparameters and faster inference. The depth estimation results suggest that the\nproposed techniques can be used as an alternative to stereo depth estimation in\ncritical applications in robotics and self-driving cars. The source code will\nbe available in the following: \\url{https://github.com/MI-Hussain/RVMDE}.",
          "link": "http://arxiv.org/abs/2109.05265",
          "publishedOn": "2021-09-14T07:20:12.796Z",
          "wordCount": null,
          "title": "RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lange_B/0/1/0/all/0/1\">Bernard Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Safe and proactive planning in robotic systems generally requires accurate\npredictions of the environment. Prior work on environment prediction applied\nvideo frame prediction techniques to bird's-eye view environment\nrepresentations, such as occupancy grids. ConvLSTM-based frameworks used\npreviously often result in significant blurring and vanishing of moving\nobjects, thus hindering their applicability for use in safety-critical\napplications. In this work, we propose two extensions to the ConvLSTM to\naddress these issues. We present the Temporal Attention Augmented ConvLSTM\n(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks\nfor spatiotemporal occupancy prediction, and demonstrate improved performance\nover baseline architectures on the real-world KITTI and Waymo datasets.",
          "link": "http://arxiv.org/abs/2010.09662",
          "publishedOn": "2021-09-14T07:20:12.794Z",
          "wordCount": null,
          "title": "Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nahum_Y/0/1/0/all/0/1\">Yotam Nahum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Tolila_E/0/1/0/all/0/1\">Eyar Ben-Tolila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anavy_L/0/1/0/all/0/1\">Leon Anavy</a>",
          "description": "As the global need for large-scale data storage is rising exponentially,\nexisting storage technologies are approaching their theoretical and functional\nlimits in terms of density and energy consumption, making DNA based storage a\npotential solution for the future of data storage. Several studies introduced\nDNA based storage systems with high information density (petabytes/gram).\nHowever, DNA synthesis and sequencing technologies yield erroneous outputs.\nAlgorithmic approaches for correcting these errors depend on reading multiple\ncopies of each sequence and result in excessive reading costs. The\nunprecedented success of Transformers as a deep learning architecture for\nlanguage modeling has led to its repurposing for solving a variety of tasks\nacross various domains. In this work, we propose a novel approach for\nsingle-read reconstruction using an encoder-decoder Transformer architecture\nfor DNA based data storage. We address the error correction process as a\nself-supervised sequence-to-sequence task and use synthetic noise injection to\ntrain the model using only the decoded reads. Our approach exploits the\ninherent redundancy of each decoded file to learn its underlying structure. To\ndemonstrate our proposed approach, we encode text, image and code-script files\nto DNA, produce errors with high-fidelity error simulator, and reconstruct the\noriginal files from the noisy reads. Our model achieves lower error rates when\nreconstructing the original data from a single read of each DNA strand compared\nto state-of-the-art algorithms using 2-3 copies. This is the first\ndemonstration of using deep learning models for single-read reconstruction in\nDNA based storage which allows for the reduction of the overall cost of the\nprocess. We show that this approach is applicable for various domains and can\nbe generalized to new domains as well.",
          "link": "http://arxiv.org/abs/2109.05478",
          "publishedOn": "2021-09-14T07:20:12.791Z",
          "wordCount": null,
          "title": "Single-Read Reconstruction for DNA Data Storage Using Transformers. (arXiv:2109.05478v1 [cs.ET])"
        },
        {
          "id": "http://arxiv.org/abs/1905.03927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamanchi_C/0/1/0/all/0/1\">Chandramouli Kamanchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diddigi_R/0/1/0/all/0/1\">Raghuram Bharadwaj Diddigi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1\">Shalabh Bhatnagar</a>",
          "description": "Value iteration is a fixed point iteration technique utilized to obtain the\noptimal value function and policy in a discounted reward Markov Decision\nProcess (MDP). Here, a contraction operator is constructed and applied\nrepeatedly to arrive at the optimal solution. Value iteration is a first order\nmethod and therefore it may take a large number of iterations to converge to\nthe optimal solution. Successive relaxation is a popular technique that can be\napplied to solve a fixed point equation. It has been shown in the literature\nthat, under a special structure of the MDP, successive over-relaxation\ntechnique computes the optimal value function faster than standard value\niteration. In this work, we propose a second order value iteration procedure\nthat is obtained by applying the Newton-Raphson method to the successive\nrelaxation value iteration scheme. We prove the global convergence of our\nalgorithm to the optimal solution asymptotically and show the second order\nconvergence. Through experiments, we demonstrate the effectiveness of our\nproposed approach.",
          "link": "http://arxiv.org/abs/1905.03927",
          "publishedOn": "2021-09-14T07:20:12.789Z",
          "wordCount": null,
          "title": "Generalized Second Order Value Iteration in Markov Decision Processes. (arXiv:1905.03927v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.",
          "link": "http://arxiv.org/abs/2007.00049",
          "publishedOn": "2021-09-14T07:20:12.788Z",
          "wordCount": null,
          "title": "OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05077",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhehua Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_O/0/1/0/all/0/1\">Ozgur S. Oguz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leibold_M/0/1/0/all/0/1\">Marion Leibold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buss_M/0/1/0/all/0/1\">Martin Buss</a>",
          "description": "Safe reinforcement learning aims to learn a control policy while ensuring\nthat neither the system nor the environment gets damaged during the learning\nprocess. For implementing safe reinforcement learning on highly nonlinear and\nhigh-dimensional dynamical systems, one possible approach is to find a\nlow-dimensional safe region via data-driven feature extraction methods, which\nprovides safety estimates to the learning algorithm. As the reliability of the\nlearned safety estimates is data-dependent, we investigate in this work how\ndifferent training data will affect the safe reinforcement learning approach.\nBy balancing between the learning performance and the risk of being unsafe, a\ndata generation method that combines two sampling methods is proposed to\ngenerate representative training data. The performance of the method is\ndemonstrated with a three-link inverted pendulum example.",
          "link": "http://arxiv.org/abs/2109.05077",
          "publishedOn": "2021-09-14T07:20:12.784Z",
          "wordCount": null,
          "title": "Data Generation Method for Learning a Low-dimensional Safe Region in Safe Reinforcement Learning. (arXiv:2109.05077v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horton_M/0/1/0/all/0/1\">Maxwell Horton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1\">Carlos Guestrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1\">Mohammad Rastegari</a>",
          "description": "Recent observations have advanced our understanding of the neural network\noptimization landscape, revealing the existence of (1) paths of high accuracy\ncontaining diverse solutions and (2) wider minima offering improved\nperformance. Previous methods observing diverse paths require multiple training\nruns. In contrast we aim to leverage both property (1) and (2) with a single\nmethod and in a single training run. With a similar computational cost as\ntraining one model, we learn lines, curves, and simplexes of high-accuracy\nneural networks. These neural network subspaces contain diverse solutions that\ncan be ensembled, approaching the ensemble performance of independently trained\nnetworks without the training cost. Moreover, using the subspace midpoint\nboosts accuracy, calibration, and robustness to label noise, outperforming\nStochastic Weight Averaging.",
          "link": "http://arxiv.org/abs/2102.10472",
          "publishedOn": "2021-09-14T07:20:12.780Z",
          "wordCount": null,
          "title": "Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05547",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1\">Junyu Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sun_J/0/1/0/all/0/1\">Jinzhao Sun</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yuan_X/0/1/0/all/0/1\">Xiao Yuan</a>",
          "description": "Rapid developments of quantum information technology show promising\nopportunities for simulating quantum field theory in near-term quantum devices.\nIn this work, we formulate the theory of (time-dependent) variational quantum\nsimulation, explicitly designed for quantum simulation of quantum field theory.\nWe develop hybrid quantum-classical algorithms for crucial ingredients in\nparticle scattering experiments, including encoding, state preparation, and\ntime evolution, with several numerical simulations to demonstrate our\nalgorithms in the 1+1 dimensional $\\lambda \\phi^4$ quantum field theory. These\nalgorithms could be understood as near-term analogs of the Jordan-Lee-Preskill\nalgorithm, the basic algorithm for simulating quantum field theory using\nuniversal quantum devices. Our contribution also includes a bosonic version of\nthe Unitary Coupled Cluster ansatz with physical interpretation in quantum\nfield theory, a discussion about the subspace fidelity, a comparison among\ndifferent bases in the 1+1 dimensional $\\lambda \\phi^4$ theory, and the\n\"spectral crowding\" in the quantum field theory simulation.",
          "link": "http://arxiv.org/abs/2109.05547",
          "publishedOn": "2021-09-14T07:20:12.779Z",
          "wordCount": null,
          "title": "Towards a variational Jordan-Lee-Preskill quantum algorithm. (arXiv:2109.05547v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05175",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazuhiko Shinoda</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hoshino_T/0/1/0/all/0/1\">Takahiro Hoshino</a>",
          "description": "It is important to estimate the local average treatment effect (LATE) when\ncompliance with a treatment assignment is incomplete. The previously proposed\nmethods for LATE estimation required all relevant variables to be jointly\nobserved in a single dataset; however, it is sometimes difficult or even\nimpossible to collect such data in many real-world problems for technical or\nprivacy reasons. We consider a novel problem setting in which LATE, as a\nfunction of covariates, is nonparametrically identified from the combination of\nseparately observed datasets. For estimation, we show that the direct least\nsquares method, which was originally developed for estimating the average\ntreatment effect under complete compliance, is applicable to our setting.\nHowever, model selection and hyperparameter tuning for the direct least squares\nestimator can be unstable in practice since it is defined as a solution to the\nminimax problem. We then propose a weighted least squares estimator that\nenables simpler model selection by avoiding the minimax objective formulation.\nUnlike the inverse probability weighted (IPW) estimator, the proposed estimator\ndirectly uses the pre-estimated weight without inversion, avoiding the problems\ncaused by the IPW methods. We demonstrate the effectiveness of our method\nthrough experiments using synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2109.05175",
          "publishedOn": "2021-09-14T07:20:12.778Z",
          "wordCount": null,
          "title": "Estimation of Local Average Treatment Effect by Data Combination. (arXiv:2109.05175v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wegmeth_L/0/1/0/all/0/1\">Lukas Wegmeth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>",
          "description": "In this work we propose a solution to the UbiComp 2021 Challenge by Stabilo\nin which handwritten mathematical terms are supposed to be automatically\nclassified based on time series sensor data captured on the DigiPen. The input\ndata set contains data of different writers, with label strings constructed\nfrom a total of 15 different possible characters. The label should first be\nsplit into separate characters to classify them one by one. This issue is\nsolved by applying a data-dependant and rule-based information extraction\nalgorithm to the labeled data. Using the resulting data, two classifiers are\nconstructed. The first is a binary classifier that is able to predict, for\nunknown data, if a sample is part of a writing activity, and consists of a Deep\nNeural Network feature extractor in concatenation with a Random Forest that is\ntrained to classify the extracted features at an F1 score of >90%. The second\nclassifier is a Deep Neural Network that combines convolution layers with\nrecurrent layers to predict windows with a single label, out of the 15 possible\nclasses, at an F1 score of >60%. A simulation of the challenge evaluation\nprocedure reports a Levensthein Distance of 8 and shows that the chosen\napproach still lacks in overall accuracy and real-time applicability.",
          "link": "http://arxiv.org/abs/2109.05594",
          "publishedOn": "2021-09-14T07:20:12.774Z",
          "wordCount": null,
          "title": "Detecting Handwritten Mathematical Terms with Sensor Based Data. (arXiv:2109.05594v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zach_C/0/1/0/all/0/1\">Christopher Zach</a>",
          "description": "In this work we unify a number of inference learning methods, that are\nproposed in the literature as alternative training algorithms to the ones based\non regular error back-propagation. These inference learning methods were\ndeveloped with very diverse motivations, mainly aiming to enhance the\nbiological plausibility of deep neural networks and to improve the intrinsic\nparallelism of training methods. We show that these superficially very\ndifferent methods can all be obtained by successively applying a particular\nreformulation of bilevel optimization programs. As a by-product it becomes also\nevident that all considered inference learning methods include back-propagation\nas a special case, and therefore at least approximate error back-propagation in\ntypical settings. Finally, we propose Fenchel back-propagation, that replaces\nthe propagation of infinitesimal corrections performed in standard\nback-propagation with finite targets as the learning signal. Fenchel\nback-propagation can therefore be seen as an instance of learning via explicit\ntarget propagation.",
          "link": "http://arxiv.org/abs/2105.07231",
          "publishedOn": "2021-09-14T07:20:12.771Z",
          "wordCount": null,
          "title": "Bilevel Programs Meet Deep Learning: A Unifying View on Inference Learning Methods. (arXiv:2105.07231v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sewak_M/0/1/0/all/0/1\">Mohit Sewak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Sanjay K. Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathore_H/0/1/0/all/0/1\">Hemant Rathore</a>",
          "description": "Supervised Deep Learning requires plenty of labeled data to converge, and\nhence perform optimally for task-specific learning. Therefore, we propose a\nnovel mechanism named DRo (for Deep Routing) for data-scarce domains like\nsecurity. The DRo approach builds upon some of the recent developments in\nDeep-Clustering. In particular, it exploits the self-augmented training\nmechanism using synthetically generated local perturbations. DRo not only\nallays the challenges with sparse-labeled data but also offers many unique\nadvantages. We also developed a system named DRoID that uses the DRo mechanism\nfor enhancing the performance of an existing Malware Detection System that uses\n(low information features like the) Android implicit Intent(s) as the only\nfeatures. We conduct experiments on DRoID using a popular and standardized\nAndroid malware dataset and found that the DRo mechanism could successfully\nreduce the false-alarms generated by the downstream classifier by 67.9%, and\nalso simultaneously boosts its accuracy by 11.3%. This is significant not only\nbecause the gains achieved are unparalleled but also because the features used\nwere never considered rich enough to train a classifier on; and hence no decent\nperformance could ever be reported by any malware classification system\ntill-date using these features in isolation. Owing to the results achieved, the\nDRo mechanism claims a dominant position amongst all known systems that aims to\nenhance the classification performance of deep learning models with\nsparse-labeled data.",
          "link": "http://arxiv.org/abs/2109.05470",
          "publishedOn": "2021-09-14T07:20:12.760Z",
          "wordCount": null,
          "title": "DRo: A data-scarce mechanism to revolutionize the performance of Deep Learning based Security Systems. (arXiv:2109.05470v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qixuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaojian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>",
          "description": "The recent breakthroughs of deep neural networks (DNNs) and the advent of\nbillions of Internet of Things (IoT) devices have excited an explosive demand\nfor intelligent IoT devices equipped with domain-specific DNN accelerators.\nHowever, the deployment of DNN accelerator enabled intelligent functionality\ninto real-world IoT devices still remains particularly challenging. First,\npowerful DNNs often come at prohibitive complexities, whereas IoT devices often\nsuffer from stringent resource constraints. Second, while DNNs are vulnerable\nto adversarial attacks especially on IoT devices exposed to complex real-world\nenvironments, many IoT applications require strict security. Existing DNN\naccelerators mostly tackle only one of the two aforementioned challenges (i.e.,\nefficiency or adversarial robustness) while neglecting or even sacrificing the\nother. To this end, we propose a 2-in-1 Accelerator, an integrated\nalgorithm-accelerator co-design framework aiming at winning both the\nadversarial robustness and efficiency of DNN accelerators. Specifically, we\nfirst propose a Random Precision Switch (RPS) algorithm that can effectively\ndefend DNNs against adversarial attacks by enabling random DNN quantization as\nan in-situ model switch. Furthermore, we propose a new precision-scalable\naccelerator featuring (1) a new precision-scalable MAC unit architecture which\nspatially tiles the temporal MAC units to boost both the achievable efficiency\nand flexibility and (2) a systematically optimized dataflow that is searched by\nour generic accelerator optimizer. Extensive experiments and ablation studies\nvalidate that our 2-in-1 Accelerator can not only aggressively boost both the\nadversarial robustness and efficiency of DNN accelerators under various\nattacks, but also naturally support instantaneous robustness-efficiency\ntrade-offs adapting to varied resources without the necessity of DNN\nretraining.",
          "link": "http://arxiv.org/abs/2109.05223",
          "publishedOn": "2021-09-14T07:20:12.730Z",
          "wordCount": null,
          "title": "2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency. (arXiv:2109.05223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Afek Ilay Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Painsky_A/0/1/0/all/0/1\">Amichai Painsky</a>",
          "description": "Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular\ndata, which produce state of the art results in many prediction tasks. Despite\nits popularity, the GBM framework suffers from a fundamental flaw in its base\nlearners. Specifically, most implementations utilize decision trees that are\ntypically biased towards categorical variables with large cardinalities. The\neffect of this bias was extensively studied over the years, mostly in terms of\npredictive performance. In this work, we extend the scope and study the effect\nof biased base learners on GBM feature importance (FI) measures. We show that\nalthough these implementation demonstrate highly competitive predictive\nperformance, they still, surprisingly, suffer from bias in FI. By utilizing\ncross-validated (CV) unbiased base learners, we fix this flaw at a relatively\nlow computational cost. We demonstrate the suggested framework in a variety of\nsynthetic and real-world setups, showing a significant improvement in all GBM\nFI measures while maintaining relatively the same level of prediction accuracy.",
          "link": "http://arxiv.org/abs/2109.05468",
          "publishedOn": "2021-09-14T07:20:12.717Z",
          "wordCount": null,
          "title": "Feature Importance in Gradient Boosting Trees with Cross-Validation Feature Selection. (arXiv:2109.05468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Shubham K Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayekar_P/0/1/0/all/0/1\">Prathamesh Mayekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_H/0/1/0/all/0/1\">Himanshu Tyagi</a>",
          "description": "We consider over-the-air convex optimization on a d dimensional space where\ncoded gradients are sent over an additive Gaussian noise channel with variance\n\\sigma^2. The codewords satisfy an average power constraint P, resulting in the\nsignal-to-noise ratio (SNR) of P/\\sigma^2. We derive bounds for the convergence\nrates for over-the-air optimization. Our first result is a lower bound for the\nconvergence rate showing that any code must slowdown the convergence rate by a\nfactor of roughly \\sqrt{d/log(1 + SNR)}. Next, we consider a popular class of\nschemes called analog coding, where a linear function of the gradient is sent.\nWe show that a simple scaled transmission analog coding scheme results in a\nslowdown in convergence rate by a factor of \\sqrt{d(1 + 1/SNR)}. This matches\nthe previous lower bound up to constant factors for low SNR, making the scaled\ntransmission scheme optimal at low SNR. However, we show that this slowdown is\nnecessary for any analog coding scheme. In particular, a slowdown in\nconvergence by a factor of \\sqrt{d} for analog coding remains even when SNR\ntends to infinity. Remarkably, we present a simple quantize-and-modulate scheme\nthat uses Amplitude Shift Keying and almost attains the optimal convergence\nrate at all SNRs.",
          "link": "http://arxiv.org/abs/2109.05222",
          "publishedOn": "2021-09-14T07:20:12.700Z",
          "wordCount": null,
          "title": "Fundamental limits of over-the-air optimization: Are analog schemes optimal?. (arXiv:2109.05222v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Haoran Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yaofeng Desmond Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1\">Biswadip Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Amit Chakraborty</a>",
          "description": "Emergency vehicles (EMVs) play a crucial role in responding to time-critical\nevents such as medical emergencies and fire outbreaks in an urban area. The\nless time EMVs spend traveling through the traffic, the more likely it would\nhelp save people's lives and reduce property loss. To reduce the travel time of\nEMVs, prior work has used route optimization based on historical traffic-flow\ndata and traffic signal pre-emption based on the optimal route. However,\ntraffic signal pre-emption dynamically changes the traffic flow which, in turn,\nmodifies the optimal route of an EMV. In addition, traffic signal pre-emption\npractices usually lead to significant disturbances in traffic flow and\nsubsequently increase the travel time for non-EMVs. In this paper, we propose\nEMVLight, a decentralized reinforcement learning (RL) framework for\nsimultaneous dynamic routing and traffic signal control. EMVLight extends\nDijkstra's algorithm to efficiently update the optimal route for the EMVs in\nreal time as it travels through the traffic network. The decentralized RL\nagents learn network-level cooperative traffic signal phase strategies that not\nonly reduce EMV travel time but also reduce the average travel time of non-EMVs\nin the network. This benefit has been demonstrated through comprehensive\nexperiments with synthetic and real-world maps. These experiments show that\nEMVLight outperforms benchmark transportation engineering techniques and\nexisting RL-based signal control methods.",
          "link": "http://arxiv.org/abs/2109.05429",
          "publishedOn": "2021-09-14T07:20:12.686Z",
          "wordCount": null,
          "title": "EMVLight: A Decentralized Reinforcement Learning Framework for EfficientPassage of Emergency Vehicles. (arXiv:2109.05429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arya_S/0/1/0/all/0/1\">Shailesh Arya</a>",
          "description": "The vision of the internet of things (IoT) is a reality now. IoT devices are\ngetting cheaper, smaller. They are becoming more and more computationally and\nenergy-efficient. The global market of IoT-based video analytics has seen\nsignificant growth in recent years and it is expected to be a growing market\nsegment. For any IoT-based video analytics application, few key points\nrequired, such as cost-effectiveness, widespread use, flexible design, accurate\nscene detection, reusability of the framework. Video-based smart doorbell\nsystem is one such application domain for video analytics where many commercial\nofferings are available in the consumer market. However, such existing\nofferings are costly, monolithic, and proprietary. Also, there will be a\ntrade-off between accuracy and portability. To address the foreseen problems,\nI'm proposing a distributed framework for video analytics with a use case of a\nsmart doorbell system. The proposed framework uses AWS cloud services as a base\nplatform and to meet the price affordability constraint, the system was\nimplemented on affordable Raspberry Pi. The smart doorbell will be able to\nrecognize the known/unknown person with at most accuracy. The smart doorbell\nsystem is also having additional detection functionalities such as harmful\nweapon detection, noteworthy vehicle detection, animal/pet detection. An iOS\napplication is specifically developed for this implementation which can receive\nthe notification from the smart doorbell in real-time. Finally, the paper also\nmentions the classical approaches for video analytics, their feasibility in\nimplementing with this use-case, and comparative analysis in terms of accuracy\nand time required to detect an object in the frame is carried out. Results\nconclude that AWS cloud-based approach is worthy for this smart doorbell use\ncase.",
          "link": "http://arxiv.org/abs/2105.06508",
          "publishedOn": "2021-09-14T07:20:12.682Z",
          "wordCount": null,
          "title": "Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Song Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiamou Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaiqi Zhao</a>",
          "description": "Traffic flow forecasting is a crucial task in urban computing. The challenge\narises as traffic flows often exhibit intrinsic and latent spatio-temporal\ncorrelations that cannot be identified by extracting the spatial and temporal\npatterns of traffic data separately. We argue that such correlations are\nuniversal and play a pivotal role in traffic flow. We put forward spacetime\ninterval learning as a paradigm to explicitly capture these correlations\nthrough a unified analysis of both spatial and temporal features. Unlike the\nstate-of-the-art methods, which are restricted to a particular road network, we\nmodel the universal spatio-temporal correlations that are transferable from\ncities to cities. To this end, we propose a new spacetime interval learning\nframework that constructs a local-spacetime context of a traffic sensor\ncomprising the data from its neighbors within close time points. Based on this\nidea, we introduce spacetime neural network (STNN), which employs novel\nspacetime convolution and attention mechanism to learn the universal\nspatio-temporal correlations. The proposed STNN captures local traffic\npatterns, which does not depend on a specific network structure. As a result, a\ntrained STNN model can be applied on any unseen traffic networks. We evaluate\nthe proposed STNN on two public real-world traffic datasets and a simulated\ndataset on dynamic networks. The experiment results show that STNN not only\nimproves prediction accuracy by 15% over state-of-the-art methods, but is also\neffective in handling the case when the traffic network undergoes dynamic\nchanges as well as the superior generalization capability.",
          "link": "http://arxiv.org/abs/2109.05225",
          "publishedOn": "2021-09-14T07:20:12.651Z",
          "wordCount": null,
          "title": "Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting. (arXiv:2109.05225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuerong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "Training robust supervised deep learning models for many geospatial\napplications of computer vision is difficult due to dearth of class-balanced\nand diverse training data. Conversely, obtaining enough training data for many\napplications is financially prohibitive or may be infeasible, especially when\nthe application involves modeling rare or extreme events. Synthetically\ngenerating data (and labels) using a generative model that can sample from a\ntarget distribution and exploit the multi-scale nature of images can be an\ninexpensive solution to address scarcity of labeled data. Towards this goal, we\npresent a deep conditional generative model, called VAE-Info-cGAN, that\ncombines a Variational Autoencoder (VAE) with a conditional Information\nMaximizing Generative Adversarial Network (InfoGAN), for synthesizing\nsemantically rich images simultaneously conditioned on a pixel-level condition\n(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC\ncan only vary in the channel dimension from the synthesized image and is meant\nto be a task-specific input. The FLC is modeled as an attribute vector in the\nlatent space of the generated image which controls the contributions of various\ncharacteristic attributes germane to the target distribution. Experiments on a\nGPS trajectories dataset show that the proposed model can accurately generate\nvarious forms of spatiotemporal aggregates across different geographic\nlocations while conditioned only on a raster representation of the road\nnetwork. The primary intended application of the VAE-Info-cGAN is synthetic\ndata (and label) generation for targeted data augmentation for computer\nvision-based modeling of problems relevant to geospatial analysis and remote\nsensing.",
          "link": "http://arxiv.org/abs/2109.05201",
          "publishedOn": "2021-09-14T07:20:12.647Z",
          "wordCount": null,
          "title": "Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2005.05276",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heese_R/0/1/0/all/0/1\">Raoul Heese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morand_L/0/1/0/all/0/1\">Lukas Morand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helm_D/0/1/0/all/0/1\">Dirk Helm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortz_M/0/1/0/all/0/1\">Michael Bortz</a>",
          "description": "Using data from a simulated cup drawing process, we demonstrate how the\ninherent geometrical structure of cup meshes can be used to effectively prune\nan artificial neural network in a straightforward way.",
          "link": "http://arxiv.org/abs/2005.05276",
          "publishedOn": "2021-09-14T07:20:12.645Z",
          "wordCount": null,
          "title": "CupNet -- Pruning a network for geometric data. (arXiv:2005.05276v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.02728",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1\">Meimei Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1\">David B. Dunson</a>",
          "description": "There has been huge interest in studying human brain connectomes inferred\nfrom different imaging modalities and exploring their relationship with human\ntraits, such as cognition. Brain connectomes are usually represented as\nnetworks, with nodes corresponding to different regions of interest (ROIs) and\nedges to connection strengths between ROIs. Due to the high-dimensionality and\nnon-Euclidean nature of networks, it is challenging to depict their population\ndistribution and relate them to human traits. Current approaches focus on\nsummarizing the network using either pre-specified topological features or\nprincipal components analysis (PCA). In this paper, building on recent advances\nin deep learning, we develop a nonlinear latent factor model to characterize\nthe population distribution of brain graphs and infer the relationships between\nbrain structural connectomes and human traits. We refer to our method as Graph\nAuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging\ndatasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human\nConnectome Project (HCP) for adults, to understand the structural brain\nconnectome and its relationship with cognition. Numerical results demonstrate\nhuge advantages of GATE over competitors in terms of prediction accuracy,\nstatistical inference and computing efficiency. We found that structural\nconnectomes have a stronger association with a wide range of human cognitive\ntraits than was apparent using previous approaches.",
          "link": "http://arxiv.org/abs/1911.02728",
          "publishedOn": "2021-09-14T07:20:12.643Z",
          "wordCount": null,
          "title": "Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets. (arXiv:1911.02728v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.01241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1\">Devavrat Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Christina Lee Yu</a>",
          "description": "Consider the task of estimating a 3-order $n \\times n \\times n$ tensor from\nnoisy observations of randomly chosen entries in the sparse regime. We\nintroduce a similarity based collaborative filtering algorithm for sparse\ntensor estimation and argue that it achieves sample complexity that nearly\nmatches the conjectured computationally efficient lower bound on the sample\ncomplexity for the setting of low-rank tensors. Our algorithm uses the matrix\nobtained from the flattened tensor to compute similarity, and estimates the\ntensor entries using a nearest neighbor estimator. We prove that the algorithm\nrecovers a low rank tensor with maximum entry-wise error (MEE) and\nmean-squared-error (MSE) decaying to $0$ as long as each entry is observed\nindependently with probability $p = \\Omega(n^{-3/2 + \\kappa})$ for any\narbitrarily small $\\kappa > 0$. % as long as tensor has finite rank $r =\n\\Theta(1)$. More generally, we establish robustness of the estimator, showing\nthat when arbitrary noise bounded by $\\epsilon \\geq 0$ is added to each\nobservation, the estimation error with respect to MEE and MSE degrades by ${\\sf\npoly}(\\epsilon)$. Consequently, even if the tensor may not have finite rank but\ncan be approximated within $\\epsilon \\geq 0$ by a finite rank tensor, then the\nestimation error converges to ${\\sf poly}(\\epsilon)$. Our analysis sheds\ninsight into the conjectured sample complexity lower bound, showing that it\nmatches the connectivity threshold of the graph used by our algorithm for\nestimating similarity between coordinates.",
          "link": "http://arxiv.org/abs/1908.01241",
          "publishedOn": "2021-09-14T07:20:12.642Z",
          "wordCount": null,
          "title": "Robust Max Entrywise Error Bounds for Sparse Tensor Estimation via Similarity Based Collaborative Filtering. (arXiv:1908.01241v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1712.10024",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Semenova_V/0/1/0/all/0/1\">Vira Semenova</a>",
          "description": "This paper provides estimation and inference methods for an identified set's\nboundary (i.e., support function) where the selection among a very large number\nof covariates is based on modern regularized tools. I characterize the boundary\nusing a semiparametric moment equation. Combining Neyman-orthogonality and\nsample splitting ideas, I construct a root-N consistent, uniformly\nasymptotically Gaussian estimator of the boundary and propose a multiplier\nbootstrap procedure to conduct inference. I apply this result to the partially\nlinear model and the partially linear IV model with an interval-valued outcome.",
          "link": "http://arxiv.org/abs/1712.10024",
          "publishedOn": "2021-09-14T07:20:12.638Z",
          "wordCount": null,
          "title": "Debiased Machine Learning of Set-Identified Linear Models. (arXiv:1712.10024v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tajwar_F/0/1/0/all/0/1\">Fahim Tajwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ananya Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>",
          "description": "Out-of-distribution detection is an important component of reliable ML\nsystems. Prior literature has proposed various methods (e.g., MSP (Hendrycks &\nGimpel, 2017), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018)),\nclaiming they are state-of-the-art by showing they outperform previous methods\non a selected set of in-distribution (ID) and out-of-distribution (OOD)\ndatasets. In this work, we show that none of these methods are inherently\nbetter at OOD detection than others on a standardized set of 16 (ID, OOD)\npairs. We give possible explanations for these inconsistencies with simple toy\ndatasets where whether one method outperforms another depends on the structure\nof the ID and OOD datasets in question. Finally, we show that a method\noutperforming another on a certain (ID, OOD) pair may not do so in a low-data\nregime. In the low-data regime, we propose a distance-based method, Pairwise\nOOD detection (POD), which is based on Siamese networks and improves over\nMahalanobis by sidestepping the expensive covariance estimation step. Our\nresults suggest that the OOD detection problem may be too broad, and we should\nconsider more specific structures for leverage.",
          "link": "http://arxiv.org/abs/2109.05554",
          "publishedOn": "2021-09-14T07:20:12.634Z",
          "wordCount": null,
          "title": "No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets. (arXiv:2109.05554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_N/0/1/0/all/0/1\">Nariman L. Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamanian_S/0/1/0/all/0/1\">Soroush Zamanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafieezadeh_A/0/1/0/all/0/1\">Abdollah Shafieezadeh</a>",
          "description": "Flow network models can capture the underlying physics and operational\nconstraints of many networked systems including the power grid and\ntransportation and water networks. However, analyzing reliability of systems\nusing computationally expensive flow-based models faces substantial challenges,\nespecially for rare events. Existing actively trained meta-models, which\npresent a new promising direction in reliability analysis, are not applicable\nto networks due to the inability of these methods to handle high-dimensional\nproblems as well as discrete or mixed variable inputs. This study presents the\nfirst adaptive surrogate-based Network Reliability Analysis using Bayesian\nAdditive Regression Trees (ANR-BART). This approach integrates BART and Monte\nCarlo simulation (MCS) via an active learning method that identifies the most\nvaluable training samples based on the credible intervals derived by BART over\nthe space of predictor variables as well as the proximity of the points to the\nestimated limit state. Benchmark power grids including IEEE 30, 57, 118, and\n300-bus systems and their power flow models for cascading failure analysis are\nconsidered to investigate ANR-BART, MCS, subset simulation, and\npassively-trained optimal deep neural networks and BART. Results indicate that\nANR-BART is robust and yields accurate estimates of network failure\nprobability, while significantly reducing the computational cost of reliability\nanalysis.",
          "link": "http://arxiv.org/abs/2109.05360",
          "publishedOn": "2021-09-14T07:20:12.581Z",
          "wordCount": null,
          "title": "Adaptive network reliability analysis: Methodology and applications to power grid. (arXiv:2109.05360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kookjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trask_N/0/1/0/all/0/1\">Nathaniel Trask</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stinis_P/0/1/0/all/0/1\">Panos Stinis</a>",
          "description": "Discovery of dynamical systems from data forms the foundation for data-driven\nmodeling and recently, structure-preserving geometric perspectives have been\nshown to provide improved forecasting, stability, and physical realizability\nguarantees. We present here a unification of the Sparse Identification of\nNonlinear Dynamics (SINDy) formalism with neural ordinary differential\nequations. The resulting framework allows learning of both \"black-box\" dynamics\nand learning of structure preserving bracket formalisms for both reversible and\nirreversible dynamics. We present a suite of benchmarks demonstrating\neffectiveness and structure preservation, including for chaotic systems.",
          "link": "http://arxiv.org/abs/2109.05364",
          "publishedOn": "2021-09-14T07:20:12.566Z",
          "wordCount": null,
          "title": "Structure-preserving Sparse Identification of Nonlinear Dynamics for Data-driven Modeling. (arXiv:2109.05364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01846",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dianlin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cong_W/0/1/0/all/0/1\">Wenxiang Cong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shaoyu Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_P/0/1/0/all/0/1\">Pingkun Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hengyong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vardhanabhuti_V/0/1/0/all/0/1\">Varut Vardhanabhuti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>",
          "description": "Tomographic image reconstruction with deep learning is an emerging field, but\na recent landmark study reveals that several deep reconstruction networks are\nunstable for computed tomography (CT) and magnetic resonance imaging (MRI).\nSpecifically, three kinds of instabilities were reported: (1) strong image\nartefacts from tiny perturbations, (2) small features missing in a deeply\nreconstructed image, and (3) decreased imaging performance with increased input\ndata. On the other hand, compressed sensing (CS) inspired reconstruction\nmethods do not suffer from these instabilities because of their built-in kernel\nawareness. For deep reconstruction to realize its full potential and become a\nmainstream approach for tomographic imaging, it is thus critically important to\nmeet this challenge by stabilizing deep reconstruction networks. Here we\npropose an Analytic Compressed Iterative Deep (ACID) framework to address this\nchallenge. ACID synergizes a deep reconstruction network trained on big data,\nkernel awareness from CS-inspired processing, and iterative refinement to\nminimize the data residual relative to real measurement. Our study demonstrates\nthat the deep reconstruction using ACID is accurate and stable, and sheds light\non the converging mechanism of the ACID iteration under a Bounded Relative\nError Norm (BREN) condition. In particular, the study shows that ACID-based\nreconstruction is resilient against adversarial attacks, superior to classic\nsparsity-regularized reconstruction alone, and eliminates the three kinds of\ninstabilities. We anticipate that this integrative data-driven approach will\nhelp promote development and translation of deep tomographic image\nreconstruction networks into clinical applications.",
          "link": "http://arxiv.org/abs/2008.01846",
          "publishedOn": "2021-09-14T07:20:12.559Z",
          "wordCount": null,
          "title": "Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hulsebos_M/0/1/0/all/0/1\">Madelon Hulsebos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gathani_S/0/1/0/all/0/1\">Sneha Gathani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gale_J/0/1/0/all/0/1\">James Gale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1\">Isil Dillig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1\">Paul Groth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>",
          "description": "Understanding the semantics of tables at scale is crucial for tasks like data\nintegration, preparation, and search. Table understanding methods aim at\ndetecting a table's topic, semantic column types, column relations, or\nentities. With the rise of deep learning, powerful models have been developed\nfor these tasks with excellent accuracy on benchmarks. However, we observe that\nthere exists a gap between the performance of these models on these benchmarks\nand their applicability in practice. In this paper, we address the question:\nwhat do we need for these models to work in practice?\n\nWe discuss three challenges of deploying table understanding models and\npropose a framework to address them. These challenges include 1) difficulty in\ncustomizing models to specific domains, 2) lack of training data for typical\ndatabase tables often found in enterprises, and 3) lack of confidence in the\ninferences made by models. We present SigmaTyper which implements this\nframework for the semantic column type detection task. SigmaTyper encapsulates\na hybrid model trained on GitTables and integrates a lightweight\nhuman-in-the-loop approach to customize the model. Lastly, we highlight avenues\nfor future research that further close the gap towards making table\nunderstanding effective in practice.",
          "link": "http://arxiv.org/abs/2109.05173",
          "publishedOn": "2021-09-14T07:20:12.548Z",
          "wordCount": null,
          "title": "Making Table Understanding Work in Practice. (arXiv:2109.05173v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Diptanu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>",
          "description": "Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.",
          "link": "http://arxiv.org/abs/2109.05074",
          "publishedOn": "2021-09-14T07:20:12.545Z",
          "wordCount": null,
          "title": "FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yiming Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Post-hoc interpretation aims to explain a trained model and reveal how the\nmodel arrives at a decision. Though research on post-hoc interpretations has\ndeveloped rapidly, one growing pain in this field is the difficulty in\nevaluating interpretations. There are some crucial logic traps behind existing\nevaluation methods, which are ignored by most works. In this opinion piece, we\nsummarize four kinds evaluation methods and point out the corresponding logic\ntraps behind them. We argue that we should be clear about these traps rather\nthan ignore them and draw conclusions assertively.",
          "link": "http://arxiv.org/abs/2109.05463",
          "publishedOn": "2021-09-14T07:20:12.542Z",
          "wordCount": null,
          "title": "The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03176",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_Y/0/1/0/all/0/1\">Ye He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Balasubramanian_K/0/1/0/all/0/1\">Krishnakumar Balasubramanian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Erdogdu_M/0/1/0/all/0/1\">Murat A. Erdogdu</a>",
          "description": "The randomized midpoint method, proposed by [SL19], has emerged as an optimal\ndiscretization procedure for simulating the continuous time Langevin\ndiffusions. Focusing on the case of strong-convex and smooth potentials, in\nthis paper, we analyze several probabilistic properties of the randomized\nmidpoint discretization method for both overdamped and underdamped Langevin\ndiffusions. We first characterize the stationary distribution of the discrete\nchain obtained with constant step-size discretization and show that it is\nbiased away from the target distribution. Notably, the step-size needs to go to\nzero to obtain asymptotic unbiasedness. Next, we establish the asymptotic\nnormality for numerical integration using the randomized midpoint method and\nhighlight the relative advantages and disadvantages over other discretizations.\nOur results collectively provide several insights into the behavior of the\nrandomized midpoint discretization method, including obtaining confidence\nintervals for numerical integrations.",
          "link": "http://arxiv.org/abs/2011.03176",
          "publishedOn": "2021-09-14T07:20:12.540Z",
          "wordCount": null,
          "title": "On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method. (arXiv:2011.03176v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lingheng Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorbet_R/0/1/0/all/0/1\">Rob Gorbet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulic_D/0/1/0/all/0/1\">Dana Kuli&#x107;</a>",
          "description": "A promising characteristic of Deep Reinforcement Learning (DRL) is its\ncapability to learn optimal policy in an end-to-end manner without relying on\nfeature engineering. However, most approaches assume a fully observable state\nspace, i.e. fully observable Markov Decision Processes (MDPs). In real-world\nrobotics, this assumption is unpractical, because of issues such as sensor\nsensitivity limitations and sensor noise, and the lack of knowledge about\nwhether the observation design is complete or not. These scenarios lead to\nPartially Observable MDPs (POMDPs). In this paper, we propose\nLong-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient\n(LSTM-TD3) by introducing a memory component to TD3, and compare its\nperformance with other DRL algorithms in both MDPs and POMDPs. Our results\ndemonstrate the significant advantages of the memory component in addressing\nPOMDPs, including the ability to handle missing and noisy observation data.",
          "link": "http://arxiv.org/abs/2102.12344",
          "publishedOn": "2021-09-14T07:20:12.521Z",
          "wordCount": null,
          "title": "Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lequn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Contextual bandit algorithms have become widely used for recommendation in\nonline systems (e.g. marketplaces, music streaming, news), where they now wield\nsubstantial influence on which items get exposed to the users. This raises\nquestions of fairness to the items -- and to the sellers, artists, and writers\nthat benefit from this exposure. We argue that the conventional bandit\nformulation can lead to an undesirable and unfair winner-takes-all allocation\nof exposure. To remedy this problem, we propose a new bandit objective that\nguarantees merit-based fairness of exposure to the items while optimizing\nutility to the users. We formulate fairness regret and reward regret in this\nsetting, and present algorithms for both stochastic multi-armed bandits and\nstochastic linear bandits. We prove that the algorithms achieve sub-linear\nfairness regret and reward regret. Beyond the theoretical analysis, we also\nprovide empirical evidence that these algorithms can fairly allocate exposure\nto different arms effectively.",
          "link": "http://arxiv.org/abs/2103.02735",
          "publishedOn": "2021-09-14T07:20:12.497Z",
          "wordCount": null,
          "title": "Fairness of Exposure in Stochastic Bandits. (arXiv:2103.02735v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matena_M/0/1/0/all/0/1\">Michael Matena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkan_K/0/1/0/all/0/1\">Karishma Malkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_J/0/1/0/all/0/1\">Jake Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>",
          "description": "The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.",
          "link": "http://arxiv.org/abs/2102.11972",
          "publishedOn": "2021-09-14T07:20:12.491Z",
          "wordCount": null,
          "title": "Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05352",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoeffmann_k/0/1/0/all/0/1\">klaus Schoeffmann</a>",
          "description": "Semantic segmentation in cataract surgery has a wide range of applications\ncontributing to surgical outcome enhancement and clinical risk reduction.\nHowever, the varying issues in segmenting the different relevant instances make\nthe designation of a unique network quite challenging. This paper proposes a\nsemantic segmentation network termed as DeepPyram that can achieve superior\nperformance in segmenting relevant objects in cataract surgery videos with\nvarying issues. This superiority mainly originates from three modules: (i)\nPyramid View Fusion, which provides a varying-angle global view of the\nsurrounding region centering at each pixel position in the input convolutional\nfeature map; (ii) Deformable Pyramid Reception, which enables a wide deformable\nreceptive field that can adapt to geometric transformations in the object of\ninterest; and (iii) Pyramid Loss that adaptively supervises multi-scale\nsemantic feature maps. These modules can effectively boost semantic\nsegmentation performance, especially in the case of transparency,\ndeformability, scalability, and blunt edges in objects. The proposed approach\nis evaluated using four datasets of cataract surgery for objects with different\ncontextual features and compared with thirteen state-of-the-art segmentation\nnetworks. The experimental results confirm that DeepPyram outperforms the rival\napproaches without imposing additional trainable parameters. Our comprehensive\nablation study further proves the effectiveness of the proposed modules.",
          "link": "http://arxiv.org/abs/2109.05352",
          "publishedOn": "2021-09-14T07:20:12.489Z",
          "wordCount": null,
          "title": "DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Harinen_T/0/1/0/all/0/1\">Totte Harinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipowicz_A/0/1/0/all/0/1\">Alexandre Filipowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimi_S/0/1/0/all/0/1\">Shabnam Hakimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iliev_R/0/1/0/all/0/1\">Rumen Iliev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klenk_M/0/1/0/all/0/1\">Matthew Klenk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumner_E/0/1/0/all/0/1\">Emily Sumner</a>",
          "description": "Different advertising messages work for different people. Machine learning\ncan be an effective way to personalise climate communications. In this paper we\nuse machine learning to reanalyse findings from a recent study, showing that\nonline advertisements increased some people's belief in climate change while\nresulting in decreased belief in others. In particular, we show that the effect\nof the advertisements could change depending on people's age and ethnicity.",
          "link": "http://arxiv.org/abs/2109.05104",
          "publishedOn": "2021-09-14T07:20:12.488Z",
          "wordCount": null,
          "title": "Machine learning reveals how personalized climate communication can both succeed and backfire. (arXiv:2109.05104v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05461",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Assef Zare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_N/0/1/0/all/0/1\">Narges Shafaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moridian_P/0/1/0/all/0/1\">Parisa Moridian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halaji_M/0/1/0/all/0/1\">Majid Halaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>",
          "description": "Many works have been done to handle the uncertainties in the data using type\n1 fuzzy regression. Few type 2 fuzzy regression works used interval type 2 for\nindeterminate modeling using type 1 fuzzy membership. The current survey\nproposes a triangular type-2 fuzzy regression (TT2FR) model to ameliorate the\nefficiency of the model by handling the uncertainty in the data. The triangular\nsecondary membership function is used instead of widely used interval type\nmodels. In the proposed model, vagueness in primary and secondary fuzzy sets is\nminimized and also, a specified x-plane of observed value is included in the\nsame {\\alpha}- plane of the predicted value. Complex calculations of the type-2\nfuzzy (T2F) model are simplified by reducing three dimensional type-2 fuzzy set\n(3DT2FS) into two dimensional interval type-2 fuzzy (2DIT2F) models. The\ncurrent survey presents a new regression model of T2F by considering the more\ngeneral form of T2F membership functions and thus avoids high complexity. The\nperformance of the developed model is evaluated using the TAIEX and COVID-19\nforecasting datasets. Our developed model reached the highest performance as\ncompared to the other state-of-art techniques. Our developed method is ready to\nbe tested with more uncertain data and has the potential to use to predict the\nweather and stock prediction.",
          "link": "http://arxiv.org/abs/2109.05461",
          "publishedOn": "2021-09-14T07:20:12.472Z",
          "wordCount": null,
          "title": "Accurate Prediction Using Triangular Type-2 Fuzzy Linear Regression. (arXiv:2109.05461v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_B/0/1/0/all/0/1\">Bashir Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Naresh Boddeti</a>",
          "description": "Adversarial representation learning aims to learn data representations for a\ntarget task while removing unwanted sensitive information at the same time.\nExisting methods learn model parameters iteratively through stochastic gradient\ndescent-ascent, which is often unstable and unreliable in practice. To overcome\nthis challenge, we adopt closed-form solvers for the adversary and target task.\nWe model them as kernel ridge regressors and analytically determine an\nupper-bound on the optimal dimensionality of representation. Our solution,\ndubbed OptNet-ARL, reduces to a stable one one-shot optimization problem that\ncan be solved reliably and efficiently. OptNet-ARL can be easily generalized to\nthe case of multiple target tasks and sensitive attributes. Numerical\nexperiments, on both small and large scale datasets, show that, from an\noptimization perspective, OptNet-ARL is stable and exhibits three to five times\nfaster convergence. Performance wise, when the target and sensitive attributes\nare dependent, OptNet-ARL learns representations that offer a better trade-off\nfront between (a) utility and bias for fair classification and (b) utility and\nprivacy by mitigating leakage of private information than existing solutions.",
          "link": "http://arxiv.org/abs/2109.05535",
          "publishedOn": "2021-09-14T07:20:12.464Z",
          "wordCount": null,
          "title": "Adversarial Representation Learning With Closed-Form Solvers. (arXiv:2109.05535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jana_A/0/1/0/all/0/1\">Ananya Jana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minacapelli_C/0/1/0/all/0/1\">Carlos D. Minacapelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rustgi_V/0/1/0/all/0/1\">Vinod Rustgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "The COVID-19 corona virus has claimed 4.1 million lives, as of July 24, 2021.\nA variety of machine learning models have been applied to related data to\npredict important factors such as the severity of the disease, infection rate\nand discover important prognostic factors. Often the usefulness of the findings\nfrom the use of these techniques is reduced due to lack of method\ninterpretability. Some recent progress made on the interpretability of machine\nlearning models has the potential to unravel more insights while using\nconventional machine learning models. In this work, we analyze COVID-19 blood\nwork data with some of the popular machine learning models; then we employ\nstate-of-the-art post-hoc local interpretability techniques(e.g.- SHAP, LIME),\nand global interpretability techniques(e.g. - symbolic metamodeling) to the\ntrained black-box models to draw interpretable conclusions. In the gamut of\nmachine learning algorithms, regressions remain one of the simplest and most\nexplainable models with clear mathematical formulation. We explore one of the\nmost recent techniques called symbolic metamodeling to find the mathematical\nexpression of the machine learning models for COVID-19. We identify Acute\nKidney Injury (AKI), initial Albumin level (ALBI), Aspartate aminotransferase\n(ASTI), Total Bilirubin initial(TBILI) and D-Dimer initial (DIMER) as major\nprognostic factors of the disease severity. Our contributions are- (i) uncover\nthe underlying mathematical expression for the black-box models on COVID-19\nseverity prediction task (ii) we are the first to apply symbolic metamodeling\nto this task, and (iii) discover important features and feature interactions.",
          "link": "http://arxiv.org/abs/2109.05087",
          "publishedOn": "2021-09-14T07:20:12.461Z",
          "wordCount": null,
          "title": "Global and Local Interpretation of black-box Machine Learning models to determine prognostic factors from early COVID-19 data. (arXiv:2109.05087v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mridul Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qinbo Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>",
          "description": "We consider the problem of tabular infinite horizon concave utility\nreinforcement learning (CURL) with convex constraints. Various learning\napplications with constraints, such as robotics, do not allow for policies that\ncan violate constraints. To this end, we propose a model-based learning\nalgorithm that achieves zero constraint violations. To obtain this result, we\nassume that the concave objective and the convex constraints have a solution\ninterior to the set of feasible occupation measures. We then solve a tighter\noptimization problem to ensure that the constraints are never violated despite\nthe imprecise model knowledge and model stochasticity. We also propose a novel\nBellman error based analysis for tabular infinite-horizon setups which allows\nto analyse stochastic policies. Combining the Bellman error based analysis and\ntighter optimization equation, for $T$ interactions with the environment, we\nobtain a regret guarantee for objective which grows as $\\Tilde{O}(1/\\sqrt{T})$,\nexcluding other factors.",
          "link": "http://arxiv.org/abs/2109.05439",
          "publishedOn": "2021-09-14T07:20:12.436Z",
          "wordCount": null,
          "title": "Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1\">Sina Ghiassian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "Many off-policy prediction learning algorithms have been proposed in the past\ndecade, but it remains unclear which algorithms learn faster than others. We\nempirically compare 11 off-policy prediction learning algorithms with linear\nfunction approximation on two small tasks: the Rooms task, and the High\nVariance Rooms task. The tasks are designed such that learning fast in them is\nchallenging. In the Rooms task, the product of importance sampling ratios can\nbe as large as $2^{14}$ and can sometimes be two. To control the high variance\ncaused by the product of the importance sampling ratios, step size should be\nset small, which in turn slows down learning. The High Variance Rooms task is\nmore extreme in that the product of the ratios can become as large as\n$2^{14}\\times 25$. This paper builds upon the empirical study of off-policy\nprediction learning algorithms by Ghiassian and Sutton (2021). We consider the\nsame set of algorithms as theirs and employ the same experimental methodology.\nThe algorithms considered are: Off-policy TD($\\lambda$), five Gradient-TD\nalgorithms, two Emphatic-TD algorithms, Tree Backup($\\lambda$),\nVtrace($\\lambda$), and ABTD($\\zeta$). We found that the algorithms' performance\nis highly affected by the variance induced by the importance sampling ratios.\nThe data shows that Tree Backup($\\lambda$), Vtrace($\\lambda$), and\nABTD($\\zeta$) are not affected by the high variance as much as other algorithms\nbut they restrict the effective bootstrapping parameter in a way that is too\nlimiting for tasks where high variance is not present. We observed that\nEmphatic TD($\\lambda$) tends to have lower asymptotic error than other\nalgorithms, but might learn more slowly in some cases. We suggest algorithms\nfor practitioners based on their problem of interest, and suggest approaches\nthat can be applied to specific algorithms that might result in substantially\nimproved algorithms.",
          "link": "http://arxiv.org/abs/2109.05110",
          "publishedOn": "2021-09-14T07:20:12.428Z",
          "wordCount": null,
          "title": "An Empirical Comparison of Off-policy Prediction Learning Algorithms in the Four Rooms Environment. (arXiv:2109.05110v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zahedi_L/0/1/0/all/0/1\">Leila Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_F/0/1/0/all/0/1\">Farid Ghareh Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1\">M. Hadi Amini</a>",
          "description": "Machine learning techniques lend themselves as promising decision-making and\nanalytic tools in a wide range of applications. Different ML algorithms have\nvarious hyper-parameters. In order to tailor an ML model towards a specific\napplication, a large number of hyper-parameters should be tuned. Tuning the\nhyper-parameters directly affects the performance (accuracy and run-time).\nHowever, for large-scale search spaces, efficiently exploring the ample number\nof combinations of hyper-parameters is computationally challenging. Existing\nautomated hyper-parameter tuning techniques suffer from high time complexity.\nIn this paper, we propose HyP-ABC, an automatic innovative hybrid\nhyper-parameter optimization algorithm using the modified artificial bee colony\napproach, to measure the classification accuracy of three ML algorithms, namely\nrandom forest, extreme gradient boosting, and support vector machine. Compared\nto the state-of-the-art techniques, HyP-ABC is more efficient and has a limited\nnumber of parameters to be tuned, making it worthwhile for real-world\nhyper-parameter optimization problems. We further compare our proposed HyP-ABC\nalgorithm with state-of-the-art techniques. In order to ensure the robustness\nof the proposed method, the algorithm takes a wide range of feasible\nhyper-parameter values, and is tested using a real-world educational dataset.",
          "link": "http://arxiv.org/abs/2109.05319",
          "publishedOn": "2021-09-14T07:20:12.427Z",
          "wordCount": null,
          "title": "HyP-ABC: A Novel Automated Hyper-Parameter Tuning Algorithm Using Evolutionary Optimization. (arXiv:2109.05319v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.11715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>",
          "description": "End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.",
          "link": "http://arxiv.org/abs/2011.11715",
          "publishedOn": "2021-09-14T07:20:12.421Z",
          "wordCount": null,
          "title": "Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhirong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedov_D/0/1/0/all/0/1\">Denis Sedov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_S/0/1/0/all/0/1\">Samuel Kaski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corander_J/0/1/0/all/0/1\">Jukka Corander</a>",
          "description": "Neighbor Embedding (NE) aims to preserve pairwise similarities between data\nitems and has been shown to yield an effective principle for data\nvisualization. However, even the best existing NE methods such as Stochastic\nNeighbor Embedding (SNE) may leave large-scale patterns hidden, for example,\nclusters, despite strong signals being present in the data. To address this, we\npropose a new cluster visualization method based on the Neighbor Embedding\nprinciple. We first present a family of Neighbor Embedding methods which\ngeneralizes SNE by using non-normalized Kullback-Leibler divergence with a\nscale parameter. In this family, much better cluster visualizations often\nappear with a parameter value different from the one corresponding to SNE. We\nalso develop an efficient software which employs asynchronous stochastic block\ncoordinate descent to optimize the new family of objective functions. Our\nexperimental results demonstrate that the method consistently and substantially\nimproves visualization of data clusters compared with the state-of-the-art NE\napproaches.",
          "link": "http://arxiv.org/abs/2108.08003",
          "publishedOn": "2021-09-14T07:20:12.412Z",
          "wordCount": null,
          "title": "Stochastic Cluster Embedding. (arXiv:2108.08003v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xugang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kai Lu</a>",
          "description": "Graph neural networks exhibit remarkable performance in graph data analysis.\nHowever, the robustness of GNN models remains a challenge. As a result, they\nare not reliable enough to be deployed in critical applications. Recent studies\ndemonstrate that GNNs could be easily fooled with adversarial perturbations,\nespecially structural perturbations. Such vulnerability is attributed to the\nexcessive dependence on the structure information to make predictions. To\nachieve better robustness, it is desirable to build the prediction of GNNs with\nmore comprehensive features. Graph data, in most cases, has two views of\ninformation, namely structure information and feature information. In this\npaper, we propose CoG, a simple yet effective co-training framework to combine\nthese two views for the purpose of robustness. CoG trains sub-models from the\nfeature view and the structure view independently and allows them to distill\nknowledge from each other by adding their most confident unlabeled data into\nthe training set. The orthogonality of these two views diversifies the\nsub-models, thus enhancing the robustness of their ensemble. We evaluate our\nframework on three popular datasets, and results show that CoG significantly\nimproves the robustness of graph models against adversarial attacks without\nsacrificing their performance on clean data. We also show that CoG still\nachieves good robustness when both node features and graph structures are\nperturbed.",
          "link": "http://arxiv.org/abs/2109.05558",
          "publishedOn": "2021-09-14T07:20:12.402Z",
          "wordCount": null,
          "title": "CoG: a Two-View Co-training Framework for Defending Adversarial Attacks on Graph. (arXiv:2109.05558v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shengyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1\">Liangjun Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhitang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "It is a long-standing question to discover causal relations among a set of\nvariables in many empirical sciences. Recently, Reinforcement Learning (RL) has\nachieved promising results in causal discovery from observational data.\nHowever, searching the space of directed graphs and enforcing acyclicity by\nimplicit penalties tend to be inefficient and restrict the existing RL-based\nmethod to small scale problems. In this work, we propose a novel RL-based\napproach for causal discovery, by incorporating RL into the ordering-based\nparadigm. Specifically, we formulate the ordering search problem as a\nmulti-step Markov decision process, implement the ordering generating process\nwith an encoder-decoder architecture, and finally use RL to optimize the\nproposed model based on the reward mechanisms designed for~each ordering. A\ngenerated ordering would then be processed using variable selection to obtain\nthe final causal graph. We analyze the consistency and computational complexity\nof the proposed method, and empirically show that a pretrained model can be\nexploited to accelerate training. Experimental results on both synthetic and\nreal data sets shows that the proposed method achieves a much improved\nperformance over existing RL-based method.",
          "link": "http://arxiv.org/abs/2105.06631",
          "publishedOn": "2021-09-14T07:20:12.383Z",
          "wordCount": null,
          "title": "Ordering-Based Causal Discovery with Reinforcement Learning. (arXiv:2105.06631v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05317",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ahrens_M/0/1/0/all/0/1\">Maximilian Ahrens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashwin_J/0/1/0/all/0/1\">Julian Ashwin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calliess_J/0/1/0/all/0/1\">Jan-Peter Calliess</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>",
          "description": "Causal inference using observational text data is becoming increasingly\npopular in many research areas. This paper presents the Bayesian Topic\nRegression (BTR) model that uses both text and numerical information to model\nan outcome variable. It allows estimation of both discrete and continuous\ntreatment effects. Furthermore, it allows for the inclusion of additional\nnumerical confounding factors next to text data. To this end, we combine a\nsupervised Bayesian topic model with a Bayesian regression framework and\nperform supervised representation learning for the text features jointly with\nthe regression parameter training, respecting the Frisch-Waugh-Lovell theorem.\nOur paper makes two main contributions. First, we provide a regression\nframework that allows causal inference in settings when both text and numerical\nconfounders are of relevance. We show with synthetic and semi-synthetic\ndatasets that our joint approach recovers ground truth with lower bias than any\nbenchmark model, when text and numerical features are correlated. Second,\nexperiments on two real-world datasets demonstrate that a joint and supervised\nlearning strategy also yields superior prediction results compared to\nstrategies that estimate regression weights for text and non-text features\nseparately, being even competitive with more complex deep neural networks.",
          "link": "http://arxiv.org/abs/2109.05317",
          "publishedOn": "2021-09-14T07:20:12.379Z",
          "wordCount": null,
          "title": "Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1\">Sitao Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1\">Chenqing Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qincheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiaqi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingde Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiao-Wen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the\ngraph structures based on the relational inductive bias (homophily assumption).\nThough GNNs are believed to outperform NNs in real-world tasks, performance\nadvantages of GNNs over graph-agnostic NNs seem not generally satisfactory.\nHeterophily has been considered as a main cause and numerous works have been\nput forward to address it. In this paper, we first show that not all cases of\nheterophily are harmful for GNNs with aggregation operation. Then, we propose\nnew metrics based on a similarity matrix which considers the influence of both\ngraph structure and input features on GNNs. The metrics demonstrate advantages\nover the commonly used homophily metrics by tests on synthetic graphs. From the\nmetrics and the observations, we find some cases of harmful heterophily can be\naddressed by diversification operation. With this fact and knowledge of\nfilterbanks, we propose the Adaptive Channel Mixing (ACM) framework to\nadaptively exploit aggregation, diversification and identity channels in each\nGNN layer to address harmful heterophily. We validate the ACM-augmented\nbaselines with 10 real-world node classification tasks. They consistently\nachieve significant performance gain and exceed the state-of-the-art GNNs on\nmost of the tasks without incurring significant computational burden.",
          "link": "http://arxiv.org/abs/2109.05641",
          "publishedOn": "2021-09-14T07:20:12.337Z",
          "wordCount": null,
          "title": "Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?. (arXiv:2109.05641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "Learning to solve sparse-reward reinforcement learning problems is difficult,\ndue to the lack of guidance towards the goal. But in some problems, prior\nknowledge can be used to augment the learning process. Reward shaping is a way\nto incorporate prior knowledge into the original reward function in order to\nspeed up the learning. While previous work has investigated the use of expert\nknowledge to generate potential functions, in this work, we study whether we\ncan use a search algorithm(A*) to automatically generate a potential function\nfor reward shaping in Sokoban, a well-known planning task. The results showed\nthat learning with shaped reward function is faster than learning from scratch.\nOur results indicate that distance functions could be a suitable function for\nSokoban. This work demonstrates the possibility of solving multiple instances\nwith the help of reward shaping. The result can be compressed into a single\npolicy, which can be seen as the first phrase towards training a general policy\nthat is able to solve unseen instances.",
          "link": "http://arxiv.org/abs/2109.05022",
          "publishedOn": "2021-09-14T07:20:12.320Z",
          "wordCount": null,
          "title": "Potential-based Reward Shaping in Sokoban. (arXiv:2109.05022v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopalan_P/0/1/0/all/0/1\">Parikshit Gopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1\">Adam Tauman Kalai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reingold_O/0/1/0/all/0/1\">Omer Reingold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharan_V/0/1/0/all/0/1\">Vatsal Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieder_U/0/1/0/all/0/1\">Udi Wieder</a>",
          "description": "Loss minimization is a dominant paradigm in machine learning, where a\npredictor is trained to minimize some loss function that depends on an\nuncertain event (e.g., \"will it rain tomorrow?''). Different loss functions\nimply different learning algorithms and, at times, very different predictors.\nWhile widespread and appealing, a clear drawback of this approach is that the\nloss function may not be known at the time of learning, requiring the algorithm\nto use a best-guess loss function. We suggest a rigorous new paradigm for loss\nminimization in machine learning where the loss function can be ignored at the\ntime of learning and only be taken into account when deciding an action.\n\nWe introduce the notion of an (${\\mathcal{L}},\\mathcal{C}$)-omnipredictor,\nwhich could be used to optimize any loss in a family ${\\mathcal{L}}$. Once the\nloss function is set, the outputs of the predictor can be post-processed (a\nsimple univariate data-independent transformation of individual predictions) to\ndo well compared with any hypothesis from the class $\\mathcal{C}$. The post\nprocessing is essentially what one would perform if the outputs of the\npredictor were true probabilities of the uncertain events. In a sense,\nomnipredictors extract all the predictive power from the class $\\mathcal{C}$,\nirrespective of the loss function in $\\mathcal{L}$.\n\nWe show that such \"loss-oblivious'' learning is feasible through a connection\nto multicalibration, a notion introduced in the context of algorithmic\nfairness. In addition, we show how multicalibration can be viewed as a solution\nconcept for agnostic boosting, shedding new light on past results. Finally, we\ntransfer our insights back to the context of algorithmic fairness by providing\nomnipredictors for multi-group loss minimization.",
          "link": "http://arxiv.org/abs/2109.05389",
          "publishedOn": "2021-09-14T07:20:12.320Z",
          "wordCount": null,
          "title": "Omnipredictors. (arXiv:2109.05389v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Desislavov_R/0/1/0/all/0/1\">Radosvet Desislavov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Plumed_F/0/1/0/all/0/1\">Fernando Mart&#xed;nez-Plumed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1\">Jos&#xe9; Hern&#xe1;ndez-Orallo</a>",
          "description": "The progress of some AI paradigms such as deep learning is said to be linked\nto an exponential growth in the number of parameters. There are many studies\ncorroborating these trends, but does this translate into an exponential\nincrease in energy consumption? In order to answer this question we focus on\ninference costs rather than training costs, as the former account for most of\nthe computing effort, solely because of the multiplicative factors. Also, apart\nfrom algorithmic innovations, we account for more specific and powerful\nhardware (leading to higher FLOPS) that is usually accompanied with important\nenergy efficiency optimisations. We also move the focus from the first\nimplementation of a breakthrough paper towards the consolidated version of the\ntechniques one or two year later. Under this distinctive and comprehensive\nperspective, we study relevant models in the areas of computer vision and\nnatural language processing: for a sustained increase in performance we see a\nmuch softer growth in energy consumption than previously anticipated. The only\ncaveat is, yet again, the multiplicative factor, as future AI increases\npenetration and becomes more pervasive.",
          "link": "http://arxiv.org/abs/2109.05472",
          "publishedOn": "2021-09-14T07:20:12.320Z",
          "wordCount": null,
          "title": "Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>",
          "description": "Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.",
          "link": "http://arxiv.org/abs/2109.05052",
          "publishedOn": "2021-09-14T07:20:12.312Z",
          "wordCount": null,
          "title": "Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05578",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hallgren_F/0/1/0/all/0/1\">Fredrik Hallgren</a>",
          "description": "Kernel methods are powerful but computationally demanding techniques for\nnon-linear learning. A popular remedy, the Nystr\\\"om method has been shown to\nbe able to scale up kernel methods to very large datasets with little loss in\naccuracy. However, kernel PCA with the Nystr\\\"om method has not been widely\nstudied. In this paper we derive kernel PCA with the Nystr\\\"om method and study\nits accuracy, providing a finite-sample confidence bound on the difference\nbetween the Nystr\\\"om and standard empirical reconstruction errors. The\nbehaviours of the method and bound are illustrated through extensive computer\nexperiments on real-world data. As an application of the method we present\nkernel principal component regression with the Nystr\\\"om method.",
          "link": "http://arxiv.org/abs/2109.05578",
          "publishedOn": "2021-09-14T07:20:12.243Z",
          "wordCount": null,
          "title": "Kernel PCA with the Nystr\\\"om method. (arXiv:2109.05578v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05526",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yue-Jie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zai-Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian-Hu/0/1/0/all/0/1\">Jian-Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao-Shen/0/1/0/all/0/1\">Yao-Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>",
          "description": "The fingerprint classification is an important and effective method to\nquicken the process and improve the accuracy in the fingerprint matching\nprocess. Conventional supervised methods need a large amount of pre-labeled\ndata and thus consume immense human resources. In this paper, we propose a new\nand efficient unsupervised deep learning method that can extract fingerprint\nfeatures and classify fingerprint patterns automatically. In this approach, a\nnew model named constraint convolutional auto-encoder (CCAE) is used to extract\nfingerprint features and a hybrid clustering strategy is applied to obtain the\nfinal clusters. A set of experiments in the NIST-DB4 dataset shows that the\nproposed unsupervised method exhibits the efficient performance on fingerprint\nclassification. For example, the CCAE achieves an accuracy of 97.3% on only\n1000 unlabeled fingerprints in the NIST-DB4.",
          "link": "http://arxiv.org/abs/2109.05526",
          "publishedOn": "2021-09-14T07:20:12.153Z",
          "wordCount": null,
          "title": "An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05466",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tekbiyik_K/0/1/0/all/0/1\">K&#xfc;r&#x15f;at Tekb&#x131;y&#x131;k</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurduseven_O/0/1/0/all/0/1\">Okan Yurduseven</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurt_G/0/1/0/all/0/1\">G&#xfc;ne&#x15f; Karabulut Kurt</a>",
          "description": "In this paper, we present a single-pixel compressive direction of arrival\n(DoA) estimation technique leveraging a graph attention network (GAT) based\ndeep-learning framework. The physical layer compression is achieved using a\ncoded-aperture technique, probing the spectrum of far-field sources incident on\nthe aperture using a set of spatio-temporally incoherent modes. This\ninformation is then encoded and compressed into the channel of the\ncoded-aperture. The coded-aperture based receiver exhibits a single-channel,\nreplacing the conventional multichannel raster scan based solutions for DoA\nestimation. The GAT network enables the compressive DoA estimation framework to\nlearn the DoA information directly from the measurements acquired using the\ncoded-aperture. This step eliminates the need for an additional reconstruction\nstep and significantly simplifies the processing layer to obtain the DoA\nestimate. We show that the presented GAT integrated single-pixel radar\nframework can retrieve high fidelity DoA information even under relatively low\nsignal-to-noise ratio (SNR) levels.",
          "link": "http://arxiv.org/abs/2109.05466",
          "publishedOn": "2021-09-14T07:20:12.135Z",
          "wordCount": null,
          "title": "Graph Attention Network Based Single-Pixel Compressive Direction of Arrival Estimation. (arXiv:2109.05466v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05391",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>, <a href=\"http://arxiv.org/find/math/1/au:+Byun_J/0/1/0/all/0/1\">Ji-Eun Byun</a>",
          "description": "Gradients and subgradients are central to optimization and sensitivity\nanalysis of buffered failure probabilities. We furnish a characterization of\nsubgradients based on subdifferential calculus in the case of finite\nprobability distributions and, under additional assumptions, also a gradient\nexpression for general distributions. Several examples illustrate the\napplication of the results, especially in the context of optimality conditions.",
          "link": "http://arxiv.org/abs/2109.05391",
          "publishedOn": "2021-09-14T07:20:12.125Z",
          "wordCount": null,
          "title": "Gradients and Subgradients of Buffered Failure Probability. (arXiv:2109.05391v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ushio_A/0/1/0/all/0/1\">Asahi Ushio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liberatore_F/0/1/0/all/0/1\">Federico Liberatore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1\">Jose Camacho-Collados</a>",
          "description": "Term weighting schemes are widely used in Natural Language Processing and\nInformation Retrieval. In particular, term weighting is the basis for keyword\nextraction. However, there are relatively few evaluation studies that shed\nlight about the strengths and shortcomings of each weighting scheme. In fact,\nin most cases researchers and practitioners resort to the well-known tf-idf as\ndefault, despite the existence of other suitable alternatives, including\ngraph-based models. In this paper, we perform an exhaustive and large-scale\nempirical comparison of both statistical and graph-based term weighting methods\nin the context of keyword extraction. Our analysis reveals some interesting\nfindings such as the advantages of the less-known lexical specificity with\nrespect to tf-idf, or the qualitative differences between statistical and\ngraph-based methods. Finally, based on our findings we discuss and devise some\nsuggestions for practitioners. Source code to reproduce our experimental\nresults, including a keyword extraction library, are available in the following\nrepository: https://github.com/asahi417/kex",
          "link": "http://arxiv.org/abs/2104.08028",
          "publishedOn": "2021-09-14T07:20:12.119Z",
          "wordCount": null,
          "title": "Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction. (arXiv:2104.08028v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casanova_A/0/1/0/all/0/1\">Arantxa Casanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Careil_M/0/1/0/all/0/1\">Marl&#xe8;ne Careil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdzal_M/0/1/0/all/0/1\">Michal Drozdzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1\">Adriana Romero-Soriano</a>",
          "description": "Generative Adversarial Networks (GANs) can generate near photo realistic\nimages in narrow domains such as human faces. Yet, modeling complex\ndistributions of datasets such as ImageNet and COCO-Stuff remains challenging\nin unconditional settings. In this paper, we take inspiration from kernel\ndensity estimation techniques and introduce a non-parametric approach to\nmodeling distributions of complex datasets. We partition the data manifold into\na mixture of overlapping neighborhoods described by a datapoint and its nearest\nneighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),\nwhich learns the distribution around each datapoint. Experimental results on\nImageNet and COCO-Stuff show that IC-GAN significantly improves over\nunconditional models and unsupervised data partitioning baselines. Moreover, we\nshow that IC-GAN can effortlessly transfer to datasets not seen during training\nby simply changing the conditioning instances, and still generate realistic\nimages. Finally, we extend IC-GAN to the class-conditional case and show\nsemantically controllable generation and competitive quantitative results on\nImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our\ncode and trained models to reproduce the reported results.",
          "link": "http://arxiv.org/abs/2109.05070",
          "publishedOn": "2021-09-14T07:20:12.116Z",
          "wordCount": null,
          "title": "Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05159",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiarun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1\">Chuan Sun</a>",
          "description": "With the development of deep learning, medical image classification has been\nsignificantly improved. However, deep learning requires massive data with\nlabels. While labeling the samples by human experts is expensive and\ntime-consuming, collecting labels from crowd-sourcing suffers from the noises\nwhich may degenerate the accuracy of classifiers. Therefore, approaches that\ncan effectively handle label noises are highly desired. Unfortunately, recent\nprogress on handling label noise in deep learning has gone largely unnoticed by\nthe medical image. To fill the gap, this paper proposes a noise-tolerant\nmedical image classification framework named Co-Correcting, which significantly\nimproves classification accuracy and obtains more accurate labels through\ndual-network mutual learning, label probability estimation, and curriculum\nlabel correcting. On two representative medical image datasets and the MNIST\ndataset, we test six latest Learning-with-Noisy-Labels methods and conduct\ncomparative studies. The experiments show that Co-Correcting achieves the best\naccuracy and generalization under different noise ratios in various tasks. Our\nproject can be found at: https://github.com/JiarunLiu/Co-Correcting.",
          "link": "http://arxiv.org/abs/2109.05159",
          "publishedOn": "2021-09-14T07:20:12.114Z",
          "wordCount": null,
          "title": "Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.04886",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasoulas_G/0/1/0/all/0/1\">George Dasoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaman_K/0/1/0/all/0/1\">Kevin Scaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virmaux_A/0/1/0/all/0/1\">Aladin Virmaux</a>",
          "description": "Attention based neural networks are state of the art in a large range of\napplications. However, their performance tends to degrade when the number of\nlayers increases. In this work, we show that enforcing Lipschitz continuity by\nnormalizing the attention scores can significantly improve the performance of\ndeep attention models. First, we show that, for deep graph attention networks\n(GAT), gradient explosion appears during training, leading to poor performance\nof gradient-based training algorithms. To address this issue, we derive a\ntheoretical analysis of the Lipschitz continuity of attention modules and\nintroduce LipschitzNorm, a simple and parameter-free normalization for\nself-attention mechanisms that enforces the model to be Lipschitz continuous.\nWe then apply LipschitzNorm to GAT and Graph Transformers and show that their\nperformance is substantially improved in the deep setting (10 to 30 layers).\nMore specifically, we show that a deep GAT model with LipschitzNorm achieves\nstate of the art results for node label prediction tasks that exhibit\nlong-range dependencies, while showing consistent improvements over their\nunnormalized counterparts in benchmark node classification tasks.",
          "link": "http://arxiv.org/abs/2103.04886",
          "publishedOn": "2021-09-14T07:20:11.968Z",
          "wordCount": null,
          "title": "Lipschitz Normalization for Self-Attention Layers with Application to Graph Neural Networks. (arXiv:2103.04886v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruining He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravula_A/0/1/0/all/0/1\">Anirudh Ravula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanagal_B/0/1/0/all/0/1\">Bhargav Kanagal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>",
          "description": "Transformer is the backbone of modern NLP models. In this paper, we propose\nRealFormer, a simple and generic technique to create Residual Attention Layer\nTransformer networks that significantly outperform the canonical Transformer\nand its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked\nLanguage Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA,\nNatural Questions, and OpenKP. We also observe empirically that RealFormer\nstabilizes training and leads to models with sparser attention. Source code and\npre-trained checkpoints for RealFormer can be found at\nhttps://github.com/google-research/google-research/tree/master/realformer.",
          "link": "http://arxiv.org/abs/2012.11747",
          "publishedOn": "2021-09-14T07:20:11.935Z",
          "wordCount": null,
          "title": "RealFormer: Transformer Likes Residual Attention. (arXiv:2012.11747v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amadio_F/0/1/0/all/0/1\">Fabio Amadio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgado_Guerrero_J/0/1/0/all/0/1\">Juan Antonio Delgado-Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colome_A/0/1/0/all/0/1\">Adri&#xe0; Colom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torras_C/0/1/0/all/0/1\">Carme Torras</a>",
          "description": "Over the last years, robotic cloth manipulation has gained relevance within\nthe research community. While significant advances have been made in robotic\nmanipulation of rigid objects, the manipulation of non-rigid objects such as\ncloth garments is still a challenging problem. The uncertainty on how cloth\nbehaves often requires the use of model-based approaches. However, cloth models\nhave a very high dimensionality. Therefore, it is difficult to find a middle\npoint between providing a manipulator with a dynamics model of cloth and\nworking with a state space of tractable dimensionality. For this reason, most\ncloth manipulation approaches in literature perform static or quasi-static\nmanipulation. In this paper, we propose a variation of Gaussian Process\nDynamical Models (GPDMs) to model cloth dynamics in a low-dimensional manifold.\nGPDMs project a high-dimensional state space into a smaller dimension latent\nspace which is capable of keeping the dynamic properties. Using such approach,\nwe add control variables to the original formulation. In this way, it is\npossible to take into account the robot commands exerted on the cloth dynamics.\nWe call this new version Controlled Gaussian Process Dynamical Model (CGPDM).\nMoreover, we propose an alternative parametric structure for the model, that is\nricher than the one employed in previous GPDM realizations. The modeling\ncapacity of our proposal has been tested in both a simulated and a real\nscenario, where CGPDM proved to be capable of generalizing over a wide range of\nmovements and correctly predicting the cloth motions obtained by previously\nunseen sequences of control actions.",
          "link": "http://arxiv.org/abs/2103.06615",
          "publishedOn": "2021-09-14T07:20:11.885Z",
          "wordCount": null,
          "title": "Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillen_S/0/1/0/all/0/1\">Sean Gillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozmen_A/0/1/0/all/0/1\">Asutay Ozmen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byl_K/0/1/0/all/0/1\">Katie Byl</a>",
          "description": "Researchers have demonstrated that Deep Reinforcement Learning (DRL) is a\npowerful tool for finding policies that perform well on complex robotic\nsystems. However, these policies are often unpredictable and can induce highly\nvariable behavior when evaluated with only slightly different initial\nconditions. Training considerations constrain DRL algorithm designs in that\nmost algorithms must use stochastic policies during training. The resulting\npolicy used during deployment, however, can and frequently is a deterministic\none that uses the Maximum Likelihood Action (MLA) at each step. In this work,\nwe show that a direct random search is very effective at fine-tuning DRL\npolicies by directly optimizing them using deterministic rollouts. We\nillustrate this across a large collection of reinforcement learning\nenvironments, using a wide variety of policies obtained from different\nalgorithms. Our results show that this method yields more consistent and higher\nperforming agents on the environments we tested. Furthermore, we demonstrate\nhow this method can be used to extend our previous work on shrinking the\ndimensionality of the reachable state space of closed-loop systems run under\nDeep Neural Network (DNN) policies.",
          "link": "http://arxiv.org/abs/2109.05604",
          "publishedOn": "2021-09-14T07:20:11.554Z",
          "wordCount": null,
          "title": "Direct Random Search for Fine Tuning of Deep Reinforcement Learning Policies. (arXiv:2109.05604v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.06304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Siavoshani_M/0/1/0/all/0/1\">Mahdi Jafari Siavoshani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khajepour_A/0/1/0/all/0/1\">Amir Hossein Khajepour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziaei_A/0/1/0/all/0/1\">Amirmohammad Ziaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatmiri_A/0/1/0/all/0/1\">Amir Ali Gatmiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_A/0/1/0/all/0/1\">Ali Taheri</a>",
          "description": "Protecting users' privacy over the Internet is of great importance; however,\nit becomes harder and harder to maintain due to the increasing complexity of\nnetwork protocols and components. Therefore, investigating and understanding\nhow data is leaked from the information transmission platforms and protocols\ncan lead us to a more secure environment.\n\nIn this paper, we propose a framework to systematically find the most\nvulnerable information fields in a network protocol. To this end, focusing on\nthe transport layer security (TLS) protocol, we perform different\nmachine-learning-based fingerprinting attacks on the collected data from more\nthan 70 domains (websites) to understand how and where this information leakage\noccurs in the TLS protocol. Then, by employing the interpretation techniques\ndeveloped in the machine learning community and applying our framework, we find\nthe most vulnerable information fields in the TLS protocol. Our findings\ndemonstrate that the TLS handshake (which is mainly unencrypted), the TLS\nrecord length appearing in the TLS application data header, and the\ninitialization vector (IV) field are among the most critical leaker parts in\nthis protocol, respectively.",
          "link": "http://arxiv.org/abs/2011.06304",
          "publishedOn": "2021-09-14T07:20:11.413Z",
          "wordCount": null,
          "title": "Machine Learning Interpretability Meets TLS Fingerprinting. (arXiv:2011.06304v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1\">Kun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhixia Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongpo Xu</a>",
          "description": "Adaptive gradient algorithm (AdaGrad) and its variants, such as RMSProp,\nAdam, AMSGrad, etc, have been widely used in deep learning. Although these\nalgorithms are faster in the early phase of training, their generalization\nperformance is often not as good as stochastic gradient descent (SGD). Hence, a\ntrade-off method of transforming Adam to SGD after a certain iteration to gain\nthe merits of both algorithms is theoretically and practically significant. To\nthat end, we propose a decreasing scaling transition scheme to achieve a smooth\nand stable transition from Adam to SGD, which is called DSTAdam. The\nconvergence of the proposed DSTAdam is also proved in an online convex setting.\nFinally, the effectiveness of the DSTAdam is verified on the CIFAR-10/100\ndatasets. Our implementation is available at:\nhttps://github.com/kunzeng/DSTAdam.",
          "link": "http://arxiv.org/abs/2106.06749",
          "publishedOn": "2021-09-14T07:20:10.603Z",
          "wordCount": 595,
          "title": "A decreasing scaling transition scheme from Adam to SGD. (arXiv:2106.06749v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Seong Jin Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myoung Ho Kim</a>",
          "description": "Link prediction is one of the key problems for graph-structured data. With\nthe advancement of graph neural networks, graph autoencoders (GAEs) and\nvariational graph autoencoders (VGAEs) have been proposed to learn graph\nembeddings in an unsupervised way. It has been shown that these methods are\neffective for link prediction tasks. However, they do not work well in link\npredictions when a node whose degree is zero (i.g., isolated node) is involved.\nWe have found that GAEs/VGAEs make embeddings of isolated nodes close to zero\nregardless of their content features. In this paper, we propose a novel\nVariational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization\nto derive better embeddings for isolated nodes. We show that our VGNAEs\noutperform the existing state-of-the-art models for link prediction tasks. The\ncode is available at https://github.com/SeongJinAhn/VGNAE.",
          "link": "http://arxiv.org/abs/2108.08046",
          "publishedOn": "2021-09-14T07:20:10.595Z",
          "wordCount": 592,
          "title": "Variational Graph Normalized Auto-Encoders. (arXiv:2108.08046v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.06905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goswami_S/0/1/0/all/0/1\">Somdatta Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Minglang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Karniadakis</a>",
          "description": "Failure trajectories, identifying the probable failure zones, and damage\nstatistics are some of the key quantities of relevance in brittle fracture\napplications. High-fidelity numerical solvers that reliably estimate these\nrelevant quantities exist but they are computationally demanding requiring a\nhigh resolution of the crack. Moreover, independent intensive simulations need\nto be carried out even for a small change in domain parameters and/or material\nproperties. Therefore, fast and generalizable surrogate models are needed to\nalleviate the computational burden but the discontinuous nature of fracture\nmechanics presents a major challenge to developing such models. We propose a\nphysics-informed variational formulation of DeepONet (V-DeepONet) for brittle\nfracture analysis. V-DeepONet is trained to map the initial configuration of\nthe defect to the relevant fields of interests (e.g., damage and displacement\nfields). Once the network is trained, the entire global solution can be rapidly\nobtained for any initial crack configuration and loading steps on that domain.\nWhile the original DeepONet is solely data-driven, we take a different path to\ntrain the V-DeepONet by imposing the governing equations in variational form\nand we also use some labelled data. We demonstrate the effectiveness of\nV-DeepOnet through two benchmarks of brittle fracture, and we verify its\naccuracy using results from high-fidelity solvers. Encoding the physical laws\nand also some data to train the network renders the surrogate model capable of\naccurately performing both interpolation and extrapolation tasks, considering\nthat fracture modeling is very sensitive to fluctuations. The proposed hybrid\ntraining of V-DeepONet is superior to state-of-the-art methods and can be\napplied to a wide array of dynamical systems with complex responses.",
          "link": "http://arxiv.org/abs/2108.06905",
          "publishedOn": "2021-09-14T07:20:10.589Z",
          "wordCount": 744,
          "title": "A physics-informed variational DeepONet for predicting the crack path in brittle materials. (arXiv:2108.06905v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05629",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_O/0/1/0/all/0/1\">Oscar Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holter_S/0/1/0/all/0/1\">Steffen Holter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertini_E/0/1/0/all/0/1\">Enrico Bertini</a>",
          "description": "Rapid improvements in the performance of machine learning models have pushed\nthem to the forefront of data-driven decision-making. Meanwhile, the increased\nintegration of these models into various application domains has further\nhighlighted the need for greater interpretability and transparency. To identify\nproblems such as bias, overfitting, and incorrect correlations, data scientists\nrequire tools that explain the mechanisms with which these model decisions are\nmade. In this paper we introduce AdViCE, a visual analytics tool that aims to\nguide users in black-box model debugging and validation. The solution rests on\ntwo main visual user interface innovations: (1) an interactive visualization\ndesign that enables the comparison of decisions on user-defined data subsets;\n(2) an algorithm and visual design to compute and visualize counterfactual\nexplanations - explanations that depict model outcomes when data features are\nperturbed from their original values. We provide a demonstration of the tool\nthrough a use case that showcases the capabilities and potential limitations of\nthe proposed approach.",
          "link": "http://arxiv.org/abs/2109.05629",
          "publishedOn": "2021-09-14T07:20:10.440Z",
          "wordCount": 629,
          "title": "AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation. (arXiv:2109.05629v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1\">Pieter Floris Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenniger_G/0/1/0/all/0/1\">Gideon Maillette de Buy Wenniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>",
          "description": "Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art NLP\nmodels. Here, we present an empirical study that compares different\nuncertainty-based algorithms with BERT$_{base}$ as the used classifier. We\nevaluate the algorithms on two NLP classification datasets: Stanford Sentiment\nTreebank and KvK-Frontpages. Additionally, we explore heuristics that aim to\nsolve presupposed problems of uncertainty-based AL; namely, that it is\nunscalable and that it is prone to selecting outliers. Furthermore, we explore\nthe influence of the query-pool size on the performance of AL. Whereas it was\nfound that the proposed heuristics for AL did not improve performance of AL;\nour results show that using uncertainty-based AL with BERT$_{base}$ outperforms\nrandom sampling of data. This difference in performance can decrease as the\nquery-pool size gets larger.",
          "link": "http://arxiv.org/abs/2109.04847",
          "publishedOn": "2021-09-13T07:20:35.255Z",
          "wordCount": 650,
          "title": "Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>",
          "description": "The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.",
          "link": "http://arxiv.org/abs/2010.09697",
          "publishedOn": "2021-09-13T07:20:35.225Z",
          "wordCount": 712,
          "title": "Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>",
          "description": "Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.",
          "link": "http://arxiv.org/abs/2104.05845",
          "publishedOn": "2021-09-13T07:20:35.169Z",
          "wordCount": 616,
          "title": "Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04912",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lees_A/0/1/0/all/0/1\">Alyssa Lees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>",
          "description": "We present ReasonBert, a pre-training method that augments language models\nwith the ability to reason over long-range relations and multiple, possibly\nhybrid contexts. Unlike existing pre-training methods that only harvest\nlearning signals from local contexts of naturally occurring texts, we propose a\ngeneralized notion of distant supervision to automatically connect multiple\npieces of text and tables to create pre-training examples that require\nlong-range reasoning. Different types of reasoning are simulated, including\nintersecting multiple pieces of evidence, bridging from one piece of evidence\nto another, and detecting unanswerable cases. We conduct a comprehensive\nevaluation on a variety of extractive question answering datasets ranging from\nsingle-hop to multi-hop and from text-only to table-only to hybrid that require\nvarious reasoning capabilities and show that ReasonBert achieves remarkable\nimprovement over an array of strong baselines. Few-shot experiments further\ndemonstrate that our pre-training method substantially improves sample\nefficiency.",
          "link": "http://arxiv.org/abs/2109.04912",
          "publishedOn": "2021-09-13T07:20:35.055Z",
          "wordCount": 609,
          "title": "ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Siddiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapasset_L/0/1/0/all/0/1\">Laurent Lapasset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mothe_J/0/1/0/all/0/1\">Josiane Mothe</a>",
          "description": "An aircraft conflict occurs when two or more aircraft cross at a certain\ndistance at the same time. Specific air traffic controllers are assigned to\nsolve such conflicts. A controller needs to consider various types of\ninformation in order to solve a conflict. The most common and preliminary\ninformation is the coordinate position of the involved aircraft. Additionally,\na controller has to take into account more information such as flight planning,\nweather, restricted territory, etc. The most important challenges a controller\nhas to face are: to think about the issues involved and make a decision in a\nvery short time. Due to the increased number of aircraft, it is crucial to\nreduce the workload of the controllers and help them make quick decisions. A\nconflict can be solved in many ways, therefore, we consider this problem as a\nmulti-label classification problem. In doing so, we are proposing a multi-label\nclassification model which provides multiple heading advisories for a given\nconflict. This model we named CRMLnet is based on a novel application of a\nmulti-layer neural network and helps the controllers in their decisions. When\ncompared to other machine learning models, our CRMLnet has achieved the best\nresults with an accuracy of 98.72% and ROC of 0.999. The simulated data set\nthat we have developed and used in our experiments will be delivered to the\nresearch community.",
          "link": "http://arxiv.org/abs/2109.04767",
          "publishedOn": "2021-09-13T07:20:35.026Z",
          "wordCount": 678,
          "title": "Multi-label Classification of Aircraft Heading Changes Using Neural Network to Resolve Conflicts. (arXiv:2109.04767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Traditional minwise hashing (MinHash) requires applying $K$ independent\npermutations to estimate the Jaccard similarity in massive binary (0/1) data,\nwhere $K$ can be (e.g.,) 1024 or even larger, depending on applications. The\nrecent work on C-MinHash (Li and Li, 2021) has shown, with rigorous proofs,\nthat only two permutations are needed. An initial permutation is applied to\nbreak whatever structures which might exist in the data, and a second\npermutation is re-used $K$ times to produce $K$ hashes, via a circulant\nshifting fashion. (Li and Li, 2021) has proved that, perhaps surprisingly, even\nthough the $K$ hashes are correlated, the estimation variance is strictly\nsmaller than the variance of the traditional MinHash.\n\nIt has been demonstrated in (Li and Li, 2021) that the initial permutation in\nC-MinHash is indeed necessary. For the ease of theoretical analysis, they have\nused two independent permutations. In this paper, we show that one can actually\nsimply use one permutation. That is, one single permutation is used for both\nthe initial pre-processing step to break the structures in the data and the\ncirculant hashing step to generate $K$ hashes. Although the theoretical\nanalysis becomes very complicated, we are able to explicitly write down the\nexpression for the expectation of the estimator. The new estimator is no longer\nunbiased but the bias is extremely small and has essentially no impact on the\nestimation accuracy (mean square errors). An extensive set of experiments are\nprovided to verify our claim for using just one permutation.",
          "link": "http://arxiv.org/abs/2109.04595",
          "publishedOn": "2021-09-13T07:20:34.900Z",
          "wordCount": 690,
          "title": "C-MinHash: Practically Reducing Two Permutations to Just One. (arXiv:2109.04595v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Recovering texture information from the aliasing regions has always been a\nmajor challenge for Single Image Super Resolution (SISR) task. These regions\nare often submerged in noise so that we have to restore texture details while\nsuppressing noise. To address this issue, we propose a Balanced Attention\nMechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and\nMaxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to\nsuppress extreme noise in the large scale feature maps while MSAM preserves\nhigh-frequency texture details. Thanks to the parallel structure, these two\nmodules not only conduct self-optimization, but also mutual optimization to\nobtain the balance of noise reduction and high-frequency texture restoration\nduring the back propagation process, and the parallel structure makes the\ninference faster. To verify the effectiveness and robustness of BAM, we applied\nit to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently\nimprove the networks performance, and for those originally with attention\nmechanism, the substitution with BAM further reduces the amount of parameters\nand increases the inference speed. Moreover, we present a dataset with rich\ntexture aliasing regions in real scenes, named realSR7. Experiments prove that\nBAM achieves better super-resolution results on the aliasing area.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-09-13T07:20:34.891Z",
          "wordCount": 689,
          "title": "BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.03495",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Hinde_O/0/1/0/all/0/1\">&#xd3;scar Garc&#xed;a-Hinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Verdejo_V/0/1/0/all/0/1\">Vanessa G&#xf3;mez-Verdejo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Ramon_M/0/1/0/all/0/1\">Manel Mart&#xed;nez-Ram&#xf3;n</a>",
          "description": "Multitask Gaussian processes (MTGP) are the Gaussian process (GP) framework's\nsolution for multioutput regression problems in which the $T$ elements of the\nregressors cannot be considered conditionally independent given the\nobservations. Standard MTGP models assume that there exist both a multitask\ncovariance matrix as a function of an intertask matrix, and a noise covariance\nmatrix. These matrices need to be approximated by a low rank simplification of\norder $P$ in order to reduce the number of parameters to be learnt from $T^2$\nto $TP$. Here we introduce a novel approach that simplifies the multitask\nlearning by reducing it to a set of conditioned univariate GPs without the need\nfor any low rank approximations, therefore completely eliminating the\nrequirement to select an adequate value for hyperparameter $P$. At the same\ntime, by extending this approach with both a hierarchical and an approximate\nmodel, the proposed extensions are capable of recovering the multitask\ncovariance and noise matrices after learning only $2T$ parameters, avoiding the\nvalidation of any model hyperparameter and reducing the overall complexity of\nthe model as well as the risk of overfitting. Experimental results over\nsynthetic and real problems confirm the advantages of this inference approach\nin its ability to accurately recover the original noise and signal matrices, as\nwell as the achieved performance improvement in comparison to other state of\nart MTGP approaches. We have also integrated the model with standard GP\ntoolboxes, showing that it is computationally competitive with state of the art\noptions.",
          "link": "http://arxiv.org/abs/2006.03495",
          "publishedOn": "2021-09-13T07:20:34.500Z",
          "wordCount": 736,
          "title": "A conditional one-output likelihood formulation for multitask Gaussian processes. (arXiv:2006.03495v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04925",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiren Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1\">Ilia Shumailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusi_N/0/1/0/all/0/1\">Nicolo Fusi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullins_R/0/1/0/all/0/1\">Robert Mullins</a>",
          "description": "Network Architecture Search (NAS) methods have recently gathered much\nattention. They design networks with better performance and use a much shorter\nsearch time compared to traditional manual tuning. Despite their efficiency in\nmodel deployments, most NAS algorithms target a single task on a fixed hardware\nsystem. However, real-life few-shot learning environments often cover a great\nnumber of tasks (T ) and deployments on a wide variety of hardware platforms (H\n).\n\nThe combinatorial search complexity T times H creates a fundamental search\nefficiency challenge if one naively applies existing NAS methods to these\nscenarios. To overcome this issue, we show, for the first time, how to rapidly\nadapt model architectures to new tasks in a many-task many-hardware few-shot\nlearning setup by integrating Model Agnostic Meta Learning (MAML) into the NAS\nflow. The proposed NAS method (H-Meta-NAS) is hardware-aware and performs\noptimisation in the MAML framework. H-Meta-NAS shows a Pareto dominance\ncompared to a variety of NAS and manual baselines in popular few-shot learning\nbenchmarks with various hardware platforms and constraints. In particular, on\nthe 5-way 1-shot Mini-ImageNet classification task, the proposed method\noutperforms the best manual baseline by a large margin (5.21% in accuracy)\nusing 60% less computation.",
          "link": "http://arxiv.org/abs/2109.04925",
          "publishedOn": "2021-09-13T07:20:34.466Z",
          "wordCount": 634,
          "title": "Rapid Model Architecture Adaption for Meta-Learning. (arXiv:2109.04925v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demotte_P/0/1/0/all/0/1\">Piyumal Demotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>",
          "description": "Text classification systems based on contextual embeddings are not viable\noptions for many of the low resource languages. On the other hand, recently\nintroduced capsule networks have shown performance in par with these text\nclassification models. Thus, they could be considered as a viable alternative\nfor text classification for languages that do not have pre-trained contextual\nembedding models. However, current capsule networks depend upon spatial\npatterns without considering the sequential features of the text. They are also\nsub-optimal in capturing the context-level information in longer sequences.\nThis paper presents a novel Dual-State Capsule (DS-Caps) network-based\ntechnique for text classification, which is optimized to mitigate these issues.\nTwo varieties of states, namely sentence-level and word-level, are integrated\nwith capsule layers to capture deeper context-level information for language\nmodeling. The dynamic routing process among capsules was also optimized using\nthe context-level information obtained through sentence-level states. The\nDS-Caps networks outperform the existing capsule network architectures for\nmultiple datasets, particularly for tasks with longer sequences of text. We\nalso demonstrate the superiority of DS-Caps in text classification for a low\nresource language.",
          "link": "http://arxiv.org/abs/2109.04762",
          "publishedOn": "2021-09-13T07:20:34.454Z",
          "wordCount": 622,
          "title": "Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heunchul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jaeseong Jeong</a>",
          "description": "A multi-agent deep reinforcement learning (MADRL) is a promising approach to\nchallenging problems in wireless environments involving multiple\ndecision-makers (or actors) with high-dimensional continuous action space. In\nthis paper, we present a MADRL-based approach that can jointly optimize\nprecoders to achieve the outer-boundary, called pareto-boundary, of the\nachievable rate region for a multiple-input single-output (MISO) interference\nchannel (IFC). In order to address two main challenges, namely, multiple actors\n(or agents) with partial observability and multi-dimensional continuous action\nspace in MISO IFC setup, we adopt a multi-agent deep deterministic policy\ngradient (MA-DDPG) framework in which decentralized actors with partial\nobservability can learn a multi-dimensional continuous policy in a centralized\nmanner with the aid of shared critic with global information. Meanwhile, we\nwill also address a phase ambiguity issue with the conventional complex\nbaseband representation of signals widely used in radio communications. In\norder to mitigate the impact of phase ambiguity on training performance, we\npropose a training method, called phase ambiguity elimination (PAE), that leads\nto faster learning and better performance of MA-DDPG in wireless communication\nsystems. The simulation results exhibit that MA-DDPG is capable of learning a\nnear-optimal precoding strategy in a MISO IFC environment. To the best of our\nknowledge, this is the first work to demonstrate that the MA-DDPG framework can\njointly optimize precoders to achieve the pareto-boundary of achievable rate\nregion in a multi-cell multi-user multi-antenna system.",
          "link": "http://arxiv.org/abs/2109.04986",
          "publishedOn": "2021-09-13T07:20:33.977Z",
          "wordCount": 719,
          "title": "Multi-agent deep reinforcement learning (MADRL) meets multi-user MIMO systems. (arXiv:2109.04986v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariev_S/0/1/0/all/0/1\">Sherzod Kariev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abduraufov_O/0/1/0/all/0/1\">Otabek Abduraufov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajili_M/0/1/0/all/0/1\">Mammad Hajili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaytbaev_A/0/1/0/all/0/1\">Abror Khaytbaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzodbek Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_E/0/1/0/all/0/1\">Esra Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>",
          "description": "Recent advances in neural machine translation (NMT) have pushed the quality\nof machine translation systems to the point where they are becoming widely\nadopted to build competitive systems. However, there is still a large number of\nlanguages that are yet to reap the benefits of NMT. In this paper, we provide\nthe first large-scale case study of the practical application of MT in the\nTurkic language family in order to realize the gains of NMT for Turkic\nlanguages under high-resource to extremely low-resource scenarios. In addition\nto presenting an extensive analysis that identifies the bottlenecks towards\nbuilding competitive systems to ameliorate data scarcity, our study has several\nkey contributions, including, i) a large parallel corpus covering 22 Turkic\nlanguages consisting of common public datasets in combination with new datasets\nof approximately 2 million parallel sentences, ii) bilingual baselines for 26\nlanguage pairs, iii) novel high-quality test sets in three different\ntranslation domains and iv) human evaluation scores. All models, scripts, and\ndata will be released to the public.",
          "link": "http://arxiv.org/abs/2109.04593",
          "publishedOn": "2021-09-13T07:20:33.743Z",
          "wordCount": 653,
          "title": "A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yuchen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoef_T/0/1/0/all/0/1\">Tessa Verhoef</a>",
          "description": "Natural languages display a trade-off among different strategies to convey\nsyntactic structure, such as word order or inflection. This trade-off, however,\nhas not appeared in recent simulations of iterated language learning with\nneural network agents (Chaabouni et al., 2019b). We re-evaluate this result in\nlight of three factors that play an important role in comparable experiments\nfrom the Language Evolution field: (i) speaker bias towards efficient\nmessaging, (ii) non systematic input languages, and (iii) learning bottleneck.\nOur simulations show that neural agents mainly strive to maintain the utterance\ntype distribution observed during learning, instead of developing a more\nefficient or systematic language.",
          "link": "http://arxiv.org/abs/2104.07637",
          "publishedOn": "2021-09-13T07:20:33.628Z",
          "wordCount": 591,
          "title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1\">Julian Suk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haan_P/0/1/0/all/0/1\">Pim de Haan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lippe_P/0/1/0/all/0/1\">Phillip Lippe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1\">Christoph Brune</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1\">Jelmer M. Wolterink</a>",
          "description": "Computational fluid dynamics (CFD) is a valuable tool for personalised,\nnon-invasive evaluation of hemodynamics in arteries, but its complexity and\ntime-consuming nature prohibit large-scale use in practice. Recently, the use\nof deep learning for rapid estimation of CFD parameters like wall shear stress\n(WSS) on surface meshes has been investigated. However, existing approaches\ntypically depend on a hand-crafted re-parametrisation of the surface mesh to\nmatch convolutional neural network architectures. In this work, we propose to\ninstead use mesh convolutional neural networks that directly operate on the\nsame finite-element surface mesh as used in CFD. We train and evaluate our\nmethod on two datasets of synthetic coronary artery models with and without\nbifurcation, using a ground truth obtained from CFD simulation. We show that\nour flexible deep learning model can accurately predict 3D WSS vectors on this\nsurface mesh. Our method processes new meshes in less than 5 [s], consistently\nachieves a normalised mean absolute error of $\\leq$ 1.6 [%], and peaks at 90.5\n[%] median approximation accuracy over the held-out test set, comparing\nfavorably to previously published work. This shows the feasibility of CFD\nsurrogate modelling using mesh convolutional neural networks for hemodynamic\nparameter estimation in artery models.",
          "link": "http://arxiv.org/abs/2109.04797",
          "publishedOn": "2021-09-13T07:20:33.512Z",
          "wordCount": 676,
          "title": "Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zongyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baohua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guoqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Longyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>",
          "description": "Anomaly detection plays a crucial role in various real-world applications,\nincluding healthcare and finance systems. Owing to the limited number of\nanomaly labels in these complex systems, unsupervised anomaly detection methods\nhave attracted great attention in recent years. Two major challenges faced by\nthe existing unsupervised methods are: (i) distinguishing between normal and\nabnormal data in the transition field, where normal and abnormal data are\nhighly mixed together; (ii) defining an effective metric to maximize the gap\nbetween normal and abnormal data in a hypothesis space, which is built by a\nrepresentation learner. To that end, this work proposes a novel scoring network\nwith a score-guided regularization to learn and enlarge the anomaly score\ndisparities between normal and abnormal data. With such score-guided strategy,\nthe representation learner can gradually learn more informative representation\nduring the model training stage, especially for the samples in the transition\nfield. We next propose a score-guided autoencoder (SG-AE), incorporating the\nscoring network into an autoencoder framework for anomaly detection, as well as\nother three state-of-the-art models, to further demonstrate the effectiveness\nand transferability of the design. Extensive experiments on both synthetic and\nreal-world datasets demonstrate the state-of-the-art performance of these\nscore-guided models (SGMs).",
          "link": "http://arxiv.org/abs/2109.04684",
          "publishedOn": "2021-09-13T07:20:33.486Z",
          "wordCount": 642,
          "title": "Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saha_G/0/1/0/all/0/1\">Gobinda Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>",
          "description": "Artificial learning systems aspire to mimic human intelligence by continually\nlearning from a stream of tasks without forgetting past knowledge. One way to\nenable such learning is to store past experiences in the form of input examples\nin episodic memory and replay them when learning new tasks. However,\nperformance of such method suffers as the size of the memory becomes smaller.\nIn this paper, we propose a new approach for experience replay, where we select\nthe past experiences by looking at the saliency maps which provide visual\nexplanations for the model's decision. Guided by these saliency maps, we pack\nthe memory with only the parts or patches of the input images important for the\nmodel's prediction. While learning a new task, we replay these memory patches\nwith appropriate zero-padding to remind the model about its past decisions. We\nevaluate our algorithm on diverse image classification datasets and report\nbetter performance than the state-of-the-art approaches. With qualitative and\nquantitative analyses we show that our method captures richer summary of past\nexperiences without any memory increase, and hence performs well with small\nepisodic memory.",
          "link": "http://arxiv.org/abs/2109.04954",
          "publishedOn": "2021-09-13T07:20:33.430Z",
          "wordCount": 631,
          "title": "Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04640",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhengling Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1\">Raymond K.W. Wong</a>",
          "description": "Offline policy evaluation (OPE) is considered a fundamental and challenging\nproblem in reinforcement learning (RL). This paper focuses on the value\nestimation of a target policy based on pre-collected data generated from a\npossibly different policy, under the framework of infinite-horizon Markov\ndecision processes. Motivated by the recently developed marginal importance\nsampling method in RL and the covariate balancing idea in causal inference, we\npropose a novel estimator with approximately projected state-action balancing\nweights for the policy value estimation. We obtain the convergence rate of\nthese weights, and show that the proposed value estimator is semi-parametric\nefficient under technical conditions. In terms of asymptotics, our results\nscale with both the number of trajectories and the number of decision points at\neach trajectory. As such, consistency can still be achieved with a limited\nnumber of subjects when the number of decision points diverges. In addition, we\nmake a first attempt towards characterizing the difficulty of OPE problems,\nwhich may be of independent interest. Numerical experiments demonstrate the\npromising performance of our proposed estimator.",
          "link": "http://arxiv.org/abs/2109.04640",
          "publishedOn": "2021-09-13T07:20:33.363Z",
          "wordCount": 614,
          "title": "Projected State-action Balancing Weights for Offline Reinforcement Learning. (arXiv:2109.04640v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00794",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sahito_A/0/1/0/all/0/1\">Attaullah Sahito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_E/0/1/0/all/0/1\">Eibe Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfahringer_B/0/1/0/all/0/1\">Bernhard Pfahringer</a>",
          "description": "Neural networks have been successfully used as classification models yielding\nstate-of-the-art results when trained on a large number of labeled samples.\nThese models, however, are more difficult to train successfully for\nsemi-supervised problems where small amounts of labeled instances are available\nalong with a large number of unlabeled instances. This work explores a new\ntraining method for semi-supervised learning that is based on similarity\nfunction learning using a Siamese network to obtain a suitable embedding. The\nlearned representations are discriminative in Euclidean space, and hence can be\nused for labeling unlabeled instances using a nearest-neighbor classifier.\nConfident predictions of unlabeled instances are used as true labels for\nretraining the Siamese network on the expanded training set. This process is\napplied iteratively. We perform an empirical study of this iterative\nself-training algorithm. For improving unlabeled predictions, local learning\nwith global consistency [22] is also evaluated.",
          "link": "http://arxiv.org/abs/2109.00794",
          "publishedOn": "2021-09-13T07:20:33.092Z",
          "wordCount": null,
          "title": "Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jifeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fanqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Junteng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>",
          "description": "Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a\n3D solid volume, exhibits advantages in numerous application domains. However,\nexisting methods generally synthesize solid texture with specific features,\nwhich may result in the failure of capturing diversified textural information.\nIn this paper, we propose a novel generative adversarial nets-based approach\n(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our\nmulti-scale discriminators evaluate the similarity between patch from exemplar\nand slice from the generated volume, promoting the generator to synthesize\nrealistic solid textures. Experimental results demonstrate that the proposed\nmethod can generate high-quality solid textures with similar visual\ncharacteristics to the exemplar.",
          "link": "http://arxiv.org/abs/2102.03973",
          "publishedOn": "2021-09-13T07:20:33.090Z",
          "wordCount": null,
          "title": "Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sandnes_A/0/1/0/all/0/1\">Anders T. Sandnes</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Grimstad_B/0/1/0/all/0/1\">Bjarne Grimstad</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Kolbjornsen_O/0/1/0/all/0/1\">Odd Kolbj&#xf8;rnsen</a> (2) ((1) Solution Seeker AS, (2) Department of Mathematics, University of Oslo, (3) Department of Engineering Cybernetics, Norwegian University of Science and Technology)",
          "description": "Virtual flow metering (VFM) is a cost-effective and non-intrusive technology\nfor inferring multiphase flow rates in petroleum assets. Inferences about flow\nrates are fundamental to decision support systems that operators extensively\nrely on. Data-driven VFM, where mechanistic models are replaced with machine\nlearning models, has recently gained attention due to its promise of lower\nmaintenance costs. While excellent performances in small sample studies have\nbeen reported in the literature, there is still considerable doubt about the\nrobustness of data-driven VFM. In this paper, we propose a new multi-task\nlearning (MTL) architecture for data-driven VFM. Our method differs from\nprevious methods in that it enables learning across oil and gas wells. We study\nthe method by modeling 55 wells from four petroleum assets and compare the\nresults with two single-task baseline models. Our findings show that MTL\nimproves robustness over single-task methods, without sacrificing performance.\nMTL yields a 25-50% error reduction on average for the assets where single-task\narchitectures are struggling.",
          "link": "http://arxiv.org/abs/2103.08713",
          "publishedOn": "2021-09-13T07:20:28.991Z",
          "wordCount": 655,
          "title": "Multi-task learning for virtual flow metering. (arXiv:2103.08713v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rojas_J/0/1/0/all/0/1\">Junior Rojas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifakis_E/0/1/0/all/0/1\">Eftychios Sifakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavan_L/0/1/0/all/0/1\">Ladislav Kavan</a>",
          "description": "We present a differentiable soft-body physics simulator that can be composed\nwith neural networks as a differentiable layer. In contrast to other\ndifferentiable physics approaches that use explicit forward models to define\nstate transitions, we focus on implicit state transitions defined via function\nminimization. Implicit state transitions appear in implicit numerical\nintegration methods, which offer the benefits of large time steps and excellent\nnumerical stability, but require a special treatment to achieve\ndifferentiability due to the absence of an explicit differentiable forward\npass. In contrast to other implicit differentiation approaches that require\nexplicit formulas for the force function and the force Jacobian matrix, we\npresent an energy-based approach that allows us to compute these derivatives\nautomatically and in a matrix-free fashion via reverse-mode automatic\ndifferentiation. This allows for more flexibility and productivity when\ndefining physical models and is particularly important in the context of neural\nnetwork training, which often relies on reverse-mode automatic differentiation\n(backpropagation). We demonstrate the effectiveness of our differentiable\nsimulator in policy optimization for locomotion tasks and show that it achieves\nbetter sample efficiency than model-free reinforcement learning.",
          "link": "http://arxiv.org/abs/2102.05791",
          "publishedOn": "2021-09-13T07:20:28.947Z",
          "wordCount": 647,
          "title": "Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Codifying commonsense knowledge in machines is a longstanding goal of\nartificial intelligence. Recently, much progress toward this goal has been made\nwith automatic knowledge base (KB) construction techniques. However, such\ntechniques focus primarily on the acquisition of positive (true) KB statements,\neven though negative (false) statements are often also important for\ndiscriminative reasoning over commonsense KBs. As a first step toward the\nlatter, this paper proposes NegatER, a framework that ranks potential negatives\nin commonsense KBs using a contextual language model (LM). Importantly, as most\nKBs do not contain negatives, NegatER relies only on the positive knowledge in\nthe LM and does not require ground-truth negative examples. Experiments\ndemonstrate that, compared to multiple contrastive data augmentation\napproaches, NegatER yields negatives that are more grammatical, coherent, and\ninformative -- leading to statistically significant accuracy improvements in a\nchallenging KB completion task and confirming that the positive knowledge in\nLMs can be \"re-purposed\" to generate negative knowledge.",
          "link": "http://arxiv.org/abs/2011.07497",
          "publishedOn": "2021-09-13T07:20:28.916Z",
          "wordCount": 632,
          "title": "NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.15413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naive formulations often\ndegrade performance and in particular, identifying the tasks that would benefit\nfrom co-training remains a challenging design question. In this paper, we\nanalyze the dynamics of information transfer, or transference, across tasks\nthroughout training. Specifically, we develop a similarity measure that can\nquantify transference among tasks and use this quantity to both better\nunderstand the optimization dynamics of multi-task learning as well as improve\noverall learning performance. In the latter case, we propose two methods to\nleverage our transference metric. The first operates at a macro-level by\nselecting which tasks should train together while the second functions at a\nmicro-level by determining how to combine task gradients at each training step.\nWe find these methods can lead to significant improvement over prior work on\nthree supervised multi-task learning benchmarks and one multi-task\nreinforcement learning paradigm.",
          "link": "http://arxiv.org/abs/2010.15413",
          "publishedOn": "2021-09-13T07:20:28.678Z",
          "wordCount": 652,
          "title": "Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manias_D/0/1/0/all/0/1\">Dimitrios Michael Manias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1\">Abdallah Shami</a>",
          "description": "As the number of Internet of Things (IoT) devices and systems have surged,\nIoT data analytics techniques have been developed to detect malicious\ncyber-attacks and secure IoT systems; however, concept drift issues often occur\nin IoT data analytics, as IoT data is often dynamic data streams that change\nover time, causing model degradation and attack detection failure. This is\nbecause traditional data analytics models are static models that cannot adapt\nto data distribution changes. In this paper, we propose a Performance Weighted\nProbability Averaging Ensemble (PWPAE) framework for drift adaptive IoT anomaly\ndetection through IoT data stream analytics. Experiments on two public datasets\nshow the effectiveness of our proposed PWPAE method compared against\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05013",
          "publishedOn": "2021-09-13T07:20:28.072Z",
          "wordCount": 598,
          "title": "PWPAE: An Ensemble Framework for Concept Drift Adaptation in IoT Data Streams. (arXiv:2109.05013v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cintrano_C/0/1/0/all/0/1\">Christian Cintrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_E/0/1/0/all/0/1\">Enrique Alba</a>",
          "description": "This article presents the problem of locating electric vehicle (EV) charging\nstations in a city by defining the Electric Vehicle Charging Stations Locations\n(EV-CSL) problem. The idea is to minimize the distance the citizens have to\ntravel to charge their vehicles. EV-CSL takes into account the maximum number\nof charging stations to install and the electric power requirements. Two\nmetaheuristics are applied to address the relying optimization problem: a\ngenetic algorithm (GA) and a variable neighborhood search (VNS). The\nexperimental analysis over a realistic scenario of Malaga city, Spain, shows\nthat the metaheuristics are able to find competitive solutions which\ndramatically improve the actual installation of the stations in Malaga. GA\nprovided statistically the best results.",
          "link": "http://arxiv.org/abs/2109.04975",
          "publishedOn": "2021-09-13T07:20:27.910Z",
          "wordCount": 591,
          "title": "Citizen centric optimal electric vehicle charging stations locations in a full city: case of Malaga. (arXiv:2109.04975v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.",
          "link": "http://arxiv.org/abs/2108.07971",
          "publishedOn": "2021-09-13T07:20:27.816Z",
          "wordCount": 570,
          "title": "De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1\">Philipp Henzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>",
          "description": "We learn a latent space for easy capture, consistent interpolation, and\nefficient reproduction of visual material appearance. When users provide a\nphoto of a stationary natural material captured under flashlight illumination,\nfirst it is converted into a latent material code. Then, in the second step,\nconditioned on the material code, our method produces an infinite and diverse\nspatial field of BRDF model parameters (diffuse albedo, normals, roughness,\nspecular albedo) that subsequently allows rendering in complex scenes and\nilluminations, matching the appearance of the input photograph. Technically, we\njointly embed all flash images into a latent space using a convolutional\nencoder, and -- conditioned on these latent codes -- convert random spatial\nfields into fields of BRDF parameters using a convolutional neural network\n(CNN). We condition these BRDF parameters to match the visual characteristics\n(statistics and spectra of visual features) of the input under matching light.\nA user study compares our approach favorably to previous work, even those with\naccess to BRDF supervision.",
          "link": "http://arxiv.org/abs/2102.11861",
          "publishedOn": "2021-09-13T07:20:27.513Z",
          "wordCount": 641,
          "title": "Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>",
          "description": "We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.",
          "link": "http://arxiv.org/abs/2104.06022",
          "publishedOn": "2021-09-13T07:20:27.496Z",
          "wordCount": 569,
          "title": "Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09485",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunlu_O/0/1/0/all/0/1\">Onur G&#xfc;nl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_M/0/1/0/all/0/1\">Matthieu Bloch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1\">Rafael F. Schaefer</a>",
          "description": "We consider a distributed function computation problem in which parties\nobserving noisy versions of a remote source facilitate the computation of a\nfunction of their observations at a fusion center through public communication.\nThe distributed function computation is subject to constraints, including not\nonly reliability and storage but also privacy and secrecy. Specifically, 1) the\nremote source should remain private from an eavesdropper and the fusion center,\nmeasured in terms of the information leaked about the remote source; 2) the\nfunction computed should remain secret from the eavesdropper, measured in terms\nof the information leaked about the arguments of the function, to ensure\nsecrecy regardless of the exact function used. We derive the exact rate regions\nfor lossless and lossy single-function computation and illustrate the lossy\nsingle-function computation rate region for an information bottleneck example,\nin which the optimal auxiliary random variables are characterized for\nbinary-input symmetric-output channels. We extend the approach to lossless and\nlossy asynchronous multiple-function computations with joint secrecy and\nprivacy constraints, in which case inner and outer bounds for the rate regions\ndiffering only in the Markov chain conditions imposed are characterized.",
          "link": "http://arxiv.org/abs/2106.09485",
          "publishedOn": "2021-09-13T07:20:27.485Z",
          "wordCount": 698,
          "title": "Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendeville_A/0/1/0/all/0/1\">Antoine Vendeville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shi Zhou</a>",
          "description": "We explore a method to influence or even control the diversity of opinions\nwithin a polarised social group. We leverage the voter model in which users\nhold binary opinions and repeatedly update their beliefs based on others they\nconnect with. Stubborn agents who never change their minds (\"zealots\") are also\ndisseminated through the network, which is modelled by a connected graph.\nBuilding on earlier results, we provide a closed-form expression for the\naverage opinion of the group at equilibrium. This leads us to a strategy to\ninject zealots into a polarised network in order to shift the average opinion\ntowards any target value. We account for the possible presence of a backfire\neffect, which may lead the group to react negatively and reinforce its level of\npolarisation in response. Our results are supported by numerical experiments on\nsynthetic data.",
          "link": "http://arxiv.org/abs/2006.07265",
          "publishedOn": "2021-09-13T07:20:27.261Z",
          "wordCount": 654,
          "title": "Towards control of opinion diversity by introducing zealots into a polarised social group. (arXiv:2006.07265v5 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09532",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Marcondes_D/0/1/0/all/0/1\">Diego Marcondes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simonis_A/0/1/0/all/0/1\">Adilson Simonis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrera_J/0/1/0/all/0/1\">Junior Barrera</a>",
          "description": "This paper presents an extension of the classical agnostic PAC learning model\nin which learning problems are modelled not only by a Hypothesis Space\n$\\mathcal{H}$, but also by a Learning Space $\\mathbb{L}(\\mathcal{H})$, which is\na cover of $\\mathcal{H}$, constrained by a VC-dimension property, that is a\nsuitable domain for Model Selection algorithms. Our main contribution is a data\ndriven general learning algorithm to perform regularized Model Selection on\n$\\mathbb{L}(\\mathcal{H})$. A remarkable, formally proved, consequence of this\napproach are conditions on $\\mathbb{L}(\\mathcal{H})$ and on the loss function\nthat lead to estimated out-of-sample error surfaces which are true U-curves on\n$\\mathbb{L}(\\mathcal{H})$ chains, enabling a more efficient search on\n$\\mathbb{L}(\\mathcal{H})$. To our knowledge, this is the first rigorous result\nasserting that a non exhaustive search of a family of candidate models can\nreturn an optimal solution. In this new framework, an U-curve optimization\nalgorithm becomes a natural component of Model Selection, hence of learning\nalgorithms. The abstract general framework proposed here may have important\nimplications on modern learning models and on areas such as Neural Architecture\nSearch.",
          "link": "http://arxiv.org/abs/2001.09532",
          "publishedOn": "2021-09-13T07:20:27.234Z",
          "wordCount": 672,
          "title": "Learning the Hypotheses Space from data: Learning Space and U-curve Property. (arXiv:2001.09532v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarrahi_A/0/1/0/all/0/1\">Ali Jarrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safari_L/0/1/0/all/0/1\">Leila Safari</a>",
          "description": "In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.",
          "link": "http://arxiv.org/abs/2109.04835",
          "publishedOn": "2021-09-13T07:20:27.083Z",
          "wordCount": 637,
          "title": "FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1\">Debasrita Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ashish Ghosh</a>",
          "description": "Binary change detection in bi-temporal co-registered hyperspectral images is\na challenging task due to a large number of spectral bands present in the data.\nResearchers, therefore, try to handle it by reducing dimensions. The proposed\nwork aims to build a novel feature extraction system using a feature fusion\ndeep convolutional autoencoder for detecting changes between a pair of such\nbi-temporal co-registered hyperspectral images. The feature fusion considers\nfeatures across successive levels and multiple receptive fields and therefore\nadds a competitive edge over the existing feature extraction methods. The\nchange detection technique described is completely unsupervised and is much\nmore elegant than other supervised or semi-supervised methods which require\nsome amount of label information. Different methods have been applied to the\nextracted features to find the changes in the two images and it is found that\nthe proposed method clearly outperformed the state of the art methods in\nunsupervised change detection for all the datasets.",
          "link": "http://arxiv.org/abs/2109.04990",
          "publishedOn": "2021-09-13T07:20:27.077Z",
          "wordCount": 610,
          "title": "Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>",
          "description": "Motivated by suggested question generation in conversational news\nrecommendation systems, we propose a model for generating question-answer pairs\n(QA pairs) with self-contained, summary-centric questions and\nlength-constrained, article-summarizing answers. We begin by collecting a new\ndataset of news articles with questions as titles and pairing them with\nsummaries of varying length. This dataset is used to learn a QA pair generation\nmodel producing summaries as answers that balance brevity with sufficiency\njointly with their corresponding questions. We then reinforce the QA pair\ngeneration process with a differentiable reward function to mitigate exposure\nbias, a common problem in natural language generation. Both automatic metrics\nand human evaluation demonstrate these QA pairs successfully capture the\ncentral gists of the articles and achieve high answer accuracy.",
          "link": "http://arxiv.org/abs/2109.04689",
          "publishedOn": "2021-09-13T07:20:27.018Z",
          "wordCount": 586,
          "title": "Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04660",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jangho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jayeon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yeji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">KiYoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "With the growth of deep neural networks (DNN), the number of DNN parameters\nhas drastically increased. This makes DNN models hard to be deployed on\nresource-limited embedded systems. To alleviate this problem, dynamic pruning\nmethods have emerged, which try to find diverse sparsity patterns during\ntraining by utilizing Straight-Through-Estimator (STE) to approximate gradients\nof pruned weights. STE can help the pruned weights revive in the process of\nfinding dynamic sparsity patterns. However, using these coarse gradients causes\ntraining instability and performance degradation owing to the unreliable\ngradient signal of the STE approximation. In this work, to tackle this issue,\nwe introduce refined gradients to update the pruned weights by forming dual\nforwarding paths from two sets (pruned and unpruned) of weights. We propose a\nnovel Dynamic Collective Intelligence Learning (DCIL) which makes use of the\nlearning synergy between the collective intelligence of both weight sets. We\nverify the usefulness of the refined gradients by showing enhancements in the\ntraining stability and the model performance on the CIFAR and ImageNet\ndatasets. DCIL outperforms various previously proposed pruning schemes\nincluding other dynamic pruning methods with enhanced stability during\ntraining.",
          "link": "http://arxiv.org/abs/2109.04660",
          "publishedOn": "2021-09-13T07:20:26.894Z",
          "wordCount": 644,
          "title": "Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "The transformer-based pre-trained language models have been tremendously\nsuccessful in most of the conventional NLP tasks. But they often struggle in\nthose tasks where numerical understanding is required. Some possible reasons\ncan be the tokenizers and pre-training objectives which are not specifically\ndesigned to learn and preserve numeracy. Here we investigate the ability of\ntext-to-text transfer learning model (T5), which has outperformed its\npredecessors in the conventional NLP tasks, to learn numeracy. We consider four\nnumeracy tasks: numeration, magnitude order prediction, finding minimum and\nmaximum in a series, and sorting. We find that, although T5 models perform\nreasonably well in the interpolation setting, they struggle considerably in the\nextrapolation setting across all four tasks.",
          "link": "http://arxiv.org/abs/2109.04672",
          "publishedOn": "2021-09-13T07:20:26.888Z",
          "wordCount": 576,
          "title": "Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "We study the optimization version of the set partition problem (where the\ndifference between the partition sums are minimized), which has numerous\napplications in decision theory literature. While the set partitioning problem\nis NP-hard and requires exponential complexity to solve (i.e., intractable); we\nformulate a weaker version of this NP-hard problem, where the goal is to find a\nlocally optimal solution. We show that our proposed algorithms can find a\nlocally optimal solution in near linear time. Our algorithms require neither\npositive nor integer elements in the input set, hence, they are more widely\napplicable.",
          "link": "http://arxiv.org/abs/2109.04809",
          "publishedOn": "2021-09-13T07:20:26.880Z",
          "wordCount": 551,
          "title": "Efficient Locally Optimal Number Set Partitioning for Scheduling, Allocation and Fair Selection. (arXiv:2109.04809v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quillen_C/0/1/0/all/0/1\">Carl Quillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dushyant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goderre_A/0/1/0/all/0/1\">Andrew Goderre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lainez_J/0/1/0/all/0/1\">Jos&#xe9; La&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanovic_L/0/1/0/all/0/1\">Ljubomir Milanovi&#x107;</a>",
          "description": "When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech\nRecognition (ASR) backend shows promising results. Recent literature has shown\ntraditional beamformer designs, such as MVDR (Minimum Variance Distortionless\nResponse) or fixed beamformers can be successfully integrated as the frontend\ninto an E2E ASR system with learnable parameters. In this work, we propose the\nself-attention channel combinator (SACC) ASR frontend, which leverages the\nself-attention mechanism to combine multichannel audio signals in the magnitude\nspectral domain. Experiments conducted on a multichannel playback test data\nshows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed\nbeamformer-based frontend, both jointly optimized with a ContextNet-based ASR\nbackend. We also demonstrate the connection between the SACC and the\ntraditional beamformers, and analyze the intermediate outputs of the SACC.",
          "link": "http://arxiv.org/abs/2109.04783",
          "publishedOn": "2021-09-13T07:20:26.864Z",
          "wordCount": 606,
          "title": "Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1\">Ilias Diakonikolas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jongho Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1\">Christos Tzamos</a>",
          "description": "We study the fundamental problem of ReLU regression, where the goal is to fit\nRectified Linear Units (ReLUs) to data. This supervised learning task is\nefficiently solvable in the realizable setting, but is known to be\ncomputationally hard with adversarial label noise. In this work, we focus on\nReLU regression in the Massart noise model, a natural and well-studied\nsemi-random noise model. In this model, the label of every point is generated\naccording to a function in the class, but an adversary is allowed to change\nthis value arbitrarily with some probability, which is {\\em at most} $\\eta <\n1/2$. We develop an efficient algorithm that achieves exact parameter recovery\nin this model under mild anti-concentration assumptions on the underlying\ndistribution. Such assumptions are necessary for exact recovery to be\ninformation-theoretically possible. We demonstrate that our algorithm\nsignificantly outperforms naive applications of $\\ell_1$ and $\\ell_2$\nregression on both synthetic and real data.",
          "link": "http://arxiv.org/abs/2109.04623",
          "publishedOn": "2021-09-13T07:20:26.859Z",
          "wordCount": 595,
          "title": "ReLU Regression with Massart Noise. (arXiv:2109.04623v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04738",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mosel_J/0/1/0/all/0/1\">Julian von der Mosel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trautsch_A/0/1/0/all/0/1\">Alexander Trautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herbold_S/0/1/0/all/0/1\">Steffen Herbold</a>",
          "description": "Transformers are the current state-of-the-art of natural language processing\nin many domains and are using traction within software engineering research as\nwell. Such models are pre-trained on large amounts of data, usually from the\ngeneral domain. However, we only have a limited understanding regarding the\nvalidity of transformers within the software engineering domain, i.e., how good\nsuch models are at understanding words and sentences within a software\nengineering context and how this improves the state-of-the-art. Within this\narticle, we shed light on this complex, but crucial issue. We compare BERT\ntransformer models trained with software engineering data with transformers\nbased on general domain data in multiple dimensions: their vocabulary, their\nability to understand which words are missing, and their performance in\nclassification tasks. Our results show that for tasks that require\nunderstanding of the software engineering context, pre-training with software\nengineering data is valuable, while general domain models are sufficient for\ngeneral language understanding, also within the software engineering domain.",
          "link": "http://arxiv.org/abs/2109.04738",
          "publishedOn": "2021-09-13T07:20:26.794Z",
          "wordCount": 621,
          "title": "On the validity of pre-trained transformers for natural language processing in the software engineering domain. (arXiv:2109.04738v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04561",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tu_L/0/1/0/all/0/1\">Liyun Tu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Talbot_A/0/1/0/all/0/1\">Austin Talbot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gallagher_N/0/1/0/all/0/1\">Neil Gallagher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carlson_D/0/1/0/all/0/1\">David Carlson</a>",
          "description": "Probabilistic generative models are attractive for scientific modeling\nbecause their inferred parameters can be used to generate hypotheses and design\nexperiments. This requires that the learned model provide an accurate\nrepresentation of the input data and yield a latent space that effectively\npredicts outcomes relevant to the scientific question. Supervised Variational\nAutoencoders (SVAEs) have previously been used for this purpose, where a\ncarefully designed decoder can be used as an interpretable generative model\nwhile the supervised objective ensures a predictive latent representation.\nUnfortunately, the supervised objective forces the encoder to learn a biased\napproximation to the generative posterior distribution, which renders the\ngenerative parameters unreliable when used in scientific models. This issue has\nremained undetected as reconstruction losses commonly used to evaluate model\nperformance do not detect bias in the encoder. We address this\npreviously-unreported issue by developing a second order supervision framework\n(SOS-VAE) that influences the decoder to induce a predictive latent\nrepresentation. This ensures that the associated encoder maintains a reliable\ngenerative interpretation. We extend this technique to allow the user to\ntrade-off some bias in the generative parameters for improved predictive\nperformance, acting as an intermediate option between SVAEs and our new\nSOS-VAE. We also use this methodology to address missing data issues that often\narise when combining recordings from multiple scientific experiments. We\ndemonstrate the effectiveness of these developments using synthetic data and\nelectrophysiological recordings with an emphasis on how our learned\nrepresentations can be used to design scientific experiments.",
          "link": "http://arxiv.org/abs/2109.04561",
          "publishedOn": "2021-09-13T07:20:26.546Z",
          "wordCount": 693,
          "title": "Supervising the Decoder of Variational Autoencoders to Improve Scientific Utility. (arXiv:2109.04561v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno Miguel Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>",
          "description": "Selective rationalization aims to produce decisions along with rationales\n(e.g., text highlights or word alignments between two sentences). Commonly,\nrationales are modeled as stochastic binary masks, requiring sampling-based\ngradient estimators, which complicates training and requires careful\nhyperparameter tuning. Sparse attention mechanisms are a deterministic\nalternative, but they lack a way to regularize the rationale extraction (e.g.,\nto control the sparsity of a text highlight or the number of alignments). In\nthis paper, we present a unified framework for deterministic extraction of\nstructured explanations via constrained inference on a factor graph, forming a\ndifferentiable layer. Our approach greatly eases training and rationale\nregularization, generally outperforming previous work on what comes to\nperformance and plausibility of the extracted rationales. We further provide a\ncomparative study of stochastic and deterministic methods for rationale\nextraction for classification and natural language inference tasks, jointly\nassessing their predictive power, quality of the explanations, and model\nvariability.",
          "link": "http://arxiv.org/abs/2109.04552",
          "publishedOn": "2021-09-13T07:20:26.220Z",
          "wordCount": 596,
          "title": "SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karra_K/0/1/0/all/0/1\">Kiran Karra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashcraft_C/0/1/0/all/0/1\">Chace Ashcraft</a>",
          "description": "The application of self-supervised methods has resulted in broad improvements\nto neural network performance by leveraging large, untapped collections of\nunlabeled data to learn generalized underlying structure. In this work, we\nharness unsupervised data augmentation (UDA) to mitigate backdoor or Trojan\nattacks on deep neural networks. We show that UDA is more effective at removing\nthe effects of a trigger than current state-of-the-art methods for both feature\nspace and point triggers. These results demonstrate that UDA is both an\neffective and practical approach to mitigating the effects of backdoors on\nneural networks.",
          "link": "http://arxiv.org/abs/2109.04566",
          "publishedOn": "2021-09-13T07:20:26.183Z",
          "wordCount": 533,
          "title": "SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks. (arXiv:2109.04566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongkuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orme_Rogers_J/0/1/0/all/0/1\">James Orme-Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1\">Rajgopal Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1\">Viktor Prasanna</a>",
          "description": "Temporal Knowledge Graphs store events in the form of subjects, relations,\nobjects, and timestamps which are often represented by dynamic heterogeneous\ngraphs. Event forecasting is a critical and challenging task in Temporal\nKnowledge Graph reasoning that predicts the subject or object of an event in\nthe future. To obtain temporal embeddings multi-step away in the future,\nexisting methods learn generative models that capture the joint distribution of\nthe observed events. To reduce the high computation costs, these methods rely\non unrealistic assumptions of independence and approximations in training and\ninference. In this work, we propose SeDyT, a discriminative framework that\nperforms sequence modeling on the dynamic entity embeddings to solve the\nmulti-step event forecasting problem. SeDyT consists of two components: a\nTemporal Graph Neural Network that generates dynamic entity embeddings in the\npast and a sequence model that predicts the entity embeddings in the future.\nCompared with the generative models, SeDyT does not rely on any heuristic-based\nprobability model and has low computation complexity in both training and\ninference. SeDyT is compatible with most Temporal Graph Neural Networks and\nsequence models. We also design an efficient training method that trains the\ntwo components in one gradient descent propagation. We evaluate the performance\nof SeDyT on five popular datasets. By combining temporal Graph Neural Network\nmodels and sequence models, SeDyT achieves an average of 2.4% MRR improvement\nwhen not using the validation set and more than 10% MRR improvement when using\nthe validation set.",
          "link": "http://arxiv.org/abs/2109.04550",
          "publishedOn": "2021-09-13T07:20:26.167Z",
          "wordCount": 703,
          "title": "SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04707",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>",
          "description": "Meta-learning has achieved great success in leveraging the historical learned\nknowledge to facilitate the learning process of the new task. However, merely\nlearning the knowledge from the historical tasks, adopted by current\nmeta-learning algorithms, may not generalize well to testing tasks when they\nare not well-supported by training tasks. This paper studies a low-resource\ntext classification problem and bridges the gap between meta-training and\nmeta-testing tasks by leveraging the external knowledge bases. Specifically, we\npropose KGML to introduce additional representation for each sentence learned\nfrom the extracted sentence-specific knowledge graph. The extensive experiments\non three datasets demonstrate the effectiveness of KGML under both supervised\nadaptation and unsupervised adaptation settings.",
          "link": "http://arxiv.org/abs/2109.04707",
          "publishedOn": "2021-09-13T07:20:25.756Z",
          "wordCount": 556,
          "title": "Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11702",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preuss_M/0/1/0/all/0/1\">Mike Preuss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "Transfer learning can speed up training in machine learning and is regularly\nused in classification tasks. It reuses prior knowledge from other tasks to\npre-train networks for new tasks. In reinforcement learning, learning actions\nfor a behavior policy that can be applied to new environments is still a\nchallenge, especially for tasks that involve much planning. Sokoban is a\nchallenging puzzle game. It has been used widely as a benchmark in\nplanning-based reinforcement learning. In this paper, we show how prior\nknowledge improves learning in Sokoban tasks. We find that reusing feature\nrepresentations learned previously can accelerate learning new, more complex,\ninstances. In effect, we show how curriculum learning, from simple to complex\ntasks, works in Sokoban. Furthermore, feature representations learned in\nsimpler instances are more general, and thus lead to positive transfers towards\nmore complex tasks, but not vice versa. We have also studied which part of the\nknowledge is most important for transfer to succeed, and identify which layers\nshould be used for pre-training.",
          "link": "http://arxiv.org/abs/2105.11702",
          "publishedOn": "2021-09-13T07:20:25.750Z",
          "wordCount": 632,
          "title": "Transfer Learning and Curriculum Learning in Sokoban. (arXiv:2105.11702v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strong_C/0/1/0/all/0/1\">Christopher A. Strong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeljic_A/0/1/0/all/0/1\">Aleksandar Zelji&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Julian_K/0/1/0/all/0/1\">Kyle D. Julian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Guy Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1\">Clark Barrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Neural networks can learn complex, non-convex functions, and it is\nchallenging to guarantee their correct behavior in safety-critical contexts.\nMany approaches exist to find failures in networks (e.g., adversarial\nexamples), but these cannot guarantee the absence of failures. Verification\nalgorithms address this need and provide formal guarantees about a neural\nnetwork by answering \"yes or no\" questions. For example, they can answer\nwhether a violation exists within certain bounds. However, individual \"yes or\nno\" questions cannot answer qualitative questions such as \"what is the largest\nerror within these bounds\"; the answers to these lie in the domain of\noptimization. Therefore, we propose strategies to extend existing verifiers to\nperform optimization and find: (i) the most extreme failure in a given input\nregion and (ii) the minimum input perturbation required to cause a failure. A\nnaive approach using a bisection search with an off-the-shelf verifier results\nin many expensive and overlapping calls to the verifier. Instead, we propose an\napproach that tightly integrates the optimization process into the verification\nprocedure, achieving better runtime performance than the naive approach. We\nevaluate our approach implemented as an extension of Marabou, a\nstate-of-the-art neural network verifier, and compare its performance with the\nbisection approach and MIPVerify, an optimization-based verifier. We observe\ncomplementary performance between our extension of Marabou and MIPVerify.",
          "link": "http://arxiv.org/abs/2010.03258",
          "publishedOn": "2021-09-13T07:20:25.483Z",
          "wordCount": null,
          "title": "Global Optimization of Objective Functions Represented by ReLU Networks. (arXiv:2010.03258v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>",
          "description": "Influence functions approximate the \"influences\" of training data-points for\ntest predictions and have a wide variety of applications. Despite the\npopularity, their computational cost does not scale well with model and\ntraining data size. We present FastIF, a set of simple modifications to\ninfluence functions that significantly improves their run-time. We use\nk-Nearest Neighbors (kNN) to narrow the search space down to a subset of good\ncandidate data points, identify the configurations that best balance the\nspeed-quality trade-off in estimating the inverse Hessian-vector product, and\nintroduce a fast parallel variant. Our proposed method achieves about 80X\nspeedup while being highly correlated with the original influence values. With\nthe availability of the fast influence functions, we demonstrate their\nusefulness in four applications. First, we examine whether influential\ndata-points can \"explain\" test time behavior using the framework of\nsimulatability. Second, we visualize the influence interactions between\ntraining and test data-points. Third, we show that we can correct model errors\nby additional fine-tuning on certain influential data-points, improving the\naccuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we\nexperiment with a similar setup but fine-tuning on datapoints not seen during\ntraining, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI\ndatasets respectively. Overall, our fast influence functions can be efficiently\napplied to large models and datasets, and our experiments demonstrate the\npotential of influence functions in model interpretation and correcting model\nerrors. Code is available at\nhttps://github.com/salesforce/fast-influence-functions",
          "link": "http://arxiv.org/abs/2012.15781",
          "publishedOn": "2021-09-13T07:20:25.470Z",
          "wordCount": null,
          "title": "FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>",
          "description": "Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.",
          "link": "http://arxiv.org/abs/2105.03432",
          "publishedOn": "2021-09-13T07:20:25.469Z",
          "wordCount": null,
          "title": "Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09574",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.",
          "link": "http://arxiv.org/abs/2104.09574",
          "publishedOn": "2021-09-13T07:20:25.468Z",
          "wordCount": null,
          "title": "Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>",
          "description": "Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.",
          "link": "http://arxiv.org/abs/2109.04611",
          "publishedOn": "2021-09-13T07:20:25.463Z",
          "wordCount": null,
          "title": "Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>",
          "description": "Relational knowledge bases (KBs) are commonly used to represent world\nknowledge in machines. However, while advantageous for their high degree of\nprecision and interpretability, KBs are usually organized according to\nmanually-defined schemas, which limit their expressiveness and require\nsignificant human efforts to engineer and maintain. In this review, we take a\nnatural language processing perspective to these limitations, examining how\nthey may be addressed in part by training deep contextual language models (LMs)\nto internalize and express relational knowledge in more flexible forms. We\npropose to organize knowledge representation strategies in LMs by the level of\nKB supervision provided, from no KB supervision at all to entity- and\nrelation-level supervision. Our contributions are threefold: (1) We provide a\nhigh-level, extensible taxonomy for knowledge representation in LMs; (2) Within\nour taxonomy, we highlight notable models, evaluation tasks, and findings, in\norder to provide an up-to-date review of current knowledge representation\ncapabilities in LMs; and (3) We suggest future research directions that build\nupon the complementary aspects of LMs and KBs as knowledge representations.",
          "link": "http://arxiv.org/abs/2104.05837",
          "publishedOn": "2021-09-13T07:20:25.463Z",
          "wordCount": null,
          "title": "Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02618",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lenz_O/0/1/0/all/0/1\">Oliver Urs Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peralta_D/0/1/0/all/0/1\">Daniel Peralta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelis_C/0/1/0/all/0/1\">Chris Cornelis</a>",
          "description": "We provide a thorough treatment of one-class classification with\nhyperparameter optimisation for five data descriptors: Support Vector Machine\n(SVM), Nearest Neighbour Distance (NND), Localised Nearest Neighbour Distance\n(LNND), Local Outlier Factor (LOF) and Average Localised Proximity (ALP). The\nhyperparameters of SVM and LOF have to be optimised through cross-validation,\nwhile NND, LNND and ALP allow an efficient form of leave-one-out validation and\nthe reuse of a single nearest-neighbour query. We experimentally evaluate the\neffect of hyperparameter optimisation with 246 classification problems drawn\nfrom 50 datasets. From a selection of optimisation algorithms, the recent\nMalherbe-Powell proposal optimises the hyperparameters of all data descriptors\nmost efficiently. We calculate the increase in test AUROC and the amount of\noverfitting as a function of the number of hyperparameter evaluations. After 50\nevaluations, ALP and SVM significantly outperform LOF, NND and LNND, and LOF\nand NND outperform LNND. The performance of ALP and SVM is comparable, but ALP\ncan be optimised more efficiently so constitutes a good default choice.\nAlternatively, using validation AUROC as a selection criterion between ALP or\nSVM gives the best overall result, and NND is the least computationally\ndemanding option. We thus end up with a clear trade-off between three choices,\nallowing practitioners to make an informed decision.",
          "link": "http://arxiv.org/abs/2102.02618",
          "publishedOn": "2021-09-13T07:20:25.461Z",
          "wordCount": null,
          "title": "Optimised one-class classification performance. (arXiv:2102.02618v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.16410",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates quality assessment on pseudo labels by (meta) learning from the\nsuccessful and failed attempts on Relation Classification Network as an\nadditional meta-objective. To reduce the influence of noisy pseudo labels,\nMetaSRE adopts a pseudo label selection and exploitation scheme which assesses\npseudo label quality on unlabeled samples and only exploits high-quality pseudo\nlabels in a self-training fashion to incrementally augment labeled samples for\nboth robustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.",
          "link": "http://arxiv.org/abs/2010.16410",
          "publishedOn": "2021-09-13T07:20:25.448Z",
          "wordCount": null,
          "title": "Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04875",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenwei Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bagheri_A/0/1/0/all/0/1\">Ayoub Bagheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heijden_P/0/1/0/all/0/1\">P.G.M van der Heijden</a>",
          "description": "Compositional data are non-negative data collected in a rectangular matrix\nwith a constant row sum. Due to the non-negativity the focus is on conditional\nproportions that add up to 1 for each row. A row of conditional proportions is\ncalled an observed budget. Latent budget analysis (LBA) assumes a mixture of\nlatent budgets that explains the observed budgets. LBA is usually fitted to a\ncontingency table, where the rows are levels of one or more explanatory\nvariables and the columns the levels of a response variable. In prospective\nstudies, there is only knowledge about the explanatory variables of individuals\nand interest goes out to predicting the response variable. Thus, a form of LBA\nis needed that has the functionality of prediction. Previous studies proposed a\nconstrained neural network (NN) extension of LBA that was hampered by an\nunsatisfying prediction ability. Here we propose LBA-NN, a feed forward NN\nmodel that yields a similar interpretation to LBA but equips LBA with a better\nability of prediction. A stable and plausible interpretation of LBA-NN is\nobtained through the use of importance plots and table, that show the relative\nimportance of all explanatory variables on the response variable. An LBA-NN-K-\nmeans approach that applies K-means clustering on the importance table is used\nto produce K clusters that are comparable to K latent budgets in LBA. Here we\nprovide different experiments where LBA-NN is implemented and compared with\nLBA. In our analysis, LBA-NN outperforms LBA in prediction in terms of\naccuracy, specificity, recall and mean square error. We provide open-source\nsoftware at GitHub.",
          "link": "http://arxiv.org/abs/2109.04875",
          "publishedOn": "2021-09-13T07:20:25.447Z",
          "wordCount": null,
          "title": "Neural Networks for Latent Budget Analysis of Compositional Data. (arXiv:2109.04875v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Alexander Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>",
          "description": "The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.",
          "link": "http://arxiv.org/abs/2109.04825",
          "publishedOn": "2021-09-13T07:20:25.439Z",
          "wordCount": null,
          "title": "Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04941",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Samarth Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1\">Gauri Joshi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yagan_O/0/1/0/all/0/1\">Osman Ya&#x11f;an</a>",
          "description": "In this paper we consider the problem of best-arm identification in\nmulti-armed bandits in the fixed confidence setting, where the goal is to\nidentify, with probability $1-\\delta$ for some $\\delta>0$, the arm with the\nhighest mean reward in minimum possible samples from the set of arms\n$\\mathcal{K}$. Most existing best-arm identification algorithms and analyses\noperate under the assumption that the rewards corresponding to different arms\nare independent of each other. We propose a novel correlated bandit framework\nthat captures domain knowledge about correlation between arms in the form of\nupper bounds on expected conditional reward of an arm, given a reward\nrealization from another arm. Our proposed algorithm C-LUCB, which generalizes\nthe LUCB algorithm utilizes this partial knowledge of correlations to sharply\nreduce the sample complexity of best-arm identification. More interestingly, we\nshow that the total samples obtained by C-LUCB are of the form\n$\\mathcal{O}\\left(\\sum_{k \\in \\mathcal{C}}\n\\log\\left(\\frac{1}{\\delta}\\right)\\right)$ as opposed to the typical\n$\\mathcal{O}\\left(\\sum_{k \\in \\mathcal{K}}\n\\log\\left(\\frac{1}{\\delta}\\right)\\right)$ samples required in the independent\nreward setting. The improvement comes, as the $\\mathcal{O}(\\log(1/\\delta))$\nterm is summed only for the set of competitive arms $\\mathcal{C}$, which is a\nsubset of the original set of arms $\\mathcal{K}$. The size of the set\n$\\mathcal{C}$, depending on the problem setting, can be as small as $2$, and\nhence using C-LUCB in the correlated bandits setting can lead to significant\nperformance improvements. Our theoretical findings are supported by experiments\non the Movielens and Goodreads recommendation datasets.",
          "link": "http://arxiv.org/abs/2109.04941",
          "publishedOn": "2021-09-13T07:20:25.438Z",
          "wordCount": null,
          "title": "Best-Arm Identification in Correlated Multi-Armed Bandits. (arXiv:2109.04941v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>",
          "description": "While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.",
          "link": "http://arxiv.org/abs/2109.04699",
          "publishedOn": "2021-09-13T07:20:25.435Z",
          "wordCount": null,
          "title": "EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.01002",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xiang Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1\">Minghua Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Low_S/0/1/0/all/0/1\">Steven H. Low</a>",
          "description": "High percentage penetrations of renewable energy generations introduce\nsignificant uncertainty into power systems. It requires grid operators to solve\nalternative current optimal power flow (AC-OPF) problems more frequently for\neconomical and reliable operation in both transmission and distribution grids.\nIn this paper, we develop a Deep Neural Network (DNN) approach, called DeepOPF,\nfor solving AC-OPF problems in a fraction of the time used by conventional\nsolvers. A key difficulty for applying machine learning techniques for solving\nAC-OPF problems lies in ensuring that the obtained solutions respect the\nequality and inequality physical and operational constraints. Generalized the\n2-stage procedure in [1], [2], DeepOPF first trains a DNN model to predict a\nset of independent operating variables and then directly compute the remaining\ndependable ones by solving power flow equations. Such an approach not only\npreserves the power-flow balance equality constraints but also reduces the\nnumber of variables to predict by the DNN, cutting down the number of neurons\nand training data needed. DeepOPF then employs a penalty approach with a\nzero-order gradient estimation technique in the training process to preserve\nthe remaining inequality constraints. As another contribution, we drive a\ncondition for tuning the size of the DNN according to the desired approximation\naccuracy, which measures the DNN generalization capability. It provides\ntheoretical justification for using DNN to solve the AC-OPF problem. Simulation\nresults of IEEE 30/118/300-bus and a synthetic 2000-bus test cases show that\nDeepOPF speeds up the computing time by up to two orders of magnitude as\ncompared to a state-of-the-art solver, at the expense of $<$0.1% cost\ndifference.",
          "link": "http://arxiv.org/abs/2007.01002",
          "publishedOn": "2021-09-13T07:20:25.434Z",
          "wordCount": null,
          "title": "DeepOPF: A Feasibility-Optimized Deep Neural Network Approach for AC Optimal Power Flow Problems. (arXiv:2007.01002v4 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bridge_C/0/1/0/all/0/1\">Christopher P. Bridge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorman_C/0/1/0/all/0/1\">Chris Gorman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pieper_S/0/1/0/all/0/1\">Steven Pieper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doyle_S/0/1/0/all/0/1\">Sean W. Doyle</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lennerz_J/0/1/0/all/0/1\">Jochen K. Lennerz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clunie_D/0/1/0/all/0/1\">David A. Clunie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fedorov_A/0/1/0/all/0/1\">Andriy Y. Fedorov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herrmann_M/0/1/0/all/0/1\">Markus D. Herrmann</a>",
          "description": "Machine learning is revolutionizing image-based diagnostics in pathology and\nradiology. ML models have shown promising results in research settings, but\ntheir lack of interoperability has been a major barrier for clinical\nintegration and evaluation. The DICOM a standard specifies Information Object\nDefinitions and Services for the representation and communication of digital\nimages and related information, including image-derived annotations and\nanalysis results. However, the complexity of the standard represents an\nobstacle for its adoption in the ML community and creates a need for software\nlibraries and tools that simplify working with data sets in DICOM format. Here\nwe present the highdicom library, which provides a high-level application\nprogramming interface for the Python programming language that abstracts\nlow-level details of the standard and enables encoding and decoding of\nimage-derived information in DICOM format in a few lines of Python code. The\nhighdicom library ties into the extensive Python ecosystem for image processing\nand machine learning. Simultaneously, by simplifying creation and parsing of\nDICOM-compliant files, highdicom achieves interoperability with the medical\nimaging systems that hold the data used to train and run ML models, and\nultimately communicate and store model outputs for clinical use. We demonstrate\nthrough experiments with slide microscopy and computed tomography imaging,\nthat, by bridging these two ecosystems, highdicom enables developers to train\nand evaluate state-of-the-art ML models in pathology and radiology while\nremaining compliant with the DICOM standard and interoperable with clinical\nsystems at all stages. To promote standardization of ML research and streamline\nthe ML model development and deployment process, we made the library available\nfree and open-source.",
          "link": "http://arxiv.org/abs/2106.07806",
          "publishedOn": "2021-09-13T07:20:25.430Z",
          "wordCount": null,
          "title": "Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1\">Ning Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinfeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bingyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rong Zhang</a>",
          "description": "Deep learning models exhibit a preference for statistical fitting over\nlogical reasoning. Spurious correlations might be memorized when there exists\nstatistical bias in training data, which severely limits the model performance\nespecially in small data scenarios. In this work, we introduce Counterfactual\nAdversarial Training framework (CAT) to tackle the problem from a causality\nperspective. Particularly, for a specific sample, CAT first generates a\ncounterfactual representation through latent space interpolation in an\nadversarial manner, and then performs Counterfactual Risk Minimization (CRM) on\neach original-counterfactual pair to adjust sample-wise loss weight\ndynamically, which encourages the model to explore the true causal effect.\nExtensive experiments demonstrate that CAT achieves substantial performance\nimprovement over SOTA across different downstream tasks, including sentence\nclassification, natural language inference and question answering.",
          "link": "http://arxiv.org/abs/2109.04746",
          "publishedOn": "2021-09-13T07:20:25.427Z",
          "wordCount": null,
          "title": "Counterfactual Adversarial Learning with Representation Interpolation. (arXiv:2109.04746v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnaghi_P/0/1/0/all/0/1\">Payam Barnaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>",
          "description": "Federated learning is proposed as an alternative to centralized machine\nlearning since its client-server structure provides better privacy protection\nand scalability in real-world applications. In many applications, such as smart\nhomes with IoT devices, local data on clients are generated from different\nmodalities such as sensory, visual, and audio data. Existing federated learning\nsystems only work on local data from a single modality, which limits the\nscalability of the systems.\n\nIn this paper, we propose a multimodal and semi-supervised federated learning\nframework that trains autoencoders to extract shared or correlated\nrepresentations from different local data modalities on clients. In addition,\nwe propose a multimodal FedAvg algorithm to aggregate local autoencoders\ntrained on different data modalities. We use the learned global autoencoder for\na downstream classification task with the help of auxiliary labelled data on\nthe server. We empirically evaluate our framework on different modalities\nincluding sensory data, depth camera videos, and RGB camera videos. Our\nexperimental results demonstrate that introducing data from multiple modalities\ninto federated learning can improve its accuracy. In addition, we can use\nlabelled data from only one modality for supervised learning on the server and\napply the learned model to testing data from other modalities to achieve decent\naccuracy (e.g., approximately 70% as the best performance), especially when\ncombining contributions from both unimodal clients and multimodal clients.",
          "link": "http://arxiv.org/abs/2109.04833",
          "publishedOn": "2021-09-13T07:20:25.427Z",
          "wordCount": null,
          "title": "Multimodal Federated Learning. (arXiv:2109.04833v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>",
          "description": "Pretraining techniques leveraging enormous datasets have driven recent\nadvances in text summarization. While folk explanations suggest that knowledge\ntransfer accounts for pretraining's benefits, little is known about why it\nworks or what makes a pretraining task or dataset suitable. In this paper, we\nchallenge the knowledge transfer story, showing that pretraining on documents\nconsisting of character n-grams selected at random, we can nearly match the\nperformance of models pretrained on real corpora. This work holds the promise\nof eliminating upstream corpora, which may alleviate some concerns over\noffensive language, bias, and copyright issues. To see whether the small\nresidual benefit of using real data could be accounted for by the structure of\nthe pretraining task, we design several tasks motivated by a qualitative study\nof summarization corpora. However, these tasks confer no appreciable benefit,\nleaving open the possibility of a small role for knowledge transfer.",
          "link": "http://arxiv.org/abs/2109.04953",
          "publishedOn": "2021-09-13T07:20:25.425Z",
          "wordCount": null,
          "title": "Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tianxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>",
          "description": "Knowledge distillation (KD) has gained much attention due to its\neffectiveness in compressing large-scale pre-trained models. In typical KD\nmethods, the small student model is trained to match the soft targets generated\nby the big teacher model. However, the interaction between student and teacher\nis one-way. The teacher is usually fixed once trained, resulting in static soft\ntargets to be distilled. This one-way interaction leads to the teacher's\ninability to perceive the characteristics of the student and its training\nprogress. To address this issue, we propose Interactive Knowledge Distillation\n(IKD), which also allows the teacher to learn to teach from the feedback of the\nstudent. In particular, IKD trains the teacher model to generate specific soft\ntarget at each training step for a certain student. Joint optimization for both\nteacher and student is achieved by two iterative steps: a course step to\noptimize student with the soft target of teacher, and an exam step to optimize\nteacher with the feedback of student. IKD is a general framework that is\northogonal to most existing knowledge distillation methods. Experimental\nresults show that IKD outperforms traditional KD methods on various NLP tasks.",
          "link": "http://arxiv.org/abs/2109.04641",
          "publishedOn": "2021-09-13T07:20:25.424Z",
          "wordCount": null,
          "title": "Learning to Teach with Student Feedback. (arXiv:2109.04641v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.02174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuxin Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>",
          "description": "Recent studies indicate that hierarchical Vision Transformer with a macro\narchitecture of interleaved non-overlapped window-based self-attention \\&\nshifted-window operation is able to achieve state-of-the-art performance in\nvarious visual recognition tasks, and challenges the ubiquitous convolutional\nneural networks (CNNs) using densely slid kernels. Most follow-up works attempt\nto replace the shifted-window operation with other kinds of cross-window\ncommunication paradigms, while treating self-attention as the de-facto standard\nfor window-based information aggregation. In this manuscript, we question\nwhether self-attention is the only choice for hierarchical Vision Transformer\nto attain strong performance, and the effects of different kinds of\ncross-window communication. To this end, we replace self-attention layers with\nembarrassingly simple linear mapping layers, and the resulting proof-of-concept\narchitecture termed as LinMapper can achieve very strong performance in\nImageNet-1k image recognition. Moreover, we find that LinMapper is able to\nbetter leverage the pre-trained representations from image recognition and\ndemonstrates excellent transfer learning properties on downstream dense\nprediction tasks such as object detection and instance segmentation. We also\nexperiment with other alternatives to self-attention for content aggregation\ninside each non-overlapped window under different cross-window communication\napproaches, which all give similar competitive results. Our study reveals that\nthe \\textbf{macro architecture} of Swin model families, other than specific\naggregation layers or specific means of cross-window communication, may be more\nresponsible for its strong performance and is the real challenger to the\nubiquitous CNN's dense sliding window paradigm. Code and models will be\npublicly available to facilitate future research.",
          "link": "http://arxiv.org/abs/2107.02174",
          "publishedOn": "2021-09-13T07:20:25.272Z",
          "wordCount": 720,
          "title": "What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuqi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_W/0/1/0/all/0/1\">Waleed Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tieyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Overwijk_A/0/1/0/all/0/1\">Arnold Overwijk</a>",
          "description": "Dense retrieval requires high-quality text sequence embeddings to support\neffective search in the representation space. Autoencoder-based language models\nare appealing in dense retrieval as they train the encoder to output\nhigh-quality embedding that can reconstruct the input texts. However, in this\npaper, we provide theoretical analyses and show empirically that an autoencoder\nlanguage model with a low reconstruction loss may not provide good sequence\nrepresentations because the decoder may take shortcuts by exploiting language\npatterns. To address this, we propose a new self-learning method that\npre-trains the autoencoder using a \\textit{weak} decoder, with restricted\ncapacity and attention flexibility to push the encoder to provide better text\nrepresentations. Our experiments on web search, news recommendation, and open\ndomain question answering show that our pre-trained model significantly boosts\nthe effectiveness and few-shot ability of dense retrieval models. Our code is\navailable at https://github.com/microsoft/SEED-Encoder/.",
          "link": "http://arxiv.org/abs/2102.09206",
          "publishedOn": "2021-09-13T07:20:25.231Z",
          "wordCount": 629,
          "title": "Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder. (arXiv:2102.09206v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06002",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ok_J/0/1/0/all/0/1\">Jungseul Ok</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1\">Alexandre Proutiere</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "We study the problem of clustering a set of items from binary user feedback.\nSuch a problem arises in crowdsourcing platforms solving large-scale labeling\ntasks with minimal effort put on the users. For example, in some of the recent\nreCAPTCHA systems, users clicks (binary answers) can be used to efficiently\nlabel images. In our inference problem, items are grouped into initially\nunknown non-overlapping clusters. To recover these clusters, the learner\nsequentially presents to users a finite list of items together with a question\nwith a binary answer selected from a fixed finite set. For each of these items,\nthe user provides a noisy answer whose expectation is determined by the item\ncluster and the question and by an item-specific parameter characterizing the\n{\\it hardness} of classifying the item. The objective is to devise an algorithm\nwith a minimal cluster recovery error rate. We derive problem-specific\ninformation-theoretical lower bounds on the error rate satisfied by any\nalgorithm, for both uniform and adaptive (list, question) selection strategies.\nFor uniform selection, we present a simple algorithm built upon the K-means\nalgorithm and whose performance almost matches the fundamental limits. For\nadaptive selection, we develop an adaptive algorithm that is inspired by the\nderivation of the information-theoretical error lower bounds, and in turn\nallocates the budget in an efficient way. The algorithm learns to select items\nhard to cluster and relevant questions more often. We compare the performance\nof our algorithms with or without the adaptive selection strategy numerically\nand illustrate the gain achieved by being adaptive.",
          "link": "http://arxiv.org/abs/1910.06002",
          "publishedOn": "2021-09-13T07:20:25.185Z",
          "wordCount": 718,
          "title": "Optimal Clustering from Noisy Binary Feedback. (arXiv:1910.06002v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chee_K/0/1/0/all/0/1\">Kong Yao Chee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>",
          "description": "In this work, we consider the problem of deriving and incorporating accurate\ndynamic models for model predictive control (MPC) with an application to\nquadrotor control. MPC relies on precise dynamic models to achieve the desired\nclosed-loop performance. However, the presence of uncertainties in complex\nsystems and the environments they operate in poses a challenge in obtaining\nsufficiently accurate representations of the system dynamics. In this work, we\nmake use of a deep learning tool, knowledge-based neural ordinary differential\nequations (KNODE), to augment a model obtained from first principles. The\nresulting hybrid model encompasses both a nominal first-principle model and a\nneural network learnt from simulated or real-world experimental data. Using a\nquadrotor, we benchmark our hybrid model against a state-of-the-art Gaussian\nProcess (GP) model and show that the hybrid model provides more accurate\npredictions of the quadrotor dynamics and is able to generalize beyond the\ntraining data. To improve closed-loop performance, the hybrid model is\nintegrated into a novel MPC framework, known as KNODE-MPC. Results show that\nthe integrated framework achieves 73% improvement in simulations and more than\n14% in physical experiments, in terms of trajectory tracking performance.",
          "link": "http://arxiv.org/abs/2109.04821",
          "publishedOn": "2021-09-13T07:20:25.112Z",
          "wordCount": 674,
          "title": "KNODE-MPC: A Knowledge-based Data-driven Predictive Control Framework for Aerial Robots. (arXiv:2109.04821v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1\">Gene Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wai-tian Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>",
          "description": "Algorithm unfolding creates an interpretable and parsimonious neural network\narchitecture by implementing each iteration of a model-based algorithm as a\nneural layer. However, unfolding a proximal splitting algorithm with a positive\nsemi-definite (PSD) cone projection operator per iteration is expensive, due to\nthe required full matrix eigen-decomposition. In this paper, leveraging a\nrecent linear algebraic theorem called Gershgorin disc perfect alignment\n(GDPA), we unroll a projection-free algorithm for semi-definite programming\nrelaxation (SDR) of a binary graph classifier, where the PSD cone constraint is\nreplaced by a set of \"tightest possible\" linear constraints per iteration. As a\nresult, each iteration only requires computing a linear program (LP) and one\nextreme eigenvector. Inside the unrolled network, we optimize parameters via\nstochastic gradient descent (SGD) that determine graph edge weights in two\nways: i) a metric matrix that computes feature distances, and ii) a sparse\nweight matrix computed via local linear embedding (LLE). Experimental results\nshow that our unrolled network outperformed pure model-based graph classifiers,\nand achieved comparable performance to pure data-driven networks but using far\nfewer parameters.",
          "link": "http://arxiv.org/abs/2109.04697",
          "publishedOn": "2021-09-13T07:20:25.067Z",
          "wordCount": 620,
          "title": "Unfolding Projection-free SDP Relaxation of Binary Graph Classifier via GDPA Linearization. (arXiv:2109.04697v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>",
          "description": "The ability to quickly learn new knowledge (e.g. new classes or data\ndistributions) is a big step towards human-level intelligence. In this paper,\nwe consider scenarios that require learning new classes or data distributions\nquickly and incrementally over time, as it often occurs in real-world dynamic\nenvironments. We propose \"Memory-based Hebbian Parameter Adaptation\" (Hebb) to\ntackle the two major challenges (i.e., catastrophic forgetting and sample\nefficiency) towards this goal in a unified framework. To mitigate catastrophic\nforgetting, Hebb augments a regular neural classifier with a continuously\nupdated memory module to store representations of previous data. To improve\nsample efficiency, we propose a parameter adaptation method based on the\nwell-known Hebbian theory, which directly \"wires\" the output network's\nparameters with similar representations retrieved from the memory. We\nempirically verify the superior performance of Hebb through extensive\nexperiments on a wide range of learning tasks (image classification, language\nmodel) and learning scenarios (continual, incremental, online). We demonstrate\nthat Hebb effectively mitigates catastrophic forgetting, and it indeed learns\nnew knowledge better and faster than the current state-of-the-art.",
          "link": "http://arxiv.org/abs/2108.12596",
          "publishedOn": "2021-09-13T07:20:25.059Z",
          "wordCount": 636,
          "title": "Representation Memorization for Fast Learning New Knowledge without Forgetting. (arXiv:2108.12596v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangkai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>",
          "description": "Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to construct\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named Mixsum, in addition to the cross-entropy-loss.\nExperiments on real-world text classification datasets (Amazon-5, Yelp-5, AG\nNews, and IMDb) demonstrate the effectiveness of the proposed contrastive\nlearning framework with summarization-based data augmentation and Mixsum\nregularization.",
          "link": "http://arxiv.org/abs/2104.05094",
          "publishedOn": "2021-09-13T07:20:25.052Z",
          "wordCount": 629,
          "title": "Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.05500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>",
          "description": "Real-world applications often require improved models by leveraging a range\nof cheap incidental supervision signals. These could include partial labels,\nnoisy labels, knowledge-based constraints, and cross-domain or cross-task\nannotations -- all having statistical associations with gold annotations but\nnot exactly the same. However, we currently lack a principled way to measure\nthe benefits of these signals to a given target task, and the common practice\nof evaluating these benefits is through exhaustive experiments with various\nmodels and hyperparameters. This paper studies whether we can, in a single\nframework, quantify the benefits of various types of incidental signals for a\ngiven target task without going through combinatorial experiments. We propose a\nunified PAC-Bayesian motivated informativeness measure, PABI, that\ncharacterizes the uncertainty reduction provided by incidental supervision\nsignals. We demonstrate PABI's effectiveness by quantifying the value added by\nvarious types of incidental signals to sequence tagging tasks. Experiments on\nnamed entity recognition (NER) and question answering (QA) show that PABI's\npredictions correlate well with learning performance, providing a promising way\nto determine, ahead of learning, which supervision signals would be beneficial.",
          "link": "http://arxiv.org/abs/2006.05500",
          "publishedOn": "2021-09-13T07:20:25.042Z",
          "wordCount": 648,
          "title": "Foreseeing the Benefits of Incidental Supervision. (arXiv:2006.05500v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pulastya_V/0/1/0/all/0/1\">Vaibhav Pulastya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuti_G/0/1/0/all/0/1\">Gaurav Nuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Due to the over-emphasize of the quantity of data, the data quality has often\nbeen overlooked. However, not all training data points contribute equally to\nlearning. In particular, if mislabeled, it might actively damage the\nperformance of the model and the ability to generalize out of distribution, as\nthe model might end up learning spurious artifacts present in the dataset. This\nproblem gets compounded by the prevalence of heavily parameterized and complex\ndeep neural networks, which can, with their high capacity, end up memorizing\nthe noise present in the dataset. This paper proposes a novel statistic --\nnoise score, as a measure for the quality of each data point to identify such\nmislabeled samples based on the variations in the latent space representation.\nIn our work, we use the representations derived by the inference network of\ndata quality supervised variational autoencoder (AQUAVS). Our method leverages\nthe fact that samples belonging to the same class will have similar latent\nrepresentations. Therefore, by identifying the outliers in the latent space, we\ncan find the mislabeled samples. We validate our proposed statistic through\nexperimentation by corrupting MNIST, FashionMNIST, and CIFAR10/100 datasets in\ndifferent noise settings for the task of identifying mislabelled samples. We\nfurther show significant improvements in accuracy for the classification task\nfor each dataset.",
          "link": "http://arxiv.org/abs/2109.05000",
          "publishedOn": "2021-09-13T07:20:25.035Z",
          "wordCount": 668,
          "title": "Assessing the Quality of the Datasets by Identifying Mislabeled Samples. (arXiv:2109.05000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>",
          "description": "This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework, by using \"entailment\" pairs\nas positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nprevious best results. We also show -- both theoretically and empirically --\nthat contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.",
          "link": "http://arxiv.org/abs/2104.08821",
          "publishedOn": "2021-09-13T07:20:25.007Z",
          "wordCount": 675,
          "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04916",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Innocenti_M/0/1/0/all/0/1\">Maria Elena Innocenti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Amaya_J/0/1/0/all/0/1\">Jorge Amaya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Raeder_J/0/1/0/all/0/1\">Joachim Raeder</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dupuis_R/0/1/0/all/0/1\">Romain Dupuis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ferdousi_B/0/1/0/all/0/1\">Banafsheh Ferdousi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lapenta_G/0/1/0/all/0/1\">Giovanni Lapenta</a>",
          "description": "In magnetospheric missions, burst mode data sampling should be triggered in\nthe presence of processes of scientific or operational interest. We present an\nunsupervised classification method for magnetospheric regions, that could\nconstitute the first-step of a multi-step method for the automatic\nidentification of magnetospheric processes of interest. Our method is based on\nSelf Organizing Maps (SOMs), and we test it preliminarily on data points from\nglobal magnetospheric simulations obtained with the OpenGGCM-CTIM-RCM code. The\ndimensionality of the data is reduced with Principal Component Analysis before\nclassification. The classification relies exclusively on local plasma\nproperties at the selected data points, without information on their\nneighborhood or on their temporal evolution. We classify the SOM nodes into an\nautomatically selected number of classes, and we obtain clusters that map to\nwell defined magnetospheric regions. We validate our classification results by\nplotting the classified data in the simulated space and by comparing with\nK-means classification. For the sake of result interpretability, we examine the\nSOM feature maps (magnetospheric variables are called features in the context\nof classification), and we use them to unlock information on the clusters. We\nrepeat the classification experiments using different sets of features, we\nquantitatively compare different classification results, and we obtain insights\non which magnetospheric variables make more effective features for unsupervised\nclassification.",
          "link": "http://arxiv.org/abs/2109.04916",
          "publishedOn": "2021-09-13T07:20:24.981Z",
          "wordCount": 675,
          "title": "Unsupervised classification of simulated magnetospheric regions. (arXiv:2109.04916v1 [physics.space-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Srivatsan Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zishen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Kshitij Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1\">Paul Whatmough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1\">Aleksandra Faust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neuman_S/0/1/0/all/0/1\">Sabrina Neuman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Gu-Yeon Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brooks_D/0/1/0/all/0/1\">David Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>",
          "description": "Building domain-specific accelerators for autonomous unmanned aerial vehicles\n(UAVs) is challenging due to a lack of systematic methodology for designing\nonboard compute. Balancing a computing system for a UAV requires considering\nboth the cyber (e.g., sensor rate, compute performance) and physical (e.g.,\npayload weight) characteristics that affect overall performance. Iterating over\nthe many component choices results in a combinatorial explosion of the number\nof possible combinations: from 10s of thousands to billions, depending on\nimplementation details. Manually selecting combinations of these components is\ntedious and expensive. To navigate the {cyber-physical design space}\nefficiently, we introduce \\emph{AutoPilot}, a framework that automates\nfull-system UAV co-design. AutoPilot uses Bayesian optimization to navigate a\nlarge design space and automatically select a combination of autonomy algorithm\nand hardware accelerator while considering the cross-product effect of other\ncyber and physical UAV components. We show that the AutoPilot methodology\nconsistently outperforms general-purpose hardware selections like Xavier NX and\nJetson TX2, as well as dedicated hardware accelerators built for autonomous\nUAVs, across a range of representative scenarios (three different UAV types and\nthree deployment environments). Designs generated by AutoPilot increase the\nnumber of missions on average by up to 2.25x, 1.62x, and 1.43x for nano, micro,\nand mini-UAVs respectively over baselines. Our work demonstrates the need for\nholistic full-UAV co-design to achieve maximum overall UAV performance and the\nneed for automated flows to simplify the design process for autonomous\ncyber-physical systems.",
          "link": "http://arxiv.org/abs/2102.02988",
          "publishedOn": "2021-09-13T07:20:24.955Z",
          "wordCount": 739,
          "title": "AutoPilot: Automating SoC Design Space Exploration for SWaP Constrained Autonomous UAVs. (arXiv:2102.02988v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thummerer_T/0/1/0/all/0/1\">Tobias Thummerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tintenherr_J/0/1/0/all/0/1\">Johannes Tintenherr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikelsons_L/0/1/0/all/0/1\">Lars Mikelsons</a>",
          "description": "Hybrid modeling, the combination of first principle and machine learning\nmodels, is an emerging research field that gathers more and more attention.\nEven if hybrid models produce formidable results for academic examples, there\nare still different technical challenges that hinder the use of hybrid modeling\nin real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a\nnumerical ODE solver and an ANN, we are paving the way for the use of a variety\nof first principle models from different modeling tools as parts of hybrid\nmodels. This contribution handles the hybrid modeling of a complex, real-world\nexample: Starting with a simplified 1D-fluid model of the human cardiovascular\nsystem (arterial side), the aim is to learn neglected physical effects like\narterial elasticity from data. We will show that the hybrid modeling process is\nmore comfortable, needs less system knowledge and is therefore less error-prone\ncompared to modeling solely based on first principle. Further, the resulting\nhybrid model has improved in computation performance, compared to a pure first\nprinciple white-box model, while still fulfilling the requirements regarding\naccuracy of the considered hemodynamic quantities. The use of the presented\ntechniques is explained in a general manner and the considered use-case can\nserve as example for other modeling and simulation applications in and beyond\nthe medical domain.",
          "link": "http://arxiv.org/abs/2109.04880",
          "publishedOn": "2021-09-13T07:20:24.932Z",
          "wordCount": 660,
          "title": "Hybrid modeling of the human cardiovascular system using NeuralFMUs. (arXiv:2109.04880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikolakopoulos_A/0/1/0/all/0/1\">Athanasios N. Nikolakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Collaborative recommendation approaches based on nearest-neighbors are still\nhighly popular today due to their simplicity, their efficiency, and their\nability to produce accurate and personalized recommendations. This chapter\noffers a comprehensive survey of neighborhood-based methods for the item\nrecommendation problem. It presents the main characteristics and benefits of\nsuch methods, describes key design choices for implementing a\nneighborhood-based recommender system, and gives practical information on how\nto make these choices. A broad range of methods is covered in the chapter,\nincluding traditional algorithms like k-nearest neighbors as well as advanced\napproaches based on matrix factorization, sparse coding and random walks.",
          "link": "http://arxiv.org/abs/2109.04584",
          "publishedOn": "2021-09-13T07:20:24.917Z",
          "wordCount": 566,
          "title": "Trust your neighbors: A comprehensive survey of neighborhood-based methods for recommender systems. (arXiv:2109.04584v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lisus_D/0/1/0/all/0/1\">Daniil Lisus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cossette_C/0/1/0/all/0/1\">Charles Champagne Cossette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalaby_M/0/1/0/all/0/1\">Mohammed Shalaby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forbes_J/0/1/0/all/0/1\">James Richard Forbes</a>",
          "description": "It is essential that a robot has the ability to determine its position and\norientation to execute tasks autonomously. Heading estimation is especially\nchallenging in indoor environments where magnetic distortions make\nmagnetometer-based heading estimation difficult. Ultra-wideband (UWB)\ntransceivers are common in indoor localization problems. This letter\nexperimentally demonstrates how to use UWB range and received signal strength\n(RSS) measurements to estimate robot heading. The RSS of a UWB antenna varies\nwith its orientation. As such, a Gaussian process (GP) is used to learn a\ndata-driven relationship from UWB range and RSS inputs to orientation outputs.\nCombined with a gyroscope in an invariant extended Kalman filter, this realizes\na heading estimation method that uses only UWB and gyroscope measurements.",
          "link": "http://arxiv.org/abs/2109.04868",
          "publishedOn": "2021-09-13T07:20:24.901Z",
          "wordCount": 582,
          "title": "Heading Estimation Using Ultra-Wideband Received Signal Strength and Gaussian Processes. (arXiv:2109.04868v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grari_V/0/1/0/all/0/1\">Vincent Grari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Detyniecki_M/0/1/0/all/0/1\">Marcin Detyniecki</a>",
          "description": "In recent years, most fairness strategies in machine learning models focus on\nmitigating unwanted biases by assuming that the sensitive information is\nobserved. However this is not always possible in practice. Due to privacy\npurposes and var-ious regulations such as RGPD in EU, many personal sensitive\nattributes are frequently not collected. We notice a lack of approaches for\nmitigating bias in such difficult settings, in particular for achieving\nclassical fairness objectives such as Demographic Parity and Equalized Odds. By\nleveraging recent developments for approximate inference, we propose an\napproach to fill this gap. Based on a causal graph, we rely on a new\nvariational auto-encoding based framework named SRCVAE to infer a sensitive\ninformation proxy, that serve for bias mitigation in an adversarial fairness\napproach. We empirically demonstrate significant improvements over existing\nworks in the field. We observe that the generated proxy's latent space recovers\nsensitive information and that our approach achieves a higher accuracy while\nobtaining the same level of fairness on two real datasets, as measured using\ncom-mon fairness definitions.",
          "link": "http://arxiv.org/abs/2109.04999",
          "publishedOn": "2021-09-13T07:20:24.840Z",
          "wordCount": null,
          "title": "Fairness without the sensitive attribute via Causal Variational Autoencoder. (arXiv:2109.04999v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04744",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1\">Eyke H&#xfc;llermeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohr_F/0/1/0/all/0/1\">Felix Mohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1\">Alexander Tornede</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wever_M/0/1/0/all/0/1\">Marcel Wever</a>",
          "description": "The notion of bounded rationality originated from the insight that perfectly\nrational behavior cannot be realized by agents with limited cognitive or\ncomputational resources. Research on bounded rationality, mainly initiated by\nHerbert Simon, has a longstanding tradition in economics and the social\nsciences, but also plays a major role in modern AI and intelligent agent\ndesign. Taking actions under bounded resources requires an agent to reflect on\nhow to use these resources in an optimal way - hence, to reason and make\ndecisions on a meta-level. In this paper, we will look at automated machine\nlearning (AutoML) and related problems from the perspective of bounded\nrationality, essentially viewing an AutoML tool as an agent that has to train a\nmodel on a given set of data, and the search for a good way of doing so (a\nsuitable \"ML pipeline\") as deliberation on a meta-level.",
          "link": "http://arxiv.org/abs/2109.04744",
          "publishedOn": "2021-09-13T07:20:24.835Z",
          "wordCount": null,
          "title": "Automated Machine Learning, Bounded Rationality, and Rational Metareasoning. (arXiv:2109.04744v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>",
          "description": "As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema(definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5)is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.",
          "link": "http://arxiv.org/abs/2109.04645",
          "publishedOn": "2021-09-13T07:20:24.832Z",
          "wordCount": null,
          "title": "CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04626",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1\">Todd Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>",
          "description": "The PC algorithm is the state-of-the-art algorithm for causal structure\ndiscovery on observational data. It can be computationally expensive in the\nworst case due to the conditional independence tests are performed in an\nexhaustive-searching manner. This makes the algorithm computationally\nintractable when the task contains several hundred or thousand nodes,\nparticularly when the true underlying causal graph is dense. We propose a\ncritical observation that the conditional set rendering two nodes independent\nis non-unique, and including certain redundant nodes do not sacrifice result\naccuracy. Based on this finding, the innovations of our work are two-folds.\nFirst, we innovate on a reserve order linkage pruning PC algorithm which\nsignificantly increases the algorithm's efficiency. Second, we propose a\nparallel computing strategy for statistical independence tests by leveraging\ntensor computation, which brings further speedup. We also prove the proposed\nalgorithm does not induce statistical power loss under mild graph and data\ndimensionality assumptions. Experimental results show that the single-threaded\nversion of the proposed algorithm can achieve a 6-fold speedup compared to the\nPC algorithm on a dense 95-node graph, and the parallel version can make a\n825-fold speed-up. We also provide proof that the proposed algorithm is\nconsistent under the same set of conditions with conventional PC algorithm.",
          "link": "http://arxiv.org/abs/2109.04626",
          "publishedOn": "2021-09-13T07:20:24.831Z",
          "wordCount": 661,
          "title": "A Fast PC Algorithm with Reversed-order Pruning and A Parallelization Strategy. (arXiv:2109.04626v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>",
          "description": "The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.",
          "link": "http://arxiv.org/abs/2109.04876",
          "publishedOn": "2021-09-13T07:20:24.772Z",
          "wordCount": null,
          "title": "Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasaiah_D/0/1/0/all/0/1\">Deepthi Sreenivasaiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otterbach_J/0/1/0/all/0/1\">Johannes Otterbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wollmann_T/0/1/0/all/0/1\">Thomas Wollmann</a>",
          "description": "Image segmentation is a common and challenging task in autonomous driving.\nAvailability of sufficient pixel-level annotations for the training data is a\nhurdle. Active learning helps learning from small amounts of data by suggesting\nthe most promising samples for labeling. In this work, we propose a new\npool-based method for active learning, which proposes promising patches\nextracted from full image, in each acquisition step. The problem is framed in\nan exploration-exploitation framework by combining an embedding based on\nUniform Manifold Approximation to model representativeness with entropy as\nuncertainty measure to model informativeness. We applied our proposed method to\nthe autonomous driving datasets CamVid and Cityscapes and performed a\nquantitative comparison with state-of-the-art baselines. We find that our\nactive learning method achieves better performance compared to previous\nmethods.",
          "link": "http://arxiv.org/abs/2106.11858",
          "publishedOn": "2021-09-13T07:20:24.676Z",
          "wordCount": 602,
          "title": "MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16144",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>, <a href=\"http://arxiv.org/find/math/1/au:+Molinaro_R/0/1/0/all/0/1\">Roberto Molinaro</a>",
          "description": "Physics informed neural networks (PINNs) have recently been widely used for\nrobust and accurate approximation of PDEs. We provide rigorous upper bounds on\nthe generalization error of PINNs approximating solutions of the forward\nproblem for PDEs. An abstract formalism is introduced and stability properties\nof the underlying PDE are leveraged to derive an estimate for the\ngeneralization error in terms of the training error and number of training\nsamples. This abstract framework is illustrated with several examples of\nnonlinear PDEs. Numerical experiments, validating the proposed theory, are also\npresented.",
          "link": "http://arxiv.org/abs/2006.16144",
          "publishedOn": "2021-09-13T07:20:24.657Z",
          "wordCount": 563,
          "title": "Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs. (arXiv:2006.16144v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.04265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Negrea_J/0/1/0/all/0/1\">Jeffrey Negrea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziugaite_G/0/1/0/all/0/1\">Gintare Karolina Dziugaite</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_D/0/1/0/all/0/1\">Daniel M. Roy</a>",
          "description": "We propose to study the generalization error of a learned predictor $\\hat h$\nin terms of that of a surrogate (potentially randomized) predictor that is\ncoupled to $\\hat h$ and designed to trade empirical risk for control of\ngeneralization error. In the case where $\\hat h$ interpolates the data, it is\ninteresting to consider theoretical surrogate classifiers that are partially\nderandomized or rerandomized, e.g., fit to the training data but with modified\nlabel noise. We also show that replacing $\\hat h$ by its conditional\ndistribution with respect to an arbitrary $\\sigma$-field is a convenient way to\nderandomize. We study two examples, inspired by the work of Nagarajan and\nKolter (2019) and Bartlett et al. (2019), where the learned classifier $\\hat h$\ninterpolates the training data with high probability, has small risk, and, yet,\ndoes not belong to a nonrandom class with a tight uniform bound on two-sided\ngeneralization error. At the same time, we bound the risk of $\\hat h$ in terms\nof surrogates constructed by conditioning and denoising, respectively, and\nshown to belong to nonrandom classes with uniformly small generalization error.",
          "link": "http://arxiv.org/abs/1912.04265",
          "publishedOn": "2021-09-13T07:20:24.638Z",
          "wordCount": 698,
          "title": "In Defense of Uniform Convergence: Generalization via derandomization with an application to interpolating predictors. (arXiv:1912.04265v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chheda_T/0/1/0/all/0/1\">Tejas Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Purujit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "A major factor contributing to the success of modern representation learning\nis the ease of performing various vector operations. Recently, objects with\ngeometric structures (eg. distributions, complex or hyperbolic vectors, or\nregions such as cones, disks, or boxes) have been explored for their\nalternative inductive biases and additional representational capacities. In\nthis work, we introduce Box Embeddings, a Python library that enables\nresearchers to easily apply and extend probabilistic box embeddings.",
          "link": "http://arxiv.org/abs/2109.04997",
          "publishedOn": "2021-09-13T07:20:24.631Z",
          "wordCount": 552,
          "title": "Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.03959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gupta_S/0/1/0/all/0/1\">Samarth Gupta</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chaudhari_S/0/1/0/all/0/1\">Shreyas Chaudhari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Joshi_G/0/1/0/all/0/1\">Gauri Joshi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yagan_O/0/1/0/all/0/1\">Osman Ya&#x11f;an</a>",
          "description": "We consider a multi-armed bandit framework where the rewards obtained by\npulling different arms are correlated. We develop a unified approach to\nleverage these reward correlations and present fundamental generalizations of\nclassic bandit algorithms to the correlated setting. We present a unified proof\ntechnique to analyze the proposed algorithms. Rigorous analysis of C-UCB (the\ncorrelated bandit version of Upper-confidence-bound) reveals that the algorithm\nends up pulling certain sub-optimal arms, termed as non-competitive, only O(1)\ntimes, as opposed to the O(log T) pulls required by classic bandit algorithms\nsuch as UCB, TS etc. We present regret-lower bound and show that when arms are\ncorrelated through a latent random source, our algorithms obtain order-optimal\nregret. We validate the proposed algorithms via experiments on the MovieLens\nand Goodreads datasets, and show significant improvement over classical bandit\nalgorithms.",
          "link": "http://arxiv.org/abs/1911.03959",
          "publishedOn": "2021-09-13T07:20:24.586Z",
          "wordCount": 631,
          "title": "Multi-Armed Bandits with Correlated Arms. (arXiv:1911.03959v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>",
          "description": "A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.",
          "link": "http://arxiv.org/abs/2104.06644",
          "publishedOn": "2021-09-13T07:20:24.567Z",
          "wordCount": 653,
          "title": "Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>",
          "description": "We study the problem of training named entity recognition (NER) models using\nonly distantly-labeled data, which can be automatically obtained by matching\nentity mentions in the raw text with entity types in a knowledge base. The\nbiggest challenge of distantly-supervised NER is that the distant supervision\nmay induce incomplete and noisy labels, rendering the straightforward\napplication of supervised learning ineffective. In this paper, we propose (1) a\nnoise-robust learning scheme comprised of a new loss function and a noisy label\nremoval step, for training NER models on distantly-labeled data, and (2) a\nself-training method that uses contextualized augmentations created by\npre-trained language models to improve the generalization ability of the NER\nmodel. On three benchmark datasets, our method achieves superior performance,\noutperforming existing distantly-supervised NER models by significant margins.",
          "link": "http://arxiv.org/abs/2109.05003",
          "publishedOn": "2021-09-13T07:20:24.560Z",
          "wordCount": 595,
          "title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>",
          "description": "We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.",
          "link": "http://arxiv.org/abs/2105.03824",
          "publishedOn": "2021-09-13T07:20:24.545Z",
          "wordCount": 714,
          "title": "FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05019",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_S/0/1/0/all/0/1\">Sarwan Ali; Murray Patterson</a>",
          "description": "With the rapid global spread of COVID-19, more and more data related to this\nvirus is becoming available, including genomic sequence data. The total number\nof genomic sequences that are publicly available on platforms such as GISAID is\ncurrently several million, and is increasing with every day. The availability\nof such \\textit{Big Data} creates a new opportunity for researchers to study\nthis virus in detail. This is particularly important with all of the dynamics\nof the COVID-19 variants which emerge and circulate. This rich data source will\ngive us insights on the best ways to perform genomic surveillance for this and\nfuture pandemic threats, with the ultimate goal of mitigating or eliminating\nsuch threats. Analyzing and processing the several million genomic sequences is\na challenging task. Although traditional methods for sequence classification\nare proven to be effective, they are not designed to deal with these specific\ntypes of genomic sequences. Moreover, most of the existing methods also face\nthe issue of scalability. Previous studies which were tailored to coronavirus\ngenomic data proposed to use spike sequences (corresponding to a subsequence of\nthe genome), rather than using the complete genomic sequence, to perform\ndifferent machine learning (ML) tasks such as classification and clustering.\nHowever, those methods suffer from scalability issues. In this paper, we\npropose an approach called Spike2Vec, an efficient and scalable feature vector\nrepresentation for each spike sequence that can be used for downstream ML\ntasks. Through experiments, we show that Spike2Vec is not only scalable on\nseveral million spike sequences, but also outperforms the baseline models in\nterms of prediction accuracy, F1-score, etc.",
          "link": "http://arxiv.org/abs/2109.05019",
          "publishedOn": "2021-09-13T07:20:24.538Z",
          "wordCount": 753,
          "title": "Spike2Vec: An Efficient and Scalable Embedding Approach for COVID-19 Spike Sequences. (arXiv:2109.05019v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/1911.00804",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albuquerque_I/0/1/0/all/0/1\">Isabela Albuquerque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Jo&#xe3;o Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darvishi_M/0/1/0/all/0/1\">Mohammad Darvishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falk_T/0/1/0/all/0/1\">Tiago H. Falk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1\">Ioannis Mitliagkas</a>",
          "description": "Supervised learning results typically rely on assumptions of i.i.d. data.\nUnfortunately, those assumptions are commonly violated in practice. In this\nwork, we tackle such problem by focusing on domain generalization: a\nformalization where the data generating process at test time may yield samples\nfrom never-before-seen domains (distributions). Our work relies on the\nfollowing lemma: by minimizing a notion of discrepancy between all pairs from a\nset of given domains, we also minimize the discrepancy between any pairs of\nmixtures of domains. Using this result, we derive a generalization bound for\nour setting. We then show that low risk over unseen domains can be achieved by\nrepresenting the data in a space where (i) the training distributions are\nindistinguishable, and (ii) relevant information for the task at hand is\npreserved. Minimizing the terms in our bound yields an adversarial formulation\nwhich estimates and minimizes pairwise discrepancies. We validate our proposed\nstrategy on standard domain generalization benchmarks, outperforming a number\nof recently introduced methods. Notably, we tackle a real-world application\nwhere the underlying data corresponds to multi-channel electroencephalography\ntime series from different subjects, each considered as a distinct domain.",
          "link": "http://arxiv.org/abs/1911.00804",
          "publishedOn": "2021-09-13T07:20:24.516Z",
          "wordCount": 719,
          "title": "Generalizing to unseen domains via distribution matching. (arXiv:1911.00804v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanoh_R/0/1/0/all/0/1\">Ryuichi Kanoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "In practical situations, the ensemble tree model is one of the most popular\nmodels along with neural networks. A soft tree is one of the variants of a\ndecision tree. Instead of using a greedy method for searching splitting rules,\nthe soft tree is trained using a gradient method in which the whole splitting\noperation is formulated in a differentiable form. Although ensembles of such\nsoft trees have been increasingly used in recent years, little theoretical work\nhas been done for understanding their behavior. In this paper, by considering\nan ensemble of infinite soft trees, we introduce and study the Tree Neural\nTangent Kernel (TNTK), which provides new insights into the behavior of the\ninfinite ensemble of soft trees. Using the TNTK, we succeed in theoretically\nfinding several non-trivial properties, such as the effect of the oblivious\ntree structure and the degeneracy of the TNTK induced by the deepening of the\ntrees. Moreover, we empirically examine the performance of an ensemble of\ninfinite soft trees using the TNTK.",
          "link": "http://arxiv.org/abs/2109.04983",
          "publishedOn": "2021-09-13T07:20:24.509Z",
          "wordCount": 610,
          "title": "A Neural Tangent Kernel Perspective of Infinite Tree Ensembles. (arXiv:2109.04983v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00218",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhicong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Quanquan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1\">Stanley Osher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>",
          "description": "Federated learning aims to protect data privacy by collaboratively learning a\nmodel without sharing private data among users. However, an adversary may still\nbe able to infer the private training data by attacking the released model.\nDifferential privacy provides a statistical protection against such attacks at\nthe price of significantly degrading the accuracy or utility of the trained\nmodels. In this paper, we investigate a utility enhancement scheme based on\nLaplacian smoothing for differentially private federated learning (DP-Fed-LS),\nwhere the parameter aggregation with injected Gaussian noise is improved in\nstatistical precision without losing privacy budget. Our key observation is\nthat the aggregated gradients in federated learning often enjoy a type of\nsmoothness, i.e. sparsity in the graph Fourier basis with polynomial decays of\nFourier coefficients as frequency grows, which can be exploited by the\nLaplacian smoothing efficiently. Under a prescribed differential privacy\nbudget, convergence error bounds with tight rates are provided for DP-Fed-LS\nwith uniform subsampling of heterogeneous Non-IID data, revealing possible\nutility improvement of Laplacian smoothing in effective dimensionality and\nvariance reduction, among others. Experiments over MNIST, SVHN, and Shakespeare\ndatasets show that the proposed method can improve model accuracy with\nDP-guarantee and membership privacy under both uniform and Poisson subsampling\nmechanisms.",
          "link": "http://arxiv.org/abs/2005.00218",
          "publishedOn": "2021-09-13T07:20:24.486Z",
          "wordCount": 677,
          "title": "Differentially Private Federated Learning with Laplacian Smoothing. (arXiv:2005.00218v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>",
          "description": "Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.",
          "link": "http://arxiv.org/abs/2103.01378",
          "publishedOn": "2021-09-13T07:20:24.480Z",
          "wordCount": 645,
          "title": "Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sariyildiz_M/0/1/0/all/0/1\">Mert Bulent Sariyildiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantidis_Y/0/1/0/all/0/1\">Yannis Kalantidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larlus_D/0/1/0/all/0/1\">Diane Larlus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1\">Karteek Alahari</a>",
          "description": "Measuring concept generalization, i.e., the extent to which models trained on\na set of (seen) visual concepts can be leveraged to recognize a new set of\n(unseen) concepts, is a popular way of evaluating visual representations,\nespecially in a self-supervised learning framework. Nonetheless, the choice of\nunseen concepts for such an evaluation is usually made arbitrarily, and\nindependently from the seen concepts used to train representations, thus\nignoring any semantic relationships between the two. In this paper, we argue\nthat the semantic relationships between seen and unseen concepts affect\ngeneralization performance and propose ImageNet-CoG, a novel benchmark on the\nImageNet-21K (IN-21K) dataset that enables measuring concept generalization in\na principled way. Our benchmark leverages expert knowledge that comes from\nWordNet in order to define a sequence of unseen IN-21K concept sets that are\nsemantically more and more distant from the ImageNet-1K (IN-1K) subset, a\nubiquitous training set. This allows us to benchmark visual representations\nlearned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31\nconvolution and transformer-based models and show how different architectures,\nlevels of supervision, regularization techniques and use of web data impact the\nconcept generalization performance.",
          "link": "http://arxiv.org/abs/2012.05649",
          "publishedOn": "2021-09-13T07:20:24.438Z",
          "wordCount": 683,
          "title": "Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11578",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Marcondes_D/0/1/0/all/0/1\">Diego Marcondes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simonis_A/0/1/0/all/0/1\">Adilson Simonis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrera_J/0/1/0/all/0/1\">Junior Barrera</a>",
          "description": "In part \\textit{I} we proposed a structure for a general Hypotheses Space\n$\\mathcal{H}$, the Learning Space $\\mathbb{L}(\\mathcal{H})$, which can be\nemployed to avoid \\textit{overfitting} when estimating in a complex space with\nrelative shortage of examples. Also, we presented the U-curve property, which\ncan be taken advantage of in order to select a Hypotheses Space without\nexhaustively searching $\\mathbb{L}(\\mathcal{H})$. In this paper, we carry\nfurther our agenda, by showing the consistency of a model selection framework\nbased on Learning Spaces, in which one selects from data the Hypotheses Space\non which to learn. The method developed in this paper adds to the\nstate-of-the-art in model selection, by extending Vapnik-Chervonenkis Theory to\n\\textit{random} Hypotheses Spaces, i.e., Hypotheses Spaces learned from data.\nIn this framework, one estimates a random subspace $\\hat{\\mathcal{M}} \\in\n\\mathbb{L}(\\mathcal{H})$ which converges with probability one to a target\nHypotheses Space $\\mathcal{M}^{\\star} \\in \\mathbb{L}(\\mathcal{H})$ with desired\nproperties. As the convergence implies asymptotic unbiased estimators, we have\na consistent framework for model selection, showing that it is feasible to\nlearn the Hypotheses Space from data. Furthermore, we show that the\ngeneralization errors of learning on $\\hat{\\mathcal{M}}$ are lesser than those\nwe commit when learning on $\\mathcal{H}$, so it is more efficient to learn on a\nsubspace learned from data.",
          "link": "http://arxiv.org/abs/2001.11578",
          "publishedOn": "2021-09-13T07:20:24.431Z",
          "wordCount": 696,
          "title": "Learning the Hypotheses Space from data Part II: Convergence and Feasibility. (arXiv:2001.11578v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.05809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theodoridis_S/0/1/0/all/0/1\">Sergios Theodoridis</a>",
          "description": "Hidden Markov Models (HMMs) comprise a powerful generative approach for\nmodeling sequential data and time-series in general. However, the commonly\nemployed assumption of the dependence of the current time frame to a single or\nmultiple immediately preceding frames is unrealistic; more complicated dynamics\npotentially exist in real world scenarios. This paper revisits conventional\nsequential modeling approaches, aiming to address the problem of capturing\ntime-varying temporal dependency patterns. To this end, we propose a different\nformulation of HMMs, whereby the dependence on past frames is dynamically\ninferred from the data. Specifically, we introduce a hierarchical extension by\npostulating an additional latent variable layer; therein, the (time-varying)\ntemporal dependence patterns are treated as latent variables over which\ninference is performed. We leverage solid arguments from the Variational Bayes\nframework and derive a tractable inference algorithm based on the\nforward-backward algorithm. As we experimentally show, our approach can model\nhighly complex sequential data and can effectively handle data with missing\nvalues.",
          "link": "http://arxiv.org/abs/2002.05809",
          "publishedOn": "2021-09-13T07:20:24.419Z",
          "wordCount": 649,
          "title": "Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>",
          "description": "This paper is concerned with multi-modal data fusion (MMDF) under unexpected\nmodality failures in nonlinear non-Gaussian dynamic processes. An efficient\nframework to tackle this problem is proposed. In particular, a notion termed\nmodality \"\\emph{usefulness}\", which takes a value of 1 or 0, is used for\nindicating whether the observation of this modality is useful or not. For $n$\nmodalities involved, $2^n$ combinations of their \"\\emph{usefulness}\" values\nexist. Each combination defines one hypothetical model of the true data\ngenerative process. Then the problem of concern is formalized as a task of\nnonlinear non-Gaussian state filtering under model uncertainty, which is\naddressed by a dynamic model averaging (DMA) based particle filter (PF)\nalgorithm. This DMA algorithm employs $2^n$ models, while all models share the\nsame state-transition function and a unique set of particle values. That makes\nthe computational complexity of this algorithm only slightly larger than a\nsingle model based PF algorithm, especially for scenarios in which $n$ is\nsmall. Experimental results show that the proposed solution outperforms\nremarkably state-of-the-art methods. Code and data are available at\nhttps://github.com/robinlau1981/fusion.",
          "link": "http://arxiv.org/abs/2105.06018",
          "publishedOn": "2021-09-13T07:20:24.412Z",
          "wordCount": 667,
          "title": "Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective. (arXiv:2105.06018v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.02375",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel L. Rubin</a>",
          "description": "Federated learning is an emerging research paradigm for enabling\ncollaboratively training deep learning models without sharing patient data.\nHowever, the data from different institutions are usually heterogeneous across\ninstitutions, which may reduce the performance of models trained using\nfederated learning. In this study, we propose a novel heterogeneity-aware\nfederated learning method, SplitAVG, to overcome the performance drops from\ndata heterogeneity in federated learning. Unlike previous federated methods\nthat require complex heuristic training or hyper parameter tuning, our SplitAVG\nleverages the simple network split and feature map concatenation strategies to\nencourage the federated model training an unbiased estimator of the target data\ndistribution. We compare SplitAVG with seven state-of-the-art federated\nlearning methods, using centrally hosted training data as the baseline on a\nsuite of both synthetic and real-world federated datasets. We find that the\nperformance of models trained using all the comparison federated learning\nmethods degraded significantly with the increasing degrees of data\nheterogeneity. In contrast, SplitAVG method achieves comparable results to the\nbaseline method under all heterogeneous settings, that it achieves 96.2% of the\naccuracy and 110.4% of the mean absolute error obtained by the baseline in a\ndiabetic retinopathy binary classification dataset and a bone age prediction\ndataset, respectively, on highly heterogeneous data partitions. We conclude\nthat SplitAVG method can effectively overcome the performance drops from\nvariability in data distributions across institutions. Experimental results\nalso show that SplitAVG can be adapted to different base networks and\ngeneralized to various types of medical imaging tasks.",
          "link": "http://arxiv.org/abs/2107.02375",
          "publishedOn": "2021-09-13T07:20:24.313Z",
          "wordCount": 729,
          "title": "SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milan_G/0/1/0/all/0/1\">Giulia Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassio_L/0/1/0/all/0/1\">Luca Vassio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drago_I/0/1/0/all/0/1\">Idilio Drago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellia_M/0/1/0/all/0/1\">Marco Mellia</a>",
          "description": "Our life is getting filled by Internet of Things (IoT) devices. These devices\noften rely on closed or poorly documented protocols, with unknown formats and\nsemantics. Learning how to interact with such devices in an autonomous manner\nis the key for interoperability and automatic verification of their\ncapabilities. In this paper, we propose RL-IoT, a system that explores how to\nautomatically interact with possibly unknown IoT devices. We leverage\nreinforcement learning (RL) to recover the semantics of protocol messages and\nto take control of the device to reach a given goal, while minimizing the\nnumber of interactions. We assume to know only a database of possible IoT\nprotocol messages, whose semantics are however unknown. RL-IoT exchanges\nmessages with the target IoT device, learning those commands that are useful to\nreach the given goal. Our results show that RL-IoT is able to solve both simple\nand complex tasks. With properly tuned parameters, RL-IoT learns how to perform\nactions with the target device, a Yeelight smart bulb in our case study,\ncompleting non-trivial patterns with as few as 400 interactions. RL-IoT paves\nthe road for automatic interactions with poorly documented IoT protocols, thus\nenabling interoperable systems.",
          "link": "http://arxiv.org/abs/2105.00884",
          "publishedOn": "2021-09-13T07:20:24.305Z",
          "wordCount": 696,
          "title": "RL-IoT: Reinforcement Learning to Interact with IoT Devices. (arXiv:2105.00884v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1\">Chulhee Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_S/0/1/0/all/0/1\">Shankar Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobahi_H/0/1/0/all/0/1\">Hossein Mobahi</a>",
          "description": "We study the implicit bias of gradient flow (i.e., gradient descent with\ninfinitesimal step size) on linear neural network training. We propose a tensor\nformulation of neural networks that includes fully-connected, diagonal, and\nconvolutional networks as special cases, and investigate the linear version of\nthe formulation called linear tensor networks. With this formulation, we can\ncharacterize the convergence direction of the network parameters as singular\nvectors of a tensor defined by the network. For $L$-layer linear tensor\nnetworks that are orthogonally decomposable, we show that gradient flow on\nseparable classification finds a stationary point of the $\\ell_{2/L}$\nmax-margin problem in a \"transformed\" input space defined by the network. For\nunderdetermined regression, we prove that gradient flow finds a global minimum\nwhich minimizes a norm-like function that interpolates between weighted\n$\\ell_1$ and $\\ell_2$ norms in the transformed input space. Our theorems\nsubsume existing results in the literature while removing standard convergence\nassumptions. We also provide experiments that corroborate our analysis.",
          "link": "http://arxiv.org/abs/2010.02501",
          "publishedOn": "2021-09-13T07:20:24.298Z",
          "wordCount": 673,
          "title": "A Unifying View on Implicit Bias in Training Linear Neural Networks. (arXiv:2010.02501v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.06127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>",
          "description": "Transformers that are pre-trained on multilingual corpora, such as, mBERT and\nXLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In\nthe zero-shot transfer setting, only English training data is used, and the\nfine-tuned model is evaluated on another target language. While this works\nsurprisingly well, substantial variance has been observed in target language\nperformance between different fine-tuning runs, and in the zero-shot setup, no\ntarget-language development data is available to select among multiple\nfine-tuned models. Prior work has relied on English dev data to select among\nmodels that are fine-tuned with different learning rates, number of steps and\nother hyperparameters, often resulting in suboptimal choices. In this paper, we\nshow that it is possible to select consistently better models when small\namounts of annotated data are available in auxiliary pivot languages. We\npropose a machine learning approach to model selection that uses the fine-tuned\nmodel's own internal representations to predict its cross-lingual capabilities.\nIn extensive experiments we find that this method consistently selects better\nmodels than English validation data across twenty five languages (including\neight low-resource languages), and often achieves results that are comparable\nto model selection using target language development data.",
          "link": "http://arxiv.org/abs/2010.06127",
          "publishedOn": "2021-09-13T07:20:24.237Z",
          "wordCount": 656,
          "title": "Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.11107",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mattheakis_M/0/1/0/all/0/1\">Marios Mattheakis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sondak_D/0/1/0/all/0/1\">David Sondak</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Protopapas_P/0/1/0/all/0/1\">Pavlos Protopapas</a>",
          "description": "There has been a wave of interest in applying machine learning to study\ndynamical systems. In particular, neural networks have been applied to solve\nthe equations of motion, and therefore, track the evolution of a system. In\ncontrast to other applications of neural networks and machine learning,\ndynamical systems possess invariants such as energy, momentum, and angular\nmomentum, depending on their underlying symmetries. Traditional numerical\nintegration methods sometimes violate these conservation laws, propagating\nerrors in time, ultimately reducing the predictability of the method. We\npresent a data-free Hamiltonian neural network that solves the differential\nequations that govern dynamical systems. This is an equation-driven\nunsupervised learning method where the optimization process of the network\ndepends solely on the predicted functions without using any ground truth data.\nThis unsupervised model learns solutions that satisfy identically, up to an\narbitrarily small error, Hamilton's equations and, therefore, conserve the\nHamiltonian invariants. Once the network is optimized, the proposed\narchitecture is considered a symplectic unit due to the introduction of an\nefficient parametric form of solutions. In addition, the choice of an\nappropriate activation function drastically improves the predictability of the\nnetwork. An error analysis is derived and states that the numerical errors\ndepend on the overall network performance. The symplectic architecture is then\nemployed to solve the equations for the nonlinear oscillator and the chaotic\nHenon-Heiles dynamical system. In both systems, a symplectic Euler integrator\nrequires two orders more evaluation points than the Hamiltonian network in\norder to achieve the same order of the numerical error in the predicted phase\nspace trajectories.",
          "link": "http://arxiv.org/abs/2001.11107",
          "publishedOn": "2021-09-13T07:20:24.216Z",
          "wordCount": 726,
          "title": "Hamiltonian neural networks for solving equations of motion. (arXiv:2001.11107v3 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_T/0/1/0/all/0/1\">Tobias Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastinger_J/0/1/0/all/0/1\">Julia Gastinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sztyler_T/0/1/0/all/0/1\">Timo Sztyler</a>",
          "description": "Process mining deals with extraction of knowledge from business process\nexecution logs. Traditional process mining tasks, like process model generation\nor conformance checking, rely on a minimalistic feature set where each event is\ncharacterized only by its case identifier, activity type, and timestamp. In\ncontrast, the success of modern machine learning is based on models that take\nany available data as direct input and build layers of features automatically\nduring training. In this work, we introduce ProcK (Process & Knowledge), a\nnovel pipeline to build business process prediction models that take into\naccount both sequential data in the form of event logs and rich semantic\ninformation represented in a graph-structured knowledge base. The hybrid\napproach enables ProcK to flexibly make use of all information residing in the\ndatabases of organizations. Components to extract inter-linked event logs and\nknowledge bases from relational databases are part of the pipeline. We\ndemonstrate the power of ProcK by training it for prediction tasks on the OULAD\ne-learning dataset, where we achieve state-of-the-art performance on the tasks\nof predicting student dropout from courses and predicting their success. We\nalso apply our method on a number of additional machine learning tasks,\nincluding exam score prediction and early predictions that only take into\naccount data recorded during the first weeks of the courses.",
          "link": "http://arxiv.org/abs/2109.04881",
          "publishedOn": "2021-09-13T07:20:24.181Z",
          "wordCount": 664,
          "title": "ProcK: Machine Learning for Knowledge-Intensive Processes. (arXiv:2109.04881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zugner_D/0/1/0/all/0/1\">Daniel Z&#xfc;gner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubet_F/0/1/0/all/0/1\">Fran&#xe7;ois-Xavier Aubet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satorras_V/0/1/0/all/0/1\">Victor Garcia Satorras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Januschowski_T/0/1/0/all/0/1\">Tim Januschowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasthaus_J/0/1/0/all/0/1\">Jan Gasthaus</a>",
          "description": "We study a recent class of models which uses graph neural networks (GNNs) to\nimprove forecasting in multivariate time series.\n\nThe core assumption behind these models is that there is a latent graph\nbetween the time series (nodes) that governs the evolution of the multivariate\ntime series.\n\nBy parameterizing a graph in a differentiable way, the models aim to improve\nforecasting quality.\n\nWe compare four recent models of this class on the forecasting task. Further,\nwe perform ablations to study their behavior under changing conditions, e.g.,\nwhen disabling the graph-learning modules and providing the ground-truth\nrelations instead. Based on our findings, we propose novel ways of combining\nthe existing architectures.",
          "link": "http://arxiv.org/abs/2109.04979",
          "publishedOn": "2021-09-13T07:20:24.173Z",
          "wordCount": 563,
          "title": "A Study of Joint Graph Inference and Forecasting. (arXiv:2109.04979v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">Michael Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">Paula Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">Susanna Dodd</a>",
          "description": "A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes, or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. To address this, we propose a method that\nuses both word-level and sentence-level information to simultaneously perform\noutcome span detection and outcome type classification. In addition to\ninjecting contextual information to hidden vectors, we use label attention to\nappropriately weight both word and sentence level information. Experimental\nresults on several benchmark datasets for health outcome detection show that\nour proposed method consistently outperforms decoupled methods, reporting\ncompetitive results.",
          "link": "http://arxiv.org/abs/2104.07789",
          "publishedOn": "2021-09-13T07:20:24.155Z",
          "wordCount": 676,
          "title": "Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>",
          "description": "Unlike well-structured text, such as news reports and encyclopedia articles,\ndialogue content often comes from two or more interlocutors, exchanging\ninformation with each other. In such a scenario, the topic of a conversation\ncan vary upon progression and the key information for a certain topic is often\nscattered across multiple utterances of different speakers, which poses\nchallenges to abstractly summarize dialogues. To capture the various topic\ninformation of a conversation and outline salient facts for the captured\ntopics, this work proposes two topic-aware contrastive learning objectives,\nnamely coherence detection and sub-summary generation objectives, which are\nexpected to implicitly model the topic change and handle information scattering\nchallenges for the dialogue summarization task. The proposed contrastive\nobjectives are framed as auxiliary tasks for the primary dialogue summarization\ntask, united via an alternative parameter updating strategy. Extensive\nexperiments on benchmark datasets demonstrate that the proposed simple method\nsignificantly outperforms strong baselines and achieves new state-of-the-art\nperformance. The code and trained models are publicly available via\n\\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.",
          "link": "http://arxiv.org/abs/2109.04994",
          "publishedOn": "2021-09-13T07:20:24.147Z",
          "wordCount": 624,
          "title": "Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13605",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tuncel_Y/0/1/0/all/0/1\">Yigit Tuncel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhat_G/0/1/0/all/0/1\">Ganapati Bhat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jaehyun Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ogras_U/0/1/0/all/0/1\">Umit Ogras</a>",
          "description": "Energy harvesting offers an attractive and promising mechanism to power\nlow-energy devices. However, it alone is insufficient to enable an\nenergy-neutral operation, which can eliminate tedious battery charging and\nreplacement requirements. Achieving an energy-neutral operation is challenging\nsince the uncertainties in harvested energy undermine the quality of service\nrequirements. To address this challenge, we present a runtime energy-allocation\nframework that optimizes the utility of the target device under energy\nconstraints using a rollout algorithm, which is a sequential approach to solve\ndynamic optimization problems. The proposed framework uses an efficient\niterative algorithm to compute initial energy allocations at the beginning of a\nday. The initial allocations are then corrected at every interval to compensate\nfor the deviations from the expected energy harvesting pattern. We evaluate\nthis framework using solar and motion energy harvesting modalities and American\nTime Use Survey data from 4772 different users. Compared to prior techniques,\nthe proposed framework achieves up to 35% higher utility even under\nenergy-limited scenarios. Moreover, measurements on a wearable device prototype\nshow that the proposed framework has 1000x smaller energy overhead than\niterative approaches with a negligible loss in utility.",
          "link": "http://arxiv.org/abs/2102.13605",
          "publishedOn": "2021-09-13T07:20:24.101Z",
          "wordCount": 664,
          "title": "ECO: Enabling Energy-Neutral IoT Devices through Runtime Allocation of Harvested Energy. (arXiv:2102.13605v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wentai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Ligang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Rui Mao</a>",
          "description": "Federated Learning (FL) has shown great potential as a privacy-preserving\nsolution to learning from decentralized data that are only accessible to end\ndevices (i.e., clients). In many scenarios however, a large proportion of the\nclients are probably in possession of low-quality data that are biased, noisy\nor even irrelevant. As a result, they could significantly slow down the\nconvergence of the global model we aim to build and also compromise its\nquality. In light of this, we propose FedProf, a novel algorithm for optimizing\nFL under such circumstances without breaching data privacy. The key of our\napproach is a data representation profiling and matching scheme that uses the\nglobal model to dynamically profile data representations and allows for\nlow-cost, lightweight representation matching. Based on the scheme we\nadaptively score each client and adjust its participation probability so as to\nmitigate the impact of low-value clients on the training process. We have\nconducted extensive experiments on public datasets using various FL settings.\nThe results show that FedProf effectively reduces the number of communication\nrounds and overall time (up to 4.5x speedup) for the global model to converge\nand provides accuracy gain.",
          "link": "http://arxiv.org/abs/2102.01733",
          "publishedOn": "2021-09-13T07:20:24.093Z",
          "wordCount": 709,
          "title": "FedProf: Selective Federated Learning with Representation Profiling. (arXiv:2102.01733v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04615",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Miao_S/0/1/0/all/0/1\">Sentao Miao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yining Wang</a>",
          "description": "In the recent decades, the advance of information technology and abundant\npersonal data facilitate the application of algorithmic personalized pricing.\nHowever, this leads to the growing concern of potential violation of privacy\ndue to adversarial attack. To address the privacy issue, this paper studies a\ndynamic personalized pricing problem with \\textit{unknown} nonparametric demand\nmodels under data privacy protection. Two concepts of data privacy, which have\nbeen widely applied in practices, are introduced: \\textit{central differential\nprivacy (CDP)} and \\textit{local differential privacy (LDP)}, which is proved\nto be stronger than CDP in many cases. We develop two algorithms which make\npricing decisions and learn the unknown demand on the fly, while satisfying the\nCDP and LDP gurantees respectively. In particular, for the algorithm with CDP\nguarantee, the regret is proved to be at most $\\tilde\nO(T^{(d+2)/(d+4)}+\\varepsilon^{-1}T^{d/(d+4)})$. Here, the parameter $T$\ndenotes the length of the time horizon, $d$ is the dimension of the\npersonalized information vector, and the key parameter $\\varepsilon>0$ measures\nthe strength of privacy (smaller $\\varepsilon$ indicates a stronger privacy\nprotection). On the other hand, for the algorithm with LDP guarantee, its\nregret is proved to be at most $\\tilde\nO(\\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$, which is near-optimal as we prove a\nlower bound of $\\Omega(\\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$ for any\nalgorithm with LDP guarantee.",
          "link": "http://arxiv.org/abs/2109.04615",
          "publishedOn": "2021-09-13T07:20:23.673Z",
          "wordCount": 655,
          "title": "Differential Privacy in Personalized Pricing with Nonparametric Demand Models. (arXiv:2109.04615v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Z/0/1/0/all/0/1\">Zewei Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1\">Houping Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fenglong Ma</a>",
          "description": "Federated Semi-Supervised Learning (FedSSL) has gained rising attention from\nboth academic and industrial researchers, due to its unique characteristics of\nco-training machine learning models with isolated yet unlabeled data. Most\nexisting FedSSL methods focus on the classical scenario, i.e, the labeled and\nunlabeled data are stored at the client side. However, in real world\napplications, client users may not provide labels without any incentive. Thus,\nthe scenario of labels at the server side is more practical. Since unlabeled\ndata and labeled data are decoupled, most existing FedSSL approaches may fail\nto deal with such a scenario. To overcome this problem, in this paper, we\npropose FedCon, which introduces a new learning paradigm, i.e., contractive\nlearning, to FedSSL. Experimental results on three datasets show that FedCon\nachieves the best performance with the contractive framework compared with\nstate-of-the-art baselines under both IID and Non-IID settings. Besides,\nablation studies demonstrate the characteristics of the proposed FedCon\nframework.",
          "link": "http://arxiv.org/abs/2109.04533",
          "publishedOn": "2021-09-13T07:20:23.667Z",
          "wordCount": 595,
          "title": "FedCon: A Contrastive Framework for Federated Semi-Supervised Learning. (arXiv:2109.04533v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gebauer_N/0/1/0/all/0/1\">Niklas W. A. Gebauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gastegger_M/0/1/0/all/0/1\">Michael Gastegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hessmann_S/0/1/0/all/0/1\">Stefaan S. P. Hessmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutt_K/0/1/0/all/0/1\">Kristof T. Sch&#xfc;tt</a>",
          "description": "The rational design of molecules with desired properties is a long-standing\nchallenge in chemistry. Generative neural networks have emerged as a powerful\napproach to sample novel molecules from a learned distribution. Here, we\npropose a conditional generative neural network for 3d molecular structures\nwith specified structural and chemical properties. This approach is agnostic to\nchemical bonding and enables targeted sampling of novel molecules from\nconditional distributions, even in domains where reference calculations are\nsparse. We demonstrate the utility of our method for inverse design by\ngenerating molecules with specified composition or motifs, discovering\nparticularly stable molecules, and jointly targeting multiple electronic\nproperties beyond the training regime.",
          "link": "http://arxiv.org/abs/2109.04824",
          "publishedOn": "2021-09-13T07:20:23.660Z",
          "wordCount": 574,
          "title": "Inverse design of 3d molecular structures with conditional generative neural networks. (arXiv:2109.04824v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>",
          "description": "Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.",
          "link": "http://arxiv.org/abs/2109.04838",
          "publishedOn": "2021-09-13T07:20:23.646Z",
          "wordCount": 595,
          "title": "Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04522",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Feyzmahdavian_H/0/1/0/all/0/1\">Hamid Reza Feyzmahdavian</a>, <a href=\"http://arxiv.org/find/math/1/au:+Johansson_M/0/1/0/all/0/1\">Mikael Johansson</a>",
          "description": "We introduce novel convergence results for asynchronous iterations which\nappear in the analysis of parallel and distributed optimization algorithms. The\nresults are simple to apply and give explicit estimates for how the degree of\nasynchrony impacts the convergence rates of the iterates. Our results shorten,\nstreamline and strengthen existing convergence proofs for several asynchronous\noptimization methods, and allow us to establish convergence guarantees for\npopular algorithms that were thus far lacking a complete theoretical\nunderstanding. Specifically, we use our results to derive better iteration\ncomplexity bounds for proximal incremental aggregated gradient methods, to\nprovide less conservative analyses of the speedup conditions for asynchronous\nblock-coordinate implementations of Krasnoselskii-Mann iterations, and to\nquantify the convergence rates for totally asynchronous iterations under\nvarious assumptions on communication delays and update rates.",
          "link": "http://arxiv.org/abs/2109.04522",
          "publishedOn": "2021-09-13T07:20:23.640Z",
          "wordCount": 587,
          "title": "Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04554",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kar_D/0/1/0/all/0/1\">Debajyoti Kar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medya_S/0/1/0/all/0/1\">Sourav Medya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1\">Debmalya Mandal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1\">Arlei Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1\">Palash Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Swagato Sanyal</a>",
          "description": "Ensuring fairness in machine learning algorithms is a challenging and\nimportant task. We consider the problem of clustering a set of points while\nensuring fairness constraints. While there have been several attempts to\ncapture group fairness in the k-clustering problem, fairness at an individual\nlevel is not well-studied. We introduce a new notion of individual fairness in\nk-clustering based on features that are not necessarily used for clustering. We\nshow that this problem is NP-hard and does not admit a constant factor\napproximation. We then design a randomized algorithm that guarantees\napproximation both in terms of minimizing the clustering distance objective as\nwell as individual fairness under natural restrictions on the distance metric\nand fairness constraints. Finally, our experimental results validate that our\nalgorithm produces lower clustering costs compared to existing algorithms while\nbeing competitive in individual fairness.",
          "link": "http://arxiv.org/abs/2109.04554",
          "publishedOn": "2021-09-13T07:20:23.609Z",
          "wordCount": 585,
          "title": "Feature-based Individual Fairness in k-Clustering. (arXiv:2109.04554v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04565",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thiruloga_S/0/1/0/all/0/1\">S. V. Thiruloga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukkala_V/0/1/0/all/0/1\">V. K. Kukkala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">S. Pasricha</a>",
          "description": "Modern vehicles have multiple electronic control units (ECUs) that are\nconnected together as part of a complex distributed cyber-physical system\n(CPS). The ever-increasing communication between ECUs and external electronic\nsystems has made these vehicles particularly susceptible to a variety of\ncyber-attacks. In this work, we present a novel anomaly detection framework\ncalled TENET to detect anomalies induced by cyber-attacks on vehicles. TENET\nuses temporal convolutional neural networks with an integrated attention\nmechanism to detect anomalous attack patterns. TENET is able to achieve an\nimprovement of 32.70% in False Negative Rate, 19.14% in the Mathews Correlation\nCoefficient, and 17.25% in the ROC-AUC metric, with 94.62% fewer model\nparameters, 86.95% decrease in memory footprint, and 48.14% lower inference\ntime when compared to the best performing prior work on automotive anomaly\ndetection.",
          "link": "http://arxiv.org/abs/2109.04565",
          "publishedOn": "2021-09-13T07:20:23.603Z",
          "wordCount": 583,
          "title": "TENET: Temporal CNN with Attention for Anomaly Detection in Automotive Cyber-Physical Systems. (arXiv:2109.04565v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04608",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_Moreno_L/0/1/0/all/0/1\">Luis Miranda-Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lijun Sun</a>",
          "description": "Spatiotemporal forecasting plays an essential role in various applications in\nintelligent transportation systems (ITS), such as route planning, navigation,\nand traffic control and management. Deep Spatiotemporal graph neural networks\n(GNNs), which capture both spatial and temporal patterns, have achieved great\nsuccess in traffic forecasting applications. Understanding how GNNs-based\nforecasting work and the vulnerability and robustness of these models becomes\ncritical to real-world applications. For example, if spatiotemporal GNNs are\nvulnerable in real-world traffic prediction applications, a hacker can easily\nmanipulate the results and cause serious traffic congestion and even a\ncity-scale breakdown. However, despite that recent studies have demonstrated\nthat deep neural networks (DNNs) are vulnerable to carefully designed\nperturbations in multiple domains like objection classification and graph\nrepresentation, current adversarial works cannot be directly applied to\nspatiotemporal forecasting due to the causal nature and spatiotemporal\nmechanisms in forecasting models. To fill this gap, in this paper we design\nSpatially Focused Attack (SFA) to break spatiotemporal GNNs by attacking a\nsingle vertex. To achieve this, we first propose the inverse estimation to\naddress the causality issue; then, we apply genetic algorithms with a universal\nattack method as the evaluation function to locate the weakest vertex; finally,\nperturbations are generated by solving an inverse estimation-based optimization\nproblem. We conduct experiments on real-world traffic data and our results show\nthat perturbations in one vertex designed by SA can be diffused into a large\npart of the graph.",
          "link": "http://arxiv.org/abs/2109.04608",
          "publishedOn": "2021-09-13T07:20:23.596Z",
          "wordCount": 673,
          "title": "Spatially Focused Attack against Spatiotemporal Graph Neural Networks. (arXiv:2109.04608v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochiai_K/0/1/0/all/0/1\">Keiichi Ochiai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demizu_T/0/1/0/all/0/1\">Tsukasa Demizu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishiguro_S/0/1/0/all/0/1\">Shin Ishiguro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_S/0/1/0/all/0/1\">Shohei Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawana_A/0/1/0/all/0/1\">Akihiro Kawana</a>",
          "description": "Reducing air pollution, such as CO2 and PM2.5 emissions, is one of the most\nimportant issues for many countries worldwide. Selecting an environmentally\nfriendly transport mode can be an effective approach of individuals to reduce\nair pollution in daily life. In this study, we propose a method to simulate the\neffectiveness of an eco-friendly transport mode selection for reducing air\npollution by using map search logs. We formulate the transport mode selection\nas a combinatorial optimization problem with the constraints regarding the\ntotal amount of CO2 emissions as an example of air pollution and the average\ntravel time. The optimization results show that the total amount of CO2\nemissions can be reduced by 9.23%, whereas the average travel time can in fact\nbe reduced by 9.96%. Our research proposal won first prize in Regular Machine\nLearning Competition Track Task 2 at KDD Cup 2019.",
          "link": "http://arxiv.org/abs/2109.04831",
          "publishedOn": "2021-09-13T07:20:23.571Z",
          "wordCount": 604,
          "title": "Simulating the Effects of Eco-Friendly Transportation Selections for Air Pollution Reduction. (arXiv:2109.04831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04518",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Thien Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuchi_K/0/1/0/all/0/1\">Kazuto Fukuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akimoto_Y/0/1/0/all/0/1\">Youhei Akimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakuma_J/0/1/0/all/0/1\">Jun Sakuma</a>",
          "description": "We aim to explain a black-box classifier with the form: `data X is classified\nas class Y because X \\textit{has} A, B and \\textit{does not have} C' in which\nA, B, and C are high-level concepts. The challenge is that we have to discover\nin an unsupervised manner a set of concepts, i.e., A, B and C, that is useful\nfor the explaining the classifier. We first introduce a structural generative\nmodel that is suitable to express and discover such concepts. We then propose a\nlearning process that simultaneously learns the data distribution and\nencourages certain concepts to have a large causal influence on the classifier\noutput. Our method also allows easy integration of user's prior knowledge to\ninduce high interpretability of concepts. Using multiple datasets, we\ndemonstrate that our method can discover useful binary concepts for\nexplanation.",
          "link": "http://arxiv.org/abs/2109.04518",
          "publishedOn": "2021-09-13T07:20:23.566Z",
          "wordCount": 584,
          "title": "Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation. (arXiv:2109.04518v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhengyang Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Meng-Hao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Ke Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "As an essential ingredient of modern deep learning, attention mechanism,\nespecially self-attention, plays a vital role in the global correlation\ndiscovery. However, is hand-crafted attention irreplaceable when modeling the\nglobal context? Our intriguing finding is that self-attention is not better\nthan the matrix decomposition (MD) model developed 20 years ago regarding the\nperformance and computational cost for encoding the long-distance dependencies.\nWe model the global context issue as a low-rank recovery problem and show that\nits optimization algorithms can help design global information blocks. This\npaper then proposes a series of Hamburgers, in which we employ the optimization\nalgorithms for solving MDs to factorize the input representations into\nsub-matrices and reconstruct a low-rank embedding. Hamburgers with different\nMDs can perform favorably against the popular global context module\nself-attention when carefully coping with gradients back-propagated through\nMDs. Comprehensive experiments are conducted in the vision tasks where it is\ncrucial to learn the global context, including semantic segmentation and image\ngeneration, demonstrating significant improvements over self-attention and its\nvariants.",
          "link": "http://arxiv.org/abs/2109.04553",
          "publishedOn": "2021-09-13T07:20:23.559Z",
          "wordCount": 618,
          "title": "Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sabetpour_N/0/1/0/all/0/1\">Nasim Sabetpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Adithya Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sihong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>",
          "description": "Annotations quality and quantity positively affect the performance of\nsequence labeling, a vital task in Natural Language Processing. Hiring domain\nexperts to annotate a corpus set is very costly in terms of money and time.\nCrowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have been\ndeployed to assist in this purpose. However, these platforms are prone to human\nerrors due to the lack of expertise; hence, one worker's annotations cannot be\ndirectly used to train the model. Existing literature in annotation aggregation\nmore focuses on binary or multi-choice problems. In recent years, handling the\nsequential label aggregation tasks on imbalanced datasets with complex\ndependencies between tokens has been challenging. To conquer the challenge, we\npropose an optimization-based method that infers the best set of aggregated\nannotations using labels provided by workers. The proposed Aggregation method\nfor Sequential Labels from Crowds ($AggSLC$) jointly considers the\ncharacteristics of sequential labeling tasks, workers' reliabilities, and\nadvanced machine learning techniques. We evaluate $AggSLC$ on different\ncrowdsourced data for Named Entity Recognition (NER), Information Extraction\ntasks in biomedical (PICO), and the simulated dataset. Our results show that\nthe proposed method outperforms the state-of-the-art aggregation methods. To\nachieve insights into the framework, we study $AggSLC$ components'\neffectiveness through ablation studies by evaluating our model in the absence\nof the prediction module and inconsistency loss function. Theoretical analysis\nof our algorithm's convergence points that the proposed $AggSLC$ halts after a\nfinite number of iterations.",
          "link": "http://arxiv.org/abs/2109.04470",
          "publishedOn": "2021-09-13T07:20:23.553Z",
          "wordCount": 677,
          "title": "Truth Discovery in Sequence Labels from Crowds. (arXiv:2109.04470v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04617",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fifty_C/0/1/0/all/0/1\">Christopher Fifty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amid_E/0/1/0/all/0/1\">Ehsan Amid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tianhe Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1\">Rohan Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>",
          "description": "Multi-task learning can leverage information learned by one task to benefit\nthe training of other tasks. Despite this capacity, naively training all tasks\ntogether in one model often degrades performance, and exhaustively searching\nthrough combinations of task groupings can be prohibitively expensive. As a\nresult, efficiently identifying the tasks that would benefit from co-training\nremains a challenging design question without a clear solution. In this paper,\nwe suggest an approach to select which tasks should train together in\nmulti-task learning models. Our method determines task groupings in a single\ntraining run by co-training all tasks together and quantifying the effect to\nwhich one task's gradient would affect another task's loss. On the large-scale\nTaskonomy computer vision dataset, we find this method can decrease test loss\nby 10.0\\% compared to simply training all tasks together while operating 11.6\ntimes faster than a state-of-the-art task grouping method.",
          "link": "http://arxiv.org/abs/2109.04617",
          "publishedOn": "2021-09-13T07:20:23.527Z",
          "wordCount": 597,
          "title": "Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1\">Sebastian Flennerhag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroecker_Y/0/1/0/all/0/1\">Yannick Schroecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1\">Tom Zahavy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1\">Hado van Hasselt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_D/0/1/0/all/0/1\">David Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "Meta-learning empowers artificial intelligence to increase its efficiency by\nlearning how to learn. Unlocking this potential involves overcoming a\nchallenging meta-optimisation problem that often exhibits ill-conditioning, and\nmyopic meta-objectives. We propose an algorithm that tackles these issues by\nletting the meta-learner teach itself. The algorithm first bootstraps a target\nfrom the meta-learner, then optimises the meta-learner by minimising the\ndistance to that target under a chosen (pseudo-)metric. Focusing on\nmeta-learning with gradients, we establish conditions that guarantee\nperformance improvements and show that the improvement is related to the target\ndistance. Thus, by controlling curvature, the distance measure can be used to\nease meta-optimization, for instance by reducing ill-conditioning. Further, the\nbootstrapping mechanism can extend the effective meta-learning horizon without\nrequiring backpropagation through all updates. The algorithm is versatile and\neasy to implement. We achieve a new state-of-the art for model-free agents on\nthe Atari ALE benchmark, improve upon MAML in few-shot learning, and\ndemonstrate how our approach opens up new possibilities by meta-learning\nefficient exploration in a Q-learning agent.",
          "link": "http://arxiv.org/abs/2109.04504",
          "publishedOn": "2021-09-13T07:20:23.521Z",
          "wordCount": 616,
          "title": "Bootstrapped Meta-Learning. (arXiv:2109.04504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04720",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_D/0/1/0/all/0/1\">Dongwook Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jonghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jinsung Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sang-Ki Ko</a>",
          "description": "Although the values of individual soccer players have become astronomical,\nsubjective judgments still play a big part in the player analysis. Recently,\nthere have been new attempts to quantitatively grasp players' styles using\nvideo-based event stream data. However, they have some limitations in\nscalability due to high annotation costs and sparsity of event stream data. In\nthis paper, we build a triplet network named 6MapNet that can effectively\ncapture the movement styles of players using in-game GPS data. Without any\nannotation of soccer-specific actions, we use players' locations and velocities\nto generate two types of heatmaps. Our subnetworks then map these heatmap pairs\ninto feature vectors whose similarity corresponds to the actual similarity of\nplaying styles. The experimental results show that players can be accurately\nidentified with only a small number of matches by our method.",
          "link": "http://arxiv.org/abs/2109.04720",
          "publishedOn": "2021-09-13T07:20:23.501Z",
          "wordCount": 607,
          "title": "6MapNet: Representing soccer players from tracking data by a triplet network. (arXiv:2109.04720v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04624",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>",
          "description": "Text style can reveal sensitive attributes of the author (e.g. race or age)\nto the reader, which can, in turn, lead to privacy violations and bias in both\nhuman and algorithmic decisions based on text. For example, the style of\nwriting in job applications might reveal protected attributes of the candidate\nwhich could lead to bias in hiring decisions, regardless of whether hiring\ndecisions are made algorithmically or by humans. We propose a VAE-based\nframework that obfuscates stylistic features of human-generated text through\nstyle transfer by automatically re-writing the text itself. Our framework\noperationalizes the notion of obfuscated style in a flexible way that enables\ntwo distinct notions of obfuscated style: (1) a minimal notion that effectively\nintersects the various styles seen in training, and (2) a maximal notion that\nseeks to obfuscate by adding stylistic features of all sensitive attributes to\ntext, in effect, computing a union of styles. Our style-obfuscation framework\ncan be used for multiple purposes, however, we demonstrate its effectiveness in\nimproving the fairness of downstream classifiers. We also conduct a\ncomprehensive study on style pooling's effect on fluency, semantic consistency,\nand attribute removal from text, in two and three domain style obfuscation.",
          "link": "http://arxiv.org/abs/2109.04624",
          "publishedOn": "2021-09-13T07:20:23.495Z",
          "wordCount": 638,
          "title": "Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness. (arXiv:2109.04624v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04572",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_M/0/1/0/all/0/1\">Mayukh Bhattacharyya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_U/0/1/0/all/0/1\">Udita Ghosh</a>",
          "description": "Out of the numerous hazards posing a threat to sustainable environmental\nconditions in the 21st century, only a few have a graver impact than air\npollution. Its importance in determining the health and living standards in\nurban settings is only expected to increase with time. Various factors ranging\nfrom emissions from traffic and power plants, household emissions, natural\ncauses are known to be primary causal agents or influencers behind rising air\npollution levels. However, the lack of large scale data involving the major\nfactors has hindered the research on the causes and relations governing the\nvariability of the different air pollutants. Through this work, we introduce a\nlarge scale city-wise dataset for exploring the relationships among these\nagents over a long period of time. We analyze and explore the dataset to bring\nout inferences which we can derive by modeling the data. Also, we provide a set\nof benchmarks for the problem of estimating or forecasting pollutant levels\nwith a set of diverse models and methodologies. Through our paper, we seek to\nprovide a ground base for further research into this domain that will demand\ncritical attention of ours in the near future.",
          "link": "http://arxiv.org/abs/2109.04572",
          "publishedOn": "2021-09-13T07:20:23.473Z",
          "wordCount": 645,
          "title": "Deciphering Environmental Air Pollution with Large Scale City Data. (arXiv:2109.04572v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogert_K/0/1/0/all/0/1\">Kenneth Bogert</a>",
          "description": "The principle of maximum entropy is a broadly applicable technique for\ncomputing a distribution with the least amount of information possible while\ncommonly constrained to match empirically estimated feature expectations. We\nseek to generalize this principle to scenarios where the empirical feature\nexpectations cannot be computed because the model variables are only partially\nobserved, which introduces a dependency on the learned model. Extending and\ngeneralizing the principle of latent maximum entropy, we introduce uncertain\nmaximum entropy and describe an expectation-maximization based solution to\napproximately solve these problems. We show that our technique generalizes the\nprinciple of maximum entropy and latent maximum entropy and discuss a generally\napplicable regularization technique for adding error terms to feature\nexpectation constraints in the event of limited data.",
          "link": "http://arxiv.org/abs/2109.04530",
          "publishedOn": "2021-09-13T07:20:23.316Z",
          "wordCount": 569,
          "title": "Notes on Generalizing the Maximum Entropy Principle to Uncertain Data. (arXiv:2109.04530v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>",
          "description": "Extracting moral sentiment from text is a vital component in understanding\npublic opinion, social movements, and policy decisions. The Moral Foundation\nTheory identifies five moral foundations, each associated with a positive and\nnegative polarity. However, moral sentiment is often motivated by its targets,\nwhich can correspond to individuals or collective entities. In this paper, we\nintroduce morality frames, a representation framework for organizing moral\nattitudes directed at different entities, and come up with a novel and\nhigh-quality annotated dataset of tweets written by US politicians. Then, we\npropose a relational learning model to predict moral attitudes towards entities\nand moral foundations jointly. We do qualitative and quantitative evaluations,\nshowing that moral sentiment towards entities differs highly across political\nideologies.",
          "link": "http://arxiv.org/abs/2109.04535",
          "publishedOn": "2021-09-13T07:20:23.309Z",
          "wordCount": 579,
          "title": "Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04526",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Lin_C/0/1/0/all/0/1\">Christy Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sussman_D/0/1/0/all/0/1\">Daniel Sussman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ishwar_P/0/1/0/all/0/1\">Prakash Ishwar</a>",
          "description": "Random walk based node embedding algorithms learn vector representations of\nnodes by optimizing an objective function of node embedding vectors and\nskip-bigram statistics computed from random walks on the network. They have\nbeen applied to many supervised learning problems such as link prediction and\nnode classification and have demonstrated state-of-the-art performance. Yet,\ntheir properties remain poorly understood. This paper studies properties of\nrandom walk based node embeddings in the unsupervised setting of discovering\nhidden block structure in the network, i.e., learning node representations\nwhose cluster structure in Euclidean space reflects their adjacency structure\nwithin the network. We characterize the ergodic limits of the embedding\nobjective, its generalization, and related convex relaxations to derive\ncorresponding non-randomized versions of the node embedding objectives. We also\ncharacterize the optimal node embedding Grammians of the non-randomized\nobjectives for the expected graph of a two-community Stochastic Block Model\n(SBM). We prove that the solution Grammian has rank $1$ for a suitable nuclear\nnorm relaxation of the non-randomized objective. Comprehensive experimental\nresults on SBM random networks reveal that our non-randomized ergodic\nobjectives yield node embeddings whose distribution is Gaussian-like, centered\nat the node embeddings of the expected network within each community, and\nconcentrate in the linear degree-scaling regime as the number of nodes\nincreases.",
          "link": "http://arxiv.org/abs/2109.04526",
          "publishedOn": "2021-09-13T07:20:23.199Z",
          "wordCount": 655,
          "title": "Ergodic Limits, Relaxations, and Geometric Properties of Random Walk Node Embeddings. (arXiv:2109.04526v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Silva_M/0/1/0/all/0/1\">Mirela Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceschin_F/0/1/0/all/0/1\">Fabr&#xed;cio Ceschin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_P/0/1/0/all/0/1\">Prakash Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brant_C/0/1/0/all/0/1\">Christopher Brant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilda_S/0/1/0/all/0/1\">Shlok Gilda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_J/0/1/0/all/0/1\">Juliana Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Catia S. Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gregio_A/0/1/0/all/0/1\">Andr&#xe9; Gr&#xe9;gio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1\">Daniela Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giovanini_L/0/1/0/all/0/1\">Luiz Giovanini</a>",
          "description": "Misinformation entails the dissemination of falsehoods that leads to the slow\nfracturing of society via decreased trust in democratic processes,\ninstitutions, and science. The public has grown aware of the role of social\nmedia as a superspreader of untrustworthy information, where even pandemics\nhave not been immune. In this paper, we focus on COVID-19 misinformation and\nexamine a subset of 2.1M tweets to understand misinformation as a function of\nengagement, tweet content (COVID-19- vs. non-COVID-19-related), and veracity\n(misleading or factual). Using correlation analysis, we show the most relevant\nfeature subsets among over 126 features that most heavily correlate with\nmisinformation or facts. We found that (i) factual tweets, regardless of\nwhether COVID-related, were more engaging than misinformation tweets; and (ii)\nfeatures that most heavily correlated with engagement varied depending on the\nveracity and content of the tweet.",
          "link": "http://arxiv.org/abs/2012.02164",
          "publishedOn": "2021-09-10T07:20:15.285Z",
          "wordCount": 718,
          "title": "People Still Care About Facts: Twitter Users Engage More with Factual Discourse than Misinformation--A Comparison Between COVID and General Narratives on Twitter. (arXiv:2012.02164v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04433",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bhatt_S/0/1/0/all/0/1\">Sujay Bhatt</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Samorodnitsky_G/0/1/0/all/0/1\">Gennady Samorodnitsky</a>",
          "description": "We consider a multi-armed bandit problem motivated by situations where only\nthe extreme values, as opposed to expected values in the classical bandit\nsetting, are of interest. We propose distribution free algorithms using robust\nstatistics and characterize the statistical properties. We show that the\nprovided algorithms achieve vanishing extremal regret under weaker conditions\nthan existing algorithms. Performance of the algorithms is demonstrated for the\nfinite-sample setting using numerical experiments. The results show superior\nperformance of the proposed algorithms compared to the well known algorithms.",
          "link": "http://arxiv.org/abs/2109.04433",
          "publishedOn": "2021-09-10T07:20:15.265Z",
          "wordCount": 519,
          "title": "Extreme Bandits using Robust Statistics. (arXiv:2109.04433v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04364",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghassemi_N/0/1/0/all/0/1\">Navid Ghassemi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khodatars_M/0/1/0/all/0/1\">Marjane Khodatars</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moridian_P/0/1/0/all/0/1\">Parisa Moridian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zare_A/0/1/0/all/0/1\">Assef Zare</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subasi_A/0/1/0/all/0/1\">Abdulhamit Subasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1\">J. Manuel Gorriz</a>",
          "description": "Epilepsy is one of the most crucial neurological disorders, and its early\ndiagnosis will help the clinicians to provide accurate treatment for the\npatients. The electroencephalogram (EEG) signals are widely used for epileptic\nseizures detection, which provides specialists with substantial information\nabout the functioning of the brain. In this paper, a novel diagnostic procedure\nusing fuzzy theory and deep learning techniques are introduced. The proposed\nmethod is evaluated on the Bonn University dataset with six classification\ncombinations and also on the Freiburg dataset. The tunable-Q wavelet transform\n(TQWT) is employed to decompose the EEG signals into different sub-bands. In\nthe feature extraction step, 13 different fuzzy entropies are calculated from\ndifferent sub-bands of TQWT, and their computational complexities are\ncalculated to help researchers choose the best feature sets. In the following,\nan autoencoder (AE) with six layers is employed for dimensionality reduction.\nFinally, the standard adaptive neuro-fuzzy inference system (ANFIS), and also\nits variants with grasshopper optimization algorithm (ANFIS-GOA), particle\nswarm optimization (ANFIS-PSO), and breeding swarm optimization (ANFIS-BS)\nmethods are used for classification. Using our proposed method, ANFIS-BS method\nhas obtained an accuracy of 99.74% in classifying into two classes and an\naccuracy of 99.46% in ternary classification on the Bonn dataset and 99.28% on\nthe Freiburg dataset, reaching state-of-the-art performances on both of them.",
          "link": "http://arxiv.org/abs/2109.04364",
          "publishedOn": "2021-09-10T07:20:15.237Z",
          "wordCount": 687,
          "title": "Detection of Epileptic Seizures on EEG Signals Using ANFIS Classifier, Autoencoders and Fuzzy Entropies. (arXiv:2109.04364v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04408",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>",
          "description": "Training NLP systems typically assumes access to annotated data that has a\nsingle human label per example. Given imperfect labeling from annotators and\ninherent ambiguity of language, we hypothesize that single label is not\nsufficient to learn the spectrum of language interpretation. We explore new\nlabel annotation distribution schemes, assigning multiple labels per example\nfor a small subset of training examples. Introducing such multi label examples\nat the cost of annotating fewer examples brings clear gains on natural language\ninference task and entity typing task, even when we simply first train with a\nsingle label data and then fine tune with multi label examples. Extending a\nMixUp data augmentation framework, we propose a learning algorithm that can\nlearn from uneven training examples (with zero, one, or multiple labels). This\nalgorithm efficiently combines signals from uneven training data and brings\nadditional gains in low annotation budget and cross domain settings. Together,\nour method achieves consistent gains in both accuracy and label distribution\nmetrics in two tasks, suggesting training with uneven training data can be\nbeneficial for many NLP tasks.",
          "link": "http://arxiv.org/abs/2109.04408",
          "publishedOn": "2021-09-10T07:20:15.228Z",
          "wordCount": 642,
          "title": "Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pei_F/0/1/0/all/0/1\">Felix Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Joel Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoltowski_D/0/1/0/all/0/1\">David Zoltowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anqi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_R/0/1/0/all/0/1\">Raeed H. Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_H/0/1/0/all/0/1\">Hansem Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODoherty_J/0/1/0/all/0/1\">Joseph E. O&#x27;Doherty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1\">Krishna V. Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufman_M/0/1/0/all/0/1\">Matthew T. Kaufman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Churchland_M/0/1/0/all/0/1\">Mark Churchland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazayeri_M/0/1/0/all/0/1\">Mehrdad Jazayeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_L/0/1/0/all/0/1\">Lee E. Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillow_J/0/1/0/all/0/1\">Jonathan Pillow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1\">Il Memming Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Eva L. Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandarinath_C/0/1/0/all/0/1\">Chethan Pandarinath</a>",
          "description": "Advances in neural recording present increasing opportunities to study neural\nactivity in unprecedented detail. Latent variable models (LVMs) are promising\ntools for analyzing this rich activity across diverse neural systems and\nbehaviors, as LVMs do not depend on known relationships between the activity\nand external experimental variables. However, progress in latent variable\nmodeling is currently impeded by a lack of standardization, resulting in\nmethods being developed and compared in an ad hoc manner. To coordinate these\nmodeling efforts, we introduce a benchmark suite for latent variable modeling\nof neural population activity. We curate four datasets of neural spiking\nactivity from cognitive, sensory, and motor areas to promote models that apply\nto the wide variety of activity seen across these areas. We identify\nunsupervised evaluation as a common framework for evaluating models across\ndatasets, and apply several baselines that demonstrate benchmark diversity. We\nrelease this benchmark through EvalAI. this http URL",
          "link": "http://arxiv.org/abs/2109.04463",
          "publishedOn": "2021-09-10T07:20:15.215Z",
          "wordCount": 634,
          "title": "Neural Latents Benchmark '21: Evaluating latent variable models of neural population activity. (arXiv:2109.04463v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiquier_M/0/1/0/all/0/1\">Mia Chiquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>",
          "description": "We find that images contain intrinsic structure that enables the reversal of\nmany adversarial attacks. Attack vectors cause not only image classifiers to\nfail, but also collaterally disrupt incidental structure in the image. We\ndemonstrate that modifying the attacked image to restore the natural structure\nwill reverse many types of attacks, providing a defense. Experiments\ndemonstrate significantly improved robustness for several state-of-the-art\nmodels across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results\nshow that our defense is still effective even if the attacker is aware of the\ndefense mechanism. Since our defense is deployed during inference instead of\ntraining, it is compatible with pre-trained networks as well as most other\ndefenses. Our results suggest deep networks are vulnerable to adversarial\nexamples partly because their representations do not enforce the natural\nstructure of images.",
          "link": "http://arxiv.org/abs/2103.14222",
          "publishedOn": "2021-09-10T07:20:15.197Z",
          "wordCount": 626,
          "title": "Adversarial Attacks are Reversible with Natural Supervision. (arXiv:2103.14222v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04356",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Haining Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paiva_A/0/1/0/all/0/1\">Antonio Paiva</a>",
          "description": "The proliferation of IoT sensors and their deployment in various industries\nand applications has brought about numerous analysis opportunities in this Big\nData era. However, drift of those sensor measurements poses major challenges to\nautomate data analysis and the ability to effectively train and deploy models\non a continuous basis. In this paper we study and test several approaches from\nthe literature with regard to their ability to cope with and adapt to sensor\ndrift under realistic conditions. Most of these approaches are recent and thus\nare representative of the current state-of-the-art. The testing was performed\non a publicly available gas sensor dataset exhibiting drift over time. The\nresults show substantial drops in sensing performance due to sensor drift in\nspite of the approaches. We then discuss several issues identified with current\napproaches and outline directions for future research to tackle them.",
          "link": "http://arxiv.org/abs/2109.04356",
          "publishedOn": "2021-09-10T07:20:15.182Z",
          "wordCount": 622,
          "title": "Assessing Machine Learning Approaches to Address IoT Sensor Drift. (arXiv:2109.04356v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1\">Anna Korhonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>",
          "description": "Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30\nseconds without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective universal lexical and sentence encoders.",
          "link": "http://arxiv.org/abs/2104.08027",
          "publishedOn": "2021-09-10T07:20:15.163Z",
          "wordCount": 694,
          "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hanna_O/0/1/0/all/0/1\">Osama A. Hanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ezzeldin_Y/0/1/0/all/0/1\">Yahya H. Ezzeldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1\">Christina Fragouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diggavi_S/0/1/0/all/0/1\">Suhas Diggavi</a>",
          "description": "We consider machine learning applications that train a model by leveraging\ndata distributed over a trusted network, where communication constraints can\ncreate a performance bottleneck. A number of recent approaches propose to\novercome this bottleneck through compression of gradient updates. However, as\nmodels become larger, so does the size of the gradient updates. In this paper,\nwe propose an alternate approach to learn from distributed data that quantizes\ndata instead of gradients, and can support learning over applications where the\nsize of gradient updates is prohibitive. Our approach leverages the dependency\nof the computed gradient on data samples, which lie in a much smaller space in\norder to perform the quantization in the smaller dimension data space. At the\ncost of an extra gradient computation, the gradient estimate can be refined by\nconveying the difference between the gradient at the quantized data point and\nthe original gradient using a small number of bits. Lastly, in order to save\ncommunication, our approach adds a layer that decides whether to transmit a\nquantized data sample or not based on its importance for learning. We analyze\nthe convergence of the proposed approach for smooth convex and non-convex\nobjective functions and show that we can achieve order optimal convergence\nrates with communication that mostly depends on the data rather than the model\n(gradient) dimension. We use our proposed algorithm to train ResNet models on\nthe CIFAR-10 and ImageNet datasets, and show that we can achieve an order of\nmagnitude savings over gradient compression methods. These communication\nsavings come at the cost of increasing computation at the learning agent, and\nthus our approach is beneficial in scenarios where communication load is the\nmain problem.",
          "link": "http://arxiv.org/abs/2012.07913",
          "publishedOn": "2021-09-10T07:20:15.156Z",
          "wordCount": 755,
          "title": "Quantizing data for distributed learning. (arXiv:2012.07913v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Cat P. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Juncheng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>",
          "description": "We formulate an asymmetric (or non-commutative) distance between tasks based\non Fisher Information Matrices. We provide proof of consistency for our\ndistance through theorems and experiments on various classification tasks. We\nthen apply our proposed measure of task distance in transfer learning on visual\ntasks in the Taskonomy dataset. Additionally, we show how the proposed distance\nbetween a target task and a set of baseline tasks can be used to reduce the\nneural architecture search space for the target task. The complexity reduction\nin search space for task-specific architectures is achieved by building on the\noptimized architectures for similar tasks instead of doing a full search\nwithout using this side information. Experimental results demonstrate the\nefficacy of the proposed approach and its improvements over other methods.",
          "link": "http://arxiv.org/abs/2103.12827",
          "publishedOn": "2021-09-10T07:20:15.149Z",
          "wordCount": 628,
          "title": "Fisher Task Distance and Its Applications in Transfer Learning and Neural Architecture Search. (arXiv:2103.12827v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>",
          "description": "Truth can vary over time. Fact-checking decisions on claim veracity should\ntherefore take into account temporal information of both the claim and\nsupporting or refuting evidence. In this work, we investigate the hypothesis\nthat the timestamp of a Web page is crucial to how it should be ranked for a\ngiven claim. We delineate four temporal ranking methods that constrain evidence\nranking differently and simulate hypothesis-specific evidence rankings given\nthe evidence timestamps as gold standard. Evidence ranking in three\nfact-checking models is ultimately optimized using a learning-to-rank loss\nfunction. Our study reveals that time-aware evidence ranking not only surpasses\nrelevance assumptions based purely on semantic similarity or position in a\nsearch results list, but also improves veracity predictions of time-sensitive\nclaims in particular.",
          "link": "http://arxiv.org/abs/2009.06402",
          "publishedOn": "2021-09-10T07:20:15.142Z",
          "wordCount": 627,
          "title": "Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar R.Zaiane</a>",
          "description": "Most recent semantic segmentation methods adopt a U-Net framework with an\nencoder-decoder architecture. It is still challenging for U-Net with a simple\nskip connection scheme to model the global multi-scale context: 1) Not each\nskip connection setting is effective due to the issue of incompatible feature\nsets of encoder and decoder stage, even some skip connection negatively\ninfluence the segmentation performance; 2) The original U-Net is worse than the\none without any skip connection on some datasets. Based on our findings, we\npropose a new segmentation framework, named UCTransNet (with a proposed CTrans\nmodule in U-Net), from the channel perspective with attention mechanism.\nSpecifically, the CTrans module is an alternate of the U-Net skip connections,\nwhich consists of a sub-module to conduct the multi-scale Channel Cross fusion\nwith Transformer (named CCT) and a sub-module Channel-wise Cross-Attention\n(named CCA) to guide the fused multi-scale channel-wise information to\neffectively connect to the decoder features for eliminating the ambiguity.\nHence, the proposed connection consisting of the CCT and CCA is able to replace\nthe original skip connection to solve the semantic gaps for an accurate\nautomatic medical image segmentation. The experimental results suggest that our\nUCTransNet produces more precise segmentation performance and achieves\nconsistent improvements over the state-of-the-art for semantic segmentation\nacross different datasets and conventional architectures involving transformer\nor U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.",
          "link": "http://arxiv.org/abs/2109.04335",
          "publishedOn": "2021-09-10T07:20:15.136Z",
          "wordCount": 689,
          "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04392",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Charles Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lemay_A/0/1/0/all/0/1\">Andreanne Lemay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalpathy_Cramer_J/0/1/0/all/0/1\">Jayashree Kalpathy-Cramer</a>",
          "description": "Deep learning has the potential to augment many components of the clinical\nworkflow, such as medical image interpretation. However, the translation of\nthese black box algorithms into clinical practice has been marred by the\nrelative lack of transparency compared to conventional machine learning\nmethods, hindering in clinician trust in the systems for critical medical\ndecision-making. Specifically, common deep learning approaches do not have\nintuitive ways of expressing uncertainty with respect to cases that might\nrequire further human review. Furthermore, the possibility of algorithmic bias\nhas caused hesitancy regarding the use of developed algorithms in clinical\nsettings. To these ends, we explore how conformal methods can complement deep\nlearning models by providing both clinically intuitive way (by means of\nconfidence prediction sets) of expressing model uncertainty as well as\nfacilitating model transparency in clinical workflows. In this paper, we\nconduct a field survey with clinicians to assess clinical use-cases of\nconformal predictions. Next, we conduct experiments with a mammographic breast\ndensity and dermatology photography datasets to demonstrate the utility of\nconformal predictions in \"rule-in\" and \"rule-out\" disease scenarios. Further,\nwe show that conformal predictors can be used to equalize coverage with respect\nto patient demographics such as race and skin tone. We find that a conformal\npredictions to be a promising framework with potential to increase clinical\nusability and transparency for better collaboration between deep learning\nalgorithms and clinicians.",
          "link": "http://arxiv.org/abs/2109.04392",
          "publishedOn": "2021-09-10T07:20:15.109Z",
          "wordCount": 687,
          "title": "Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thummerer_T/0/1/0/all/0/1\">Tobias Thummerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kircher_J/0/1/0/all/0/1\">Josef Kircher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikelsons_L/0/1/0/all/0/1\">Lars Mikelsons</a>",
          "description": "This paper covers two major subjects: First, the presentation of a new\nopen-source library called FMI.jl for integrating FMI into the Julia\nprogramming environment by providing the possibility to load, parameterize and\nsimulate FMUs. Further, an extension to this library called FMIFlux.jl is\nintroduced, that allows the integration of FMUs into a neural network topology\nto obtain a NeuralFMU. This structural combination of an industry typical\nblack-box model and a data-driven machine learning model combines the different\nadvantages of both modeling approaches in one single development environment.\nThis allows for the usage of advanced data driven modeling techniques for\nphysical effects that are difficult to model based on first principles.",
          "link": "http://arxiv.org/abs/2109.04351",
          "publishedOn": "2021-09-10T07:20:15.103Z",
          "wordCount": 549,
          "title": "NeuralFMU: Towards Structural Integration of FMUs into Neural Networks. (arXiv:2109.04351v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.01512",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>, <a href=\"http://arxiv.org/find/math/1/au:+To_T/0/1/0/all/0/1\">Tat Dat To</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu Hang Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Helmy_M/0/1/0/all/0/1\">Maged Helmy</a>",
          "description": "We propose in this paper New Q-Newton's method. The update rule is very\nsimple conceptually, for example $x_{n+1}=x_n-w_n$ where\n$w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\\nabla ^2f(x_n)+\\delta\n_n||\\nabla f(x_n)||^2.Id$ and $v_n=A_n^{-1}.\\nabla f(x_n)$. Here $\\delta _n$ is\nan appropriate real number so that $A_n$ is invertible, and $pr_{A_n,\\pm}$ are\nprojections to the vector subspaces generated by eigenvectors of positive\n(correspondingly negative) eigenvalues of $A_n$.\n\nThe main result of this paper roughly says that if $f$ is $C^3$ (can be\nunbounded from below) and a sequence $\\{x_n\\}$, constructed by the New\nQ-Newton's method from a random initial point $x_0$, {\\bf converges}, then the\nlimit point is a critical point and is not a saddle point, and the convergence\nrate is the same as that of Newton's method. The first author has recently been\nsuccessful incorporating Backtracking line search to New Q-Newton's method,\nthus resolving the convergence guarantee issue observed for some (non-smooth)\ncost functions. An application to quickly finding zeros of a univariate\nmeromorphic function will be discussed. Various experiments are performed,\nagainst well known algorithms such as BFGS and Adaptive Cubic Regularization\nare presented.",
          "link": "http://arxiv.org/abs/2006.01512",
          "publishedOn": "2021-09-10T07:20:15.096Z",
          "wordCount": 730,
          "title": "A fast and simple modification of Newton's method helping to avoid saddle points. (arXiv:2006.01512v4 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04352",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salehi_Y/0/1/0/all/0/1\">Yasmin Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giannacopoulos_D/0/1/0/all/0/1\">Dennis Giannacopoulos</a>",
          "description": "Correctly capturing intraoperative brain shift in image-guided neurosurgical\nprocedures is a critical task for aligning preoperative data with\nintraoperative geometry, ensuring effective surgical navigation and optimal\nsurgical precision. While the finite element method (FEM) is a proven technique\nto effectively approximate soft tissue deformation through biomechanical\nformulations, their degree of success boils down to a trade-off between\naccuracy and speed. To circumvent this problem, the most recent works in this\ndomain have proposed leveraging data-driven models obtained by training various\nmachine learning algorithms, e.g. random forests, artificial neural networks\n(ANNs), with the results of finite element analysis (FEA) to speed up tissue\ndeformation approximations by prediction. These methods, however, do not\naccount for the structure of the finite element (FE) mesh during training that\nprovides information on node connectivities as well as the distance between\nthem, which can aid with approximating tissue deformation based on the\nproximity of force load points with the rest of the mesh nodes. Therefore, this\nwork proposes a novel framework, PhysGNN, a data-driven model that approximates\nthe solution of FEA by leveraging graph neural networks (GNNs), which are\ncapable of accounting for the mesh structural information and inductive\nlearning over unstructured grids and complex topological structures.\nEmpirically, we demonstrate that the proposed architecture, PhysGNN, promises\naccurate and fast soft tissue deformation approximations while remaining\ncomputationally feasible, suitable for neurosurgical settings.",
          "link": "http://arxiv.org/abs/2109.04352",
          "publishedOn": "2021-09-10T07:20:15.087Z",
          "wordCount": 697,
          "title": "PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1\">Preethi Lahoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1\">Krishna P. Gummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Reliably predicting potential failure risks of machine learning (ML) systems\nwhen deployed with production data is a crucial aspect of trustworthy AI. This\npaper introduces Risk Advisor, a novel post-hoc meta-learner for estimating\nfailure risks and predictive uncertainties of any already-trained black-box\nclassification model. In addition to providing a risk score, the Risk Advisor\ndecomposes the uncertainty estimates into aleatoric and epistemic uncertainty\ncomponents, thus giving informative insights into the sources of uncertainty\ninducing the failures. Consequently, Risk Advisor can distinguish between\nfailures caused by data variability, data shifts and model limitations and\nadvise on mitigation actions (e.g., collecting more data to counter data\nshift). Extensive experiments on various families of black-box classification\nmodels and on real-world and synthetic datasets covering common ML failure\nscenarios show that the Risk Advisor reliably predicts deployment-time failure\nrisks in all the scenarios, and outperforms strong baselines.",
          "link": "http://arxiv.org/abs/2109.04432",
          "publishedOn": "2021-09-10T07:20:15.080Z",
          "wordCount": 611,
          "title": "Detecting and Mitigating Test-time Failure Risks via Model-agnostic Uncertainty Learning. (arXiv:2109.04432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.09298",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1\">Kaiwen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolmanovsky_I/0/1/0/all/0/1\">Ilya Kolmanovsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rizzo_D/0/1/0/all/0/1\">Denise Rizzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Girard_A/0/1/0/all/0/1\">Anouck Girard</a>",
          "description": "This paper proposes a learning reference governor (LRG) approach to enforce\nstate and control constraints in systems for which an accurate model is\nunavailable, and this approach enables the reference governor to gradually\nimprove command tracking performance through learning while enforcing the\nconstraints during learning and after learning is completed. The learning can\nbe performed either on a black-box type model of the system or directly on the\nhardware. After introducing the LRG algorithm and outlining its theoretical\nproperties, this paper investigates LRG application to fuel truck (tank truck)\nrollover avoidance. Through simulations based on a fuel truck model that\naccounts for liquid fuel sloshing effects, we show that the proposed LRG can\neffectively protect fuel trucks from rollover accidents under various operating\nconditions.",
          "link": "http://arxiv.org/abs/2101.09298",
          "publishedOn": "2021-09-10T07:20:15.063Z",
          "wordCount": 606,
          "title": "Safe Learning Reference Governor: Theory and Application to Fuel Truck Rollover Avoidance. (arXiv:2101.09298v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Han Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_C/0/1/0/all/0/1\">Chen Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1\">Bryon Aragam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi S. Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_G/0/1/0/all/0/1\">Geoffrey J. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>",
          "description": "A wide range of machine learning applications such as privacy-preserving\nlearning, algorithmic fairness, and domain adaptation/generalization among\nothers, involve learning \\emph{invariant representations} of the data that aim\nto achieve two competing goals: (a) maximize information or accuracy with\nrespect to a target response, and (b) maximize invariance or independence with\nrespect to a set of protected features (e.g.\\ for fairness, privacy, etc).\nDespite their wide applicability, theoretical understanding of the optimal\ntradeoffs -- with respect to accuracy, and invariance -- achievable by\ninvariant representations is still severely lacking. In this paper, we provide\nprecisely such an information-theoretic analysis of such tradeoffs under both\nclassification and regression settings. We provide a geometric characterization\nof the accuracy and invariance achievable by any representation of the data; we\nterm this feasible region the information plane. We provide a lower bound for\nthis feasible region for the classification case, and an exact characterization\nfor the regression case, which allows us to either bound or exactly\ncharacterize the Pareto optimal frontier between accuracy and invariance.\nAlthough our contributions are mainly theoretical, a key practical application\nof our results is in certifying the potential sub-optimality of any given\nrepresentation learning algorithm for either classification or regression\ntasks. Our results shed new light on the fundamental interplay between accuracy\nand invariance, and may be useful in guiding the design of future\nrepresentation learning algorithms.",
          "link": "http://arxiv.org/abs/2012.10713",
          "publishedOn": "2021-09-10T07:20:15.057Z",
          "wordCount": 723,
          "title": "Fundamental Limits and Tradeoffs in Invariant Representation Learning. (arXiv:2012.10713v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.09144",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tetali_H/0/1/0/all/0/1\">Harsha Vardhan Tetali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harley_J/0/1/0/all/0/1\">Joel B. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1\">Benjamin D. Haeffele</a>",
          "description": "With the recent success of representation learning methods, which includes\ndeep learning as a special case, there has been considerable interest in\ndeveloping representation learning techniques that can incorporate known\nphysical constraints into the learned representation. As one example, in many\napplications that involve a signal propagating through physical media (e.g.,\noptics, acoustics, fluid dynamics, etc), it is known that the dynamics of the\nsignal must satisfy constraints imposed by the wave equation. Here we propose a\nmatrix factorization technique that decomposes such signals into a sum of\ncomponents, where each component is regularized to ensure that it satisfies\nwave equation constraints. Although our proposed formulation is non-convex, we\nprove that our model can be efficiently solved to global optimality in\npolynomial time. We demonstrate the benefits of our work by applications in\nstructural health monitoring, where prior work has attempted to solve this\nproblem using sparse dictionary learning approaches that do not come with any\ntheoretical guarantees regarding convergence to global optimality and employ\nheuristics to capture desired physical constraints.",
          "link": "http://arxiv.org/abs/2107.09144",
          "publishedOn": "2021-09-10T07:20:15.041Z",
          "wordCount": 642,
          "title": "Wave-Informed Matrix Factorization with Global Optimality Guarantees. (arXiv:2107.09144v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vignesh Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_O/0/1/0/all/0/1\">Omer Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Neel Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1\">Yair Zick</a>",
          "description": "Equilibrium computation in markets usually considers settings where player\nvaluation functions are known. We consider the setting where player valuations\nare unknown; using a PAC learning-theoretic framework, we analyze some classes\nof common valuation functions, and provide algorithms which output direct PAC\nequilibrium allocations, not estimates based on attempting to learn valuation\nfunctions. Since there exist trivial PAC market outcomes with an unbounded\nworst-case efficiency loss, we lower-bound the efficiency of our algorithms.\nWhile the efficiency loss under general distributions is rather high, we show\nthat in some cases (e.g., unit-demand valuations), it is possible to find a PAC\nmarket equilibrium with significantly better utility.",
          "link": "http://arxiv.org/abs/2012.14838",
          "publishedOn": "2021-09-10T07:20:15.031Z",
          "wordCount": 596,
          "title": "The Price is (Probably) Right: Learning Market Equilibria from Samples. (arXiv:2012.14838v3 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+DellEva_A/0/1/0/all/0/1\">Anthony Dell&#x27;Eva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertozzi_M/0/1/0/all/0/1\">Massimo Bertozzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>",
          "description": "Image-to-image (i2i) networks struggle to capture local changes because they\ndo not affect the global scene structure. For example, translating from highway\nscenes to offroad, i2i networks easily focus on global color features but\nignore obvious traits for humans like the absence of lane markings. In this\npaper, we leverage human knowledge about spatial domain characteristics which\nwe refer to as 'local domains' and demonstrate its benefit for image-to-image\ntranslation. Relying on a simple geometrical guidance, we train a patch-based\nGAN on few source data and hallucinate a new unseen domain which subsequently\neases transfer learning to target. We experiment on three tasks ranging from\nunstructured environments to adverse weather. Our comprehensive evaluation\nsetting shows we are able to generate realistic translations, with minimal\npriors, and training only on a few images. Furthermore, when trained on our\ntranslations images we show that all tested proxy tasks are significantly\nimproved, without ever seeing target domain at training.",
          "link": "http://arxiv.org/abs/2109.04468",
          "publishedOn": "2021-09-10T07:20:15.008Z",
          "wordCount": 613,
          "title": "Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08181",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Doostmohammadian_M/0/1/0/all/0/1\">Mohammadreza Doostmohammadian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aghasi_A/0/1/0/all/0/1\">Alireza Aghasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pirani_M/0/1/0/all/0/1\">Mohammad Pirani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nekouei_E/0/1/0/all/0/1\">Ehsan Nekouei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_U/0/1/0/all/0/1\">Usman A. Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Charalambous_T/0/1/0/all/0/1\">Themistoklis Charalambous</a>",
          "description": "This paper proposes networked dynamics to solve resource allocation problems\nover time-varying multi-agent networks. The state of each agent represents the\namount of used resources (or produced utilities) while the total amount of\nresources is fixed. The idea is to optimally allocate the resources among the\ngroup of agents by minimizing the overall cost function subject to fixed sum of\nresources. Each agents' information is restricted to its own state and cost\nfunction and those of its immediate in-neighbors. This is motivated by\ndistributed applications such as mobile edge-computing, economic dispatch over\nsmart grids, and multi-agent coverage control. This work provides a fast\nconvergent solution (in comparison with linear dynamics) while considering\nrelaxed network connectivity with quantized communication links. The proposed\ndynamics reaches optimal solution over switching (possibly disconnected)\nundirected networks as far as their union over some bounded non-overlapping\ntime-intervals has a spanning-tree. We prove feasibility of the solution,\nuniqueness of the optimal state, and convergence to the optimal value under the\nproposed dynamics, where the analysis is applicable to similar 1st-order\nallocation dynamics with strongly sign-preserving nonlinearities, such as\nactuator saturation.",
          "link": "http://arxiv.org/abs/2012.08181",
          "publishedOn": "2021-09-10T07:20:15.001Z",
          "wordCount": 687,
          "title": "Fast-Convergent Dynamics for Distributed Allocation of Resources Over Switching Sparse Networks with Quantized Communication Links. (arXiv:2012.08181v3 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04460",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1\">Sumit Kumar Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ewetz_R/0/1/0/all/0/1\">Rickard Ewetz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Velasquez_A/0/1/0/all/0/1\">Alvaro Velasquez</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>",
          "description": "Deep neural networks such as AlphaFold and RoseTTAFold predict remarkably\naccurate structures of proteins compared to other algorithmic approaches. It is\nknown that biologically small perturbations in the protein sequence do not lead\nto drastic changes in the protein structure. In this paper, we demonstrate that\nRoseTTAFold does not exhibit such a robustness despite its high accuracy, and\nbiologically small perturbations for some input sequences result in radically\ndifferent predicted protein structures. This raises the challenge of detecting\nwhen these predicted protein structures cannot be trusted. We define the\nrobustness measure for the predicted structure of a protein sequence to be the\ninverse of the root-mean-square distance (RMSD) in the predicted structure and\nthe structure of its adversarially perturbed sequence. We use adversarial\nattack methods to create adversarial protein sequences, and show that the RMSD\nin the predicted protein structure ranges from 0.119\\r{A} to 34.162\\r{A} when\nthe adversarial perturbations are bounded by 20 units in the BLOSUM62 distance.\nThis demonstrates very high variance in the robustness measure of the predicted\nstructures. We show that the magnitude of the correlation (0.917) between our\nrobustness measure and the RMSD between the predicted structure and the ground\ntruth is high, that is, the predictions with low robustness measure cannot be\ntrusted. This is the first paper demonstrating the susceptibility of\nRoseTTAFold to adversarial attacks.",
          "link": "http://arxiv.org/abs/2109.04460",
          "publishedOn": "2021-09-10T07:20:14.983Z",
          "wordCount": 670,
          "title": "Protein Folding Neural Networks Are Not Robust. (arXiv:2109.04460v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/1911.03063",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ding_J/0/1/0/all/0/1\">Jie Ding</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhong Yang</a>",
          "description": "In recent years, many non-traditional classification methods, such as Random\nForest, Boosting, and neural network, have been widely used in applications.\nTheir performance is typically measured in terms of classification accuracy.\nWhile the classification error rate and the like are important, they do not\naddress a fundamental question: Is the classification method underfitted? To\nour best knowledge, there is no existing method that can assess the\ngoodness-of-fit of a general classification procedure. Indeed, the lack of a\nparametric assumption makes it challenging to construct proper tests. To\novercome this difficulty, we propose a methodology called BAGofT that splits\nthe data into a training set and a validation set. First, the classification\nprocedure to assess is applied to the training set, which is also used to\nadaptively find a data grouping that reveals the most severe regions of\nunderfitting. Then, based on this grouping, we calculate a test statistic by\ncomparing the estimated success probabilities and the actual observed responses\nfrom the validation set. The data splitting guarantees that the size of the\ntest is controlled under the null hypothesis, and the power of the test goes to\none as the sample size increases under the alternative hypothesis. For testing\nparametric classification models, the BAGofT has a broader scope than the\nexisting methods since it is not restricted to specific parametric models\n(e.g., logistic regression). Extensive simulation studies show the utility of\nthe BAGofT when assessing general classification procedures and its strengths\nover some existing methods when testing parametric classification models.",
          "link": "http://arxiv.org/abs/1911.03063",
          "publishedOn": "2021-09-10T07:20:14.977Z",
          "wordCount": 718,
          "title": "Is a Classification Procedure Good Enough? A Goodness-of-Fit Assessment Tool for Classification Learning. (arXiv:1911.03063v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08164",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "The factual knowledge acquired during pre-training and stored in the\nparameters of Language Models (LMs) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod which can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditordoes not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components need to be changed to manipulate\nfactual knowledge; our analysis shows that the updates tend to be concentrated\non a small subset of components. Source code available at\nhttps://github.com/nicola-decao/KnowledgeEditor",
          "link": "http://arxiv.org/abs/2104.08164",
          "publishedOn": "2021-09-10T07:20:14.970Z",
          "wordCount": 732,
          "title": "Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.05285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shalit_N/0/1/0/all/0/1\">Nadav Shalit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fire_M/0/1/0/all/0/1\">Michael Fire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Elia_E/0/1/0/all/0/1\">Eran Ben-Elia</a>",
          "description": "Public transport has become an essential part of urban existence with\nincreased population densities and environmental awareness. Large quantities of\ndata are currently generated, allowing for more robust methods to understand\ntravel behavior by harvesting smart card usage. However, public transport\ndatasets suffer from data integrity problems; boarding stop information may be\nmissing due to imperfect acquirement processes or inadequate reporting. We\ndeveloped a supervised machine learning method to impute missing boarding stops\nbased on ordinal classification using GTFS timetable, smart card, and\ngeospatial datasets. A new metric, Pareto Accuracy, is suggested to evaluate\nalgorithms where classes have an ordinal nature. Results are based on a case\nstudy in the city of Beer Sheva, Israel, consisting of one month of smart card\ndata. We show that our proposed method is robust to irregular travelers and\nsignificantly outperforms well-known imputation methods without the need to\nmine any additional datasets. Validation of data from another Israeli city\nusing transfer learning shows the presented model is general and context-free.\nThe implications for transportation planning and travel behavior research are\nfurther discussed.",
          "link": "http://arxiv.org/abs/2003.05285",
          "publishedOn": "2021-09-10T07:20:14.963Z",
          "wordCount": 658,
          "title": "A Supervised Machine Learning Model For Imputing Missing Boarding Stops In Smart Card Data. (arXiv:2003.05285v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Cong Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hangfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_Q/0/1/0/all/0/1\">Qi Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In this paper, we introduce the \\textit{Layer-Peeled Model}, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts of the network. We demonstrate that the\nLayer-Peeled Model, albeit simple, inherits many characteristics of\nwell-trained neural networks, thereby offering an effective tool for explaining\nand predicting common empirical patterns of deep learning training. First, when\nworking on class-balanced datasets, we prove that any solution to this model\nforms a simplex equiangular tight frame, which in part explains the recently\ndiscovered phenomenon of neural collapse \\cite{papyan2020prevalence}. More\nimportantly, when moving to the imbalanced case, our analysis of the\nLayer-Peeled Model reveals a hitherto unknown phenomenon that we term\n\\textit{Minority Collapse}, which fundamentally limits the performance of deep\nlearning models on the minority classes. In addition, we use the Layer-Peeled\nModel to gain insights into how to mitigate Minority Collapse. Interestingly,\nthis phenomenon is first predicted by the Layer-Peeled Model before being\nconfirmed by our computational experiments.",
          "link": "http://arxiv.org/abs/2101.12699",
          "publishedOn": "2021-09-10T07:20:14.871Z",
          "wordCount": null,
          "title": "Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. (arXiv:2101.12699v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00245",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1\">Linjun Zhang</a>",
          "description": "In this paper, we develop a general framework to design differentially\nprivate expectation-maximization (EM) algorithms in high-dimensional latent\nvariable models, based on the noisy iterative hard-thresholding. We derive the\nstatistical guarantees of the proposed framework and apply it to three specific\nmodels: Gaussian mixture, mixture of regression, and regression with missing\ncovariates. In each model, we establish the near-optimal rate of convergence\nwith differential privacy constraints, and show the proposed algorithm is\nminimax rate optimal up to logarithm factors. The technical tools developed for\nthe high-dimensional setting are then extended to the classic low-dimensional\nlatent variable models, and we propose a near rate-optimal EM algorithm with\ndifferential privacy guarantees in this setting. Simulation studies and real\ndata analysis are conducted to support our results.",
          "link": "http://arxiv.org/abs/2104.00245",
          "publishedOn": "2021-09-10T07:20:14.848Z",
          "wordCount": null,
          "title": "High-Dimensional Differentially-Private EM Algorithm: Methods and Near-Optimal Statistical Guarantees. (arXiv:2104.00245v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.05170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mantowsky_S/0/1/0/all/0/1\">Sven Mantowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heuer_F/0/1/0/all/0/1\">Falk Heuer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukhari_S/0/1/0/all/0/1\">Syed Saqib Bukhari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keckeisen_M/0/1/0/all/0/1\">Michael Keckeisen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_G/0/1/0/all/0/1\">Georg Schneider</a>",
          "description": "Development in the field of Single Board Computers (SBC) have been increasing\nfor several years. They provide a good balance between computing performance\nand power consumption which is usually required for mobile platforms, like\napplication in vehicles for Advanced Driver Assistance Systems (ADAS) and\nAutonomous Driving (AD). However, there is an ever-increasing need of more\npowerful and efficient SBCs which can run power intensive Deep Neural Networks\n(DNNs) in real-time and can also satisfy necessary functional safety\nrequirements such as Automotive Safety Integrity Level (ASIL). ProAI is being\ndeveloped by ZF mainly to run powerful and efficient applications such as\nmultitask DNNs and on top of that it also has the required safety certification\nfor AD. In this work, we compare and discuss state of the art SBC on the basis\nof power intensive multitask DNN architecture called Multitask-CenterNet with\nrespect to performance measures such as, FPS and power efficiency. As an\nautomotive supercomputer, ProAI delivers an excellent combination of\nperformance and efficiency, managing nearly twice the number of FPS per watt\nthan a modern workstation laptop and almost four times compared to the Jetson\nNano. Furthermore, it was also shown that there is still power in reserve for\nfurther and more complex tasks on the ProAI, based on the CPU and GPU\nutilization during the benchmark.",
          "link": "http://arxiv.org/abs/2108.05170",
          "publishedOn": "2021-09-10T07:20:14.848Z",
          "wordCount": null,
          "title": "ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study. (arXiv:2108.05170v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">April Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yufang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>",
          "description": "Jupyter notebook allows data scientists to write machine learning code\ntogether with its documentation in cells. In this paper, we propose a new task\nof code documentation generation (CDG) for computational notebooks. In contrast\nto the previous CDG tasks which focus on generating documentation for single\ncode snippets, in a computational notebook, one documentation in a markdown\ncell often corresponds to multiple code cells, and these code cells have an\ninherent structure. We proposed a new model (HAConvGNN) that uses a\nhierarchical attention mechanism to consider the relevant code cells and the\nrelevant code tokens information when generating the documentation. Tested on a\nnew corpus constructed from well-documented Kaggle notebooks, we show that our\nmodel outperforms other baseline models.",
          "link": "http://arxiv.org/abs/2104.01002",
          "publishedOn": "2021-09-10T07:20:14.837Z",
          "wordCount": null,
          "title": "HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks. (arXiv:2104.01002v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.01789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shaolei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linqi Song</a>",
          "description": "We study a cooperative multi-agent multi-armed bandits with M agents and K\narms. The goal of the agents is to minimized the cumulative regret. We adapt a\ntraditional Thompson Sampling algoirthm under the distributed setting. However,\nwith agent's ability to communicate, we note that communication may further\nreduce the upper bound of the regret for a distributed Thompson Sampling\napproach. To further improve the performance of distributed Thompson Sampling,\nwe propose a distributed Elimination based Thompson Sampling algorithm that\nallow the agents to learn collaboratively. We analyse the algorithm under\nBernoulli reward and derived a problem dependent upper bound on the cumulative\nregret.",
          "link": "http://arxiv.org/abs/2012.01789",
          "publishedOn": "2021-09-10T07:20:14.833Z",
          "wordCount": null,
          "title": "Distributed Thompson Sampling. (arXiv:2012.01789v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kanggeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1\">Won-Ki Jeong</a>",
          "description": "With the advent of advances in self-supervised learning, paired clean-noisy\ndata are no longer required in deep learning-based image denoising. However,\nexisting blind denoising methods still require the assumption with regard to\nnoise characteristics, such as zero-mean noise distribution and pixel-wise\nnoise-signal independence; this hinders wide adaptation of the method in the\nmedical domain. On the other hand, unpaired learning can overcome limitations\nrelated to the assumption on noise characteristics, which makes it more\nfeasible for collecting the training data in real-world scenarios. In this\npaper, we propose a novel image denoising scheme, Interdependent\nSelf-Cooperative Learning (ISCL), that leverages unpaired learning by combining\ncyclic adversarial learning with self-supervised residual learning. Unlike the\nexisting unpaired image denoising methods relying on matching data\ndistributions in different domains, the two architectures in ISCL, designed for\ndifferent tasks, complement each other and boost the learning process. To\nassess the performance of the proposed method, we conducted extensive\nexperiments in various biomedical image degradation scenarios, such as noise\ncaused by physical characteristics of electron microscopy (EM) devices (film\nand charging noise), and structural noise found in low-dose computer tomography\n(CT). We demonstrate that the image quality of our method is superior to\nconventional and current state-of-the-art deep learning-based image denoising\nmethods, including supervised learning.",
          "link": "http://arxiv.org/abs/2102.09858",
          "publishedOn": "2021-09-10T07:20:14.831Z",
          "wordCount": null,
          "title": "ISCL: Interdependent Self-Cooperative Learning for Unpaired Image Denoising. (arXiv:2102.09858v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xuanyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1\">Mengmeng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Triet H. M. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullah_F/0/1/0/all/0/1\">Faheem Ullah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1\">M. Ali Babar</a>",
          "description": "Internet of Things (IoT) based applications face an increasing number of\npotential security risks, which need to be systematically assessed and\naddressed. Expert-based manual assessment of IoT security is a predominant\napproach, which is usually inefficient. To address this problem, we propose an\nautomated security assessment framework for IoT networks. Our framework first\nleverages machine learning and natural language processing to analyze\nvulnerability descriptions for predicting vulnerability metrics. The predicted\nmetrics are then input into a two-layered graphical security model, which\nconsists of an attack graph at the upper layer to present the network\nconnectivity and an attack tree for each node in the network at the bottom\nlayer to depict the vulnerability information. This security model\nautomatically assesses the security of the IoT network by capturing potential\nattack paths. We evaluate the viability of our approach using a\nproof-of-concept smart building system model which contains a variety of\nreal-world IoT devices and potential vulnerabilities. Our evaluation of the\nproposed framework demonstrates its effectiveness in terms of automatically\npredicting the vulnerability metrics of new vulnerabilities with more than 90%\naccuracy, on average, and identifying the most vulnerable attack paths within\nan IoT network. The produced assessment results can serve as a guideline for\ncybersecurity professionals to take further actions and mitigate risks in a\ntimely manner.",
          "link": "http://arxiv.org/abs/2109.04029",
          "publishedOn": "2021-09-10T07:20:14.830Z",
          "wordCount": null,
          "title": "Automated Security Assessment for the Internet of Things. (arXiv:2109.04029v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04149",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xinwu Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuocheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>",
          "description": "In a ride-hailing system, an optimal relocation of vacant vehicles can\nsignificantly reduce fleet idling time and balance the supply-demand\ndistribution, enhancing system efficiency and promoting driver satisfaction and\nretention. Model-free deep reinforcement learning (DRL) has been shown to\ndynamically learn the relocating policy by actively interacting with the\nintrinsic dynamics in large-scale ride-hailing systems. However, the issues of\nsparse reward signals and unbalanced demand and supply distribution place\ncritical barriers in developing effective DRL models. Conventional exploration\nstrategy (e.g., the $\\epsilon$-greedy) may barely work under such an\nenvironment because of dithering in low-demand regions distant from\nhigh-revenue regions. This study proposes the deep relocating option policy\n(DROP) that supervises vehicle agents to escape from oversupply areas and\neffectively relocate to potentially underserved areas. We propose to learn the\nLaplacian embedding of a time-expanded relocation graph, as an approximation\nrepresentation of the system relocation policy. The embedding generates\ntask-agnostic signals, which in combination with task-dependent signals,\nconstitute the pseudo-reward function for generating DROPs. We present a\nhierarchical learning framework that trains a high-level relocation policy and\na set of low-level DROPs. The effectiveness of our approach is demonstrated\nusing a custom-built high-fidelity simulator with real-world trip record data.\nWe report that DROP significantly improves baseline models with 15.7% more\nhourly revenue and can effectively resolve the dithering issue in low-demand\nareas.",
          "link": "http://arxiv.org/abs/2109.04149",
          "publishedOn": "2021-09-10T07:20:14.829Z",
          "wordCount": null,
          "title": "DROP: Deep relocating option policy for optimal ride-hailing vehicle repositioning. (arXiv:2109.04149v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitehill_J/0/1/0/all/0/1\">Jacob Whitehill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeqian Li</a>",
          "description": "We consider a new kind of clustering problem in which clusters need not be\nindependent of each other, but rather can have compositional relationships with\nother clusters (e.g., an image set consists of rectangles, circles, as well as\ncombinations of rectangles and circles). This task is motivated by recent work\nin few-shot learning on compositional embedding models that structure the\nembedding space to distinguish the label sets, not just the individual labels,\nassigned to the examples. To tackle this clustering problem, we propose a new\nalgorithm called Compositional Affinity Propagation (CAP). In contrast to\nstandard Affinity Propagation as well as other algorithms for multi-view and\nhierarchical clustering, CAP can deduce compositionality among clusters\nautomatically. We show promising results, compared to several existing\nclustering algorithms, on the MultiMNIST, OmniGlot, and LibriSpeech datasets.\nOur work has applications to multi-object image recognition and speaker\ndiarization with simultaneous speech from multiple speakers.",
          "link": "http://arxiv.org/abs/2109.04160",
          "publishedOn": "2021-09-10T07:20:14.829Z",
          "wordCount": null,
          "title": "Compositional Affinity Propagation: When Clusters Have Compositional Structure. (arXiv:2109.04160v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14997",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "We consider the approximation rates of shallow neural networks with respect\nto the variation norm. Upper bounds on these rates have been established for\nsigmoidal and ReLU activation functions, but it has remained an important open\nproblem whether these rates are sharp. In this article, we provide a solution\nto this problem by proving sharp lower bounds on the approximation rates for\nshallow neural networks, which are obtained by lower bounding the $L^2$-metric\nentropy of the convex hull of the neural network basis functions. In addition,\nour methods also give sharp lower bounds on the Kolmogorov $n$-widths of this\nconvex hull, which show that the variation spaces corresponding to shallow\nneural networks cannot be efficiently approximated by linear methods. These\nlower bounds apply to both sigmoidal activation functions with bounded\nvariation and to activation functions which are a power of the ReLU. Our\nresults also quantify how much stronger the Barron spectral norm is than the\nvariation norm and, combined with previous results, give the asymptotics of the\n$L^\\infty$-metric entropy up to logarithmic factors in the case of the ReLU\nactivation function.",
          "link": "http://arxiv.org/abs/2106.14997",
          "publishedOn": "2021-09-10T07:20:14.829Z",
          "wordCount": null,
          "title": "Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks. (arXiv:2106.14997v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03930",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Giorgio_G/0/1/0/all/0/1\">Gnecco Giorgio</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Federico_N/0/1/0/all/0/1\">Nutarelli Federico</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Massimo_R/0/1/0/all/0/1\">Riccaboni Massimo</a>",
          "description": "This work applies Matrix Completion (MC) -- a class of machine-learning\nmethods commonly used in the context of recommendation systems -- to analyse\neconomic complexity. MC is applied to reconstruct the Revealed Comparative\nAdvantage (RCA) matrix, whose elements express the relative advantage of\ncountries in given classes of products, as evidenced by yearly trade flows. A\nhigh-accuracy binary classifier is derived from the application of MC, with the\naim of discriminating between elements of the RCA matrix that are,\nrespectively, higher or lower than one. We introduce a novel Matrix cOmpletion\niNdex of Economic complexitY (MONEY) based on MC, which is related to the\npredictability of countries' RCA (the lower the predictability, the higher the\ncomplexity). Differently from previously-developed indices of economic\ncomplexity, the MONEY index takes into account the various singular vectors of\nthe matrix reconstructed by MC, whereas other indices are based only on one/two\neigenvectors of a suitable symmetric matrix, derived from the RCA matrix.\nFinally, MC is compared with a state-of-the-art economic complexity index\n(GENEPY). We show that the false positive rate per country of a binary\nclassifier constructed starting from the average entry-wise output of MC can be\nused as a proxy of GENEPY.",
          "link": "http://arxiv.org/abs/2109.03930",
          "publishedOn": "2021-09-10T07:20:14.827Z",
          "wordCount": null,
          "title": "Matrix Completion of World Trade. (arXiv:2109.03930v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04202",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Campos_D/0/1/0/all/0/1\">Daniel Campos</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>",
          "description": "Like many scientific fields, new chemistry literature has grown at a\nstaggering pace, with thousands of papers released every month. A large portion\nof chemistry literature focuses on new molecules and reactions between\nmolecules. Most vital information is conveyed through 2-D images of molecules,\nrepresenting the underlying molecules or reactions described. In order to\nensure reproducible and machine-readable molecule representations, text-based\nmolecule descriptors like SMILES and SELFIES were created. These text-based\nmolecule representations provide molecule generation but are unfortunately\nrarely present in published literature. In the absence of molecule descriptors,\nthe generation of molecule descriptors from the 2-D images present in the\nliterature is necessary to understand chemistry literature at scale. Successful\nmethods such as Optical Structure Recognition Application (OSRA), and\nChemSchematicResolver are able to extract the locations of molecules structures\nin chemistry papers and infer molecular descriptions and reactions. While\neffective, existing systems expect chemists to correct outputs, making them\nunsuitable for unsupervised large-scale data mining. Leveraging the task\nformulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a\nmodel which leverages Deep Residual Networks for image feature extraction and\nan encoder-decoder Transformer layers for molecule description generation.\nUnlike previous Neural Network-based systems, IMG2SMI builds around the task of\nmolecule description generation, which enables IMG2SMI to outperform OSRA-based\nsystems by 163% in molecule similarity prediction as measured by the molecular\nMACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further\nresearch on this task, we release a new molecule prediction dataset. including\n81 million molecules for molecule description generation",
          "link": "http://arxiv.org/abs/2109.04202",
          "publishedOn": "2021-09-10T07:20:14.827Z",
          "wordCount": null,
          "title": "IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System. (arXiv:2109.04202v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04188",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_Llaneza_D/0/1/0/all/0/1\">Daniel Fernandez-Llaneza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gondova_A/0/1/0/all/0/1\">Andrea Gondova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vince_H/0/1/0/all/0/1\">Harris Vince</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zurek_M/0/1/0/all/0/1\">Magdalena Zurek</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konings_P/0/1/0/all/0/1\">Peter Konings</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kagelid_P/0/1/0/all/0/1\">Patrik Kagelid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hultin_L/0/1/0/all/0/1\">Leif Hultin</a>",
          "description": "Automated segmentation of human cardiac magnetic resonance datasets has been\nsteadily improving during recent years. However, these methods are not directly\napplicable in preclinical context due to limited datasets and lower image\nresolution. Successful application of deep architectures for rat cardiac\nsegmentation, although of critical importance for preclinical evaluation of\ncardiac function, has to our knowledge not yet been reported. We developed\nsegmentation models that expand on the standard U-Net architecture and\nevaluated separate models for systole and diastole phases, 2MSA, and one model\nfor all timepoints, 1MSA. Furthermore, we calibrated model outputs using a\nGaussian Process (GP)-based prior to improve phase selection. Resulting models\napproach human performance in terms of left ventricular segmentation quality\nand ejection fraction (EF) estimation in both 1MSA and 2MSA settings\n(S{\\o}rensen-Dice score 0.91 +/- 0.072 and 0.93 +/- 0.032, respectively). 2MSA\nachieved a mean absolute difference between estimated and reference EF of 3.5\n+/- 2.5 %, while 1MSA resulted in 4.1 +/- 3.0 %. Applying Gaussian Processes to\n1MSA allows to automate the selection of systole and diastole phases. Combined\nwith a novel cardiac phase selection strategy, our work presents an important\nfirst step towards a fully automated segmentation pipeline in the context of\nrat cardiac analysis.",
          "link": "http://arxiv.org/abs/2109.04188",
          "publishedOn": "2021-09-10T07:20:14.826Z",
          "wordCount": null,
          "title": "Towards Fully Automated Segmentation of Rat Cardiac MRI by Leveraging Deep Learning Frameworks. (arXiv:2109.04188v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Togootogtokh_E/0/1/0/all/0/1\">Enkhtogtokh Togootogtokh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasen_C/0/1/0/all/0/1\">Christian Klasen</a>",
          "description": "We proposed the industry level deep learning approach for speech emotion\nrecognition task. In industry, carefully proposed deep transfer learning\ntechnology shows real results due to mostly low amount of training data\navailability, machine training cost, and specialized learning on dedicated AI\ntasks. The proposed speech recognition framework, called DeepEMO, consists of\ntwo main pipelines such that preprocessing to extract efficient main features\nand deep transfer learning model to train and recognize. Main source code is in\nhttps://github.com/enkhtogtokh/deepemo repository",
          "link": "http://arxiv.org/abs/2109.04081",
          "publishedOn": "2021-09-10T07:20:14.825Z",
          "wordCount": null,
          "title": "DeepEMO: Deep Learning for Speech Emotion Recognition. (arXiv:2109.04081v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07705",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Izsak_P/0/1/0/all/0/1\">Peter Izsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berchansky_M/0/1/0/all/0/1\">Moshe Berchansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>",
          "description": "While large language models a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours using a\nsingle low-end deep learning server. We demonstrate that through a combination\nof software optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.",
          "link": "http://arxiv.org/abs/2104.07705",
          "publishedOn": "2021-09-10T07:20:14.816Z",
          "wordCount": null,
          "title": "How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rabin_M/0/1/0/all/0/1\">Md Rafiqul Islam Rabin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alipour_M/0/1/0/all/0/1\">Mohammad Amin Alipour</a>",
          "description": "A wide range of code intelligence (CI) tools, powered by deep neural\nnetworks, have been developed recently to improve programming productivity and\nperform program analysis. To reliably use such tools, developers often need to\nreason about the behavior of the underlying models and the factors that affect\nthem. This is especially challenging for tools backed by deep neural networks.\nVarious methods have tried to reduce this opacity in the vein of\n\"transparent/interpretable-AI\". However, these approaches are often specific to\na particular set of network architectures, even requiring access to the\nnetwork's parameters. This makes them difficult to use for the average\nprogrammer, which hinders the reliable adoption of neural CI systems. In this\npaper, we propose a simple, model-agnostic approach to identify critical input\nfeatures for models in CI systems, by drawing on software debugging research,\nspecifically delta debugging. Our approach, SIVAND, uses simplification\ntechniques that reduce the size of input programs of a CI model while\npreserving the predictions of the model. We show that this approach yields\nremarkably small outputs and is broadly applicable across many model\narchitectures and problem domains. We find that the models in our experiments\noften rely heavily on just a few syntactic features in input programs. We\nbelieve that SIVAND's extracted features may help understand neural CI systems'\npredictions and learned behavior.",
          "link": "http://arxiv.org/abs/2106.03353",
          "publishedOn": "2021-09-10T07:20:14.805Z",
          "wordCount": null,
          "title": "Understanding Neural Code Intelligence Through Program Simplification. (arXiv:2106.03353v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04001",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Marzban_S/0/1/0/all/0/1\">Saeed Marzban</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Delage_E/0/1/0/all/0/1\">Erick Delage</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Li_J/0/1/0/all/0/1\">Jonathan Yumeng Li</a>",
          "description": "Recently equal risk pricing, a framework for fair derivative pricing, was\nextended to consider dynamic risk measures. However, all current\nimplementations either employ a static risk measure that violates time\nconsistency, or are based on traditional dynamic programming solution schemes\nthat are impracticable in problems with a large number of underlying assets\n(due to the curse of dimensionality) or with incomplete asset dynamics\ninformation. In this paper, we extend for the first time a famous off-policy\ndeterministic actor-critic deep reinforcement learning (ACRL) algorithm to the\nproblem of solving a risk averse Markov decision process that models risk using\na time consistent recursive expectile risk measure. This new ACRL algorithm\nallows us to identify high quality time consistent hedging policies (and equal\nrisk prices) for options, such as basket options, that cannot be handled using\ntraditional methods, or in context where only historical trajectories of the\nunderlying assets are available. Our numerical experiments, which involve both\na simple vanilla option and a more exotic basket option, confirm that the new\nACRL algorithm can produce 1) in simple environments, nearly optimal hedging\npolicies, and highly accurate prices, simultaneously for a range of maturities\n2) in complex environments, good quality policies and prices using reasonable\namount of computing resources; and 3) overall, hedging strategies that actually\noutperform the strategies produced using static risk measures when the risk is\nevaluated at later points of time.",
          "link": "http://arxiv.org/abs/2109.04001",
          "publishedOn": "2021-09-10T07:20:14.794Z",
          "wordCount": null,
          "title": "Deep Reinforcement Learning for Equal Risk Pricing and Hedging under Dynamic Expectile Risk Measures. (arXiv:2109.04001v1 [q-fin.PR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Abhinav Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anubhav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jose Mathew</a>",
          "description": "Digital maps are commonly used across the globe for exploring places that\nusers are interested in, commonly referred to as points of interest (PoI). In\nonline food delivery platforms, PoIs could represent any major private\ncompounds where customers could order from such as hospitals, residential\ncomplexes, office complexes, educational institutes and hostels. In this work,\nwe propose an end-to-end unsupervised system design for obtaining polygon\nrepresentations of PoIs (PoI polygons) from address locations and address\ntexts. We preprocess the address texts using locality names and generate\nembeddings for the address texts using a deep learning-based architecture, viz.\nRoBERTa, trained on our internal address dataset. The PoI candidates are\nidentified by jointly clustering the anonymised customer phone GPS locations\n(obtained during address onboarding) and the embeddings of the address texts.\nThe final list of PoI polygons is obtained from these PoI candidates using\nnovel post-processing steps. This algorithm identified 74.8 % more PoIs than\nthose obtained using the Mummidi-Krumm baseline algorithm run on our internal\ndataset. The proposed algorithm achieves a median area precision of 98 %, a\nmedian area recall of 8 %, and a median F-score of 0.15. In order to improve\nthe recall of the algorithmic polygons, we post-process them using building\nfootprint polygons from the OpenStreetMap (OSM) database. The post-processing\nalgorithm involves reshaping the algorithmic polygon using intersecting\npolygons and closed private roads from the OSM database, and accounting for\nintersection with public roads on the OSM database. We achieve a median area\nrecall of 70 %, a median area precision of 69 %, and a median F-score of 0.69\non these post-processed polygons.",
          "link": "http://arxiv.org/abs/2109.04467",
          "publishedOn": "2021-09-10T07:20:14.780Z",
          "wordCount": null,
          "title": "Mining Points of Interest via Address Embeddings: An Unsupervised Approach. (arXiv:2109.04467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiangyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guiliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1\">Pascal Poupart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1\">Oliver Schulte</a>",
          "description": "We propose a score-based DAG structure learning method for time-series data\nthat captures linear, nonlinear, lagged and instantaneous relations among\nvariables while ensuring acyclicity throughout the entire graph. The proposed\nmethod extends nonparametric NOTEARS, a recent continuous optimization approach\nfor learning nonparametric instantaneous DAGs. The proposed method is faster\nthan constraint-based methods using nonlinear conditional independence tests.\nWe also promote the use of optimization constraints to incorporate prior\nknowledge into the structure learning process. A broad set of experiments with\nsimulated data demonstrates that the proposed method discovers better DAG\nstructures than several recent comparison methods. We also evaluate the\nproposed method on complex real-world data acquired from NHL ice hockey games\ncontaining a mixture of continuous and discrete variables. The code is\navailable at https://github.com/xiangyu-sun-789/NTS-NOTEARS/.",
          "link": "http://arxiv.org/abs/2109.04286",
          "publishedOn": "2021-09-10T07:20:14.767Z",
          "wordCount": null,
          "title": "NTS-NOTEARS: Learning Nonparametric Temporal DAGs With Time-Series Data and Prior Knowledge. (arXiv:2109.04286v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuster_T/0/1/0/all/0/1\">Tal Schuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1\">Regina Barzilay</a>",
          "description": "We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.",
          "link": "http://arxiv.org/abs/2104.08803",
          "publishedOn": "2021-09-10T07:20:14.762Z",
          "wordCount": 607,
          "title": "Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10097",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wentao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Ziqi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zeang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yangyu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bin Cui</a>",
          "description": "Graph neural networks (GNNs) have recently achieved state-of-the-art\nperformance in many graph-based applications. Despite the high expressive\npower, they typically need to perform an expensive recursive neighborhood\nexpansion in multiple training epochs and face a scalability issue. Moreover,\nmost of them are inflexible since they are restricted to fixed-hop\nneighborhoods and insensitive to actual receptive field demands for different\nnodes. We circumvent these limitations by introducing a scalable and flexible\nGraph Attention Multilayer Perceptron (GAMLP). With the separation of the\nnon-linear transformation and feature propagation, GAMLP significantly improves\nthe scalability and efficiency by performing the propagation procedure in a\npre-compute manner. With three principled receptive field attention, each node\nin GAMLP is flexible and adaptive in leveraging the propagated features over\nthe different sizes of reception field. We conduct extensive evaluations on the\nthree large open graph benchmarks (e.g., ogbn-papers100M, ogbn-products and\nogbn-mag), demonstrating that GAMLP not only achieves the state-of-art\nperformance, but also additionally provide high scalability and efficiency.",
          "link": "http://arxiv.org/abs/2108.10097",
          "publishedOn": "2021-09-10T07:20:14.756Z",
          "wordCount": 631,
          "title": "Graph Attention Multi-Layer Perceptron. (arXiv:2108.10097v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.12488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakkelund_D/0/1/0/all/0/1\">Daniel Bakkelund</a>",
          "description": "Partial orders and directed acyclic graphs are commonly recurring data\nstructures that arise naturally in numerous domains and applications and are\nused to represent ordered relations between entities in the domains. Examples\nare task dependencies in a project plan, transaction order in distributed\nledgers and execution sequences of tasks in computer programs, just to mention\na few. We study the problem of order preserving hierarchical clustering of this\nkind of ordered data. That is, if we have $a < b$ in the original data and\ndenote their respective clusters by $[a]$ and $[b]$, then we shall have $[a] <\n[b]$ in the produced clustering. The clustering is similarity based and uses\nstandard linkage functions, such as single- and complete linkage, and is an\nextension of classical hierarchical clustering.\n\nTo achieve this, we define the output from running classical hierarchical\nclustering on strictly ordered data to be partial dendrograms; sub-trees of\nclassical dendrograms with several connected components. We then construct an\nembedding of partial dendrograms over a set into the family of ultrametrics\nover the same set. An optimal hierarchical clustering is defined as the partial\ndendrogram corresponding to the ultrametric closest to the original\ndissimilarity measure, measured in the p-norm. Thus, the method is a\ncombination of classical hierarchical clustering and ultrametric fitting.\n\nA reference implementation is employed for experiments on both synthetic\nrandom data and real world data from a database of machine parts. When compared\nto existing methods, the experiments show that our method excels both in\ncluster quality and order preservation.",
          "link": "http://arxiv.org/abs/2004.12488",
          "publishedOn": "2021-09-10T07:20:14.749Z",
          "wordCount": 729,
          "title": "Order preserving hierarchical agglomerative clustering. (arXiv:2004.12488v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04447",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Banerjee_S/0/1/0/all/0/1\">Sudipto Banerjee</a>",
          "description": "Geographic Information Systems (GIS) and related technologies have generated\nsubstantial interest among statisticians with regard to scalable methodologies\nfor analyzing large spatial datasets. A variety of scalable spatial process\nmodels have been proposed that can be easily embedded within a hierarchical\nmodeling framework to carry out Bayesian inference. While the focus of\nstatistical research has mostly been directed toward innovative and more\ncomplex model development, relatively limited attention has been accorded to\napproaches for easily implementable scalable hierarchical models for the\npracticing scientist or spatial analyst. This article discusses how\npoint-referenced spatial process models can be cast as a conjugate Bayesian\nlinear regression that can rapidly deliver inference on spatial processes. The\napproach allows exact sampling directly (avoids iterative algorithms such as\nMarkov chain Monte Carlo) from the joint posterior distribution of regression\nparameters, the latent process and the predictive random variables, and can be\neasily implemented on statistical programming environments such as R.",
          "link": "http://arxiv.org/abs/2109.04447",
          "publishedOn": "2021-09-10T07:20:14.743Z",
          "wordCount": 609,
          "title": "Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Regression Framework. (arXiv:2109.04447v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1909.12205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morin_G/0/1/0/all/0/1\">Gr&#xe9;goire Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razani_R/0/1/0/all/0/1\">Ryan Razani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sari_E/0/1/0/all/0/1\">Eyy&#xfc;b Sari</a>",
          "description": "Neural network models are resource hungry. It is difficult to deploy such\ndeep networks on devices with limited resources, like smart wearables,\ncellphones, drones, and autonomous vehicles. Low bit quantization such as\nbinary and ternary quantization is a common approach to alleviate this resource\nrequirements. Ternary quantization provides a more flexible model and\noutperforms binary quantization in terms of accuracy, however doubles the\nmemory footprint and increases the computational cost. Contrary to these\napproaches, mixed quantized models allow a trade-off between accuracy and\nmemory footprint. In such models, quantization depth is often chosen manually,\nor is tuned using a separate optimization routine. The latter requires training\na quantized network multiple times. Here, we propose an adaptive combination of\nbinary and ternary quantization, namely Smart Quantization (SQ), in which the\nquantization depth is modified directly via a regularization function, so that\nthe model is trained only once. Our experimental results show that the proposed\nmethod adapts quantization depth successfully while keeping the model accuracy\nhigh on MNIST and CIFAR10 benchmarks.",
          "link": "http://arxiv.org/abs/1909.12205",
          "publishedOn": "2021-09-10T07:20:14.736Z",
          "wordCount": 649,
          "title": "Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03817",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shaghaghi_A/0/1/0/all/0/1\">Amirhossein Shaghaghi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zakeri_A/0/1/0/all/0/1\">Abolfazl Zakeri</a> (Student Member, IEEE), <a href=\"http://arxiv.org/find/eess/1/au:+Mokari_N/0/1/0/all/0/1\">Nader Mokari</a> (Senior Member, IEEE), <a href=\"http://arxiv.org/find/eess/1/au:+Javan_M/0/1/0/all/0/1\">Mohammad Reza Javan</a> (Senior Member, IEEE), <a href=\"http://arxiv.org/find/eess/1/au:+Behdadfar_M/0/1/0/all/0/1\">Mohammad Behdadfar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorswieck_E/0/1/0/all/0/1\">Eduard A Jorswieck</a> (Fellow, IEEE)",
          "description": "In this paper, we propose a Zero-Touch, deep reinforcement learning\n(DRL)-based Proactive Failure Recovery framework called ZT-PFR for stateful\nnetwork function virtualization (NFV)-enabled networks. To this end, we\nformulate a resource-efficient optimization problem minimizing the network cost\nfunction including resource cost and wrong decision penalty. As a solution, we\npropose state-of-the-art DRL-based methods such as soft-actor-critic (SAC) and\nproximal-policy-optimization (PPO). In addition, to train and test our DRL\nagents, we propose a novel impending failure model. Moreover, to keep network\nstatus information at an acceptable freshness level for appropriate\ndecision-making, we apply the concept of age of information to strike a balance\nbetween the event and scheduling-based monitoring. Several key systems and DRL\nalgorithm design insights for ZT-PFR are drawn from our analysis and simulation\nresults. For example, we use a hybrid neural network, consisting long\nshort-term memory layers in the DRL agents",
          "link": "http://arxiv.org/abs/2103.03817",
          "publishedOn": "2021-09-10T07:20:14.720Z",
          "wordCount": 659,
          "title": "Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach. (arXiv:2103.03817v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scheinert_D/0/1/0/all/0/1\">Dominik Scheinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acker_A/0/1/0/all/0/1\">Alexander Acker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamsen_L/0/1/0/all/0/1\">Lauritz Thamsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geldenhuys_M/0/1/0/all/0/1\">Morgan K. Geldenhuys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1\">Odej Kao</a>",
          "description": "Operation and maintenance of large distributed cloud applications can quickly\nbecome unmanageably complex, putting human operators under immense stress when\nproblems occur. Utilizing machine learning for identification and localization\nof anomalies in such systems supports human experts and enables fast\nmitigation. However, due to the various inter-dependencies of system\ncomponents, anomalies do not only affect their origin but propagate through the\ndistributed system. Taking this into account, we present Arvalus and its\nvariant D-Arvalus, a neural graph transformation method that models system\ncomponents as nodes and their dependencies and placement as edges to improve\nthe identification and localization of anomalies. Given a series of metric\nKPIs, our method predicts the most likely system state - either normal or an\nanomaly class - and performs localization when an anomaly is detected. During\nour experiments, we simulate a distributed cloud application deployment and\nsynthetically inject anomalies. The evaluation shows the generally good\nprediction performance of Arvalus and reveals the advantage of D-Arvalus which\nincorporates information about system component dependencies.",
          "link": "http://arxiv.org/abs/2103.05245",
          "publishedOn": "2021-09-10T07:20:14.713Z",
          "wordCount": 666,
          "title": "Learning Dependencies in Distributed Cloud Applications to Identify and Localize Anomalies. (arXiv:2103.05245v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_D/0/1/0/all/0/1\">Devang Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Vlad Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>",
          "description": "In this work, we introduce back-training, an alternative to self-training for\nunsupervised domain adaptation (UDA) from source to target domain. While\nself-training generates synthetic training data where natural inputs are\naligned with noisy outputs, back-training results in natural outputs aligned\nwith noisy inputs. This significantly reduces the gap between the target domain\nand synthetic data distribution, and reduces model overfitting to the source\ndomain. We run UDA experiments on question generation and passage retrieval\nfrom the \\textit{Natural Questions} domain to machine learning and biomedical\ndomains. We find that back-training vastly outperforms self-training by a mean\nimprovement of 7.8 BLEU-4 points on generation, and 17.6\\% top-20 retrieval\naccuracy across both domains. We further propose consistency filters to remove\nlow-quality synthetic data before training. We also release a new\ndomain-adaptation dataset- \\textit{MLQuestions} containing 35K unaligned\nquestions, 50K unaligned passages, and 3K aligned question-passage pairs.",
          "link": "http://arxiv.org/abs/2104.08801",
          "publishedOn": "2021-09-10T07:20:14.707Z",
          "wordCount": 632,
          "title": "Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04260",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Ming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yu-Wei Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Chen-Lu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhen-Duo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin-Shun Xu</a>",
          "description": "With the vigorous development of multimedia equipment and applications,\nefficient retrieval of large-scale multi-modal data has become a trendy\nresearch topic. Thereinto, hashing has become a prevalent choice due to its\nretrieval efficiency and low storage cost. Although multi-modal hashing has\ndrawn lots of attention in recent years, there still remain some problems. The\nfirst point is that existing methods are mainly designed in batch mode and not\nable to efficiently handle streaming multi-modal data. The second point is that\nall existing online multi-modal hashing methods fail to effectively handle\nunseen new classes which come continuously with streaming data chunks. In this\npaper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).\nWe design novel semantic-enhanced representation for data, which could help\nhandle the new coming classes, and thereby construct the enhanced semantic\nobjective function. An efficient and effective discrete online optimization\nalgorithm is further proposed for OASIS. Extensive experiments show that our\nmethod can exceed the state-of-the-art models. For good reproducibility and\nbenefiting the community, our code and data are already available in\nsupplementary material and will be made publicly available.",
          "link": "http://arxiv.org/abs/2109.04260",
          "publishedOn": "2021-09-10T07:20:14.699Z",
          "wordCount": 643,
          "title": "Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data. (arXiv:2109.04260v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nodet_P/0/1/0/all/0/1\">Pierre Nodet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1\">Vincent Lemaire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1\">Alexis Bondu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornuejols_A/0/1/0/all/0/1\">Antoine Cornu&#xe9;jols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouorou_A/0/1/0/all/0/1\">Adam Ouorou</a>",
          "description": "The field of Weakly Supervised Learning (WSL) has recently seen a surge of\npopularity, with numerous papers addressing different types of \"supervision\ndeficiencies\". In WSL use cases, a variety of situations exists where the\ncollected \"information\" is imperfect. The paradigm of WSL attempts to list and\ncover these problems with associated solutions. In this paper, we review the\nresearch progress on WSL with the aim to make it as a brief introduction to\nthis field. We present the three axis of WSL cube and an overview of most of\nall the elements of their facets. We propose three measurable quantities that\nacts as coordinates in the previously defined cube namely: Quality,\nAdaptability and Quantity of information. Thus we suggest that Biquality\nLearning framework can be defined as a plan of the WSL cube and propose to\nre-discover previously unrelated patches in WSL literature as a unified\nBiquality Learning literature.",
          "link": "http://arxiv.org/abs/2012.09632",
          "publishedOn": "2021-09-10T07:20:14.693Z",
          "wordCount": 632,
          "title": "From Weakly Supervised Learning to Biquality Learning: an Introduction. (arXiv:2012.09632v3 [cs.LG] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.15122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaarsveld_W/0/1/0/all/0/1\">Willem van Jaarsveld</a>",
          "description": "Recent literature established that neural networks can represent good\npolicies across a range of stochastic dynamic models in supply chain and\nlogistics. We incorporate variance reduction techniques in a newly proposed\nalgorithm, to overcome limitations of the model-free algorithms typically\nemployed to learn such neural network policies. For the classical lost sales\ninventory model, the algorithm learns neural network policies that are superior\nto those learned using model-free algorithms, while outperforming the best\nheuristic benchmarks by an order of magnitude. The algorithm is an interesting\ncandidate to apply to other stochastic dynamic problems in supply chain and\nlogistics, because the ideas in its development are generic.",
          "link": "http://arxiv.org/abs/2011.15122",
          "publishedOn": "2021-09-10T07:20:14.671Z",
          "wordCount": 575,
          "title": "Deep controlled learning of MDP policies with an application to lost-sales inventory control. (arXiv:2011.15122v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04378",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yiyue Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_W/0/1/0/all/0/1\">Wan Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1\">Michael Foshey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1\">Wojciech Matusik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>",
          "description": "Tactile sensing is critical for humans to perform everyday tasks. While\nsignificant progress has been made in analyzing object grasping from vision, it\nremains unclear how we can utilize tactile sensing to reason about and model\nthe dynamics of hand-object interactions. In this work, we employ a\nhigh-resolution tactile glove to perform four different interactive activities\non a diversified set of objects. We build our model on a cross-modal learning\nframework and generate the labels using a visual processing pipeline to\nsupervise the tactile model, which can then be used on its own during the test\ntime. The tactile model aims to predict the 3d locations of both the hand and\nthe object purely from the touch data by combining a predictive model and a\ncontrastive learning module. This framework can reason about the interaction\npatterns from the tactile data, hallucinate the changes in the environment,\nestimate the uncertainty of the prediction, and generalize to unseen objects.\nWe also provide detailed ablation studies regarding different system designs as\nwell as visualizations of the predicted trajectories. This work takes a step on\ndynamics modeling in hand-object interactions from dense tactile sensing, which\nopens the door for future applications in activity learning, human-computer\ninteractions, and imitation learning for robotics.",
          "link": "http://arxiv.org/abs/2109.04378",
          "publishedOn": "2021-09-10T07:20:14.662Z",
          "wordCount": 683,
          "title": "Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. (arXiv:2109.04378v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2008.01511",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Araujo_I/0/1/0/all/0/1\">Israel F. Araujo</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Park_D/0/1/0/all/0/1\">Daniel K. Park</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Petruccione_F/0/1/0/all/0/1\">Francesco Petruccione</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Silva_A/0/1/0/all/0/1\">Adenilton J. da Silva</a>",
          "description": "Advantages in several fields of research and industry are expected with the\nrise of quantum computers. However, the computational cost to load classical\ndata in quantum computers can impose restrictions on possible quantum speedups.\nKnown algorithms to create arbitrary quantum states require quantum circuits\nwith depth O(N) to load an N-dimensional vector. Here, we show that it is\npossible to load an N-dimensional vector with a quantum circuit with\npolylogarithmic depth and entangled information in ancillary qubits. Results\nshow that we can efficiently load data in quantum devices using a\ndivide-and-conquer strategy to exchange computational time for space. We\ndemonstrate a proof of concept on a real quantum device and present two\napplications for quantum machine learning. We expect that this new loading\nstrategy allows the quantum speedup of tasks that require to load a significant\nvolume of information to quantum devices.",
          "link": "http://arxiv.org/abs/2008.01511",
          "publishedOn": "2021-09-10T07:20:14.656Z",
          "wordCount": 606,
          "title": "A divide-and-conquer algorithm for quantum state preparation. (arXiv:2008.01511v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_G/0/1/0/all/0/1\">Guogang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>",
          "description": "E-commerce platforms usually display a mixed list of ads and organic items in\nfeed. One key problem is to allocate the limited slots in the feed to maximize\nthe overall revenue as well as improve user experience, which requires a good\nmodel for user preference. Instead of modeling the influence of individual\nitems on user behaviors, the arrangement signal models the influence of the\narrangement of items and may lead to a better allocation strategy. However,\nmost of previous strategies fail to model such a signal and therefore result in\nsuboptimal performance. To this end, we propose Cross Deep Q Network (Cross\nDQN) to extract the arrangement signal by crossing the embeddings of different\nitems and processing the crossed sequence in the feed. Our model results in\nhigher revenue and better user experience than state-of-the-art baselines in\noffline experiments. Moreover, our model demonstrates a significant improvement\nin the online A/B test and has been fully deployed on Meituan feed to serve\nmore than 300 millions of customers.",
          "link": "http://arxiv.org/abs/2109.04353",
          "publishedOn": "2021-09-10T07:20:14.650Z",
          "wordCount": 620,
          "title": "Cross DQN: Cross Deep Q Network for Ads Allocation in Feed. (arXiv:2109.04353v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04298",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Pistoia_M/0/1/0/all/0/1\">Marco Pistoia</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ahmad_S/0/1/0/all/0/1\">Syed Farhan Ahmad</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ajagekar_A/0/1/0/all/0/1\">Akshay Ajagekar</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Buts_A/0/1/0/all/0/1\">Alexander Buts</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Shouvanik Chakrabarti</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Herman_D/0/1/0/all/0/1\">Dylan Herman</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hu_S/0/1/0/all/0/1\">Shaohan Hu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jena_A/0/1/0/all/0/1\">Andrew Jena</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Minssen_P/0/1/0/all/0/1\">Pierre Minssen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Niroula_P/0/1/0/all/0/1\">Pradeep Niroula</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rattew_A/0/1/0/all/0/1\">Arthur Rattew</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yue Sun</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yalovetzky_R/0/1/0/all/0/1\">Romina Yalovetzky</a>",
          "description": "Quantum computers are expected to surpass the computational capabilities of\nclassical computers during this decade, and achieve disruptive impact on\nnumerous industry sectors, particularly finance. In fact, finance is estimated\nto be the first industry sector to benefit from Quantum Computing not only in\nthe medium and long terms, but even in the short term. This review paper\npresents the state of the art of quantum algorithms for financial applications,\nwith particular focus to those use cases that can be solved via Machine\nLearning.",
          "link": "http://arxiv.org/abs/2109.04298",
          "publishedOn": "2021-09-10T07:20:14.644Z",
          "wordCount": 539,
          "title": "Quantum Machine Learning for Finance. (arXiv:2109.04298v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04361",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_N/0/1/0/all/0/1\">Ning Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taniar_D/0/1/0/all/0/1\">David Taniar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haolan Zhang</a>",
          "description": "Motor imagery classification is of great significance to humans with mobility\nimpairments, and how to extract and utilize the effective features from motor\nimagery electroencephalogram(EEG) channels has always been the focus of\nattention. There are many different methods for the motor imagery\nclassification, but the limited understanding on human brain requires more\neffective methods for extracting the features of EEG data. Graph neural\nnetworks(GNNs) have demonstrated its effectiveness in classifying graph\nstructures; and the use of GNN provides new possibilities for brain structure\nconnection feature extraction. In this paper we propose a novel graph neural\nnetwork based on the mutual information of the raw EEG channels called\nMutualGraphNet. We use the mutual information as the adjacency matrix combined\nwith the spatial temporal graph convolution network(ST-GCN) could extract the\ntransition rules of the motor imagery electroencephalogram(EEG) channels data\nmore effectively. Experiments are conducted on motor imagery EEG data set and\nwe compare our model with the current state-of-the-art approaches and the\nresults suggest that MutualGraphNet is robust enough to learn the interpretable\nfeatures and outperforms the current state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.04361",
          "publishedOn": "2021-09-10T07:20:14.627Z",
          "wordCount": 630,
          "title": "MutualGraphNet: A novel model for motor imagery classification. (arXiv:2109.04361v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoshino_H/0/1/0/all/0/1\">Hana Hoshino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1\">Kei Ota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanezaki_A/0/1/0/all/0/1\">Asako Kanezaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokota_R/0/1/0/all/0/1\">Rio Yokota</a>",
          "description": "Inverse Reinforcement Learning (IRL) is attractive in scenarios where reward\nengineering can be tedious. However, prior IRL algorithms use on-policy\ntransitions, which require intensive sampling from the current policy for\nstable and optimal performance. This limits IRL applications in the real world,\nwhere environment interactions can become highly expensive. To tackle this\nproblem, we present Off-Policy Inverse Reinforcement Learning (OPIRL), which\n(1) adopts off-policy data distribution instead of on-policy and enables\nsignificant reduction of the number of interactions with the environment, (2)\nlearns a stationary reward function that is transferable with high\ngeneralization capabilities on changing dynamics, and (3) leverages\nmode-covering behavior for faster convergence. We demonstrate that our method\nis considerably more sample efficient and generalizes to novel environments\nthrough the experiments. Our method achieves better or comparable results on\npolicy performance baselines with significantly fewer interactions.\nFurthermore, we empirically show that the recovered reward function generalizes\nto different tasks where prior arts are prone to fail.",
          "link": "http://arxiv.org/abs/2109.04307",
          "publishedOn": "2021-09-10T07:20:14.621Z",
          "wordCount": 610,
          "title": "OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching. (arXiv:2109.04307v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruoxi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Borui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yangzhou Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenglong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>",
          "description": "In this work we propose Energy Attack, a transfer-based black-box\n$L_\\infty$-adversarial attack. The attack is parameter-free and does not\nrequire gradient approximation. In particular, we first obtain white-box\nadversarial perturbations of a surrogate model and divide these perturbations\ninto small patches. Then we extract the unit component vectors and eigenvalues\nof these patches with principal component analysis (PCA). Base on the\neigenvalues, we can model the energy distribution of adversarial perturbations.\nWe then perform black-box attacks by sampling from the perturbation patches\naccording to their energy distribution, and tiling the sampled patches to form\na full-size adversarial perturbation. This can be done without the available\naccess to victim models. Extensive experiments well demonstrate that the\nproposed Energy Attack achieves state-of-the-art performance in black-box\nattacks on various models and several datasets. Moreover, the extracted\ndistribution is able to transfer among different model architectures and\ndifferent datasets, and is therefore intrinsic to vision architectures.",
          "link": "http://arxiv.org/abs/2109.04300",
          "publishedOn": "2021-09-10T07:20:14.610Z",
          "wordCount": 601,
          "title": "Energy Attack: On Transferring Adversarial Examples. (arXiv:2109.04300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09769",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Goerigk_M/0/1/0/all/0/1\">Marc Goerigk</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kurtz_J/0/1/0/all/0/1\">Jannis Kurtz</a>",
          "description": "Robust optimization has been established as a leading methodology to approach\ndecision problems under uncertainty. To derive a robust optimization model, a\ncentral ingredient is to identify a suitable model for uncertainty, which is\ncalled the uncertainty set. An ongoing challenge in the recent literature is to\nderive uncertainty sets from given historical data that result in solutions\nthat are robust regarding future scenarios. In this paper we use an\nunsupervised deep learning method to learn and extract hidden structures from\ndata, leading to non-convex uncertainty sets and better robust solutions. We\nprove that most of the classical uncertainty classes are special cases of our\nderived sets and that optimizing over them is strongly NP-hard. Nevertheless,\nwe show that the trained neural networks can be integrated into a robust\noptimization model by formulating the adversarial problem as a convex quadratic\nmixed-integer program. This allows us to derive robust solutions through an\niterative scenario generation process. In our computational experiments, we\ncompare this approach to a similar approach using kernel-based support vector\nclustering. We find that uncertainty sets derived by the unsupervised deep\nlearning method find a better description of data and lead to robust solutions\nthat outperform the comparison method both with respect to objective value and\nfeasibility.",
          "link": "http://arxiv.org/abs/2011.09769",
          "publishedOn": "2021-09-10T07:20:14.602Z",
          "wordCount": 673,
          "title": "Data-Driven Robust Optimization using Unsupervised Deep Learning. (arXiv:2011.09769v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Timkey_W/0/1/0/all/0/1\">William Timkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1\">Marten van Schijndel</a>",
          "description": "Similarity measures are a vital tool for understanding how language models\nrepresent and process language. Standard representational similarity measures\nsuch as cosine similarity and Euclidean distance have been successfully used in\nstatic word embedding models to understand how words cluster in semantic space.\nRecently, these measures have been applied to embeddings from contextualized\nmodels such as BERT and GPT-2. In this work, we call into question the\ninformativity of such measures for contextualized language models. We find that\na small number of rogue dimensions, often just 1-3, dominate these measures.\nMoreover, we find a striking mismatch between the dimensions that dominate\nsimilarity measures and those which are important to the behavior of the model.\nWe show that simple postprocessing techniques such as standardization are able\nto correct for rogue dimensions and reveal underlying representational quality.\nWe argue that accounting for rogue dimensions is essential for any\nsimilarity-based analysis of contextual language models.",
          "link": "http://arxiv.org/abs/2109.04404",
          "publishedOn": "2021-09-10T07:20:14.584Z",
          "wordCount": 610,
          "title": "All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04253",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shulai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Quan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenli Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jingwen Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Minyi Guo</a>",
          "description": "Federated learning (FL) is a distributed machine learning paradigm that\nallows clients to collaboratively train a model over their own local data. FL\npromises the privacy of clients and its security can be strengthened by\ncryptographic methods such as additively homomorphic encryption (HE). However,\nthe efficiency of FL could seriously suffer from the statistical heterogeneity\nin both the data distribution discrepancy among clients and the global\ndistribution skewness. We mathematically demonstrate the cause of performance\ndegradation in FL and examine the performance of FL over various datasets. To\ntackle the statistical heterogeneity problem, we propose a pluggable\nsystem-level client selection method named Dubhe, which allows clients to\nproactively participate in training, meanwhile preserving their privacy with\nthe assistance of HE. Experimental results show that Dubhe is comparable with\nthe optimal greedy method on the classification accuracy, with negligible\nencryption and communication overhead.",
          "link": "http://arxiv.org/abs/2109.04253",
          "publishedOn": "2021-09-10T07:20:14.568Z",
          "wordCount": 608,
          "title": "Dubhe: Towards Data Unbiasedness with Homomorphic Encryption in Federated Learning Client Selection. (arXiv:2109.04253v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maretic_H/0/1/0/all/0/1\">Hermina Petric Maretic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheche_M/0/1/0/all/0/1\">Mireille El Gheche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chierchia_G/0/1/0/all/0/1\">Giovanni Chierchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>",
          "description": "Graph comparison deals with identifying similarities and dissimilarities\nbetween graphs. A major obstacle is the unknown alignment of graphs, as well as\nthe lack of accurate and inexpensive comparison metrics. In this work we\nintroduce the filter graph distance. It is an optimal transport based distance\nwhich drives graph comparison through the probability distribution of filtered\ngraph signals. This creates a highly flexible distance, capable of prioritising\ndifferent spectral information in observed graphs, offering a wide range of\nchoices for a comparison metric. We tackle the problem of graph alignment by\ncomputing graph permutations that minimise our new filter distances, which\nimplicitly solves the graph comparison problem. We then propose a new\napproximate cost function that circumvents many computational difficulties\ninherent to graph comparison and permits the exploitation of fast algorithms\nsuch as mirror gradient descent, without grossly sacrificing the performance.\nWe finally propose a novel algorithm derived from a stochastic version of\nmirror gradient descent, which accommodates the non-convexity of the alignment\nproblem, offering a good trade-off between performance accuracy and speed. The\nexperiments on graph alignment and classification show that the flexibility\ngained through filter graph distances can have a significant impact on\nperformance, while the difference in speed offered by the approximation cost\nmakes the framework applicable in practical settings.",
          "link": "http://arxiv.org/abs/2109.04442",
          "publishedOn": "2021-09-10T07:20:14.556Z",
          "wordCount": 656,
          "title": "fGOT: Graph Distances based on Filters and Optimal Transport. (arXiv:2109.04442v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04244",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1\">Shaojie Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vaughan_J/0/1/0/all/0/1\">Joel Vaughan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sudjianto_A/0/1/0/all/0/1\">Agus Sudjianto</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nair_V/0/1/0/all/0/1\">Vijayan Nair</a>",
          "description": "Principal component analysis (PCA) is a well-known linear dimension-reduction\nmethod that has been widely used in data analysis and modeling. It is an\nunsupervised learning technique that identifies a suitable linear subspace for\nthe input variable that contains maximal variation and preserves as much\ninformation as possible. PCA has also been used in prediction models where the\noriginal, high-dimensional space of predictors is reduced to a smaller, more\nmanageable, set before conducting regression analysis. However, this approach\ndoes not incorporate information in the response during the dimension-reduction\nstage and hence can have poor predictive performance. To address this concern,\nseveral supervised linear dimension-reduction techniques have been proposed in\nthe literature. This paper reviews selected techniques, extends some of them,\nand compares their performance through simulations. Two of these techniques,\npartial least squares (PLS) and least-squares PCA (LSPCA), consistently\noutperform the others in this study.",
          "link": "http://arxiv.org/abs/2109.04244",
          "publishedOn": "2021-09-10T07:20:14.547Z",
          "wordCount": 588,
          "title": "Supervised Linear Dimension-Reduction Methods: Review, Extensions, and Comparisons. (arXiv:2109.04244v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05174",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Gollakota_A/0/1/0/all/0/1\">Aravind Gollakota</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liang_D/0/1/0/all/0/1\">Daniel Liang</a>",
          "description": "We consider the problem of learning stabilizer states with noise in the\nProbably Approximately Correct (PAC) framework of Aaronson (2007) for learning\nquantum states. In the noiseless setting, an algorithm for this problem was\nrecently given by Rocchetto (2018), but the noisy case was left open. Motivated\nby approaches to noise tolerance from classical learning theory, we introduce\nthe Statistical Query (SQ) model for PAC-learning quantum states, and prove\nthat algorithms in this model are indeed resilient to common forms of noise,\nincluding classification and depolarizing noise. We prove an exponential lower\nbound on learning stabilizer states in the SQ model. Even outside the SQ model,\nwe prove that learning stabilizer states with noise is in general as hard as\nLearning Parity with Noise (LPN) using classical examples. Our results position\nthe problem of learning stabilizer states as a natural quantum analogue of the\nclassical problem of learning parities: easy in the noiseless setting, but\nseemingly intractable even with simple forms of noise.",
          "link": "http://arxiv.org/abs/2102.05174",
          "publishedOn": "2021-09-10T07:20:14.524Z",
          "wordCount": 627,
          "title": "On the Hardness of PAC-learning stabilizer States with Noise. (arXiv:2102.05174v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slade_E/0/1/0/all/0/1\">Emma Slade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiselgof_S/0/1/0/all/0/1\">Sonya Kiselgof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granovsky_L/0/1/0/all/0/1\">Lena Granovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+England_J/0/1/0/all/0/1\">Jeremy L. England</a>",
          "description": "Ising models are a simple generative approach to describing interacting\nbinary variables. They have proven useful in a number of biological settings\nbecause they enable one to represent observed many-body correlations as the\nseparable consequence of many direct, pairwise statistical interactions. The\ninference of Ising models from data can be computationally very challenging and\noften one must be satisfied with numerical approximations or limited precision.\nIn this paper we present a novel method for the determination of Ising\nparameters from data, called GNisi, which uses a Graph Neural network trained\non known Ising models in order to construct the parameters for unseen data. We\nshow that GNisi is more accurate than the existing state of the art software,\nand we illustrate our method by applying GNisi to gene expression data.",
          "link": "http://arxiv.org/abs/2109.04257",
          "publishedOn": "2021-09-10T07:20:14.512Z",
          "wordCount": 585,
          "title": "GNisi: A graph network for reconstructing Ising models from multivariate binarized data. (arXiv:2109.04257v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chevrot_A/0/1/0/all/0/1\">Antoine Chevrot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernotte_A/0/1/0/all/0/1\">Alexandre Vernotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legeard_B/0/1/0/all/0/1\">Bruno Legeard</a>",
          "description": "The Automatic Dependent Surveillance Broadcast protocol is one of the latest\ncompulsory advances in air surveillance. While it supports the tracking of the\never-growing number of aircraft in the air, it also introduces cybersecurity\nissues that must be mitigated e.g., false data injection attacks where an\nattacker emits fake surveillance information. The recent data sources and tools\navailable to obtain flight tracking records allow the researchers to create\ndatasets and develop Machine Learning models capable of detecting such\nanomalies in En-Route trajectories. In this context, we propose a novel\nmultivariate anomaly detection model called Discriminatory Auto-Encoder (DAE).\nIt uses the baseline of a regular LSTM-based auto-encoder but with several\ndecoders, each getting data of a specific flight phase (e.g. climbing, cruising\nor descending) during its training.To illustrate the DAE's efficiency, an\nevaluation dataset was created using real-life anomalies as well as\nrealistically crafted ones, with which the DAE as well as three anomaly\ndetection models from the literature were evaluated. Results show that the DAE\nachieves better results in both accuracy and speed of detection. The dataset,\nthe models implementations and the evaluation results are available in an\nonline repository, thereby enabling replicability and facilitating future\nexperiments.",
          "link": "http://arxiv.org/abs/2109.04247",
          "publishedOn": "2021-09-10T07:20:14.505Z",
          "wordCount": 647,
          "title": "DAE : Discriminatory Auto-Encoder for multivariate time-series anomaly detection in air transportation. (arXiv:2109.04247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.",
          "link": "http://arxiv.org/abs/2109.04312",
          "publishedOn": "2021-09-10T07:20:14.494Z",
          "wordCount": 623,
          "title": "MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1911.03437",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>",
          "description": "Transfer learning has fundamentally changed the landscape of natural language\nprocessing (NLP) research. Many existing state-of-the-art models are first\npre-trained on a large text corpus and then fine-tuned on downstream tasks.\nHowever, due to limited data resources from downstream tasks and the extremely\nlarge capacity of pre-trained models, aggressive fine-tuning often causes the\nadapted model to overfit the data of downstream tasks and forget the knowledge\nof the pre-trained model. To address the above issue in a more principled\nmanner, we propose a new computational framework for robust and efficient\nfine-tuning for pre-trained language models. Specifically, our proposed\nframework contains two important ingredients: 1. Smoothness-inducing\nregularization, which effectively manages the capacity of the model; 2. Bregman\nproximal point optimization, which is a class of trust-region methods and can\nprevent knowledge forgetting. Our experiments demonstrate that our proposed\nmethod achieves the state-of-the-art performance on multiple NLP benchmarks.",
          "link": "http://arxiv.org/abs/1911.03437",
          "publishedOn": "2021-09-10T07:20:14.470Z",
          "wordCount": 686,
          "title": "SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.12537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chasani_P/0/1/0/all/0/1\">Paraskevi Chasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likas_A/0/1/0/all/0/1\">Aristidis Likas</a>",
          "description": "Deciding on the unimodality of a dataset is an important problem in data\nanalysis and statistical modeling. It allows to obtain knowledge about the\nstructure of the dataset, ie. whether data points have been generated by a\nprobability distribution with a single or more than one peaks. Such knowledge\nis very useful for several data analysis problems, such as for deciding on the\nnumber of clusters and determining unimodal projections. We propose a technique\ncalled UU-test (Unimodal Uniform test) to decide on the unimodality of a\none-dimensional dataset. The method operates on the empirical cumulative\ndensity function (ecdf) of the dataset. It attempts to build a piecewise linear\napproximation of the ecdf that is unimodal and models the data sufficiently in\nthe sense that the data corresponding to each linear segment follows the\nuniform distribution. A unique feature of this approach is that in the case of\nunimodality, it also provides a statistical model of the data in the form of a\nUniform Mixture Model. We present experimental results in order to assess the\nability of the method to decide on unimodality and perform comparisons with the\nwell-known dip-test approach. In addition, in the case of unimodal datasets we\nevaluate the Uniform Mixture Models provided by the proposed method using the\ntest set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.",
          "link": "http://arxiv.org/abs/2008.12537",
          "publishedOn": "2021-09-10T07:20:14.462Z",
          "wordCount": null,
          "title": "The UU-test for Statistical Modeling of Unimodal Data. (arXiv:2008.12537v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">You Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopal_A/0/1/0/all/0/1\">Achintya Gopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Liwen Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Key_A/0/1/0/all/0/1\">Aaron Key</a>",
          "description": "As an important step to fulfill the Paris Agreement and achieve net-zero\nemissions by 2050, the European Commission adopted the most ambitious package\nof climate impact measures in April 2021 to improve the flow of capital towards\nsustainable activities. For these and other international measures to be\nsuccessful, reliable data is key. The ability to see the carbon footprint of\ncompanies around the world will be critical for investors to comply with the\nmeasures. However, with only a small portion of companies volunteering to\ndisclose their greenhouse gas (GHG) emissions, it is nearly impossible for\ninvestors to align their investment strategies with the measures. By training a\nmachine learning model on disclosed GHG emissions, we are able to estimate the\nemissions of other companies globally who do not disclose their emissions. In\nthis paper, we show that our model provides accurate estimates of corporate GHG\nemissions to investors such that they are able to align their investments with\nthe regulatory measures and achieve net-zero goals.",
          "link": "http://arxiv.org/abs/2109.04318",
          "publishedOn": "2021-09-10T07:20:14.428Z",
          "wordCount": 625,
          "title": "Estimation of Corporate Greenhouse Gas Emissions via Machine Learning. (arXiv:2109.04318v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14694",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1\">Jing An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lexing Ying</a>",
          "description": "Many machine learning and data science tasks require solving non-convex\noptimization problems. When the loss function is a sum of multiple terms, a\npopular method is the stochastic gradient descent. Viewed as a process for\nsampling the loss function landscape, the stochastic gradient descent is known\nto prefer flat minima. Though this is desired for certain optimization problems\nsuch as in deep learning, it causes issues when the goal is to find the global\nminimum, especially if the global minimum resides in a sharp valley.\n\nIllustrated with a simple motivating example, we show that the fundamental\nreason is that the difference in the Lipschitz constants of multiple terms in\nthe loss function causes stochastic gradient descent to experience different\nvariances at different minima. In order to mitigate this effect and perform\nfaithful optimization, we propose a combined resampling-reweighting scheme to\nbalance the variance at local minima and extend to general loss functions. We\nexplain from the numerical stability perspective how the proposed scheme is\nmore likely to select the true global minimum, and the local convergence\nanalysis perspective how it converges to a minimum faster when compared with\nthe vanilla stochastic gradient descent. Experiments from robust statistics and\ncomputational chemistry are provided to demonstrate the theoretical findings.",
          "link": "http://arxiv.org/abs/2105.14694",
          "publishedOn": "2021-09-10T07:20:14.221Z",
          "wordCount": null,
          "title": "Combining resampling and reweighting for faithful stochastic optimization. (arXiv:2105.14694v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arka Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullick_S/0/1/0/all/0/1\">Sankha Subhra Mullick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_S/0/1/0/all/0/1\">Shounak Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Swagatam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallipeddi_R/0/1/0/all/0/1\">Rammohan Mallipeddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Asit Kr. Das</a>",
          "description": "Constructing adversarial perturbations for deep neural networks is an\nimportant direction of research. Crafting image-dependent adversarial\nperturbations using white-box feedback has hitherto been the norm for such\nadversarial attacks. However, black-box attacks are much more practical for\nreal-world applications. Universal perturbations applicable across multiple\nimages are gaining popularity due to their innate generalizability. There have\nalso been efforts to restrict the perturbations to a few pixels in the image.\nThis helps to retain visual similarity with the original images making such\nattacks hard to detect. This paper marks an important step which combines all\nthese directions of research. We propose the DEceit algorithm for constructing\neffective universal pixel-restricted perturbations using only black-box\nfeedback from the target network. We conduct empirical investigations using the\nImageNet validation set on the state-of-the-art deep neural classifiers by\nvarying the number of pixels to be perturbed from a meagre 10 pixels to as high\nas all pixels in the image. We find that perturbing only about 10% of the\npixels in an image using DEceit achieves a commendable and highly transferable\nFooling Rate while retaining the visual quality. We further demonstrate that\nDEceit can be successfully applied to image dependent attacks as well. In both\nsets of experiments, we outperformed several state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2004.13002",
          "publishedOn": "2021-09-10T07:20:14.131Z",
          "wordCount": 717,
          "title": "A Black-box Adversarial Attack Strategy with Adjustable Sparsity and Generalizability for Deep Image Classifiers. (arXiv:2004.13002v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04235",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yi_P/0/1/0/all/0/1\">Peng Yi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kecheng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Z/0/1/0/all/0/1\">Zhaoqi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1\">Di Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pu_X/0/1/0/all/0/1\">Xiaorong Pu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>",
          "description": "Electroencephalogram (EEG) has shown a useful approach to produce a\nbrain-computer interface (BCI). One-dimensional (1-D) EEG signal is yet easily\ndisturbed by certain artifacts (a.k.a. noise) due to the high temporal\nresolution. Thus, it is crucial to remove the noise in received EEG signal.\nRecently, deep learning-based EEG signal denoising approaches have achieved\nimpressive performance compared with traditional ones. It is well known that\nthe characteristics of self-similarity (including non-local and local ones) of\ndata (e.g., natural images and time-domain signals) are widely leveraged for\ndenoising. However, existing deep learning-based EEG signal denoising methods\nignore either the non-local self-similarity (e.g., 1-D convolutional neural\nnetwork) or local one (e.g., fully connected network and recurrent neural\nnetwork). To address this issue, we propose a novel 1-D EEG signal denoising\nnetwork with 2-D transformer, namely EEGDnet. Specifically, we comprehensively\ntake into account the non-local and local self-similarity of EEG signal through\nthe transformer module. By fusing non-local self-similarity in self-attention\nblocks and local self-similarity in feed forward blocks, the negative impact\ncaused by noises and outliers can be reduced significantly. Extensive\nexperiments show that, compared with other state-of-the-art models, EEGDnet\nachieves much better performance in terms of both quantitative and qualitative\nmetrics.",
          "link": "http://arxiv.org/abs/2109.04235",
          "publishedOn": "2021-09-10T07:20:14.125Z",
          "wordCount": 661,
          "title": "EEGDnet: Fusing Non-Local and Local Self-Similarity for 1-D EEG Signal Denoising with 2-D Transformer. (arXiv:2109.04235v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rivera_Landos_E/0/1/0/all/0/1\">Emilio Rivera-Landos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1\">Foutse Khomh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1\">Amin Nikanjam</a>",
          "description": "Reproducibility is a crucial requirement in scientific research. When results\nof research studies and scientific papers have been found difficult or\nimpossible to reproduce, we face a challenge which is called reproducibility\ncrisis. Although the demand for reproducibility in Machine Learning (ML) is\nacknowledged in the literature, a main barrier is inherent non-determinism in\nML training and inference. In this paper, we establish the fundamental factors\nthat cause non-determinism in ML systems. A framework, ReproduceML, is then\nintroduced for deterministic evaluation of ML experiments in a real, controlled\nenvironment. ReproduceML allows researchers to investigate software\nconfiguration effects on ML training and inference. Using ReproduceML, we run a\ncase study: investigation of the impact of bugs inside ML libraries on\nperformance of ML experiments. This study attempts to quantify the impact that\nthe occurrence of bugs in a popular ML framework, PyTorch, has on the\nperformance of trained models. To do so, a comprehensive methodology is\nproposed to collect buggy versions of ML libraries and run deterministic ML\nexperiments using ReproduceML. Our initial finding is that there is no evidence\nbased on our limited dataset to show that bugs which occurred in PyTorch do\naffect the performance of trained models. The proposed methodology as well as\nReproduceML can be employed for further research on non-determinism and bugs.",
          "link": "http://arxiv.org/abs/2109.03991",
          "publishedOn": "2021-09-10T07:20:14.119Z",
          "wordCount": 667,
          "title": "The challenge of reproducible ML: an empirical study on the impact of bugs. (arXiv:2109.03991v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Asha Devi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anurag Singh</a>",
          "description": "A reliable forecast of inflows to the reservoir is a key factor in the\noptimal operation of reservoirs. Real-time operation of the reservoir based on\nforecasts of inflows can lead to substantial economic gains. However, the\nforecast of inflow is an intricate task as it has to incorporate the impacts of\nclimate and hydrological changes. Therefore, the major objective of the present\nwork is to develop a novel approach based on long short-term memory (LSTM) for\nthe forecast of inflows. Real-time inflow forecast, in other words, daily\ninflow at the reservoir helps in efficient operation of water resources. Also,\ndaily variations in the release can be monitored efficiently and the\nreliability of operation is improved. This work proposes a naive anomaly\ndetection algorithm baseline based on LSTM. In other words, a strong baseline\nto forecast flood and drought for any deep learning-based prediction model. The\npracticality of the approach has been demonstrated using the observed daily\ndata of the past 20 years from Bhakra Dam in India. The results of the\nsimulations conducted herein clearly indicate the supremacy of the LSTM\napproach over the traditional methods of forecasting. Although, experiments are\nrun on data from Bhakra Dam Reservoir in India, LSTM model, and anomaly\ndetection algorithm are general purpose and can be applied to any basin with\nminimal changes. A distinct practical advantage of the LSTM method presented\nherein is that it can adequately simulate non-stationarity and non-linearity in\nthe historical data.",
          "link": "http://arxiv.org/abs/2109.04255",
          "publishedOn": "2021-09-10T07:20:14.101Z",
          "wordCount": 677,
          "title": "Optimal Reservoir Operations using Long Short-Term Memory Network. (arXiv:2109.04255v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhipeng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhixing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Manqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bohang Zheng</a>",
          "description": "Temporal relational data, perhaps the most commonly used data type in\nindustrial machine learning applications, needs labor-intensive feature\nengineering and data analyzing for giving precise model predictions. An\nautomatic machine learning framework is needed to ease the manual efforts in\nfine-tuning the models so that the experts can focus more on other problems\nthat really need humans' engagement such as problem definition, deployment, and\nbusiness services. However, there are three main challenges for building\nautomatic solutions for temporal relational data: 1) how to effectively and\nautomatically mining useful information from the multiple tables and the\nrelations from them? 2) how to be self-adjustable to control the time and\nmemory consumption within a certain budget? and 3) how to give generic\nsolutions to a wide range of tasks? In this work, we propose our solution that\nsuccessfully addresses the above issues in an end-to-end automatic way. The\nproposed framework, AutoSmart, is the winning solution to the KDD Cup 2019 of\nthe AutoML Track, which is one of the largest AutoML competition to date (860\nteams with around 4,955 submissions). The framework includes automatic data\nprocessing, table merging, feature engineering, and model tuning, with a\ntime\\&memory controller for efficiently and automatically formulating the\nmodels. The proposed framework outperforms the baseline solution significantly\non several datasets in various domains.",
          "link": "http://arxiv.org/abs/2109.04115",
          "publishedOn": "2021-09-10T07:20:14.073Z",
          "wordCount": 679,
          "title": "AutoSmart: An Efficient and Automatic Machine Learning framework for Temporal Relational Data. (arXiv:2109.04115v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>",
          "description": "A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.",
          "link": "http://arxiv.org/abs/2010.03060",
          "publishedOn": "2021-09-10T07:20:14.066Z",
          "wordCount": 704,
          "title": "Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.00295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1\">Sashank Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Manzil Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1\">Zachary Garrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1\">Keith Rush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konecny_J/0/1/0/all/0/1\">Jakub Kone&#x10d;n&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sanjiv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1\">H. Brendan McMahan</a>",
          "description": "Federated learning is a distributed machine learning paradigm in which a\nlarge number of clients coordinate with a central server to learn a model\nwithout sharing their own training data. Standard federated optimization\nmethods such as Federated Averaging (FedAvg) are often difficult to tune and\nexhibit unfavorable convergence behavior. In non-federated settings, adaptive\noptimization methods have had notable success in combating such issues. In this\nwork, we propose federated versions of adaptive optimizers, including Adagrad,\nAdam, and Yogi, and analyze their convergence in the presence of heterogeneous\ndata for general non-convex settings. Our results highlight the interplay\nbetween client heterogeneity and communication efficiency. We also perform\nextensive experiments on these methods and show that the use of adaptive\noptimizers can significantly improve the performance of federated learning.",
          "link": "http://arxiv.org/abs/2003.00295",
          "publishedOn": "2021-09-10T07:20:14.057Z",
          "wordCount": 651,
          "title": "Adaptive Federated Optimization. (arXiv:2003.00295v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.12365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "In this article, we study approximation properties of the variation spaces\ncorresponding to shallow neural networks with a variety of activation\nfunctions. We introduce two main tools for estimating the metric entropy,\napproximation rates, and $n$-widths of these spaces. First, we introduce the\nnotion of a smoothly parameterized dictionary and give upper bounds on the\nnon-linear approximation rates, metric entropy and $n$-widths of their absolute\nconvex hull. The upper bounds depend upon the order of smoothness of the\nparameterization. This result is applied to dictionaries of ridge functions\ncorresponding to shallow neural networks, and they improve upon existing\nresults in many cases. Next, we provide a method for lower bounding the metric\nentropy and $n$-widths of variation spaces which contain certain classes of\nridge functions. This result gives sharp lower bounds on the\n$L^2$-approximation rates, metric entropy, and $n$-widths for variation spaces\ncorresponding to neural networks with a range of important activation\nfunctions, including ReLU$^k$, sigmoidal activation functions with bounded\nvariation, and the B-spline activation functions.",
          "link": "http://arxiv.org/abs/2101.12365",
          "publishedOn": "2021-09-10T07:20:14.047Z",
          "wordCount": 697,
          "title": "Sharp Bounds on the Approximation Rates, Metric Entropy, and $n$-widths of Shallow Neural Networks. (arXiv:2101.12365v7 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>",
          "description": "Recently, the textual adversarial attack models become increasingly popular\ndue to their successful in estimating the robustness of NLP models. However,\nexisting works have obvious deficiencies. (1) They usually consider only a\nsingle granularity of modification strategies (e.g. word-level or\nsentence-level), which is insufficient to explore the holistic textual space\nfor generation; (2) They need to query victim models hundreds of times to make\na successful attack, which is highly inefficient in practice. To address such\nproblems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to\neffectively generate high-quality adversarial samples with fewer queries to\nvictim models. Furthermore, we propose a reinforcement-learning based method to\ntrain a multi-granularity attack agent through behavior cloning with the expert\nknowledge from our MAYA algorithm to further reduce the query times.\nAdditionally, we also adapt the agent to attack black-box models that only\noutput labels without confidence scores. We conduct comprehensive experiments\nto evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two\ndifferent black-box attack settings and three benchmark datasets. Experimental\nresults show that our models achieve overall better attacking performance and\nproduce more fluent and grammatical adversarial samples compared to baseline\nmodels. Besides, our adversarial attack agent significantly reduces the query\ntimes in both attack settings. Our codes are released at\nhttps://github.com/Yangyi-Chen/MAYA.",
          "link": "http://arxiv.org/abs/2109.04367",
          "publishedOn": "2021-09-10T07:20:14.019Z",
          "wordCount": 670,
          "title": "Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cabanes_G/0/1/0/all/0/1\">Gu&#xe9;na&#xeb;l Cabanes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennani_Y/0/1/0/all/0/1\">Youn&#xe8;s Bennani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verde_R/0/1/0/all/0/1\">Rosanna Verde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irpino_A/0/1/0/all/0/1\">Antonio Irpino</a>",
          "description": "This paper deals with a clustering algorithm for histogram data based on a\nSelf-Organizing Map (SOM) learning. It combines a dimension reduction by SOM\nand the clustering of the data in a reduced space. Related to the kind of data,\na suitable dissimilarity measure between distributions is introduced: the $L_2$\nWasserstein distance. Moreover, the number of clusters is not fixed in advance\nbut it is automatically found according to a local data density estimation in\nthe original space. Applications on synthetic and real data sets corroborate\nthe proposed strategy.",
          "link": "http://arxiv.org/abs/2109.04301",
          "publishedOn": "2021-09-10T07:20:14.009Z",
          "wordCount": 536,
          "title": "On the use of Wasserstein metric in topological clustering of distributional data. (arXiv:2109.04301v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Srinagesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1\">Guoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>",
          "description": "Albeit the universal representational power of pre-trained language models,\nadapting them onto a specific NLP task still requires a considerably large\namount of labeled data. Effective task fine-tuning meets challenges when only a\nfew labeled examples are present for the task. In this paper, we aim to the\naddress of the problem of few shot task learning by exploiting and transferring\nfrom a different task which admits a related but disparate label space.\nSpecifically, we devise a label transfer network (LTN) to transform the labels\nfrom source task to the target task of interest for training. Both the LTN and\nthe model for task prediction are learned via a bi-level optimization\nframework, which we term as MetaXT. MetaXT offers a principled solution to best\nadapt a pre-trained language model to the target task by transferring knowledge\nfrom the source task. Empirical evaluations on cross-task transfer settings for\nfour NLP tasks, from two different types of label space disparities,\ndemonstrate the effectiveness of MetaXT, especially when the labeled data in\nthe target task is limited.",
          "link": "http://arxiv.org/abs/2109.04240",
          "publishedOn": "2021-09-10T07:20:13.995Z",
          "wordCount": 617,
          "title": "MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03999",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhifeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "The unprecedented demand for collaborative machine learning in a\nprivacy-preserving manner gives rise to a novel machine learning paradigm\ncalled federated learning (FL). Given a sufficient level of privacy guarantees,\nthe practicality of an FL system mainly depends on its time-to-accuracy\nperformance during the training process. Despite bearing some resemblance with\ntraditional distributed training, FL has four distinct challenges that\ncomplicate the optimization towards shorter time-to-accuracy: information\ndeficiency, coupling for contrasting factors, client heterogeneity, and huge\nconfiguration space. Motivated by the need for inspiring related research, in\nthis paper we survey highly relevant attempts in the FL literature and organize\nthem by the related training phases in the standard workflow: selection,\nconfiguration, and reporting. We also review exploratory work including\nmeasurement studies and benchmarking tools to friendly support FL developers.\nAlthough a few survey articles on FL already exist, our work differs from them\nin terms of the focus, classification, and implications.",
          "link": "http://arxiv.org/abs/2109.03999",
          "publishedOn": "2021-09-10T07:20:13.986Z",
          "wordCount": 607,
          "title": "System Optimization in Synchronous Federated Training: A Survey. (arXiv:2109.03999v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1\">Lance Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romana_A/0/1/0/all/0/1\">Amrit Romana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Provost_E/0/1/0/all/0/1\">Emily Mower Provost</a>",
          "description": "In recent years, deep-learning-based speech emotion recognition models have\noutperformed classical machine learning models. Previously, neural network\ndesigns, such as Multitask Learning, have accounted for variations in emotional\nexpressions due to demographic and contextual factors. However, existing models\nface a few constraints: 1) they rely on a clear definition of domains (e.g.\ngender, noise condition, etc.) and the availability of domain labels; 2) they\noften attempt to learn domain-invariant features while emotion expressions can\nbe domain-specific. In the present study, we propose the Nonparametric\nHierarchical Neural Network (NHNN), a lightweight hierarchical neural network\nmodel based on Bayesian nonparametric clustering. In comparison to Multitask\nLearning approaches, the proposed model does not require domain/task labels. In\nour experiments, the NHNN models generally outperform the models with similar\nlevels of complexity and state-of-the-art models in within-corpus and\ncross-corpus tests. Through clustering analysis, we show that the NHNN models\nare able to learn group-specific features and bridge the performance gap\nbetween groups.",
          "link": "http://arxiv.org/abs/2109.04316",
          "publishedOn": "2021-09-10T07:20:13.966Z",
          "wordCount": 618,
          "title": "Accounting for Variations in Speech Emotion Recognition with Nonparametric Hierarchical Neural Network. (arXiv:2109.04316v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04094",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">C. Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">S. Wang</a>",
          "description": "Federated learning is a distributed machine learning paradigm that trains a\nglobal model for prediction based on a number of local models at clients while\nlocal data privacy is preserved. Class imbalance is believed to be one of the\nfactors that degrades the global model performance. However, there has been\nvery little research on if and how class imbalance can affect the global\nperformance. class imbalance in federated learning is much more complex than\nthat in traditional non-distributed machine learning, due to different class\nimbalance situations at local clients. Class imbalance needs to be re-defined\nin distributed learning environments. In this paper, first, we propose two new\nmetrics to define class imbalance -- the global class imbalance degree (MID)\nand the local difference of class imbalance among clients (WCS). Then, we\nconduct extensive experiments to analyze the impact of class imbalance on the\nglobal performance in various scenarios based on our definition. Our results\nshow that a higher MID and a larger WCS degrade more the performance of the\nglobal model. Besides, WCS is shown to slow down the convergence of the global\nmodel by misdirecting the optimization.",
          "link": "http://arxiv.org/abs/2109.04094",
          "publishedOn": "2021-09-10T07:20:13.960Z",
          "wordCount": 625,
          "title": "An Experimental Study of Class Imbalance in Federated Learning. (arXiv:2109.04094v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03973",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Charles_Z/0/1/0/all/0/1\">Zachary Charles</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rush_K/0/1/0/all/0/1\">Keith Rush</a>",
          "description": "We study iterated vector fields and investigate whether they are\nconservative, in the sense that they are the gradient of some scalar-valued\nfunction. We analyze the conservatism of various iterated vector fields,\nincluding gradient vector fields associated to loss functions of generalized\nlinear models. We relate this study to optimization and derive novel\nconvergence results for federated learning algorithms. In particular, we show\nthat for certain classes of functions (including non-convex functions),\nfederated averaging is equivalent to gradient descent on a surrogate loss\nfunction. Finally, we discuss a variety of open questions spanning topics in\ngeometry, dynamical systems, and optimization.",
          "link": "http://arxiv.org/abs/2109.03973",
          "publishedOn": "2021-09-10T07:20:13.952Z",
          "wordCount": 559,
          "title": "Iterated Vector Fields and Conservatism, with Applications to Federated Learning. (arXiv:2109.03973v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hertweck_C/0/1/0/all/0/1\">Corinna Hertweck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heitz_C/0/1/0/all/0/1\">Christoph Heitz</a>",
          "description": "While the field of algorithmic fairness has brought forth many ways to\nmeasure and improve the fairness of machine learning models, these findings are\nstill not widely used in practice. We suspect that one reason for this is that\nthe field of algorithmic fairness came up with a lot of definitions of\nfairness, which are difficult to navigate. The goal of this paper is to provide\ndata scientists with an accessible introduction to group fairness metrics and\nto give some insight into the philosophical reasoning for caring about these\nmetrics. We will do this by considering in which sense socio-demographic groups\nare compared for making a statement on fairness.",
          "link": "http://arxiv.org/abs/2109.04230",
          "publishedOn": "2021-09-10T07:20:13.933Z",
          "wordCount": 585,
          "title": "A Systematic Approach to Group Fairness in Automated Decision Making. (arXiv:2109.04230v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kshetramade_R/0/1/0/all/0/1\">Reema Kshetramade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houser_J/0/1/0/all/0/1\">John Houser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Compliments and concerns in reviews are valuable for understanding users'\nshopping interests and their opinions with respect to specific aspects of\ncertain items. Existing review-based recommenders favor large and complex\nlanguage encoders that can only learn latent and uninterpretable text\nrepresentations. They lack explicit user attention and item property modeling,\nwhich however could provide valuable information beyond the ability to\nrecommend items. Therefore, we propose a tightly coupled two-stage approach,\nincluding an Aspect-Sentiment Pair Extractor (ASPE) and an\nAttention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines\nAspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as\nconcrete aspect-level evidence. Extensive experiments on seven real-world\nAmazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs\nwhich enable APRE to deliver superior accuracy over the leading baselines.",
          "link": "http://arxiv.org/abs/2109.03821",
          "publishedOn": "2021-09-10T07:20:13.915Z",
          "wordCount": 584,
          "title": "Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction. (arXiv:2109.03821v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04261",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Deperrois_N/0/1/0/all/0/1\">Nicolas Deperrois</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Petrovici_M/0/1/0/all/0/1\">Mihai A. Petrovici</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Senn_W/0/1/0/all/0/1\">Walter Senn</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jordan_J/0/1/0/all/0/1\">Jakob Jordan</a>",
          "description": "Classical theories of memory consolidation emphasize the importance of replay\nin extracting semantic information from episodic memories. However, the\ncharacteristic creative nature of dreams suggests that memory semantization may\ngo beyond merely replaying previous experiences. We propose that\nrapid-eye-movement (REM) dreaming is essential for efficient memory\nsemantization by randomly combining episodic memories to create new, virtual\nsensory experiences. We support this hypothesis by implementing a cortical\narchitecture with hierarchically organized feedforward and feedback pathways,\ninspired by generative adversarial networks (GANs). Learning in our model is\norganized across three different global brain states mimicking wakefulness,\nnon-REM (NREM) and REM sleep, optimizing different, but complementary objective\nfunctions. We train the model in an unsupervised fashion on standard datasets\nof natural images and evaluate the quality of the learned representations. Our\nresults suggest that adversarial dreaming during REM sleep is essential for\nextracting memory contents, while perturbed dreaming during NREM sleep improves\nrobustness of the latent representation to noisy sensory inputs. The model\nprovides a new computational perspective on sleep states, memory replay and\ndreams and suggests a cortical implementation of GANs.",
          "link": "http://arxiv.org/abs/2109.04261",
          "publishedOn": "2021-09-10T07:20:13.897Z",
          "wordCount": 637,
          "title": "Memory semantization through perturbed and adversarial dreaming. (arXiv:2109.04261v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alevizopoulou_S/0/1/0/all/0/1\">Sofia Alevizopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koloveas_P/0/1/0/all/0/1\">Paris Koloveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tryfonopoulos_C/0/1/0/all/0/1\">Christos Tryfonopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raftopoulou_P/0/1/0/all/0/1\">Paraskevi Raftopoulou</a>",
          "description": "The rapid development of IoT applications and their use in various fields of\neveryday life has resulted in an escalated number of different possible\ncyber-threats, and has consequently raised the need of securing IoT devices.\nCollecting Cyber-Threat Intelligence (e.g., zero-day vulnerabilities or\ntrending exploits) from various online sources and utilizing it to proactively\nsecure IoT systems or prepare mitigation scenarios has proven to be a promising\ndirection. In this work, we focus on social media monitoring and investigate\nreal-time Cyber-Threat Intelligence detection from the Twitter stream.\nInitially, we compare and extensively evaluate six different machine-learning\nbased classification alternatives trained with vulnerability descriptions and\ntested with real-world data from the Twitter stream to identify the\nbest-fitting solution. Subsequently, based on our findings, we propose a novel\nsocial media monitoring system tailored to the IoT domain; the system allows\nusers to identify recent/trending vulnerabilities and exploits on IoT devices.\nFinally, to aid research on the field and support the reproducibility of our\nresults we publicly release all annotated datasets created during this process.",
          "link": "http://arxiv.org/abs/2109.04306",
          "publishedOn": "2021-09-10T07:20:13.845Z",
          "wordCount": 631,
          "title": "Social Media Monitoring for IoT Cyber-Threats. (arXiv:2109.04306v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becking_D/0/1/0/all/0/1\">Daniel Becking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1\">Maximilian Dreyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Karsten M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1\">Sebastian Lapuschkin</a>",
          "description": "The remarkable success of deep neural networks (DNNs) in various applications\nis accompanied by a significant increase in network parameters and arithmetic\noperations. Such increases in memory and computational demands make deep\nlearning prohibitive for resource-constrained hardware platforms such as mobile\ndevices. Recent efforts aim to reduce these overheads, while preserving model\nperformance as much as possible, and include parameter reduction techniques,\nparameter quantization, and lossless compression techniques.\n\nIn this chapter, we develop and describe a novel quantization paradigm for\nDNNs: Our method leverages concepts of explainable AI (XAI) and concepts of\ninformation theory: Instead of assigning weight values based on their distances\nto the quantization clusters, the assignment function additionally considers\nweight relevances obtained from Layer-wise Relevance Propagation (LRP) and the\ninformation content of the clusters (entropy optimization). The ultimate goal\nis to preserve the most relevant weights in quantization clusters of highest\ninformation content.\n\nExperimental results show that this novel Entropy-Constrained and\nXAI-adjusted Quantization (ECQ$^{\\text{x}}$) method generates ultra\nlow-precision (2-5 bit) and simultaneously sparse neural networks while\nmaintaining or even improving model performance. Due to reduced parameter\nprecision and high number of zero-elements, the rendered networks are highly\ncompressible in terms of file size, up to $103\\times$ compared to the\nfull-precision unquantized DNN model. Our approach was evaluated on different\ntypes of models and datasets (including Google Speech Commands and CIFAR-10)\nand compared with previous work.",
          "link": "http://arxiv.org/abs/2109.04236",
          "publishedOn": "2021-09-10T07:20:13.824Z",
          "wordCount": 680,
          "title": "ECQ$^{\\text{x}}$: Explainability-Driven Quantization for Low-Bit and Sparse DNNs. (arXiv:2109.04236v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1\">Maziar Gomrokchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1\">Susan Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboutalebi_H/0/1/0/all/0/1\">Hossein Aboutalebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>",
          "description": "While significant research advances have been made in the field of deep\nreinforcement learning, a major challenge to widespread industrial adoption of\ndeep reinforcement learning that has recently surfaced but little explored is\nthe potential vulnerability to privacy breaches. In particular, there have been\nno concrete adversarial attack strategies in literature tailored for studying\nthe vulnerability of deep reinforcement learning algorithms to membership\ninference attacks. To address this gap, we propose an adversarial attack\nframework tailored for testing the vulnerability of deep reinforcement learning\nalgorithms to membership inference attacks. More specifically, we design a\nseries of experiments to investigate the impact of temporal correlation, which\nnaturally exists in reinforcement learning training data, on the probability of\ninformation leakage. Furthermore, we study the differences in the performance\nof \\emph{collective} and \\emph{individual} membership attacks against deep\nreinforcement learning algorithms. Experimental results show that the proposed\nadversarial attack framework is surprisingly effective at inferring the data\nused during deep reinforcement training with an accuracy exceeding $84\\%$ in\nindividual and $97\\%$ in collective mode on two different control tasks in\nOpenAI Gym, which raises serious privacy concerns in the deployment of models\nresulting from deep reinforcement learning. Moreover, we show that the learning\nstate of a reinforcement learning algorithm significantly influences the level\nof the privacy breach.",
          "link": "http://arxiv.org/abs/2109.03975",
          "publishedOn": "2021-09-10T07:20:13.805Z",
          "wordCount": 683,
          "title": "Where Did You Learn That From? Surprising Effectiveness of Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning. (arXiv:2109.03975v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moya_C/0/1/0/all/0/1\">Christian Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guang Lin</a>",
          "description": "Deep learning-based surrogate modeling is becoming a promising approach for\nlearning and simulating dynamical systems. Deep-learning methods, however, find\nvery challenging learning stiff dynamics. In this paper, we develop DAE-PINN,\nthe first effective deep-learning framework for learning and simulating the\nsolution trajectories of nonlinear differential-algebraic equations (DAE),\nwhich present a form of infinite stiffness and describe, for example, the\ndynamics of power networks. Our DAE-PINN bases its effectiveness on the synergy\nbetween implicit Runge-Kutta time-stepping schemes (designed specifically for\nsolving DAEs) and physics-informed neural networks (PINN) (deep neural networks\nthat we train to satisfy the dynamics of the underlying problem). Furthermore,\nour framework (i) enforces the neural network to satisfy the DAEs as\n(approximate) hard constraints using a penalty-based method and (ii) enables\nsimulating DAEs for long-time horizons. We showcase the effectiveness and\naccuracy of DAE-PINN by learning and simulating the solution trajectories of a\nthree-bus power network.",
          "link": "http://arxiv.org/abs/2109.04304",
          "publishedOn": "2021-09-10T07:20:13.798Z",
          "wordCount": 598,
          "title": "DAE-PINN: A Physics-Informed Neural Network Model for Simulating Differential-Algebraic Equations with Application to Power Networks. (arXiv:2109.04304v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03974",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Piliouras_G/0/1/0/all/0/1\">Georgios Piliouras</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>",
          "description": "Several recent works in online optimization and game dynamics have\nestablished strong negative complexity results including the formal emergence\nof instability and chaos even in small such settings, e.g., $2\\times 2$ games.\nThese results motivate the following question: Which methodological tools can\nguarantee the regularity of such dynamics and how can we apply them in standard\nsettings of interest such as discrete-time first-order optimization dynamics?\nWe show how proving the existence of invariant functions, i.e., constant of\nmotions, is a fundamental contribution in this direction and establish a\nplethora of such positive results (e.g. gradient descent, multiplicative\nweights update, alternating gradient descent and manifold gradient descent)\nboth in optimization as well as in game settings. At a technical level, for\nsome conservation laws we provide an explicit and concise closed form, whereas\nfor other ones we present non-constructive proofs using tools from dynamical\nsystems.",
          "link": "http://arxiv.org/abs/2109.03974",
          "publishedOn": "2021-09-10T07:20:13.789Z",
          "wordCount": 593,
          "title": "Constants of Motion: The Antidote to Chaos in Optimization and Game Dynamics. (arXiv:2109.03974v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1901.09018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_A/0/1/0/all/0/1\">Akshay Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1\">Miroslav Dud&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langford_J/0/1/0/all/0/1\">John Langford</a>",
          "description": "We study the exploration problem in episodic MDPs with rich observations\ngenerated from a small number of latent states. Under certain identifiability\nassumptions, we demonstrate how to estimate a mapping from the observations to\nlatent states inductively through a sequence of regression and clustering steps\n-- where previously decoded latent states provide labels for later regression\nproblems -- and use it to construct good exploration policies. We provide\nfinite-sample guarantees on the quality of the learned state decoding function\nand exploration policies, and complement our theory with an empirical\nevaluation on a class of hard exploration problems. Our method exponentially\nimproves over $Q$-learning with na\\\"ive exploration, even when $Q$-learning has\ncheating access to latent states.",
          "link": "http://arxiv.org/abs/1901.09018",
          "publishedOn": "2021-09-10T07:20:13.760Z",
          "wordCount": 639,
          "title": "Provably efficient RL with Rich Observations via Latent State Decoding. (arXiv:1901.09018v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04270",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabitza_F/0/1/0/all/0/1\">Federico Cabitza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campagner_A/0/1/0/all/0/1\">Andrea Campagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fell_M/0/1/0/all/0/1\">Michael Fell</a>",
          "description": "Most Artificial Intelligence applications are based on supervised machine\nlearning (ML), which ultimately grounds on manually annotated data. The\nannotation process is often performed in terms of a majority vote and this has\nbeen proved to be often problematic, as highlighted by recent studies on the\nevaluation of ML models. In this article we describe and advocate for a\ndifferent paradigm, which we call data perspectivism, which moves away from\ntraditional gold standard datasets, towards the adoption of methods that\nintegrate the opinions and perspectives of the human subjects involved in the\nknowledge representation step of ML processes. Drawing on previous works which\ninspired our proposal we describe the potential of our proposal for not only\nthe more subjective tasks (e.g. those related to human language) but also to\ntasks commonly understood as objective (e.g. medical decision making), and\npresent the main advantages of adopting a perspectivist stance in ML, as well\nas possible disadvantages, and various ways in which such a stance can be\nimplemented in practice. Finally, we share a set of recommendations and outline\na research agenda to advance the perspectivist stance in ML.",
          "link": "http://arxiv.org/abs/2109.04270",
          "publishedOn": "2021-09-10T07:20:13.750Z",
          "wordCount": 642,
          "title": "Toward a Perspectivist Turn in Ground Truthing for Predictive Computing. (arXiv:2109.04270v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.12809",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mimi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parnell_A/0/1/0/all/0/1\">Andrew Parnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brabazon_D/0/1/0/all/0/1\">Dermot Brabazon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benavoli_A/0/1/0/all/0/1\">Alessio Benavoli</a>",
          "description": "Bayesian optimization (BO) is an approach to globally optimizing black-box\nobjective functions that are expensive to evaluate. BO-powered experimental\ndesign has found wide application in materials science, chemistry, experimental\nphysics, drug development, etc. This work aims to bring attention to the\nbenefits of applying BO in designing experiments and to provide a BO manual,\ncovering both methodology and software, for the convenience of anyone who wants\nto apply or learn BO. In particular, we briefly explain the BO technique,\nreview all the applications of BO in additive manufacturing, compare and\nexemplify the features of different open BO libraries, unlock new potential\napplications of BO to other types of data (e.g., preferential output). This\narticle is aimed at readers with some understanding of Bayesian methods, but\nnot necessarily with knowledge of additive manufacturing; the software\nperformance overview and implementation instructions are instrumental for any\nexperimental-design practitioner. Moreover, our review in the field of additive\nmanufacturing highlights the current knowledge and technological trends of BO.",
          "link": "http://arxiv.org/abs/2107.12809",
          "publishedOn": "2021-09-10T07:20:13.741Z",
          "wordCount": 642,
          "title": "Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Konyushkova_K/0/1/0/all/0/1\">Ksenia Konyushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yutian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paine_T/0/1/0/all/0/1\">Tom Le Paine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulcehre_C/0/1/0/all/0/1\">Caglar Gulcehre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paduraru_C/0/1/0/all/0/1\">Cosmin Paduraru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mankowitz_D/0/1/0/all/0/1\">Daniel J Mankowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_N/0/1/0/all/0/1\">Nando de Freitas</a>",
          "description": "This paper addresses the problem of policy selection in domains with abundant\nlogged data, but with a very restricted interaction budget. Solving this\nproblem would enable safe evaluation and deployment of offline reinforcement\nlearning policies in industry, robotics, and recommendation domains among\nothers. Several off-policy evaluation (OPE) techniques have been proposed to\nassess the value of policies using only logged data. However, there is still a\nbig gap between the evaluation by OPE and the full online evaluation in the\nreal environment. At the same time, large amount of online interactions is\noften not feasible in practice. To overcome this problem, we introduce\n\\emph{active offline policy selection} -- a novel sequential decision approach\nthat combines logged data with online interaction to identify the best policy.\nThis approach uses OPE estimates to warm start the online evaluation. Then, in\norder to utilize the limited environment interactions wisely, it relies on a\nBayesian optimization method, with a kernel function that represents policy\nsimilarity, to decide which policy to evaluate next. We use multiple benchmarks\nwith a large number of candidate policies to show that the proposed approach\nimproves upon state-of-the-art OPE estimates and pure online policy evaluation.",
          "link": "http://arxiv.org/abs/2106.10251",
          "publishedOn": "2021-09-10T07:20:13.663Z",
          "wordCount": 676,
          "title": "Active Offline Policy Selection. (arXiv:2106.10251v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brazinskas_A/0/1/0/all/0/1\">Arthur Bra&#x17e;inskas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "Opinion summarization has been traditionally approached with unsupervised,\nweakly-supervised and few-shot learning techniques. In this work, we collect a\nlarge dataset of summaries paired with user reviews for over 31,000 products,\nenabling supervised training. However, the number of reviews per product is\nlarge (320 on average), making summarization - and especially training a\nsummarizer - impractical. Moreover, the content of many reviews is not\nreflected in the human-written summaries, and, thus, the summarizer trained on\nrandom review subsets hallucinates. In order to deal with both of these\nchallenges, we formulate the task as jointly learning to select informative\nsubsets of reviews and summarizing the opinions expressed in these subsets. The\nchoice of the review subset is treated as a latent variable, predicted by a\nsmall and simple selector. The subset is then fed into a more powerful\nsummarizer. For joint training, we use amortized variational inference and\npolicy gradient methods. Our experiments demonstrate the importance of\nselecting informative reviews resulting in improved quality of summaries and\nreduced hallucinations.",
          "link": "http://arxiv.org/abs/2109.04325",
          "publishedOn": "2021-09-10T07:20:13.557Z",
          "wordCount": 615,
          "title": "Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.07579",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohdal_O/0/1/0/all/0/1\">Ondrej Bohdal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Rajesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyeji Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy Hospedales</a>",
          "description": "Meta-learning provides a popular and effective family of methods for\ndata-efficient learning of new tasks. However, several important issues in\nmeta-learning have proven hard to study thus far. For example, performance\ndegrades in real-world settings where meta-learners must learn from a wide and\npotentially multi-modal distribution of training tasks; and when distribution\nshift exists between meta-train and meta-test task distributions. These issues\nare typically hard to study since the shape of task distributions, and shift\nbetween them are not straightforward to measure or control in standard\nbenchmarks. We propose the channel coding problem as a benchmark for\nmeta-learning. Channel coding is an important practical application where task\ndistributions naturally arise, and fast adaptation to new tasks is practically\nvaluable. We use our MetaCC benchmark to study several aspects of\nmeta-learning, including the impact of task distribution breadth and shift,\nwhich can be controlled in the coding problem. Going forward, MetaCC provides a\ntool for the community to study the capabilities and limitations of\nmeta-learning, and to drive research on practically robust and effective\nmeta-learners.",
          "link": "http://arxiv.org/abs/2107.07579",
          "publishedOn": "2021-09-10T07:20:13.550Z",
          "wordCount": 657,
          "title": "A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeseul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung Hwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1\">Hong In Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_W/0/1/0/all/0/1\">Wonmo Sung</a>",
          "description": "We propose predictive models that estimate GBM patients' health status of\none-year after treatments (Classification task), predict the long-term\nprognosis of GBM patients at an individual level (Survival task). We used total\nof 467 GBM patients' clinical profile consists of 13 features and two follow-up\ndates. For baseline models of random forest classifier(RFC) and random survival\nforest model (RSF), we introduced generalized linear model (GLM), support\nvector machine (SVM) and Cox proportional hazardous model (COX), accelerated\nfailure time model (AFT) respectively. After preprocessing and prefixing\nstratified 5-fold data set, we generated best performing models for model types\nusing recursive feature elimination process. Total 10, 4, and 13 features were\nextracted for best performing one-year survival/progression status RFC models\nand RSF model via the recursive feature elimination process. In classification\ntask, AUROC of best performing RFC recorded 0.6990 (for one-year survival\nstatus classification) and 0.7076 (for one-year progression classification)\nwhile that of second best baseline models (GLM in both cases) recorded 0.6691\nand 0.6997 respectively. About survival task, the highest C-index of 0.7157 and\nthe lowest IBS of 0.1038 came from the best performing RSF model while that of\nsecond best baseline models were 0.6556 and 0.1139 respectively. A simplified\nlinear correlation (extracted from LIME and virtual patient group analysis)\nbetween each feature and prognosis of GBM patient were consistent with proven\nmedical knowledge. Our machine learning models suggest that the top three\nprognostic factors for GBM patient survival were MGMT gene promoter, the extent\nof resection, and age. To the best of our knowledge, this study is the very\nfirst study introducing a interpretable and medical knowledge consistent GBM\nprognosis predictive models.",
          "link": "http://arxiv.org/abs/2108.13039",
          "publishedOn": "2021-09-10T07:20:13.529Z",
          "wordCount": 756,
          "title": "An Interpretable Web-based Glioblastoma Multiforme Prognosis Prediction Tool using Random Forest Model. (arXiv:2108.13039v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00922",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>",
          "description": "Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.",
          "link": "http://arxiv.org/abs/2109.00922",
          "publishedOn": "2021-09-10T07:20:13.503Z",
          "wordCount": 642,
          "title": "Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09785",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ceusters_G/0/1/0/all/0/1\">Glenn Ceusters</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_R/0/1/0/all/0/1\">Rom&#xe1;n Cant&#xfa; Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_A/0/1/0/all/0/1\">Alberte Bouso Garc&#xed;a</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Franke_R/0/1/0/all/0/1\">R&#xfc;diger Franke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deconinck_G/0/1/0/all/0/1\">Geert Deconinck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Helsen_L/0/1/0/all/0/1\">Lieve Helsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nowe_A/0/1/0/all/0/1\">Ann Now&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Messagie_M/0/1/0/all/0/1\">Maarten Messagie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Camargo_L/0/1/0/all/0/1\">Luis Ramirez Camargo</a>",
          "description": "Model-predictive-control (MPC) offers an optimal control technique to\nestablish and ensure that the total operation cost of multi-energy systems\nremains at a minimum while fulfilling all system constraints. However, this\nmethod presumes an adequate model of the underlying system dynamics, which is\nprone to modelling errors and is not necessarily adaptive. This has an\nassociated initial and ongoing project-specific engineering cost. In this\npaper, we present an on- and off-policy multi-objective reinforcement learning\n(RL) approach, that does not assume a model a priori, benchmarking this against\na linear MPC (LMPC - to reflect current practice, though non-linear MPC\nperforms better) - both derived from the general optimal control problem,\nhighlighting their differences and similarities. In a simple multi-energy\nsystem (MES) configuration case study, we show that a twin delayed deep\ndeterministic policy gradient (TD3) RL agent offers potential to match and\noutperform the perfect foresight LMPC benchmark (101.5%). This while the\nrealistic LMPC, i.e. imperfect predictions, only achieves 98%. While in a more\ncomplex MES system configuration, the RL agent's performance is generally lower\n(94.6%), yet still better than the realistic LMPC (88.9%). In both case\nstudies, the RL agents outperformed the realistic LMPC after a training period\nof 2 years using quarterly interactions with the environment. We conclude that\nreinforcement learning is a viable optimal control technique for multi-energy\nsystems given adequate constraint handling and pre-training, to avoid unsafe\ninteractions and long training periods, as is proposed in fundamental future\nwork.",
          "link": "http://arxiv.org/abs/2104.09785",
          "publishedOn": "2021-09-10T07:20:13.496Z",
          "wordCount": 740,
          "title": "Model-predictive control and reinforcement learning in multi-energy system case studies. (arXiv:2104.09785v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gauchy_C/0/1/0/all/0/1\">Clement Gauchy</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Feau_C/0/1/0/all/0/1\">Cyril Feau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Garnier_J/0/1/0/all/0/1\">Josselin Garnier</a>",
          "description": "As part of Probabilistic Risk Assessment studies, it is necessary to study\nthe fragility of mechanical and civil engineered structures when subjected to\nseismic loads. This risk can be measured with fragility curves, which express\nthe probability of failure of the structure conditionally to a seismic\nintensity measure. The estimation of fragility curves relies on time-consuming\nnumerical simulations, so that careful experimental design is required in order\nto gain the maximum information on the structure's fragility with a limited\nnumber of code evaluations. We propose and implement an active learning\nmethodology based on adaptive importance sampling in order to reduce the\nvariance of the training loss. The efficiency of the proposed method in terms\nof bias, standard deviation and prediction interval coverage are theoretically\nand numerically characterized.",
          "link": "http://arxiv.org/abs/2109.04323",
          "publishedOn": "2021-09-10T07:20:13.489Z",
          "wordCount": 575,
          "title": "Adaptive importance sampling for seismic fragility curve estimation. (arXiv:2109.04323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frikha_A/0/1/0/all/0/1\">Ahmed Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krompass_D/0/1/0/all/0/1\">Denis Krompa&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>",
          "description": "Machine learning models that can generalize to unseen domains are essential\nwhen applied in real-world scenarios involving strong domain shifts. We address\nthe challenging domain generalization (DG) problem, where a model trained on a\nset of source domains is expected to generalize well in unseen domains without\nany exposure to their data. The main challenge of DG is that the features\nlearned from the source domains are not necessarily present in the unseen\ntarget domains, leading to performance deterioration. We assume that learning a\nricher set of features is crucial to improve the transfer to a wider set of\nunknown domains. For this reason, we propose COLUMBUS, a method that enforces\nnew feature discovery via a targeted corruption of the most relevant input and\nmulti-level representations of the data. We conduct an extensive empirical\nevaluation to demonstrate the effectiveness of the proposed approach which\nachieves new state-of-the-art results by outperforming 18 DG algorithms on\nmultiple DG benchmark datasets in the DomainBed framework.",
          "link": "http://arxiv.org/abs/2109.04320",
          "publishedOn": "2021-09-10T07:20:13.481Z",
          "wordCount": 614,
          "title": "COLUMBUS: Automated Discovery of New Multi-Level Features for Domain Generalization via Knowledge Corruption. (arXiv:2109.04320v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in a geometrical graph. Although\nthere are some correlation analysis of features based on covariance, there is\nno relevant research on structural feature correlation analysis with graph\nneural networks. In this paper, we introuduce graph feature to feature\n(Fea2Fea) prediction pipelines in a low dimensional space to explore some\npreliminary results on structural feature correlation, which is based on graph\nneural network. The results show that there exists high correlation between\nsome of the structural features. An irredundant feature combination with\ninitial node features, which is filtered by graph neural network has improved\nits classification accuracy in some graph-based tasks. We compare differences\nbetween concatenation methods on connecting embeddings between features and\nshow that the simplest is the best. We generalize on the synthetic geometric\ngraphs and certify the results on prediction difficulty between structural\nfeatures.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-09-10T07:20:13.463Z",
          "wordCount": 640,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Aditya Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>",
          "description": "Efficient reasoning about the semantic, spatial, and temporal structure of a\nscene is a crucial prerequisite for autonomous driving. We present NEural\nATtention fields (NEAT), a novel representation that enables such reasoning for\nend-to-end imitation learning models. NEAT is a continuous function which maps\nlocations in Bird's Eye View (BEV) scene coordinates to waypoints and\nsemantics, using intermediate attention maps to iteratively compress\nhigh-dimensional 2D image features into a compact representation. This allows\nour model to selectively attend to relevant regions in the input while ignoring\ninformation irrelevant to the driving task, effectively associating the images\nwith the BEV representation. In a new evaluation setting involving adverse\nenvironmental conditions and challenging scenarios, NEAT outperforms several\nstrong baselines and achieves driving scores on par with the privileged CARLA\nexpert used to generate its training data. Furthermore, visualizing the\nattention maps for models with NEAT intermediate representations provides\nimproved interpretability.",
          "link": "http://arxiv.org/abs/2109.04456",
          "publishedOn": "2021-09-10T07:20:13.451Z",
          "wordCount": 605,
          "title": "NEAT: Neural Attention Fields for End-to-End Autonomous Driving. (arXiv:2109.04456v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>",
          "description": "Modeling complex physical dynamics is a fundamental task in science and\nengineering. Traditional physics-based models are sample efficient,\ninterpretable but often rely on rigid assumptions. Furthermore, direct\nnumerical approximation is usually computationally intensive, requiring\nsignificant computational resources and expertise. While deep learning (DL)\nprovides novel alternatives for efficiently recognizing complex patterns and\nemulating nonlinear dynamics, its predictions do not necessarily obey the\ngoverning laws of physical systems, nor do they generalize well across\ndifferent systems. Thus, the study of physics-guided DL emerged and has gained\ngreat progress. Physics-guided DL aims to take the best from both physics-based\nmodeling and state-of-the-art DL models to better solve scientific problems. In\nthis paper, we provide a structured overview of existing methodologies of\nintegrating prior physical knowledge or physics-based modeling into DL, with a\nspecial emphasis on learning dynamical systems. We also discuss the fundamental\nchallenges and emerging opportunities in the area.",
          "link": "http://arxiv.org/abs/2107.01272",
          "publishedOn": "2021-09-10T07:20:13.445Z",
          "wordCount": 616,
          "title": "Physics-Guided Deep Learning for Dynamical Systems: A Survey. (arXiv:2107.01272v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sunny_F/0/1/0/all/0/1\">Febin Sunny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikdast_M/0/1/0/all/0/1\">Mahdi Nikdast</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">Sudeep Pasricha</a>",
          "description": "Sparse neural networks can greatly facilitate the deployment of neural\nnetworks on resource-constrained platforms as they offer compact model sizes\nwhile retaining inference accuracy. Because of the sparsity in parameter\nmatrices, sparse neural networks can, in principle, be exploited in accelerator\narchitectures for improved energy-efficiency and latency. However, to realize\nthese improvements in practice, there is a need to explore sparsity-aware\nhardware-software co-design. In this paper, we propose a novel silicon\nphotonics-based sparse neural network inference accelerator called SONIC. Our\nexperimental analysis shows that SONIC can achieve up to 5.8x better\nperformance-per-watt and 8.4x lower energy-per-bit than state-of-the-art sparse\nelectronic neural network accelerators; and up to 13.8x better\nperformance-per-watt and 27.6x lower energy-per-bit than the best known\nphotonic neural network accelerators.",
          "link": "http://arxiv.org/abs/2109.04459",
          "publishedOn": "2021-09-10T07:20:13.438Z",
          "wordCount": 578,
          "title": "SONIC: A Sparse Neural Network Inference Accelerator with Silicon Photonics for Energy-Efficient Deep Learning. (arXiv:2109.04459v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.15010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sizhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhehao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Q/0/1/0/all/0/1\">Qinghua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolin Huang</a>",
          "description": "Deep Neural Networks (DNNs) are acknowledged as vulnerable to adversarial\nattacks, while the existing black-box attacks require extensive queries on the\nvictim DNN to achieve high success rates. For query-efficiency, surrogate\nmodels of the victim are adopted as transferable attackers in consideration of\ntheir Gradient Similarity (GS), i.e., surrogates' attack gradients are similar\nto the victim's ones to some extent. However, it is generally neglected to\nexploit their similarity on outputs, namely the Prediction Similarity (PS), to\nfilter out inefficient queries. To jointly utilize and also optimize\nsurrogates' GS and PS, we develop QueryNet, an efficient attack network that\ncan significantly reduce queries. QueryNet crafts several transferable\nAdversarial Examples (AEs) by surrogates, and then decides also by surrogates\non the most promising AE, which is then sent to query the victim. That is to\nsay, in QueryNet, surrogates are not only exploited as transferable attackers,\nbut also as transferability evaluators for AEs. The AEs are generated using\nsurrogates' GS and evaluated based on their PS, and therefore, the query\nresults could be back-propagated to optimize surrogates' parameters and also\ntheir architectures, enhancing both the GS and the PS. QueryNet has significant\nquery-efficiency, i.e., reduces queries by averagely about an order of\nmagnitude compared to recent SOTA methods according to our comprehensive and\nreal-world experiments: 11 victims (including 2 commercial models) on\nMNIST/CIFAR10/ImageNet, allowing only 8-bit image queries, and no access to the\nvictim's training data.",
          "link": "http://arxiv.org/abs/2105.15010",
          "publishedOn": "2021-09-10T07:20:13.422Z",
          "wordCount": 726,
          "title": "QueryNet: An Attack Framework with Surrogates Carrying Multiple Identities. (arXiv:2105.15010v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gholamian_S/0/1/0/all/0/1\">Sina Gholamian</a>",
          "description": "Software developers embed logging statements inside the source code as an\nimperative duty in modern software development as log files are necessary for\ntracking down runtime system issues and troubleshooting system management\ntasks. Prior research has emphasized the importance of logging statements in\nthe operation and debugging of software systems. However, the current logging\nprocess is mostly manual and ad hoc, and thus, proper placement and content of\nlogging statements remain as challenges. To overcome these challenges, methods\nthat aim to automate log placement and log content, i.e., 'where, what, and how\nto log', are of high interest. Thus, we propose to accomplish the goal of this\nresearch, that is \"to predict the log statements by utilizing source code\nclones and natural language processing (NLP)\", as these approaches provide\nadditional context and advantage for log prediction. We pursue the following\nfour research objectives: (RO1) investigate whether source code clones can be\nleveraged for log statement location prediction, (RO2) propose a clone-based\napproach for log statement prediction, (RO3) predict log statement's\ndescription with code-clone and NLP models, and (RO4) examine approaches to\nautomatically predict additional details of the log statement, such as its\nverbosity level and variables. For this purpose, we perform an experimental\nanalysis on seven open-source java projects, extract their method-level code\nclones, investigate their attributes, and utilize them for log location and\ndescription prediction. Our work demonstrates the effectiveness of log-aware\nclone detection for automated log location and description prediction and\noutperforms the prior work.",
          "link": "http://arxiv.org/abs/2109.03859",
          "publishedOn": "2021-09-10T07:20:13.394Z",
          "wordCount": 706,
          "title": "Leveraging Code Clones and Natural Language Processing for Log Statement Prediction. (arXiv:2109.03859v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chairatanakul_N/0/1/0/all/0/1\">Nuttapong Chairatanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriwatanasakdi_N/0/1/0/all/0/1\">Noppayut Sriwatanasakdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>",
          "description": "In cross-lingual text classification, it is required that task-specific\ntraining data in high-resource source languages are available, where the task\nis identical to that of a low-resource target language. However, collecting\nsuch training data can be infeasible because of the labeling cost, task\ncharacteristics, and privacy concerns. This paper proposes an alternative\nsolution that uses only task-independent word embeddings of high-resource\nlanguages and bilingual dictionaries. First, we construct a dictionary-based\nheterogeneous graph (DHG) from bilingual dictionaries. This opens the\npossibility to use graph neural networks for cross-lingual transfer. The\nremaining challenge is the heterogeneity of DHG because multiple languages are\nconsidered. To address this challenge, we propose dictionary-based\nheterogeneous graph neural network (DHGNet) that effectively handles the\nheterogeneity of DHG by two-step aggregations, which are word-level and\nlanguage-level aggregations. Experimental results demonstrate that our method\noutperforms pretrained models even though it does not access to large corpora.\nFurthermore, it can perform well even though dictionaries contain many\nincorrect translations. Its robustness allows the usage of a wider range of\ndictionaries such as an automatically constructed dictionary and crowdsourced\ndictionary, which are convenient for real-world applications.",
          "link": "http://arxiv.org/abs/2109.04400",
          "publishedOn": "2021-09-10T07:20:13.375Z",
          "wordCount": 644,
          "title": "Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>",
          "description": "We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.",
          "link": "http://arxiv.org/abs/2109.04282",
          "publishedOn": "2021-09-10T07:20:13.368Z",
          "wordCount": 575,
          "title": "Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hertweck_C/0/1/0/all/0/1\">Corinna Hertweck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raz_T/0/1/0/all/0/1\">Tim R&#xe4;z</a>",
          "description": "Impossibility results show that important fairness measures (independence,\nseparation, sufficiency) cannot be satisfied at the same time under reasonable\nassumptions. This paper explores whether we can satisfy and/or improve these\nfairness measures simultaneously to a certain degree. We introduce\ninformation-theoretic formulations of the fairness measures and define degrees\nof fairness based on these formulations. The information-theoretic formulations\nsuggest unexplored theoretical relations between the three fairness measures.\nIn the experimental part, we use the information-theoretic expressions as\nregularizers to obtain fairness-regularized predictors for three standard\ndatasets. Our experiments show that a) fairness regularization directly\nincreases fairness measures, in line with existing work, and b) some fairness\nregularizations indirectly increase other fairness measures, as suggested by\nour theoretical findings. This establishes that it is possible to increase the\ndegree to which some fairness measures are satisfied at the same time -- some\nfairness measures are gradually compatible.",
          "link": "http://arxiv.org/abs/2109.04399",
          "publishedOn": "2021-09-10T07:20:13.362Z",
          "wordCount": 582,
          "title": "Gradual (In)Compatibility of Fairness Criteria. (arXiv:2109.04399v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammar_K/0/1/0/all/0/1\">Kim Hammar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadler_R/0/1/0/all/0/1\">Rolf Stadler</a>",
          "description": "We study automated intrusion prevention using reinforcement learning. In a\nnovel approach, we formulate the problem of intrusion prevention as an optimal\nstopping problem. This formulation allows us insight into the structure of the\noptimal policies, which turn out to be threshold based. Since the computation\nof the optimal defender policy using dynamic programming is not feasible for\npractical cases, we approximate the optimal policy through reinforcement\nlearning in a simulation environment. To define the dynamics of the simulation,\nwe emulate the target infrastructure and collect measurements. Our evaluations\nshow that the learned policies are close to optimal and that they indeed can be\nexpressed using thresholds.",
          "link": "http://arxiv.org/abs/2106.07160",
          "publishedOn": "2021-09-10T07:20:13.354Z",
          "wordCount": 630,
          "title": "Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v7 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramnath_S/0/1/0/all/0/1\">Sahana Ramnath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1\">Melvin Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhirut Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1\">Aravindan Raghuveer</a>",
          "description": "Back-translation (BT) of target monolingual corpora is a widely used data\naugmentation strategy for neural machine translation (NMT), especially for\nlow-resource language pairs. To improve effectiveness of the available BT data,\nwe introduce HintedBT -- a family of techniques which provides hints (through\ntags) to the encoder and decoder. First, we propose a novel method of using\nboth high and low quality BT data by providing hints (as source tags on the\nencoder) to the model about the quality of each source-target pair. We don't\nfilter out low quality data but instead show that these hints enable the model\nto learn effectively from noisy data. Second, we address the problem of\npredicting whether a source token needs to be translated or transliterated to\nthe target language, which is common in cross-script translation tasks (i.e.,\nwhere source and target do not share the written script). For such cases, we\npropose training the model with additional hints (as target tags on the\ndecoder) that provide information about the operation required on the source\n(translation or both translation and transliteration). We conduct experiments\nand detailed analyses on standard WMT benchmarks for three cross-script\nlow/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our\nmethods compare favorably with five strong and well established baselines. We\nshow that using these hints, both separately and together, significantly\nimproves translation quality and leads to state-of-the-art performance in all\nthree language pairs in corresponding bilingual settings.",
          "link": "http://arxiv.org/abs/2109.04443",
          "publishedOn": "2021-09-10T07:20:13.336Z",
          "wordCount": 692,
          "title": "HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12210",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>",
          "description": "Ever since its debut, generative adversarial networks (GANs) have attracted\ntremendous amount of attention. Over the past years, different variations of\nGANs models have been developed and tailored to different applications in\npractice. Meanwhile, some issues regarding the performance and training of GANs\nhave been noticed and investigated from various theoretical perspectives. This\nsubchapter will start from an introduction of GANs from an analytical\nperspective, then move on to the training of GANs via SDE approximations and\nfinally discuss some applications of GANs in computing high dimensional MFGs as\nwell as tackling mathematical finance problems.",
          "link": "http://arxiv.org/abs/2104.12210",
          "publishedOn": "2021-09-10T07:20:13.252Z",
          "wordCount": 570,
          "title": "Generative Adversarial Network: Some Analytical Perspectives. (arXiv:2104.12210v2 [q-fin.MF] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_K/0/1/0/all/0/1\">Koushik Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sandeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Shilpak Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Ashish Kumar Pandey</a>",
          "description": "An activation function is a crucial component of a neural network that\nintroduces non-linearity in the network. The state-of-the-art performance of a\nneural network depends on the perfect choice of an activation function. We\npropose two novel non-monotonic smooth trainable activation functions, called\nErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve\nthe network performance significantly compared to the widely used activations\nlike ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have\n5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in\nCIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on\nPreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean\naverage precision (mAP) on SSD300 model in Pascal VOC dataset.",
          "link": "http://arxiv.org/abs/2109.04386",
          "publishedOn": "2021-09-10T07:20:13.232Z",
          "wordCount": 572,
          "title": "ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03857",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vos_D/0/1/0/all/0/1\">Dani&#xeb;l Vos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1\">Sicco Verwer</a>",
          "description": "Decision trees are a popular choice of explainable model, but just like\nneural networks, they suffer from adversarial examples. Existing algorithms for\nfitting decision trees robust against adversarial examples are greedy\nheuristics and lack approximation guarantees. In this paper we propose ROCT, a\ncollection of methods to train decision trees that are optimally robust against\nuser-specified attack models. We show that the min-max optimization problem\nthat arises in adversarial learning can be solved using a single minimization\nformulation for decision trees with 0-1 loss. We propose such formulations in\nMixed-Integer Linear Programming and Maximum Satisfiability, which widely\navailable solvers can optimize. We also present a method that determines the\nupper bound on adversarial accuracy for any model using bipartite matching. Our\nexperimental results demonstrate that the existing heuristics achieve close to\noptimal scores while ROCT achieves state-of-the-art scores.",
          "link": "http://arxiv.org/abs/2109.03857",
          "publishedOn": "2021-09-10T07:20:13.193Z",
          "wordCount": 574,
          "title": "Robust Optimal Classification Trees Against Adversarial Examples. (arXiv:2109.03857v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nicosia_M/0/1/0/all/0/1\">Massimo Nicosia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1\">Zhongdi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_Y/0/1/0/all/0/1\">Yasemin Altun</a>",
          "description": "While multilingual pretrained language models (LMs) fine-tuned on a single\nlanguage have shown substantial cross-lingual task transfer capabilities, there\nis still a wide performance gap in semantic parsing tasks when target language\nsupervision is available. In this paper, we propose a novel Translate-and-Fill\n(TaF) method to produce silver training data for a multilingual semantic\nparser. This method simplifies the popular Translate-Align-Project (TAP)\npipeline and consists of a sequence-to-sequence filler model that constructs a\nfull parse conditioned on an utterance and a view of the same parse. Our filler\nis trained on English data only but can accurately complete instances in other\nlanguages (i.e., translations of the English training utterances), in a\nzero-shot fashion. Experimental results on three multilingual semantic parsing\ndatasets show that data augmentation with TaF reaches accuracies competitive\nwith similar systems which rely on traditional alignment techniques.",
          "link": "http://arxiv.org/abs/2109.04319",
          "publishedOn": "2021-09-10T07:20:13.179Z",
          "wordCount": 598,
          "title": "Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pastor_Serrano_O/0/1/0/all/0/1\">Oscar Pastor-Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perko_Z/0/1/0/all/0/1\">Zolt&#xe1;n Perk&#xf3;</a>",
          "description": "Particle physics simulations are the cornerstone of nuclear engineering\napplications. Among them radiotherapy (RT) is crucial for society, with 50% of\ncancer patients receiving radiation treatments. For the most precise targeting\nof tumors, next generation RT treatments aim for real-time correction during\nradiation delivery, necessitating particle transport algorithms that yield\nprecise dose distributions in sub-second times even in highly heterogeneous\npatient geometries. This is infeasible with currently available, purely physics\nbased simulations. In this study, we present a data-driven dose calculation\nalgorithm predicting the dose deposited by mono-energetic proton beams for\narbitrary energies and patient geometries. Our approach frames particle\ntransport as sequence modeling, where convolutional layers extract important\nspatial features into tokens and the transformer self-attention mechanism\nroutes information between such tokens in the sequence and a beam energy token.\nWe train our network and evaluate prediction accuracy using computationally\nexpensive but accurate Monte Carlo (MC) simulations, considered the gold\nstandard in particle physics. Our proposed model is 33 times faster than\ncurrent clinical analytic pencil beam algorithms, improving upon their accuracy\nin the most heterogeneous and challenging geometries. With a relative error of\n0.34% and very high gamma pass rate of 99.59% (1%, 3 mm), it also greatly\noutperforms the only published similar data-driven proton dose algorithm, even\nat a finer grid resolution. Offering MC precision 400 times faster, our model\ncould overcome a major obstacle that has so far prohibited real-time adaptive\nproton treatments and significantly increase cancer treatment efficacy. Its\npotential to model physics interactions of other particles could also boost\nheavy ion treatment planning procedures limited by the speed of traditional\nmethods.",
          "link": "http://arxiv.org/abs/2109.03951",
          "publishedOn": "2021-09-10T07:20:13.069Z",
          "wordCount": 702,
          "title": "Learning the Physics of Particle Transport via Transformers. (arXiv:2109.03951v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Aviles_E/0/1/0/all/0/1\">Ernesto Diaz-Aviles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orellana_Rodriguez_C/0/1/0/all/0/1\">Claudia Orellana-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brigadir_I/0/1/0/all/0/1\">Igor Brigadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutty_R/0/1/0/all/0/1\">Reshma Narayanan Kutty</a>",
          "description": "Newsletters have (re-) emerged as a powerful tool for publishers to engage\nwith their readers directly and more effectively. Despite the diversity in\ntheir audiences, publishers' newsletters remain largely a one-size-fits-all\noffering, which is suboptimal. In this paper, we present NU:BRIEF, a web\napplication for publishers that enables them to personalize their newsletters\nwithout harvesting personal data. Personalized newsletters build a habit and\nbecome a great conversion tool for publishers, providing an alternative\nreaders-generated revenue model to a declining ad/clickbait-centered business\nmodel.",
          "link": "http://arxiv.org/abs/2109.03955",
          "publishedOn": "2021-09-10T07:20:13.034Z",
          "wordCount": 556,
          "title": "NU:BRIEF -- A Privacy-aware Newsletter Personalization Engine for Publishers. (arXiv:2109.03955v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03882",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Jena_S/0/1/0/all/0/1\">Sanjay Dominik Jena</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Lodi_A/0/1/0/all/0/1\">Andrea Lodi</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Sole_C/0/1/0/all/0/1\">Claudio Sole</a>",
          "description": "The Random Utility Maximization model is by far the most adopted framework to\nestimate consumer choice behavior. However, behavioral economics has provided\nstrong empirical evidence of irrational choice behavior, such as halo effects,\nthat are incompatible with this framework. Models belonging to the Random\nUtility Maximization family may therefore not accurately capture such\nirrational behavior. Hence, more general choice models, overcoming such\nlimitations, have been proposed. However, the flexibility of such models comes\nat the price of increased risk of overfitting. As such, estimating such models\nremains a challenge. In this work, we propose an estimation method for the\nrecently proposed Generalized Stochastic Preference choice model, which\nsubsumes the family of Random Utility Maximization models and is capable of\ncapturing halo effects. Specifically, we show how to use partially-ranked\npreferences to efficiently model rational and irrational customer types from\ntransaction data. Our estimation procedure is based on column generation, where\nrelevant customer types are efficiently extracted by expanding a tree-like data\nstructure containing the customer behaviors. Further, we propose a new\ndominance rule among customer types whose effect is to prioritize low orders of\ninteractions among products. An extensive set of experiments assesses the\npredictive accuracy of the proposed approach. Our results show that accounting\nfor irrational preferences can boost predictive accuracy by 12.5% on average,\nwhen tested on a real-world dataset from a large chain of grocery and drug\nstores.",
          "link": "http://arxiv.org/abs/2109.03882",
          "publishedOn": "2021-09-10T07:20:13.027Z",
          "wordCount": 685,
          "title": "On the estimation of discrete choice models to capture irrational customer behaviors. (arXiv:2109.03882v1 [econ.EM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">William Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1\">Mehdi Jafarnia-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rahul Jain</a>",
          "description": "We introduce a framework for decentralized online learning for multi-armed\nbandits (MAB) with multiple cooperative players. The reward obtained by the\nplayers in each round depends on the actions taken by all the players. It's a\nteam setting, and the objective is common. Information asymmetry is what makes\nthe problem interesting and challenging. We consider three types of information\nasymmetry: action information asymmetry when the actions of the players can't\nbe observed but the rewards received are common; reward information asymmetry\nwhen the actions of the other players are observable but rewards received are\nIID from the same distribution; and when we have both action and reward\ninformation asymmetry. For the first setting, we propose a UCB-inspired\nalgorithm that achieves $O(\\log T)$ regret whether the rewards are IID or\nMarkovian. For the second section, we offer an environment such that the\nalgorithm given for the first setting gives linear regret. For the third\nsetting, we show that a variation of the `explore then commit' algorithm\nachieves almost log regret.",
          "link": "http://arxiv.org/abs/2109.03818",
          "publishedOn": "2021-09-10T07:20:12.995Z",
          "wordCount": 603,
          "title": "Online Learning for Cooperative Multi-Player Multi-Armed Bandits. (arXiv:2109.03818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03891",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wentao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desingh_K/0/1/0/all/0/1\">Karthik Desingh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Sequential manipulation tasks require a robot to perceive the state of an\nenvironment and plan a sequence of actions leading to a desired goal state,\nwhere the ability to reason about spatial relationships among object entities\nfrom raw sensor inputs is crucial. Prior works relying on explicit state\nestimation or end-to-end learning struggle with novel objects. In this work, we\npropose SORNet (Spatial Object-Centric Representation Network), which extracts\nobject-centric representations from RGB images conditioned on canonical views\nof the objects of interest. We show that the object embeddings learned by\nSORNet generalize zero-shot to unseen object entities on three spatial\nreasoning tasks: spatial relationship classification, skill precondition\nclassification and relative direction regression, significantly outperforming\nbaselines. Further, we present real-world robotic experiments demonstrating the\nusage of the learned object embeddings in task planning for sequential\nmanipulation.",
          "link": "http://arxiv.org/abs/2109.03891",
          "publishedOn": "2021-09-10T07:20:12.989Z",
          "wordCount": 581,
          "title": "SORNet: Spatial Object-Centric Representations for Sequential Manipulation. (arXiv:2109.03891v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Desen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1\">Bo Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>",
          "description": "Reconstructing 3D object from a single image (RGB or depth) is a fundamental\nproblem in visual scene understanding and yet remains challenging due to its\nill-posed nature and complexity in real-world scenes. To address those\nchallenges, we adopt a primitive-based representation for 3D object, and\npropose a two-stage graph network for primitive-based 3D object estimation,\nwhich consists of a sequential proposal module and a graph reasoning module.\nGiven a 2D image, our proposal module first generates a sequence of 3D\nprimitives from input image with local feature attention. Then the graph\nreasoning module performs joint reasoning on a primitive graph to capture the\nglobal shape context for each primitive. Such a framework is capable of taking\ninto account rich geometry and semantic constraints during 3D structure\nrecovery, producing 3D objects with more coherent structure even under\nchallenging viewing conditions. We train the entire graph neural network in a\nstage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and\nNYU Depth V2. Extensive experiments show that our approach outperforms the\nprevious state of the arts with a considerable margin.",
          "link": "http://arxiv.org/abs/2109.04153",
          "publishedOn": "2021-09-10T07:20:12.976Z",
          "wordCount": 635,
          "title": "Single Image 3D Object Estimation with Primitive Graph Networks. (arXiv:2109.04153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.12697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "In recent years there has been a lot of focus on adversarial attacks,\nespecially on deep neural networks. Here, we argue that they are more general\nin nature and can easily affect a larger class of models, e.g., any\ndifferentiable perturbed optimizers. We further show that such attacks can be\ndetermined by the hidden confounders in a domain, thus drawing a novel\nconnection between such attacks and causality. Establishing this causal\nperspective is characterized by the influence of the structural causal model's\ndata generating process on the subsequent optimization thereby exhibiting\nintriguing parameters of the former. We reveal the existence of such parameters\nfor three combinatorial optimization problems, namely linear assignment,\nshortest path and a real world problem of energy systems. Our empirical\nexamination also unveils worrisome consequences of these attacks on\ndifferentiable perturbed optimizers thereby highlighting the criticality of our\nfindings.",
          "link": "http://arxiv.org/abs/2105.12697",
          "publishedOn": "2021-09-10T07:20:12.969Z",
          "wordCount": 636,
          "title": "Intriguing Parameters of Structural Causal Models. (arXiv:2105.12697v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04228",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Yuan_G/0/1/0/all/0/1\">Ganzhao Yuan</a>",
          "description": "Difference-of-Convex (DC) minimization, referring to the problem of\nminimizing the difference of two convex functions, has been found rich\napplications in statistical learning and studied extensively for decades.\nHowever, existing methods are primarily based on multi-stage convex relaxation,\nonly leading to weak optimality of critical points. This paper proposes a\ncoordinate descent method for minimizing DC functions based on sequential\nnonconvex approximation. Our approach iteratively solves a nonconvex\none-dimensional subproblem globally, and it is guaranteed to converge to a\ncoordinate-wise stationary point. We prove that this new optimality condition\nis always stronger than the critical point condition and the directional point\ncondition when the objective function is weakly convex. For comparisons, we\nalso include a naive variant of coordinate descent methods based on sequential\nconvex approximation in our study. When the objective function satisfies an\nadditional regularity condition called \\emph{sharpness}, coordinate descent\nmethods with an appropriate initialization converge \\emph{linearly} to the\noptimal solution set. Also, for many applications of interest, we show that the\nnonconvex one-dimensional subproblem can be computed exactly and efficiently\nusing a breakpoint searching method. We present some discussions and extensions\nof our proposed method. Finally, we have conducted extensive experiments on\nseveral statistical learning tasks to show the superiority of our approach.\n\nKeywords: Coordinate Descent, DC Minimization, DC Programming,\nDifference-of-Convex Programs, Nonconvex Optimization, Sparse Optimization,\nBinary Optimization.",
          "link": "http://arxiv.org/abs/2109.04228",
          "publishedOn": "2021-09-10T07:20:12.963Z",
          "wordCount": 655,
          "title": "Coordinate Descent Methods for DC Minimization. (arXiv:2109.04228v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04024",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mondal_W/0/1/0/all/0/1\">Washim Uddin Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mridul Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1\">Vaneet Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ukkusuri_S/0/1/0/all/0/1\">Satish V. Ukkusuri</a>",
          "description": "Mean field control (MFC) is an effective way to mitigate the curse of\ndimensionality of cooperative multi-agent reinforcement learning (MARL)\nproblems. This work considers a collection of $N_{\\mathrm{pop}}$ heterogeneous\nagents that can be segregated into $K$ classes such that the $k$-th class\ncontains $N_k$ homogeneous agents. We aim to prove approximation guarantees of\nthe MARL problem for this heterogeneous system by its corresponding MFC\nproblem. We consider three scenarios where the reward and transition dynamics\nof all agents are respectively taken to be functions of $(1)$ joint state and\naction distributions across all classes, $(2)$ individual distributions of each\nclass, and $(3)$ marginal distributions of the entire population. We show that,\nin these cases, the $K$-class MARL problem can be approximated by MFC with\nerrors given as\n$e_1=\\mathcal{O}(\\frac{\\sqrt{|\\mathcal{X}||\\mathcal{U}|}}{N_{\\mathrm{pop}}}\\sum_{k}\\sqrt{N_k})$,\n$e_2=\\mathcal{O}(\\sqrt{|\\mathcal{X}||\\mathcal{U}|}\\sum_{k}\\frac{1}{\\sqrt{N_k}})$\nand\n$e_3=\\mathcal{O}\\left(\\sqrt{|\\mathcal{X}||\\mathcal{U}|}\\left[\\frac{A}{N_{\\mathrm{pop}}}\\sum_{k\\in[K]}\\sqrt{N_k}+\\frac{B}{\\sqrt{N_{\\mathrm{pop}}}}\\right]\\right)$,\nrespectively, where $A, B$ are some constants and $|\\mathcal{X}|,|\\mathcal{U}|$\nare the sizes of state and action spaces of each agent. Finally, we design a\nNatural Policy Gradient (NPG) based algorithm that, in the three cases stated\nabove, can converge to an optimal MARL policy within $\\mathcal{O}(e_j)$ error\nwith a sample complexity of $\\mathcal{O}(e_j^{-3})$, $j\\in\\{1,2,3\\}$,\nrespectively.",
          "link": "http://arxiv.org/abs/2109.04024",
          "publishedOn": "2021-09-10T07:20:12.928Z",
          "wordCount": 660,
          "title": "On the Approximation of Cooperative Heterogeneous Multi-Agent Reinforcement Learning (MARL) using Mean Field Control (MFC). (arXiv:2109.04024v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.04558",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Passino_F/0/1/0/all/0/1\">Francesco Sanna Passino</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heard_N/0/1/0/all/0/1\">Nicholas A. Heard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rubin_Delanchy_P/0/1/0/all/0/1\">Patrick Rubin-Delanchy</a>",
          "description": "Spectral clustering is a popular method for community detection in network\ngraphs: starting from a matrix representation of the graph, the nodes are\nclustered on a low dimensional projection obtained from a truncated spectral\ndecomposition of the matrix. Estimating correctly the number of communities and\nthe dimension of the reduced latent space is critical for good performance of\nspectral clustering algorithms. Furthermore, many real-world graphs, such as\nenterprise computer networks studied in cyber-security applications, often\ndisplay heterogeneous within-community degree distributions. Such heterogeneous\ndegree distributions are usually not well captured by standard spectral\nclustering algorithms. In this article, a novel spectral clustering algorithm\nis proposed for community detection under the degree-corrected stochastic\nblockmodel. The proposed method is based on a transformation of the spectral\nembedding to spherical coordinates, and a novel modelling assumption in the\ntransformed space. The method allows for simultaneous and automated selection\nof the number of communities and the latent dimension for spectral embeddings\nof graphs with uneven node degrees. Results show improved performance over\ncompeting methods in representing computer networks.",
          "link": "http://arxiv.org/abs/2011.04558",
          "publishedOn": "2021-09-10T07:20:12.921Z",
          "wordCount": 647,
          "title": "Spectral clustering on spherical coordinates under the degree-corrected stochastic blockmodel. (arXiv:2011.04558v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04266",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakkelund_D/0/1/0/all/0/1\">Daniel Bakkelund</a>",
          "description": "We present an objective function for similarity based hierarchical clustering\nof partially ordered data that preserves the partial order in the sense that if\n$x \\leq y$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$,\nthen there is an order relation $\\leq'$ on the clusters for which $[x] \\leq'\n|y]$. The model distinguishes itself from existing methods and models for\nclustering of ordered data in that the order relation and the similarity are\ncombined to obtain an optimal hierarchical clustering seeking to satisfy both,\nand that the order relation is equipped with a pairwise level of comparability\nin the range $[0,1]$. In particular, if the similarity and the order relation\nare not aligned, then order preservation may have to yield in favor of\nclustering. Finding an optimal solution is NP-hard, so we provide a polynomial\ntime approximation algorithm, with a relative performance guarantee of\n$O(\\log^{3/2}n)$, based on successive applications of directed sparsest cut.\nThe model is an extension of the Dasgupta cost function for divisive\nhierarchical clustering.",
          "link": "http://arxiv.org/abs/2109.04266",
          "publishedOn": "2021-09-10T07:20:12.908Z",
          "wordCount": 623,
          "title": "An objective function for order preserving hierarchical clustering. (arXiv:2109.04266v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zecevic_M/0/1/0/all/0/1\">Matej Ze&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1\">Devendra Singh Dhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1\">Petar Veli&#x10d;kovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>",
          "description": "Causality can be described in terms of a structural causal model (SCM) that\ncarries information on the variables of interest and their mechanistic\nrelations. For most processes of interest the underlying SCM will only be\npartially observable, thus causal inference tries to leverage any exposed\ninformation. Graph neural networks (GNN) as universal approximators on\nstructured input pose a viable candidate for causal learning, suggesting a\ntighter integration with SCM. To this effect we present a theoretical analysis\nfrom first principles that establishes a novel connection between GNN and SCM\nwhile providing an extended view on general neural-causal models. We then\nestablish a new model class for GNN-based causal inference that is necessary\nand sufficient for causal effect identification. Our empirical illustration on\nsimulations and standard benchmarks validate our theoretical proofs.",
          "link": "http://arxiv.org/abs/2109.04173",
          "publishedOn": "2021-09-10T07:20:12.889Z",
          "wordCount": 593,
          "title": "Relating Graph Neural Networks to Structural Causal Models. (arXiv:2109.04173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zadaianchuk_A/0/1/0/all/0/1\">Andrii Zadaianchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martius_G/0/1/0/all/0/1\">Georg Martius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>",
          "description": "To successfully tackle challenging manipulation tasks, autonomous agents must\nlearn a diverse set of skills and how to combine them. Recently,\nself-supervised agents that set their own abstract goals by exploiting the\ndiscovered structure in the environment were shown to perform well on many\ndifferent tasks. In particular, some of them were applied to learn basic\nmanipulation skills in compositional multi-object environments. However, these\nmethods learn skills without taking the dependencies between objects into\naccount. Thus, the learned skills are difficult to combine in realistic\nenvironments. We propose a novel self-supervised agent that estimates relations\nbetween environment components and uses them to independently control different\nparts of the environment state. In addition, the estimated relations between\nobjects can be used to decompose a complex goal into a compatible sequence of\nsubgoals. We show that, by using this framework, an agent can efficiently and\nautomatically learn manipulation tasks in multi-object environments with\ndifferent relations between objects.",
          "link": "http://arxiv.org/abs/2109.04150",
          "publishedOn": "2021-09-10T07:20:12.882Z",
          "wordCount": 592,
          "title": "Self-supervised Reinforcement Learning with Independently Controllable Subgoals. (arXiv:2109.04150v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Zhaojing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Gang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kaiping Zheng</a>",
          "description": "Deep learning models usually require a large amount of labeled data to\nachieve satisfactory performance. In multimedia analysis, domain adaptation\nstudies the problem of cross-domain knowledge transfer from a label rich source\ndomain to a label scarce target domain, thus potentially alleviates the\nannotation requirement for deep learning models. However, we find that\ncontemporary domain adaptation methods for cross-domain image understanding\nperform poorly when source domain is noisy. Weakly Supervised Domain Adaptation\n(WSDA) studies the domain adaptation problem under the scenario where source\ndata can be noisy. Prior methods on WSDA remove noisy source data and align the\nmarginal distribution across domains without considering the fine-grained\nsemantic structure in the embedding space, which have the problem of class\nmisalignment, e.g., features of cats in the target domain might be mapped near\nfeatures of dogs in the source domain. In this paper, we propose a novel\nmethod, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we\nadopt the cluster assumption and learn cluster discriminatively with class\nprototypes in the embedding space. We propose to leverage the location\ninformation of the data points in the embedding space and model the location\ninformation with a Gaussian mixture model to identify noisy source data. We\nthen design a network which incorporates the Gaussian mixture noise model as a\nsub-module for unsupervised noise removal and propose a novel cluster-level\nadversarial adaptation method which aligns unlabeled target data with the less\nnoisy class prototypes for mapping the semantic structure across domains. We\nconduct extensive experiments to evaluate the effectiveness of our method on\nboth general images and medical images from COVID-19 and e-commerce datasets.\nThe results show that our method significantly outperforms state-of-the-art\nWSDA methods.",
          "link": "http://arxiv.org/abs/2109.04284",
          "publishedOn": "2021-09-10T07:20:12.875Z",
          "wordCount": 793,
          "title": "Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal. (arXiv:2109.04284v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08667",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yee_K/0/1/0/all/0/1\">Kyra Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tantipongpipat_U/0/1/0/all/0/1\">Uthaipon Tantipongpipat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Shubhanshu Mishra</a>",
          "description": "Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities because of an effect we term\nargmax bias. However, we demonstrate that formalized fairness metrics and\nquantitative analysis on their own are insufficient for capturing the risk of\nrepresentational harm in automatic cropping. We suggest the removal of\nsaliency-based cropping in favor of a solution that better preserves user\nagency. For developing a new solution that sufficiently address concerns\nrelated to representational harm, our critique motivates a combination of\nquantitative and qualitative methods that include human-centered design.",
          "link": "http://arxiv.org/abs/2105.08667",
          "publishedOn": "2021-09-10T07:20:12.867Z",
          "wordCount": 685,
          "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency. (arXiv:2105.08667v2 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Baoxiang Wang</a>",
          "description": "Motivated by the common strategic activities in crowdsourcing labeling, we\nstudy the problem of sequential eliciting information without verification\n(EIWV) for workers with a heterogeneous and unknown crowd. We propose a\nreinforcement learning-based approach that is effective against a wide range of\nsettings including potential irrationality and collusion among workers. With\nthe aid of a costly oracle and the inference method, our approach dynamically\ndecides the oracle calls and gains robustness even under the presence of\nfrequent collusion activities. Extensive experiments show the advantage of our\napproach. Our results also present the first comprehensive experiments of EIWV\non large-scale real datasets and the first thorough study of the effects of\nenvironmental variables.",
          "link": "http://arxiv.org/abs/2109.04226",
          "publishedOn": "2021-09-10T07:20:12.845Z",
          "wordCount": 540,
          "title": "Incentivizing an Unknown Crowd. (arXiv:2109.04226v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Di Zhang</a>",
          "description": "The proposition of lottery ticket hypothesis revealed the relationship\nbetween network structure and initialization parameters and the learning\npotential of neural networks. The original lottery ticket hypothesis performs\npruning and weight resetting after training convergence, exposing it to the\nproblem of forgotten learning knowledge and potential high cost of training.\nTherefore, we propose a strategy that combines the idea of neural network\nstructure search with a pruning algorithm to alleviate this problem. This\nalgorithm searches and extends the network structure on existing winning ticket\nsub-network to producing new winning ticket recursively. This allows the\ntraining and pruning process to continue without compromising performance. A\nnew winning ticket sub-network with deeper network structure, better\ngeneralization ability and better test performance can be obtained in this\nrecursive manner. This method can solve: the difficulty of training or\nperformance degradation of the sub-networks after pruning, the forgetting of\nthe weights of the original lottery ticket hypothesis and the difficulty of\ngenerating winning ticket sub-network when the final network structure is not\ngiven. We validate this strategy on the MNIST and CIFAR-10 datasets. And after\nrelating it to similar biological phenomena and relevant lottery ticket\nhypothesis studies in recent years, we will further propose a new hypothesis to\ndiscuss which factors that can keep a network juvenile, i.e., those possible\nfactors that influence the learning potential or generalization performance of\na neural network during training.",
          "link": "http://arxiv.org/abs/2109.03862",
          "publishedOn": "2021-09-10T07:20:12.827Z",
          "wordCount": 681,
          "title": "Juvenile state hypothesis: What we can learn from lottery ticket hypothesis researches?. (arXiv:2109.03862v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Chulhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Akhil Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acer_U/0/1/0/all/0/1\">Utku Gunay Acer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montanari_A/0/1/0/all/0/1\">Alessandro Montanari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1\">Fahim Kawsar</a>",
          "description": "We present SensiX++ - a multi-tenant runtime for adaptive model execution\nwith integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT\nsensors. SensiX++ operates on two fundamental principles - highly modular\ncomponentisation to externalise data operations with clear abstractions and\ndocument-centric manifestation for system-wide orchestration. First, a data\ncoordinator manages the lifecycle of sensors and serves models with correct\ndata through automated transformations. Next, a resource-aware model server\nexecutes multiple models in isolation through model abstraction, pipeline\nautomation and feature sharing. An adaptive scheduler then orchestrates the\nbest-effort executions of multiple models across heterogeneous accelerators,\nbalancing latency and throughput. Finally, microservices with REST APIs serve\nsynthesised model predictions, system statistics, and continuous deployment.\nCollectively, these components enable SensiX++ to serve multiple models\nefficiently with fine-grained control on edge devices while minimising data\noperation redundancy, managing data and device heterogeneity, reducing resource\ncontention and removing manual MLOps. We benchmark SensiX++ with ten different\nvision and acoustics models across various multi-tenant configurations on\ndifferent edge accelerators (Jetson AGX and Coral TPU) designed for sensory\ndevices. We report on the overall throughput and quantified benefits of various\nautomation components of SensiX++ and demonstrate its efficacy to significantly\nreduce operational complexity and lower the effort to deploy, upgrade,\nreconfigure and serve embedded models on edge devices.",
          "link": "http://arxiv.org/abs/2109.03947",
          "publishedOn": "2021-09-10T07:20:12.814Z",
          "wordCount": 667,
          "title": "SensiX++: Bringing MLOPs and Multi-tenant Model Serving to Sensory Edge Devices. (arXiv:2109.03947v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04114",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>",
          "description": "We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.",
          "link": "http://arxiv.org/abs/2109.04114",
          "publishedOn": "2021-09-10T07:20:12.776Z",
          "wordCount": 489,
          "title": "Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kexuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szekely_P/0/1/0/all/0/1\">Pedro Szekely</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>",
          "description": "Tables provide valuable knowledge that can be used to verify textual\nstatements. While a number of works have considered table-based fact\nverification, direct alignments of tabular data with tokens in textual\nstatements are rarely available. Moreover, training a generalized fact\nverification model requires abundant labeled training data. In this paper, we\npropose a novel system to address these problems. Inspired by counterfactual\ncausality, our system identifies token-level salience in the statement with\nprobing-based salience estimation. Salience estimation allows enhanced learning\nof fact verification from two perspectives. From one perspective, our system\nconducts masked salient token prediction to enhance the model for alignment and\nreasoning between the table and the statement. From the other perspective, our\nsystem applies salience-aware data augmentation to generate a more diverse set\nof training instances by replacing non-salient terms. Experimental results on\nTabFact show the effective improvement by the proposed salience-aware learning\ntechniques, leading to the new SOTA performance on the benchmark. Our code is\npublicly available at https://github.com/luka-group/Salience-aware-Learning .",
          "link": "http://arxiv.org/abs/2109.04053",
          "publishedOn": "2021-09-10T07:20:12.770Z",
          "wordCount": 616,
          "title": "Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haohai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jialun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yunpu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Temporal knowledge graph (TKG) reasoning is a crucial task that has gained\nincreasing research interest in recent years. Most existing methods focus on\nreasoning at past timestamps to complete the missing facts, and there are only\na few works of reasoning on known TKGs to forecast future facts. Compared with\nthe completion task, the forecasting task is more difficult that faces two main\nchallenges: (1) how to effectively model the time information to handle future\ntimestamps? (2) how to make inductive inference to handle previously unseen\nentities that emerge over time? To address these challenges, we propose the\nfirst reinforcement learning method for forecasting. Specifically, the agent\ntravels on historical knowledge graph snapshots to search for the answer. Our\nmethod defines a relative time encoding function to capture the timespan\ninformation, and we design a novel time-shaped reward based on Dirichlet\ndistribution to guide the model learning. Furthermore, we propose a novel\nrepresentation method for unseen entities to improve the inductive inference\nability of the model. We evaluate our method for this link prediction task at\nfuture timestamps. Extensive experiments on four benchmark datasets demonstrate\nsubstantial performance improvement meanwhile with higher explainability, less\ncalculation, and fewer parameters when compared with existing state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2109.04101",
          "publishedOn": "2021-09-10T07:20:12.761Z",
          "wordCount": 656,
          "title": "TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bera_D/0/1/0/all/0/1\">Debajyoti Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_R/0/1/0/all/0/1\">Rameshwar Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1\">Bhisham Dev Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1\">Biswadeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Representation learning using network embedding has received tremendous\nattention due to its efficacy to solve downstream tasks. Popular embedding\nmethods (such as deepwalk, node2vec, LINE) are based on a neural architecture,\nthus unable to scale on large networks both in terms of time and space usage.\nRecently, we proposed BinSketch, a sketching technique for compressing binary\nvectors to binary vectors. In this paper, we show how to extend BinSketch and\nuse it for network hashing. Our proposal named QUINT is built upon BinSketch,\nand it embeds nodes of a sparse network onto a low-dimensional space using\nsimple bi-wise operations. QUINT is the first of its kind that provides\ntremendous gain in terms of speed and space usage without compromising much on\nthe accuracy of the downstream tasks. Extensive experiments are conducted to\ncompare QUINT with seven state-of-the-art network embedding methods for two end\ntasks - link prediction and node classification. We observe huge performance\ngain for QUINT in terms of speedup (up to 7000x) and space saving (up to 800x)\ndue to its bit-wise nature to obtain node embedding. Moreover, QUINT is a\nconsistent top-performer for both the tasks among the baselines across all the\ndatasets. Our empirical observations are backed by rigorous theoretical\nanalysis to justify the effectiveness of QUINT. In particular, we prove that\nQUINT retains enough structural information which can be used further to\napproximate many topological properties of networks with high confidence.",
          "link": "http://arxiv.org/abs/2109.04206",
          "publishedOn": "2021-09-10T07:20:12.739Z",
          "wordCount": 690,
          "title": "QUINT: Node embedding using network hashing. (arXiv:2109.04206v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Han-Dong Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jihoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_O/0/1/0/all/0/1\">Okyong Choi</a>",
          "description": "Sutton, Szepesv\\'{a}ri and Maei introduced the first gradient\ntemporal-difference (GTD) learning algorithms compatible with both linear\nfunction approximation and off-policy training. The goal of this paper is (a)\nto propose some variants of GTDs with extensive comparative analysis and (b) to\nestablish new theoretical analysis frameworks for the GTDs. These variants are\nbased on convex-concave saddle-point interpretations of GTDs, which effectively\nunify all the GTDs into a single framework, and provide simple stability\nanalysis based on recent results on primal-dual gradient dynamics. Finally,\nnumerical comparative analysis is given to evaluate these approaches.",
          "link": "http://arxiv.org/abs/2109.04033",
          "publishedOn": "2021-09-10T07:20:12.731Z",
          "wordCount": 528,
          "title": "Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03856",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hanze Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Dinghao Wu</a>",
          "description": "Data augmentation has been widely used in image data and linguistic data but\nremains under-explored on graph-structured data. Existing methods focus on\naugmenting the graph data from a global perspective and largely fall into two\ngenres: structural manipulation and adversarial training with feature noise\ninjection. However, the structural manipulation approach suffers information\nloss issues while the adversarial training approach may downgrade the feature\nquality by injecting noise. In this work, we introduce the local augmentation,\nwhich enhances node features by its local subgraph structures. Specifically, we\nmodel the data argumentation as a feature generation process. Given the central\nnode's feature, our local augmentation approach learns the conditional\ndistribution of its neighbors' features and generates the neighbors' optimal\nfeature to boost the performance of downstream tasks. Based on the local\naugmentation, we further design a novel framework: LA-GNN, which can apply to\nany GNN models in a plug-and-play manner. Extensive experiments and analyses\nshow that local augmentation consistently yields performance improvement for\nvarious GNN architectures across a diverse set of benchmarks. Code is available\nat https://github.com/Soughing0823/LAGNN.",
          "link": "http://arxiv.org/abs/2109.03856",
          "publishedOn": "2021-09-10T07:20:12.628Z",
          "wordCount": 625,
          "title": "Local Augmentation for Graph Neural Networks. (arXiv:2109.03856v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03839",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongyuan Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Molei Tao</a>",
          "description": "Sampling algorithms based on discretizations of Stochastic Differential\nEquations (SDEs) compose a rich and popular subset of MCMC methods. This work\nprovides a general framework for the non-asymptotic analysis of sampling error\nin 2-Wasserstein distance, which also leads to a bound of mixing time. The\nmethod applies to any consistent discretization of contractive SDEs. When\napplied to Langevin Monte Carlo algorithm, it establishes\n$\\tilde{\\mathcal{O}}\\left( \\frac{\\sqrt{d}}{\\epsilon} \\right)$ mixing time,\nwithout warm start, under the common log-smooth and log-strongly-convex\nconditions, plus a growth condition on the 3rd-order derivative of the\npotential of target measures at infinity. This bound improves the best\npreviously known $\\tilde{\\mathcal{O}}\\left( \\frac{d}{\\epsilon} \\right)$ result\nand is optimal (in terms of order) in both dimension $d$ and accuracy tolerance\n$\\epsilon$ for target measures satisfying the aforementioned assumptions. Our\ntheoretical analysis is further validated by numerical experiments.",
          "link": "http://arxiv.org/abs/2109.03839",
          "publishedOn": "2021-09-10T07:20:12.602Z",
          "wordCount": 614,
          "title": "Mean-Square Analysis with An Application to Optimal Dimension Dependence of Langevin Monte Carlo. (arXiv:2109.03839v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Ting-Han Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_X/0/1/0/all/0/1\">Xian Yeow Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yubo Wang</a>",
          "description": "We introduce PowerGym, an open-source reinforcement learning environment for\nVolt-Var control in power distribution systems. Following OpenAI Gym APIs,\nPowerGym targets minimizing power loss and voltage violations under physical\nnetworked constraints. PowerGym provides four distribution systems (13Bus,\n34Bus, 123Bus, and 8500Node) based on IEEE benchmark systems and design\nvariants for various control difficulties. To foster generalization, PowerGym\noffers a detailed customization guide for users working with their distribution\nsystems. As a demonstration, we examine state-of-the-art reinforcement learning\nalgorithms in PowerGym and validate the environment by studying controller\nbehaviors.",
          "link": "http://arxiv.org/abs/2109.03970",
          "publishedOn": "2021-09-10T07:20:12.580Z",
          "wordCount": 538,
          "title": "PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems. (arXiv:2109.03970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03992",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Gu_Y/0/1/0/all/0/1\">Yiqi Gu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Harlim_J/0/1/0/all/0/1\">John Harlim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liang_S/0/1/0/all/0/1\">Senwei Liang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yang_H/0/1/0/all/0/1\">Haizhao Yang</a>",
          "description": "In this paper, we consider the density estimation problem associated with the\nstationary measure of ergodic It\\^o diffusions from a discrete-time series that\napproximate the solutions of the stochastic differential equations. To take an\nadvantage of the characterization of density function through the stationary\nsolution of a parabolic-type Fokker-Planck PDE, we proceed as follows. First,\nwe employ deep neural networks to approximate the drift and diffusion terms of\nthe SDE by solving appropriate supervised learning tasks. Subsequently, we\nsolve a steady-state Fokker-Plank equation associated with the estimated drift\nand diffusion coefficients with a neural-network-based least-squares method. We\nestablish the convergence of the proposed scheme under appropriate mathematical\nassumptions, accounting for the generalization errors induced by regressing the\ndrift and diffusion coefficients, and the PDE solvers. This theoretical study\nrelies on a recent perturbation theory of Markov chain result that shows a\nlinear dependence of the density estimation to the error in estimating the\ndrift term, and generalization error results of nonparametric regression and of\nPDE regression solution obtained with neural-network models. The effectiveness\nof this method is reflected by numerical simulations of a two-dimensional\nStudent's t distribution and a 20-dimensional Langevin dynamics.",
          "link": "http://arxiv.org/abs/2109.03992",
          "publishedOn": "2021-09-10T07:20:12.572Z",
          "wordCount": 642,
          "title": "Stationary Density Estimation of It\\^o Diffusions Using Deep Learning. (arXiv:2109.03992v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salahirad_A/0/1/0/all/0/1\">Alireza Salahirad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gay_G/0/1/0/all/0/1\">Gregory Gay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_E/0/1/0/all/0/1\">Ehsan Mohammadi</a>",
          "description": "In this study, we apply co-word analysis - a text mining technique based on\nthe co-occurrence of terms - to map the topology of software testing research\ntopics, with the goal of providing current and prospective researchers with a\nmap, and observations about the evolution, of the software testing field. Our\nanalysis enables the mapping of software testing research into clusters of\nconnected topics, from which emerge a total of 16 high-level research themes\nand a further 18 subthemes. This map also suggests topics that are growing in\nimportance, including topics related to web and mobile applications and\nartificial intelligence. Exploration of author and country-based collaboration\npatterns offers similar insight into the implicit and explicit factors that\ninfluence collaboration and suggests emerging sources of collaboration for\nfuture work. We make our observations - and the underlying mapping of research\ntopics and research collaborations - available so that researchers can gain a\ndeeper understanding of the topology of the software testing field, inspiration\nregarding new areas and connections to explore, and collaborators who will\nbroaden their perspectives.",
          "link": "http://arxiv.org/abs/2109.04086",
          "publishedOn": "2021-09-10T07:20:12.551Z",
          "wordCount": 631,
          "title": "Mapping Research Topics in Software Testing: A Bibliometric Analysis. (arXiv:2109.04086v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03900",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Goldman_S/0/1/0/all/0/1\">Samuel Goldman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Das_R/0/1/0/all/0/1\">Ria Das</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_K/0/1/0/all/0/1\">Kevin K. Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Coley_C/0/1/0/all/0/1\">Connor W. Coley</a>",
          "description": "Biocatalysis is a promising approach to sustainably synthesize\npharmaceuticals, complex natural products, and commodity chemicals at scale.\nHowever, the adoption of biocatalysis is limited by our ability to select\nenzymes that will catalyze their natural chemical transformation on non-natural\nsubstrates. While machine learning and in silico directed evolution are\nwell-posed for this predictive modeling challenge, efforts to date have\nprimarily aimed to increase activity against a single known substrate, rather\nthan to identify enzymes capable of acting on new substrates of interest. To\naddress this need, we curate 6 different high-quality enzyme family screens\nfrom the literature that each measure multiple enzymes against multiple\nsubstrates. We compare machine learning-based compound-protein interaction\n(CPI) modeling approaches from the literature used for predicting drug-target\ninteractions. Surprisingly, comparing these interaction-based models against\ncollections of independent (single task) enzyme-only or substrate-only models\nreveals that current CPI approaches are incapable of learning interactions\nbetween compounds and proteins in the current family level data regime. We\nfurther validate this observation by demonstrating that our no-interaction\nbaseline can outperform CPI-based models from the literature used to guide the\ndiscovery of kinase inhibitors. Given the high performance of non-interaction\nbased models, we introduce a new structure-based strategy for pooling residue\nrepresentations across a protein sequence. Altogether, this work motivates a\nprincipled path forward in order to build and evaluate meaningful predictive\nmodels for biocatalysis and other drug discovery applications.",
          "link": "http://arxiv.org/abs/2109.03900",
          "publishedOn": "2021-09-10T07:20:12.545Z",
          "wordCount": 676,
          "title": "Machine learning modeling of family wide enzyme-substrate specificity screens. (arXiv:2109.03900v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhuofu Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>",
          "description": "We investigate the use of multimodal information contained in images as an\neffective method for enhancing the commonsense of Transformer models for text\ngeneration. We perform experiments using BART and T5 on concept-to-text\ngeneration, specifically the task of generative commonsense reasoning, or\nCommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text\nGeneration. VisCTG involves captioning images representing appropriate everyday\nscenarios, and using these captions to enrich and steer the generation process.\nComprehensive evaluation and analysis demonstrate that VisCTG noticeably\nimproves model performance while successfully addressing several issues of the\nbaseline generations, including poor commonsense, fluency, and specificity.",
          "link": "http://arxiv.org/abs/2109.03892",
          "publishedOn": "2021-09-10T07:20:12.532Z",
          "wordCount": 562,
          "title": "Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>",
          "description": "While recent research on natural language inference has considerably\nbenefited from large annotated datasets, the amount of inference-related\nknowledge (including commonsense) provided in the annotated data is still\nrather limited. There have been two lines of approaches that can be used to\nfurther address the limitation: (1) unsupervised pretraining can leverage\nknowledge in much larger unstructured text data; (2) structured (often\nhuman-curated) knowledge has started to be considered in neural-network-based\nmodels for NLI. An immediate question is whether these two approaches\ncomplement each other, or how to develop models that can bring together their\nadvantages. In this paper, we propose models that leverage structured knowledge\nin different components of pre-trained models. Our results show that the\nproposed models perform better than previous BERT-based state-of-the-art\nmodels. Although our models are proposed for NLI, they can be easily extended\nto other sentence or sentence-pair classification problems.",
          "link": "http://arxiv.org/abs/2109.03941",
          "publishedOn": "2021-09-10T07:20:12.525Z",
          "wordCount": 596,
          "title": "Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Ye Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>",
          "description": "Improving Transformer efficiency has become increasingly attractive recently.\nA wide range of methods has been proposed, e.g., pruning, quantization, new\narchitectures and etc. But these methods are either sophisticated in\nimplementation or dependent on hardware. In this paper, we show that the\nefficiency of Transformer can be improved by combining some simple and\nhardware-agnostic methods, including tuning hyper-parameters, better design\nchoices and training strategies. On the WMT news translation tasks, we improve\nthe inference efficiency of a strong Transformer system by 3.80X on CPU and\n2.52X on GPU. The code is publicly available at\nhttps://github.com/Lollipop321/mini-decoder-network.",
          "link": "http://arxiv.org/abs/2109.04030",
          "publishedOn": "2021-09-10T07:20:12.496Z",
          "wordCount": 540,
          "title": "Bag of Tricks for Optimizing Transformer Efficiency. (arXiv:2109.04030v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04007",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Hu_J/0/1/0/all/0/1\">Jianjun Hu</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Stefanov_S/0/1/0/all/0/1\">Stanislav Stefanov</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Song_Y/0/1/0/all/0/1\">Yuqi Song</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Omee_S/0/1/0/all/0/1\">Sadman Sadeed Omee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Louis_S/0/1/0/all/0/1\">Steph-Yves Louis</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Siriwardane_E/0/1/0/all/0/1\">Edirisuriya M. D. Siriwardane</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zhao_Y/0/1/0/all/0/1\">Yong Zhao</a>",
          "description": "The availability and easy access of large scale experimental and\ncomputational materials data have enabled the emergence of accelerated\ndevelopment of algorithms and models for materials property prediction,\nstructure prediction, and generative design of materials. However, lack of\nuser-friendly materials informatics web servers has severely constrained the\nwide adoption of such tools in the daily practice of materials screening,\ntinkering, and design space exploration by materials scientists. Herein we\nfirst survey current materials informatics web apps and then propose and\ndevelop MaterialsAtlas.org, a web based materials informatics toolbox for\nmaterials discovery, which includes a variety of routinely needed tools for\nexploratory materials discovery, including materials composition and structure\ncheck (e.g. for neutrality, electronegativity balance, dynamic stability,\nPauling rules), materials property prediction (e.g. band gap, elastic moduli,\nhardness, thermal conductivity), and search for hypothetical materials. These\nuser-friendly tools can be freely accessed at \\url{www.materialsatlas.org}. We\nargue that such materials informatics apps should be widely developed by the\ncommunity to speed up the materials discovery processes.",
          "link": "http://arxiv.org/abs/2109.04007",
          "publishedOn": "2021-09-10T07:20:12.484Z",
          "wordCount": 635,
          "title": "MaterialsAtlas.org: A Materials Informatics Web App Platform for Materials Discovery and Survey of State-of-the-Art. (arXiv:2109.04007v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04010",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Koo_J/0/1/0/all/0/1\">John Koo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tang_M/0/1/0/all/0/1\">Minh Tang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Trosset_M/0/1/0/all/0/1\">Michael W. Trosset</a>",
          "description": "We connect two random graph models, the Popularity Adjusted Block Model\n(PABM) and the Generalized Random Dot Product Graph (GRDPG), by demonstrating\nthat the PABM is a special case of the GRDPG in which communities correspond to\nmutually orthogonal subspaces of latent vectors. This insight allows us to\nconstruct new algorithms for community detection and parameter estimation for\nthe PABM, as well as improve an existing algorithm that relies on Sparse\nSubspace Clustering. Using established asymptotic properties of Adjacency\nSpectral Embedding for the GRDPG, we derive asymptotic properties of these\nalgorithms. In particular, we demonstrate that the absolute number of community\ndetection errors tends to zero as the number of graph vertices tends to\ninfinity. Simulation experiments illustrate these properties.",
          "link": "http://arxiv.org/abs/2109.04010",
          "publishedOn": "2021-09-10T07:20:12.464Z",
          "wordCount": 571,
          "title": "Popularity Adjusted Block Models are Generalized Random Dot Product Graphs. (arXiv:2109.04010v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Docena_A/0/1/0/all/0/1\">Amel Nestor Docena</a> (Northeastern University), <a href=\"http://arxiv.org/find/cs/1/au:+Wahl_T/0/1/0/all/0/1\">Thomas Wahl</a> (Northeastern University), <a href=\"http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1\">Trevor Pearce</a> (Northeastern University), <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yunsi Fei</a> (Northeastern University)",
          "description": "Neural Networks are used today in numerous security- and safety-relevant\ndomains and are, as such, a popular target of attacks that subvert their\nclassification capabilities, by manipulating the network parameters. Prior work\nhas introduced sensitive samples -- inputs highly sensitive to parameter\nchanges -- to detect such manipulations, and proposed a gradient ascent-based\napproach to compute them. In this paper we offer an alternative, using symbolic\nconstraint solvers. We model the network and a formal specification of a\nsensitive sample in the language of the solver and ask for a solution. This\napproach supports a rich class of queries, corresponding, for instance, to the\npresence of certain types of attacks. Unlike earlier techniques, our approach\ndoes not depend on convex search domains, or on the suitability of a starting\npoint for the search. We address the performance limitations of constraint\nsolvers by partitioning the search space for the solver, and exploring the\npartitions according to a balanced schedule that still retains completeness of\nthe search. We demonstrate the impact of the use of solvers in terms of\nfunctionality and search efficiency, using a case study for the detection of\nTrojan attacks on Neural Networks.",
          "link": "http://arxiv.org/abs/2109.03966",
          "publishedOn": "2021-09-10T07:20:12.444Z",
          "wordCount": 667,
          "title": "Sensitive Samples Revisited: Detecting Neural Network Attacks Using Constraint Solvers. (arXiv:2109.03966v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiyang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Juan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongtao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Conventional unsupervised domain adaptation (UDA) methods need to access both\nlabeled source samples and unlabeled target samples simultaneously to train the\nmodel. While in some scenarios, the source samples are not available for the\ntarget domain due to data privacy and safety. To overcome this challenge,\nrecently, source-free domain adaptation (SFDA) has attracted the attention of\nresearchers, where both a trained source model and unlabeled target samples are\ngiven. Existing SFDA methods either adopt a pseudo-label based strategy or\ngenerate more samples. However, these methods do not explicitly reduce the\ndistribution shift across domains, which is the key to a good adaptation.\nAlthough there are no source samples available, fortunately, we find that some\ntarget samples are very similar to the source domain and can be used to\napproximate the source domain. This approximated domain is denoted as the\npseudo-source domain. In this paper, inspired by this observation, we propose a\nnovel method based on the pseudo-source domain. The proposed method firstly\ngenerates and augments the pseudo-source domain, and then employs distribution\nalignment with four novel losses based on pseudo-label based strategy. Among\nthem, a domain adversarial loss is introduced between the pseudo-source domain\nthe remaining target domain to reduce the distribution shift. The results on\nthree real-world datasets verify the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2109.04015",
          "publishedOn": "2021-09-10T07:20:12.437Z",
          "wordCount": 673,
          "title": "Generation, augmentation, and alignment: A pseudo-source domain based method for source-free domain adaptation. (arXiv:2109.04015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>",
          "description": "Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.",
          "link": "http://arxiv.org/abs/2109.03848",
          "publishedOn": "2021-09-10T07:20:12.426Z",
          "wordCount": 656,
          "title": "Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maiya_A/0/1/0/all/0/1\">Anirudh Maiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sricharan_I/0/1/0/all/0/1\">Inumella Sricharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1\">Anshuman Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_S/0/1/0/all/0/1\">Srinivas K. S</a>",
          "description": "The success of deep learning can be attributed to various factors such as\nincrease in computational power, large datasets, deep convolutional neural\nnetworks, optimizers etc. Particularly, the choice of optimizer affects the\ngeneralization, convergence rate, and training stability. Stochastic Gradient\nDescent (SGD) is a first order iterative optimizer that updates the gradient\nuniformly for all parameters. This uniform update may not be suitable across\nthe entire training phase. A rudimentary solution for this is to employ a\nfine-tuned learning rate scheduler which decreases learning rate as a function\nof iteration. To eliminate the dependency of learning rate schedulers, adaptive\ngradient optimizers such as AdaGrad, AdaDelta, RMSProp, Adam employ a\nparameter-wise scaling term for learning rate which is a function of the\ngradient itself. We propose Tom (Trend over Momentum) optimizer, which is a\nnovel variant of Adam that takes into account of the trend which is observed\nfor the gradients in the loss landscape traversed by the neural network. In the\nproposed Tom optimizer, an additional smoothing equation is introduced to\naddress the trend observed during the process of optimization. The smoothing\nparameter introduced for the trend requires no tuning and can be used with\ndefault values. Experimental results for classification datasets such as\nCIFAR-10, CIFAR-100 and CINIC-10 image datasets show that Tom outperforms\nAdagrad, Adadelta, RMSProp and Adam in terms of both accuracy and has a faster\nconvergence. The source code is publicly made available at\nhttps://github.com/AnirudhMaiya/Tom",
          "link": "http://arxiv.org/abs/2109.03820",
          "publishedOn": "2021-09-10T07:20:12.410Z",
          "wordCount": 684,
          "title": "Tom: Leveraging trend of the observed gradients for faster convergence. (arXiv:2109.03820v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03890",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vignesh Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1\">Yair Zick</a>",
          "description": "Explaining the decisions of black-box models has been a central theme in the\nstudy of trustworthy ML. Numerous measures have been proposed in the\nliterature; however, none of them have been able to adopt a provably causal\ntake on explainability. Building upon Halpern and Pearl's formal definition of\na causal explanation, we derive an analogous set of axioms for the\nclassification setting, and use them to derive three explanation measures. Our\nfirst measure is a natural adaptation of Chockler and Halpern's notion of\ncausal responsibility, whereas the other two correspond to existing\ngame-theoretic influence measures. We present an axiomatic treatment for our\nproposed indices, showing that they can be uniquely characterized by a set of\ndesirable properties. We compliment this with computational analysis, providing\nprobabilistic approximation schemes for all of our proposed measures. Thus, our\nwork is the first to formally bridge the gap between model explanations,\ngame-theoretic influence, and causal analysis.",
          "link": "http://arxiv.org/abs/2109.03890",
          "publishedOn": "2021-09-10T07:20:12.375Z",
          "wordCount": 586,
          "title": "Model Explanations via the Axiomatic Causal Lens. (arXiv:2109.03890v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chunting Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1\">Daniel Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1\">Marjan Ghazvininejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "Multilingual neural machine translation (MNMT) learns to translate multiple\nlanguage pairs with a single model, potentially improving both the accuracy and\nthe memory-efficiency of deployed models. However, the heavy data imbalance\nbetween languages hinders the model from performing uniformly across language\npairs. In this paper, we propose a new learning objective for MNMT based on\ndistributionally robust optimization, which minimizes the worst-case expected\nloss over the set of language pairs. We further show how to practically\noptimize this objective for large translation corpora using an iterated best\nresponse scheme, which is both effective and incurs negligible additional\ncomputational cost compared to standard empirical risk minimization. We perform\nextensive experiments on three sets of languages from two datasets and show\nthat our method consistently outperforms strong baseline methods in terms of\naverage and per-language performance under both many-to-one and one-to-many\ntranslation settings.",
          "link": "http://arxiv.org/abs/2109.04020",
          "publishedOn": "2021-09-10T07:20:12.362Z",
          "wordCount": 594,
          "title": "Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Arshiya Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotton_C/0/1/0/all/0/1\">Chase Cotton</a>",
          "description": "The generalization of deep learning has helped us, in the past, address\nchallenges such as malware identification and anomaly detection in the network\nsecurity domain. However, as effective as it is, scarcity of memory and\nprocessing power makes it difficult to perform these tasks in Internet of\nThings (IoT) devices. This research finds an easy way out of this bottleneck by\ndepreciating the need for feature engineering and subsequent processing in\nmachine learning techniques. In this study, we introduce a Featureless machine\nlearning process to perform anomaly detection. It uses unprocessed byte streams\nof packets as training data. Featureless machine learning enables a low cost\nand low memory time-series analysis of network traffic. It benefits from\neliminating the significant investment in subject matter experts and the time\nrequired for feature engineering.",
          "link": "http://arxiv.org/abs/2109.03989",
          "publishedOn": "2021-09-10T07:20:12.355Z",
          "wordCount": 587,
          "title": "Detecting Attacks on IoT Devices using Featureless 1D-CNN. (arXiv:2109.03989v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03866",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Marcondes_D/0/1/0/all/0/1\">Diego Marcondes</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simonis_A/0/1/0/all/0/1\">Adilson Simonis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrera_J/0/1/0/all/0/1\">Junior Barrera</a>",
          "description": "This paper proposes a data-driven systematic, consistent and non-exhaustive\napproach to Model Selection, that is an extension of the classical agnostic PAC\nlearning model. In this approach, learning problems are modeled not only by a\nhypothesis space $\\mathcal{H}$, but also by a Learning Space\n$\\mathbb{L}(\\mathcal{H})$, a poset of subspaces of $\\mathcal{H}$, which covers\n$\\mathcal{H}$ and satisfies a property regarding the VC dimension of related\nsubspaces, that is a suitable algebraic search space for Model Selection\nalgorithms. Our main contributions are a data-driven general learning algorithm\nto perform regularized Model Selection on $\\mathbb{L}(\\mathcal{H})$ and a\nframework under which one can, theoretically, better estimate a target\nhypothesis with a given sample size by properly modeling\n$\\mathbb{L}(\\mathcal{H})$ and employing high computational power. A remarkable\nconsequence of this approach are conditions under which a non-exhaustive search\nof $\\mathbb{L}(\\mathcal{H})$ can return an optimal solution. The results of\nthis paper lead to a practical property of Machine Learning, that the lack of\nexperimental data may be mitigated by a high computational capacity. In a\ncontext of continuous popularization of computational power, this property may\nhelp understand why Machine Learning has become so important, even where data\nis expensive and hard to get.",
          "link": "http://arxiv.org/abs/2109.03866",
          "publishedOn": "2021-09-10T07:20:12.069Z",
          "wordCount": 668,
          "title": "Learning the hypotheses space from data through a U-curve algorithm: a statistically consistent complexity regularizer for Model Selection. (arXiv:2109.03866v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03956",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Karra_S/0/1/0/all/0/1\">Satish Karra</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ahmmed_B/0/1/0/all/0/1\">Bulbul Ahmmed</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mudunuru_M/0/1/0/all/0/1\">Maruti K. Mudunuru</a>",
          "description": "Physics-informed Machine Learning has recently become attractive for learning\nphysical parameters and features from simulation and observation data. However,\nmost existing methods do not ensure that the physics, such as balance laws\n(e.g., mass, momentum, energy conservation), are constrained. Some recent works\n(e.g., physics-informed neural networks) softly enforce physics constraints by\nincluding partial differential equation (PDE)-based loss functions but need\nre-discretization of the PDEs using auto-differentiation. Training these neural\nnets on observational data showed that one could solve forward and inverse\nproblems in one shot. They evaluate the state variables and the parameters in a\nPDE. This re-discretization of PDEs is not necessarily an attractive option for\ndomain scientists that work with physics-based codes that have been developed\nfor decades with sophisticated discretization techniques to solve complex\nprocess models and advanced equations of state. This paper proposes a physics\nconstrained machine learning framework, AdjointNet, allowing domain scientists\nto embed their physics code in neural network training workflows. This\nembedding ensures that physics is constrained everywhere in the domain.\nAdditionally, the mathematical properties such as consistency, stability, and\nconvergence vital to the numerical solution of a PDE are still satisfied. We\nshow that the proposed AdjointNet framework can be used for parameter\nestimation (and uncertainty quantification by extension) and experimental\ndesign using active learning. The applicability of our framework is\ndemonstrated for four flow cases. Results show that AdjointNet-based inversion\ncan estimate process model parameters with reasonable accuracy. These examples\ndemonstrate the applicability of using existing software with no changes in\nsource code to perform accurate and reliable inversion of model parameters.",
          "link": "http://arxiv.org/abs/2109.03956",
          "publishedOn": "2021-09-10T07:20:11.914Z",
          "wordCount": 712,
          "title": "AdjointNet: Constraining machine learning models with physics-based codes. (arXiv:2109.03956v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03874",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hafshejani_S/0/1/0/all/0/1\">Sajad Fathi Hafshejani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Moaberfard_Z/0/1/0/all/0/1\">Zahra Moaberfard</a>",
          "description": "Non-negative matrix factorization (NMF) has become a popular method for\nrepresenting meaningful data by extracting a non-negative basis feature from an\nobserved non-negative data matrix. Some of the unique features of this method\nin identifying hidden data put this method amongst the powerful methods in the\nmachine learning area. The NMF is a known non-convex optimization problem and\nthe initial point has a significant effect on finding an efficient local\nsolution. In this paper, we investigate the most popular initialization\nprocedures proposed for NMF so far. We describe each method and present some of\ntheir advantages and disadvantages. Finally, some numerical results to\nillustrate the performance of each algorithm are presented.",
          "link": "http://arxiv.org/abs/2109.03874",
          "publishedOn": "2021-09-10T07:20:11.861Z",
          "wordCount": 554,
          "title": "Initialization for Nonnegative Matrix Factorization: a Comprehensive Review. (arXiv:2109.03874v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yilong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "We study Comparative Preference Classification (CPC) which aims at predicting\nwhether a preference comparison exists between two entities in a given sentence\nand, if so, which entity is preferred over the other. High-quality CPC models\ncan significantly benefit applications such as comparative question answering\nand review-based recommendations. Among the existing approaches, non-deep\nlearning methods suffer from inferior performances. The state-of-the-art graph\nneural network-based ED-GAT (Ma et al., 2020) only considers syntactic\ninformation while ignoring the critical semantic relations and the sentiments\nto the compared entities. We proposed sentiment Analysis Enhanced COmparative\nNetwork (SAECON) which improves CPC ac-curacy with a sentiment analyzer that\nlearns sentiments to individual entities via domain adaptive knowledge\ntransfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset\npresent a significant improvement on the F1 scores over the best existing CPC\napproaches.",
          "link": "http://arxiv.org/abs/2109.03819",
          "publishedOn": "2021-09-10T07:20:11.776Z",
          "wordCount": 591,
          "title": "Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03902",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Taghavi_L/0/1/0/all/0/1\">Leila Taghavi</a>",
          "description": "In the oracle identification problem we have oracle access to bits of an\nunknown string $x$ of length $n$, with the promise that it belongs to a known\nset $C\\subseteq\\{0,1\\}^n$. The goal is to identify $x$ using as few queries to\nthe oracle as possible. We develop a quantum query algorithm for this problem\nwith query complexity $O\\left(\\sqrt{\\frac{n\\log M }{\\log(n/\\log M)+1}}\\right)$,\nwhere $M$ is the size of $C$. This bound is already derived by Kothari in 2014,\nfor which we provide a more elegant simpler proof.",
          "link": "http://arxiv.org/abs/2109.03902",
          "publishedOn": "2021-09-10T07:20:11.638Z",
          "wordCount": 530,
          "title": "Simplified Quantum Algorithm for the Oracle Identification Problem. (arXiv:2109.03902v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "Markov Chain Monte Carlo (MCMC) methods are promising solutions to sample\nfrom target distributions in high dimensions. While MCMC methods enjoy nice\ntheoretical properties, like guaranteed convergence and mixing to the true\ntarget, in practice their sampling efficiency depends on the choice of the\nproposal distribution and the target at hand. This work considers using machine\nlearning to adapt the proposal distribution to the target, in order to improve\nthe sampling efficiency in the purely discrete domain. Specifically, (i) it\nproposes a new parametrization for a family of proposal distributions, called\nlocally balanced proposals, (ii) it defines an objective function based on\nmutual information and (iii) it devises a learning procedure to adapt the\nparameters of the proposal to the target, thus achieving fast convergence and\nfast mixing. We call the resulting sampler as the Locally Self-Balancing\nSampler (LSB). We show through experimental analysis on the Ising model and\nBayesian networks that LSB is indeed able to improve the efficiency over a\nstate-of-the-art sampler based on locally balanced proposals, thus reducing the\nnumber of iterations required to converge, while achieving comparable mixing\nperformance.",
          "link": "http://arxiv.org/abs/2109.03867",
          "publishedOn": "2021-09-10T07:20:11.614Z",
          "wordCount": 622,
          "title": "LSB: Local Self-Balancing MCMC in Discrete Spaces. (arXiv:2109.03867v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2103.14017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Image translation methods typically aim to manipulate a set of labeled\nattributes (given as supervision at training time e.g. domain label) while\nleaving the unlabeled attributes intact. Current methods achieve either: (i)\ndisentanglement, which exhibits low visual fidelity and can only be satisfied\nwhere the attributes are perfectly uncorrelated. (ii) visually-plausible\ntranslations, which are clearly not disentangled. In this work, we propose\nOverLORD, a single framework for disentangling labeled and unlabeled attributes\nas well as synthesizing high-fidelity images, which is composed of two stages;\n(i) Disentanglement: Learning disentangled representations with latent\noptimization. Differently from previous approaches, we do not rely on\nadversarial training or any architectural biases. (ii) Synthesis: Training\nfeed-forward encoders for inferring the learned attributes and tuning the\ngenerator in an adversarial manner to increase the perceptual quality. When the\nlabeled and unlabeled attributes are correlated, we model an additional\nrepresentation that accounts for the correlated attributes and improves\ndisentanglement. We highlight that our flexible framework covers multiple\nsettings as disentangling labeled attributes, pose and appearance, localized\nconcepts, and shape and texture. We present significantly better\ndisentanglement with higher translation quality and greater output diversity\nthan state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2103.14017",
          "publishedOn": "2021-09-09T07:20:44.892Z",
          "wordCount": 669,
          "title": "Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02377",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>",
          "description": "A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.",
          "link": "http://arxiv.org/abs/2109.02377",
          "publishedOn": "2021-09-09T07:20:44.875Z",
          "wordCount": 626,
          "title": "PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.14756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montana_M/0/1/0/all/0/1\">Michael Montana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannampallil_T/0/1/0/all/0/1\">Thomas Kannampallil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chenyang Lu</a>",
          "description": "We present an end-to-end model using streaming physiological time series to\naccurately predict near-term risk for hypoxemia, a rare, but life-threatening\ncondition known to cause serious patient harm during surgery. Our proposed\nmodel makes inference on both hypoxemia outcomes and future input sequences,\nenabled by a joint sequence autoencoder that simultaneously optimizes a\ndiscriminative decoder for label prediction, and two auxiliary decoders trained\nfor data reconstruction and forecast, which seamlessly learns future-indicative\nlatent representation. All decoders share a memory-based encoder that helps\ncapture the global dynamics of patient data. In a large surgical cohort of\n73,536 surgeries at a major academic medical center, our model outperforms all\nbaselines and gives a large performance gain over the state-of-the-art\nhypoxemia prediction system. With a high sensitivity cutoff at 80%, it presents\n99.36% precision in predicting hypoxemia and 86.81% precision in predicting the\nmuch more severe and rare hypoxemic condition, persistent hypoxemia. With\nexceptionally low rate of false alarms, our proposed model is promising in\nimproving clinical decision making and easing burden on the health system.",
          "link": "http://arxiv.org/abs/2104.14756",
          "publishedOn": "2021-09-09T07:20:44.821Z",
          "wordCount": 678,
          "title": "Predicting Intraoperative Hypoxemia with Joint Sequence Autoencoder Networks. (arXiv:2104.14756v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>",
          "description": "Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.",
          "link": "http://arxiv.org/abs/2104.01027",
          "publishedOn": "2021-09-09T07:20:44.814Z",
          "wordCount": 674,
          "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lubing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>",
          "description": "Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for the panoptic tracking problem and propose a novel\ninstance-centric metric that addresses the concerns. We present extensive\nexperiments that demonstrate the utility of Panoptic nuScenes compared to\nexisting datasets and make the online evaluation server available at\n\\url{nuScenes.org}. We believe that this extension will accelerate the research\nof novel methods for scene understanding of dynamic urban environments.",
          "link": "http://arxiv.org/abs/2109.03805",
          "publishedOn": "2021-09-09T07:20:44.801Z",
          "wordCount": 680,
          "title": "Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sencun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>",
          "description": "Current app ranking and recommendation systems are mainly based on\nuser-generated information, e.g., number of downloads and ratings. However, new\napps often have few (or even no) user feedback, suffering from the classic\ncold-start problem. How to quickly identify and then recommend new apps of high\nquality is a challenging issue. Here, a fundamental requirement is the\ncapability to accurately measure an app's quality based on its inborn features,\nrather than user-generated features. Since users obtain first-hand experience\nof an app by interacting with its views, we speculate that the inborn features\nare largely related to the visual quality of individual views in an app and the\nways the views switch to one another. In this work, we propose AppQ, a novel\napp quality grading and recommendation system that extracts inborn features of\napps based on app source code. In particular, AppQ works in parallel to perform\ncode analysis to extract app-level features as well as dynamic analysis to\ncapture view-level layout hierarchy and the switching among views. Each app is\nthen expressed as an attributed view graph, which is converted into a vector\nand fed to classifiers for recognizing its quality classes. Our evaluation with\nan app dataset from Google Play reports that AppQ achieves the best performance\nwith accuracy of 85.0\\%. This shows a lot of promise to warm-start app grading\nand recommendation systems with AppQ.",
          "link": "http://arxiv.org/abs/2109.03798",
          "publishedOn": "2021-09-09T07:20:44.764Z",
          "wordCount": 687,
          "title": "AppQ: Warm-starting App Recommendation Based on View Graphs. (arXiv:2109.03798v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2003.07040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changhun Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>",
          "description": "Incorporating graph side information into recommender systems has been widely\nused to better predict ratings, but relatively few works have focused on\ntheoretical guarantees. Ahn et al. (2018) firstly characterized the optimal\nsample complexity in the presence of graph side information, but the results\nare limited due to strict, unrealistic assumptions made on the unknown latent\npreference matrix and the structure of user clusters. In this work, we propose\na new model in which 1) the unknown latent preference matrix can have any\ndiscrete values, and 2) users can be clustered into multiple clusters, thereby\nrelaxing the assumptions made in prior work. Under this new model, we fully\ncharacterize the optimal sample complexity and develop a\ncomputationally-efficient algorithm that matches the optimal sample complexity.\nOur algorithm is robust to model errors and outperforms the existing algorithms\nin terms of prediction performance on both synthetic and real data.",
          "link": "http://arxiv.org/abs/2003.07040",
          "publishedOn": "2021-09-09T07:20:44.396Z",
          "wordCount": null,
          "title": "Discrete-Valued Latent Preference Matrix Estimation with Graph Side Information. (arXiv:2003.07040v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.13183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Ziyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiangtao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>",
          "description": "Graph Convolutional Network (GCN) has achieved extraordinary success in\nlearning effective task-specific representations of nodes in graphs. However,\nregarding Heterogeneous Information Network (HIN), existing HIN-oriented GCN\nmethods still suffer from two deficiencies: (1) they cannot flexibly explore\nall possible meta-paths and extract the most useful ones for a target object,\nwhich hinders both effectiveness and interpretability; (2) they often need to\ngenerate intermediate meta-path based dense graphs, which leads to high\ncomputational complexity. To address the above issues, we propose an\ninterpretable and efficient Heterogeneous Graph Convolutional Network (ie-HGCN)\nto learn the representations of objects in HINs. It is designed as a\nhierarchical aggregation architecture, i.e., object-level aggregation first,\nfollowed by type-level aggregation. The novel architecture can automatically\nextract useful meta-paths for each object from all possible meta-paths (within\na length limit), which brings good model interpretability. It can also reduce\nthe computational cost by avoiding intermediate HIN transformation and\nneighborhood attention. We provide theoretical analysis about the proposed\nie-HGCN in terms of evaluating the usefulness of all possible meta-paths, its\nconnection to the spectral graph convolution on HINs, and its quasi-linear time\ncomplexity. Extensive experiments on three real network datasets demonstrate\nthe superiority of ie-HGCN over the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2005.13183",
          "publishedOn": "2021-09-09T07:20:44.394Z",
          "wordCount": null,
          "title": "Interpretable and Efficient Heterogeneous Graph Convolutional Network. (arXiv:2005.13183v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06664",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jiangmiao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Linlu Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>",
          "description": "Similarity learning has been recognized as a crucial step for object\ntracking. However, existing multiple object tracking methods only use sparse\nground truth matching as the training objective, while ignoring the majority of\nthe informative regions on the images. In this paper, we present Quasi-Dense\nSimilarity Learning, which densely samples hundreds of region proposals on a\npair of images for contrastive learning. We can directly combine this\nsimilarity learning with existing detection methods to build Quasi-Dense\nTracking (QDTrack) without turning to displacement regression or motion priors.\nWe also find that the resulting distinctive feature space admits a simple\nnearest neighbor search at the inference time. Despite its simplicity, QDTrack\noutperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking\nbenchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external\ntraining data. Compared to methods with similar detectors, it boosts almost 10\npoints of MOTA and significantly decreases the number of ID switches on BDD100K\nand Waymo datasets. Our code and trained models are available at\nthis http URL",
          "link": "http://arxiv.org/abs/2006.06664",
          "publishedOn": "2021-09-09T07:20:44.393Z",
          "wordCount": 687,
          "title": "Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03709",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mate_B/0/1/0/all/0/1\">B&#xe1;lint M&#xe1;t&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleuret_F/0/1/0/all/0/1\">Fran&#xe7;ois Fleuret</a>",
          "description": "We introduce primed-PCA (pPCA), an extension of the recently proposed\nEigenGame algorithm for computing principal components in a large-scale setup.\nOur algorithm first runs EigenGame to get an approximation of the principal\ncomponents, and then applies an exact PCA in the subspace they span. Since this\nsubspace is of small dimension in any practical use of EigenGame, this second\nstep is extremely cheap computationally. Nonetheless, it improves accuracy\nsignificantly for a given computational budget across datasets. In this setup,\nthe purpose of EigenGame is to narrow down the search space, and prepare the\ndata for the second step, an exact calculation.\n\nWe show formally that pPCA improves upon EigenGame under very mild\nconditions, and we provide experimental validation on both synthetic and real\nlarge-scale datasets showing that it systematically translates to improved\nperformance. In our experiments we achieve improvements in convergence speed by\nfactors of 5-25 on the datasets of the original EigenGame paper.",
          "link": "http://arxiv.org/abs/2109.03709",
          "publishedOn": "2021-09-09T07:20:44.373Z",
          "wordCount": 591,
          "title": "Priming PCA with EigenGame. (arXiv:2109.03709v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.10820",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ji_K/0/1/0/all/0/1\">Kaiyi Ji</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zavlanos_M/0/1/0/all/0/1\">Michael M. Zavlanos</a>",
          "description": "Zeroth-order optimization (ZO) algorithms have been recently used to solve\nblack-box or simulation-based learning and control problems, where the gradient\nof the objective function cannot be easily computed but can be approximated\nusing the objective function values. Many existing ZO algorithms adopt\ntwo-point feedback schemes due to their fast convergence rate compared to\none-point feedback schemes. However, two-point schemes require two evaluations\nof the objective function at each iteration, which can be impractical in\napplications where the data are not all available a priori, e.g., in online\noptimization. In this paper, we propose a novel one-point feedback scheme that\nqueries the function value once at each iteration and estimates the gradient\nusing the residual between two consecutive points. When optimizing a\ndeterministic Lipschitz function, we show that the query complexity of ZO with\nthe proposed one-point residual feedback matches that of ZO with the existing\ntwo-point schemes. Moreover, the query complexity of the proposed algorithm can\nbe improved when the objective function has Lipschitz gradient. Then, for\nstochastic bandit optimization problems where only noisy objective function\nvalues are given, we show that ZO with one-point residual feedback achieves the\nsame convergence rate as that of two-point scheme with uncontrollable data\nsamples. We demonstrate the effectiveness of the proposed one-point residual\nfeedback via extensive numerical experiments.",
          "link": "http://arxiv.org/abs/2006.10820",
          "publishedOn": "2021-09-09T07:20:44.364Z",
          "wordCount": 692,
          "title": "A New One-Point Residual-Feedback Oracle For Black-Box Learning and Control. (arXiv:2006.10820v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>",
          "description": "In this work, we present an alternative to conventional residual connections,\nwhich is inspired by maxout nets. This means that instead of the addition in\nresidual connections, our approach only propagates the maximum value or, in the\nleaky formulation, propagates a percentage of both. In our evaluation, we show\non different public data sets that the presented approaches are comparable to\nthe residual connections and have other interesting properties, such as better\ngeneralization with a constant batch normalization, faster learning, and also\nthe possibility to generalize without additional activation functions. In\naddition, the proposed approaches work very well if ensembles together with\nresidual networks are formed.\n\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FMaximumPropagation&mode=list",
          "link": "http://arxiv.org/abs/2105.10277",
          "publishedOn": "2021-09-09T07:20:44.356Z",
          "wordCount": 567,
          "title": "Maximum and Leaky Maximum Propagation. (arXiv:2105.10277v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merkurjev_E/0/1/0/all/0/1\">Ekaterina Merkurjev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc DUy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">Guo-Wei Wei</a>",
          "description": "Machine learning methods have greatly changed science, engineering, finance,\nbusiness, and other fields. Despite the tremendous accomplishments of machine\nlearning and deep learning methods, many challenges still remain. In\nparticular, the performance of machine learning methods is often severely\naffected in case of diverse data, usually associated with smaller data sets or\ndata related to areas of study where the size of the data sets is constrained\nby the complexity and/or high cost of experiments. Moreover, data with limited\nlabeled samples is a challenge to most learning approaches. In this paper, the\naforementioned challenges are addressed by integrating graph-based frameworks,\nmultiscale structure, modified and adapted optimization procedures and\nsemi-supervised techniques. This results in two innovative multiscale Laplacian\nlearning (MLL) approaches for machine learning tasks, such as data\nclassification, and for tackling diverse data, data with limited samples and\nsmaller data sets. The first approach, called multikernel manifold learning\n(MML), integrates manifold learning with multikernel information and solves a\nregularization problem consisting of a loss function and a warped kernel\nregularizer using multiscale graph Laplacians. The second approach, called the\nmultiscale MBO (MMBO) method, introduces multiscale Laplacians to a\nmodification of the famous classical Merriman-Bence-Osher (MBO) scheme, and\nmakes use of fast solvers for finding the approximations to the extremal\neigenvectors of the graph Laplacian. We demonstrate the performance of our\nmethods experimentally on a variety of data sets, such as biological, text and\nimage data, and compare them favorably to existing approaches.",
          "link": "http://arxiv.org/abs/2109.03718",
          "publishedOn": "2021-09-09T07:20:44.341Z",
          "wordCount": 674,
          "title": "Multiscale Laplacian Learning. (arXiv:2109.03718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13847",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>",
          "description": "Word embeddings are a powerful natural lan-guage processing technique, but\nthey are ex-tremely difficult to interpret. To enable inter-pretable NLP\nmodels, we create vectors whereeach dimension isinherently interpretable.\nByinherently interpretable, we mean a systemwhere each dimension is associated\nwith somehuman-understandablehintthat can describethe meaning of that\ndimension. In order tocreate more interpretable word embeddings,we transform\npretrained dense word embed-dings into sparse embeddings. These new em-beddings\nare inherently interpretable: each oftheir dimensions is created from and\nrepre-sents a natural language word or specific gram-matical concept. We\nconstruct these embed-dings through sparse coding, where each vec-tor in the\nbasis set is itself a word embedding.Therefore, each dimension of our sparse\nvec-tors corresponds to a natural language word.We also show that models\ntrained using thesesparse embeddings can achieve good perfor-mance and are more\ninterpretable in practice,including through human evaluations.",
          "link": "http://arxiv.org/abs/2004.13847",
          "publishedOn": "2021-09-09T07:20:44.335Z",
          "wordCount": null,
          "title": "Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Common acquisition functions for active learning use either uncertainty or\ndiversity sampling, aiming to select difficult and diverse data points from the\npool of unlabeled data, respectively. In this work, leveraging the best of both\nworlds, we propose an acquisition function that opts for selecting\n\\textit{contrastive examples}, i.e. data points that are similar in the model\nfeature space and yet the model outputs maximally different predictive\nlikelihoods. We compare our approach, CAL (Contrastive Active Learning), with a\ndiverse set of acquisition functions in four natural language understanding\ntasks and seven datasets. Our experiments show that CAL performs consistently\nbetter or equal than the best performing baseline across all tasks, on both\nin-domain and out-of-domain data. We also conduct an extensive ablation study\nof our method and we further analyze all actively acquired datasets showing\nthat CAL achieves a better trade-off between uncertainty and diversity compared\nto other strategies.",
          "link": "http://arxiv.org/abs/2109.03764",
          "publishedOn": "2021-09-09T07:20:44.333Z",
          "wordCount": 602,
          "title": "Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.04678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Psarras_C/0/1/0/all/0/1\">Christos Psarras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_L/0/1/0/all/0/1\">Lars Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bro_R/0/1/0/all/0/1\">Rasmus Bro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bientinesi_P/0/1/0/all/0/1\">Paolo Bientinesi</a>",
          "description": "Tensor decompositions, such as CANDECOMP/PARAFAC (CP), are widely used in a\nvariety of applications, such as chemometrics, signal processing, and machine\nlearning. A broadly used method for computing such decompositions relies on the\nAlternating Least Squares (ALS) algorithm. When the number of components is\nsmall, regardless of its implementation, ALS exhibits low arithmetic intensity,\nwhich severely hinders its performance and makes GPU offloading ineffective. We\nobserve that, in practice, experts often have to compute multiple\ndecompositions of the same tensor, each with a small number of components\n(typically fewer than 20), to ultimately find the best ones to use for the\napplication at hand. In this paper, we illustrate how multiple decompositions\nof the same tensor can be fused together at the algorithmic level to increase\nthe arithmetic intensity. Therefore, it becomes possible to make efficient use\nof GPUs for further speedups; at the same time the technique is compatible with\nmany enhancements typically used in ALS, such as line search, extrapolation,\nand non-negativity constraints. We introduce the Concurrent ALS algorithm and\nlibrary, which offers an interface to Matlab, and a mechanism to effectively\ndeal with the issue that decompositions complete at different times.\nExperimental results on artificial and real datasets demonstrate a shorter time\nto completion due to increased arithmetic intensity.",
          "link": "http://arxiv.org/abs/2010.04678",
          "publishedOn": "2021-09-09T07:20:44.327Z",
          "wordCount": 698,
          "title": "Concurrent Alternating Least Squares for multiple simultaneous Canonical Polyadic Decompositions. (arXiv:2010.04678v2 [cs.MS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chaoyang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_K/0/1/0/all/0/1\">Keshav Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceyani_E/0/1/0/all/0/1\">Emir Ceyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Han Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lifang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liangwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yu Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junzhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1\">Murali Annavaram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1\">Salman Avestimehr</a>",
          "description": "Graph Neural Network (GNN) research is rapidly growing thanks to the capacity\nof GNNs in learning distributed representations from graph-structured data.\nHowever, centralizing a massive amount of real-world graph data for GNN\ntraining is prohibitive due to privacy concerns, regulation restrictions, and\ncommercial competitions. Federated learning (FL), a trending distributed\nlearning paradigm, provides possibilities to solve this challenge while\npreserving data privacy. Despite recent advances in vision and language\ndomains, there is no suitable platform for the FL of GNNs. To this end, we\nintroduce FedGraphNN, an open FL benchmark system that can facilitate research\non federated GNNs. FedGraphNN is built on a unified formulation of graph FL and\ncontains a wide range of datasets from different domains, popular GNN models,\nand FL algorithms, with secure and efficient system support. Particularly for\nthe datasets, we collect, preprocess, and partition 36 datasets from 7 domains,\nincluding both publicly available ones and specifically obtained ones such as\nhERG and Tencent. Our empirical analysis showcases the utility of our benchmark\nsystem, while exposing significant challenges in graph FL: federated GNNs\nperform worse in most datasets with a non-IID split than centralized GNNs; the\nGNN model that attains the best result in the centralized setting may not\nmaintain its advantage in the FL setting. These results imply that more\nresearch efforts are needed to unravel the mystery behind federated GNNs.\nMoreover, our system performance analysis demonstrates that the FedGraphNN\nsystem is computationally efficient and secure to large-scale graphs datasets.\nWe maintain the source code at https://github.com/FedML-AI/FedGraphNN.",
          "link": "http://arxiv.org/abs/2104.07145",
          "publishedOn": "2021-09-09T07:20:44.320Z",
          "wordCount": 799,
          "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks. (arXiv:2104.07145v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14933",
          "author": "<a href=\"http://arxiv.org/find/hep-ph/1/au:+Lebese_T/0/1/0/all/0/1\">Thabang Lebese</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Mellado_B/0/1/0/all/0/1\">Bruce Mellado</a>, <a href=\"http://arxiv.org/find/hep-ph/1/au:+Ruan_X/0/1/0/all/0/1\">Xifeng Ruan</a>",
          "description": "Semi-supervision in Machine Learning can be used in searches for new physics\nwhere the signal plus background regions are not labelled. This strongly\nreduces model dependency in the search for signals Beyond the Standard Model.\nThis approach displays the drawback in that over-fitting can give rise to fake\nsignals. Tossing toy Monte Carlo (MC) events can be used to estimate the\ncorresponding trials factor through a frequentist inference. However, MC events\nthat are based on full detector simulations are resource intensive. Generative\nAdversarial Networks (GANs) can be used to mimic MC generators. GANs are\npowerful generative models, but often suffer from training instability. We\nhenceforth show a review of GANs. We advocate the use of Wasserstein GAN (WGAN)\nwith weight clipping and WGAN with gradient penalty (WGAN-GP) where the norm of\ngradient of the critic is penalized with respect to its input. Following the\nemergence of multi-lepton anomalies at the LHC, we apply GANs for the\ngeneration of di-leptons final states in association with b-quarks at the LHC.\nA good agreement between the MC events and the WGAN-GP events is found for the\nobservables selected in the study.",
          "link": "http://arxiv.org/abs/2105.14933",
          "publishedOn": "2021-09-09T07:20:44.313Z",
          "wordCount": 689,
          "title": "The use of Generative Adversarial Networks to characterise new physics in multi-lepton final states at the LHC. (arXiv:2105.14933v2 [hep-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.01169",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1\">Muzammal Naseer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>",
          "description": "Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.",
          "link": "http://arxiv.org/abs/2101.01169",
          "publishedOn": "2021-09-09T07:20:44.307Z",
          "wordCount": 749,
          "title": "Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabaghi_P/0/1/0/all/0/1\">Puoya Tabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "Many high-dimensional and large-volume data sets of practical relevance have\nhierarchical structures induced by trees, graphs or time series. Such data sets\nare hard to process in Euclidean spaces and one often seeks low-dimensional\nembeddings in other space forms to perform required learning tasks. For\nhierarchical data, the space of choice is a hyperbolic space since it\nguarantees low-distortion embeddings for tree-like structures. Unfortunately,\nthe geometry of hyperbolic spaces has properties not encountered in Euclidean\nspaces that pose challenges when trying to rigorously analyze algorithmic\nsolutions. Here, for the first time, we establish a unified framework for\nlearning scalable and simple hyperbolic linear classifiers with provable\nperformance guarantees. The gist of our approach is to focus on Poincar\\'e ball\nmodels and formulate the classification problems using tangent space\nformalisms. Our results include a new hyperbolic and second-order perceptron\nalgorithm as well as an efficient and highly accurate convex optimization setup\nfor hyperbolic support vector machine classifiers. All algorithms provably\nconverge and are highly scalable as they have complexities comparable to those\nof their Euclidean counterparts. Their performance accuracies on synthetic data\nsets comprising millions of points, as well as on complex real-world data sets\nsuch as single-cell RNA-seq expression measurements, CIFAR10, Fashion-MNIST and\nmini-ImageNet.",
          "link": "http://arxiv.org/abs/2109.03781",
          "publishedOn": "2021-09-09T07:20:44.299Z",
          "wordCount": 660,
          "title": "Highly Scalable and Provably Accurate Classification in Poincare Balls. (arXiv:2109.03781v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_M/0/1/0/all/0/1\">Mert Al</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagli_S/0/1/0/all/0/1\">Semih Yagli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>",
          "description": "The rapid rise of IoT and Big Data has facilitated copious data driven\napplications to enhance our quality of life. However, the omnipresent and\nall-encompassing nature of the data collection can generate privacy concerns.\nHence, there is a strong need to develop techniques that ensure the data serve\nonly the intended purposes, giving users control over the information they\nshare. To this end, this paper studies new variants of supervised and\nadversarial learning methods, which remove the sensitive information in the\ndata before they are sent out for a particular application. The explored\nmethods optimize privacy preserving feature mappings and predictive models\nsimultaneously in an end-to-end fashion. Additionally, the models are built\nwith an emphasis on placing little computational burden on the user side so\nthat the data can be desensitized on device in a cheap manner. Experimental\nresults on mobile sensing and face datasets demonstrate that our models can\nsuccessfully maintain the utility performances of predictive models while\ncausing sensitive predictions to perform poorly.",
          "link": "http://arxiv.org/abs/2007.15710",
          "publishedOn": "2021-09-09T07:20:44.291Z",
          "wordCount": 677,
          "title": "Privacy Enhancing Machine Learning via Removal of Unwanted Dependencies. (arXiv:2007.15710v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyong Yuan</a>",
          "description": "Federated learning enables distributed devices to collaboratively learn a\nshared prediction model without centralizing on-device training data. Most of\nthe current algorithms require comparable individual efforts to train on-device\nmodels with the same structure and size, impeding participation from\nresource-constrained devices. Given the widespread yet heterogeneous devices\nnowadays, this paper proposes a new framework supporting federated learning\nacross heterogeneous on-device models via Zero-shot Knowledge Transfer, named\nby FedZKT. Specifically, FedZKT allows participating devices to independently\ndetermine their on-device models. To transfer knowledge across on-device\nmodels, FedZKT develops a zero-shot distillation approach contrary to certain\nprior research based on a public dataset or a pre-trained data generator. To\nutmostly reduce on-device workload, the resource-intensive distillation task is\nassigned to the server, which constructs a generator to adversarially train\nwith the ensemble of the received heterogeneous on-device models. The distilled\ncentral knowledge will then be sent back in the form of the corresponding\non-device model parameters, which can be easily absorbed at the device side.\nExperimental studies demonstrate the effectiveness and the robustness of FedZKT\ntowards heterogeneous on-device models and challenging federated learning\nscenarios, such as non-iid data distribution and straggler effects.",
          "link": "http://arxiv.org/abs/2109.03775",
          "publishedOn": "2021-09-09T07:20:44.265Z",
          "wordCount": 643,
          "title": "FedZKT: Zero-Shot Knowledge Transfer towards Heterogeneous On-Device Models in Federated Learning. (arXiv:2109.03775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Chih-Hong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rongjie Yan</a>",
          "description": "Deploying deep neural networks (DNNs) as core functions in autonomous driving\ncreates unique verification and validation challenges. In particular, the\ncontinuous engineering paradigm of gradually perfecting a DNN-based perception\ncan make the previously established result of safety verification no longer\nvalid. This can occur either due to the newly encountered examples (i.e., input\ndomain enlargement) inside the Operational Design Domain or due to the\nsubsequent parameter fine-tuning activities of a DNN. This paper considers\napproaches to transfer results established in the previous DNN safety\nverification problem to the modified problem setting. By considering the reuse\nof state abstractions, network abstractions, and Lipschitz constants, we\ndevelop several sufficient conditions that only require formally analyzing a\nsmall part of the DNN in the new problem. The overall concept is evaluated in a\n$1/10$-scaled vehicle that equips a DNN controller to determine the visual\nwaypoint from the perceived image.",
          "link": "http://arxiv.org/abs/2010.05689",
          "publishedOn": "2021-09-09T07:20:44.257Z",
          "wordCount": 621,
          "title": "Continuous Safety Verification of Neural Networks. (arXiv:2010.05689v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.11435",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goodman_D/0/1/0/all/0/1\">Dou Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1\">Tao Wei</a>",
          "description": "Though deep neural networks have achieved the state of the art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. In this paper, we develop improved\ntechniques for defending against adversarial examples. First, we propose an\nenhanced defense technique denoted Attention and Adversarial Logit\nPairing(AT+ALP), which encourages both attention map and logit for the pairs of\nexamples to be similar. When being applied to clean examples and their\nadversarial counterparts, AT+ALP improves accuracy on adversarial examples over\nadversarial training. We show that AT+ALP can effectively increase the average\nactivations of adversarial examples in the key area and demonstrate that it\nfocuses on discriminate features to improve the robustness of the model.\nFinally, we conduct extensive experiments using a wide range of datasets and\nthe experiment results show that our AT+ALP achieves the state of the art\ndefense performance. For example, on 17 Flower Category Database, under strong\n200-iteration PGD gray-box and black-box attacks where prior art has 34% and\n39% accuracy, our method achieves 50% and 51%. Compared with previous work, our\nwork is evaluated under highly challenging PGD attack: the maximum perturbation\n$\\epsilon \\in \\{0.25,0.5\\}$ i.e. $L_\\infty \\in \\{0.25,0.5\\}$ with 10 to 200\nattack iterations. To the best of our knowledge, such a strong attack has not\nbeen previously explored on a wide range of datasets.",
          "link": "http://arxiv.org/abs/1908.11435",
          "publishedOn": "2021-09-09T07:20:44.232Z",
          "wordCount": 718,
          "title": "Improving Adversarial Robustness via Attention and Adversarial Logit Pairing. (arXiv:1908.11435v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vanneste_S/0/1/0/all/0/1\">Simon Vanneste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanneste_A/0/1/0/all/0/1\">Astrid Vanneste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mets_K/0/1/0/all/0/1\">Kevin Mets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Ali Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mercelis_S/0/1/0/all/0/1\">Siegfried Mercelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Latre_S/0/1/0/all/0/1\">Steven Latr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellinckx_P/0/1/0/all/0/1\">Peter Hellinckx</a>",
          "description": "Learning to communicate in order to share state information is an active\nproblem in the area of multi-agent reinforcement learning (MARL). The credit\nassignment problem, the non-stationarity of the communication environment and\nthe creation of influenceable agents are major challenges within this research\nfield which need to be overcome in order to learn a valid communication\nprotocol. This paper introduces the novel multi-agent counterfactual\ncommunication learning (MACC) method which adapts counterfactual reasoning in\norder to overcome the credit assignment problem for communicating agents.\nSecondly, the non-stationarity of the communication environment while learning\nthe communication Q-function is overcome by creating the communication\nQ-function using the action policy of the other agents and the Q-function of\nthe action environment. Additionally, a social loss function is introduced in\norder to create influenceable agents which is required to learn a valid\ncommunication protocol. Our experiments show that MACC is able to outperform\nthe state-of-the-art baselines in four different scenarios in the Particle\nenvironment.",
          "link": "http://arxiv.org/abs/2006.07200",
          "publishedOn": "2021-09-09T07:20:44.223Z",
          "wordCount": 650,
          "title": "Learning to Communicate Using Counterfactual Reasoning. (arXiv:2006.07200v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.09684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilahi_I/0/1/0/all/0/1\">Inaam Ilahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usama_M/0/1/0/all/0/1\">Muhammad Usama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1\">Junaid Qadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Janjua_M/0/1/0/all/0/1\">Muhammad Umar Janjua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1\">Dinh Thai Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>",
          "description": "Deep Reinforcement Learning (DRL) has numerous applications in the real world\nthanks to its outstanding ability in quickly adapting to the surrounding\nenvironments. Despite its great advantages, DRL is susceptible to adversarial\nattacks, which precludes its use in real-life critical systems and applications\n(e.g., smart grids, traffic controls, and autonomous vehicles) unless its\nvulnerabilities are addressed and mitigated. Thus, this paper provides a\ncomprehensive survey that discusses emerging attacks in DRL-based systems and\nthe potential countermeasures to defend against these attacks. We first cover\nsome fundamental backgrounds about DRL and present emerging adversarial attacks\non machine learning techniques. We then investigate more details of the\nvulnerabilities that the adversary can exploit to attack DRL along with the\nstate-of-the-art countermeasures to prevent such attacks. Finally, we highlight\nopen issues and research challenges for developing solutions to deal with\nattacks for DRL-based intelligent systems.",
          "link": "http://arxiv.org/abs/2001.09684",
          "publishedOn": "2021-09-09T07:20:44.207Z",
          "wordCount": 641,
          "title": "Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning. (arXiv:2001.09684v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rios_A/0/1/0/all/0/1\">Anabel G&#xf3;mez-R&#xed;os</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luengo_J/0/1/0/all/0/1\">Juli&#xe1;n Luengo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_F/0/1/0/all/0/1\">Francisco Herrera</a>",
          "description": "Deep learning has outperformed other machine learning algorithms in a variety\nof tasks, and as a result, it has become more and more popular and used.\nHowever, as other machine learning algorithms, deep learning, and convolutional\nneural networks (CNNs) in particular, perform worse when the data sets present\nlabel noise. Therefore, it is important to develop algorithms that help the\ntraining of deep networks and their generalization to noise-free test sets. In\nthis paper, we propose a robust training strategy against label noise, called\nRAFNI, that can be used with any CNN. This algorithm filters and relabels\ninstances of the training set based on the predictions and their probabilities\nmade by the backbone neural network during the training process. That way, this\nalgorithm improves the generalization ability of the CNN on its own. RAFNI\nconsists of three mechanisms: two mechanisms that filter instances and one\nmechanism that relabels instances. In addition, it does not suppose that the\nnoise rate is known nor does it need to be estimated. We evaluated our\nalgorithm using different data sets of several sizes and characteristics. We\nalso compared it with state-of-the-art models using the CIFAR10 and CIFAR100\nbenchmarks under different types and rates of label noise and found that RAFNI\nachieves better results in most cases.",
          "link": "http://arxiv.org/abs/2109.03748",
          "publishedOn": "2021-09-09T07:20:44.188Z",
          "wordCount": null,
          "title": "A robust approach for deep neural networks in presence of label noise: relabelling and filtering instances during training. (arXiv:2109.03748v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2004.13805",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>",
          "description": "As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.",
          "link": "http://arxiv.org/abs/2004.13805",
          "publishedOn": "2021-09-09T07:20:44.187Z",
          "wordCount": null,
          "title": "Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salimi_Badr_A/0/1/0/all/0/1\">Armin Salimi-Badr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1\">Mohammad Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_H/0/1/0/all/0/1\">Hamidreza Saffari</a>",
          "description": "In this paper, an interpretable classifier using an interval type-2 fuzzy\nneural network for detecting patients suffering from Parkinson's Disease (PD)\nbased on analyzing the gait cycle is presented. The proposed method utilizes\nclinical features extracted from the vertical Ground Reaction Force (vGRF),\nmeasured by 16 wearable sensors placed in the soles of subjects' shoes and\nlearns interpretable fuzzy rules. Therefore, experts can verify the decision\nmade by the proposed method based on investigating the firing strength of\ninterpretable fuzzy rules. Moreover, experts can utilize the extracted fuzzy\nrules for patient diagnosing or adjust them based on their knowledge. To\nimprove the robustness of the proposed method against uncertainty and noisy\nsensor measurements, Interval Type-2 Fuzzy Logic is applied. To learn fuzzy\nrules, two paradigms are proposed: 1- A batch learning approach based on\nclustering available samples is applied to extract initial fuzzy rules, 2- A\ncomplementary online learning is proposed to improve the rule base encountering\nnew labeled samples. The performance of the method is evaluated for classifying\npatients and healthy subjects in different conditions including the presence of\nnoise or observing new instances. Moreover, the performance of the model is\ncompared to some previous supervised and unsupervised machine learning\napproaches. The final Accuracy, Precision, Recall, and F1 Score of the proposed\nmethod are 88.74%, 89.41%, 95.10%, and 92.16%. Finally, the extracted fuzzy\nsets for each feature are reported.",
          "link": "http://arxiv.org/abs/2109.02442",
          "publishedOn": "2021-09-09T07:20:44.180Z",
          "wordCount": 706,
          "title": "Parkinson's Disease Diagnosis based on Gait Cycle Analysis Through an Interpretable Interval Type-2 Neuro-Fuzzy System. (arXiv:2109.02442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.03173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1\">Milan Sonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural\nnetwork by maximizing the AUC score of the model on a dataset. Most previous\nworks of AUC maximization focus on the perspective of optimization by designing\nefficient stochastic algorithms, and studies on generalization performance of\nlarge-scale DAM on difficult tasks are missing. In this work, we aim to make\nDAM more practical for interesting real-world applications (e.g., medical image\nclassification). First, we propose a new margin-based min-max surrogate loss\nfunction for the AUC score (named as AUC min-max-margin loss or simply AUC\nmargin loss for short). It is more robust than the commonly used AUC square\nloss, while enjoying the same advantage in terms of large-scale stochastic\noptimization. Second, we conduct extensive empirical studies of our DAM method\non four difficult medical image classification tasks, namely (i) classification\nof chest x-ray images for identifying many threatening diseases, (ii)\nclassification of images of skin lesions for identifying melanoma, (iii)\nclassification of mammogram for breast cancer screening, and (iv)\nclassification of microscopic images for identifying tumor tissue. Our studies\ndemonstrate that the proposed DAM method improves the performance of optimizing\ncross-entropy loss by a large margin, and also achieves better performance than\noptimizing the existing AUC square loss on these medical image classification\ntasks. Specifically, our DAM method has achieved the 1st place on Stanford\nCheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is\nthe first work that makes DAM succeed on large-scale medical image datasets. We\nalso conduct extensive ablation studies to demonstrate the advantages of the\nnew AUC margin loss over the AUC square loss on benchmark datasets. The\nproposed method is implemented in our open-sourced library LibAUC\n(www.libauc.org).",
          "link": "http://arxiv.org/abs/2012.03173",
          "publishedOn": "2021-09-09T07:20:44.173Z",
          "wordCount": 808,
          "title": "Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06307",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Bowen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_F/0/1/0/all/0/1\">Fanghong Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_C/0/1/0/all/0/1\">Changyun Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruilong Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wen-An Zhang</a>",
          "description": "Most traditional false data injection attack (FDIA) detection approaches rely\non a key assumption, i.e., the power system can be accurately modeled. However,\nthe transmission line parameters are dynamic and cannot be accurately known\nduring operation and thus the involved modeling errors should not be neglected.\nIn this paper, an illustrative case has revealed that modeling errors in\ntransmission lines significantly weaken the detection effectiveness of\nconventional FDIA approaches. To tackle this issue, we propose an FDIA\ndetection mechanism from the perspective of transfer learning. Specifically,\nthe simulated power system is treated as a source domain, which provides\nabundant simulated normal and attack data. The real world's running system\nwhose transmission line parameters are unknown is taken as a target domain\nwhere sufficient real normal data are collected for tracking the latest system\nstates online. The designed transfer strategy that aims at making full use of\ndata in hand is divided into two optimization stages. In the first stage, a\ndeep neural network (DNN) is built by simultaneously optimizing several\nwell-designed objective terms with both simulated data and real data, and then\nit is fine-tuned via real data in the second stage. Several case studies on the\nIEEE 14-bus and 118-bus systems verify the effectiveness of the proposed\nmechanism.",
          "link": "http://arxiv.org/abs/2104.06307",
          "publishedOn": "2021-09-09T07:20:44.154Z",
          "wordCount": 723,
          "title": "Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach. (arXiv:2104.06307v3 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hulsebos_M/0/1/0/all/0/1\">Madelon Hulsebos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demiralp_C/0/1/0/all/0/1\">&#xc7;a&#x11f;atay Demiralp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groth_P/0/1/0/all/0/1\">Paul Groth</a>",
          "description": "The practical success of deep learning has sparked interest in improving\nrelational table tasks, like data search, with models trained on large table\ncorpora. Existing corpora primarily contain tables extracted from HTML pages,\nlimiting the capability to represent offline database tables. To train and\nevaluate high-capacity models for applications beyond the Web, we need\nadditional resources with tables that resemble relational database tables.\n\nHere we introduce GitTables, a corpus of currently 1.7M relational tables\nextracted from GitHub. Our continuing curation aims at growing the corpus to at\nleast 20M tables. We annotate table columns in GitTables with more than 2K\ndifferent semantic types from Schema.org and DBpedia. Our column annotations\nconsist of semantic types, hierarchical relations, range types and\ndescriptions.\n\nThe corpus is available at https://gittables.github.io. Our analysis of\nGitTables shows that its structure, content, and topical coverage differ\nsignificantly from existing table corpora. We evaluate our annotation pipeline\non hand-labeled tables from the T2Dv2 benchmark and find that our approach\nprovides results on par with human annotations. We demonstrate a use case of\nGitTables by training a semantic type detection model on it and obtain high\nprediction accuracy. We also show that the same model trained on tables from\ntheWeb generalizes poorly.",
          "link": "http://arxiv.org/abs/2106.07258",
          "publishedOn": "2021-09-09T07:20:44.147Z",
          "wordCount": 675,
          "title": "GitTables: A Large-Scale Corpus of Relational Tables. (arXiv:2106.07258v2 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Katafuchi_R/0/1/0/all/0/1\">Ryoya Katafuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1\">Terumasa Tokunaga</a>",
          "description": "This paper proposes an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis. The construction of large and publicly\navailable datasets containing labeled images of healthy and diseased crop\nplants led to growing interest in computer vision techniques for automatic\nplant disease diagnosis. Although supervised image classifiers based on deep\nlearning can be a powerful tool for plant disease diagnosis, they require a\nhuge amount of labeled data. The data mining technique of anomaly detection\nincludes unsupervised approaches that do not require rare samples for training\nclassifiers. We propose an unsupervised anomaly detection technique for\nimage-based plant disease diagnosis that is based on the reconstructability of\ncolors; a deep encoder-decoder network trained to reconstruct the colors of\n\\textit{healthy} plant images should fail to reconstruct colors of symptomatic\nregions. Our proposed method includes a new image-based framework for plant\ndisease detection that utilizes a conditional adversarial network called\npix2pix and a new anomaly score based on CIEDE2000 color difference.\nExperiments with PlantVillage dataset demonstrated the superiority of our\nproposed method compared to an existing anomaly detector at identifying\ndiseased crop images in terms of accuracy, interpretability and computational\nefficiency.",
          "link": "http://arxiv.org/abs/2011.14306",
          "publishedOn": "2021-09-09T07:20:44.138Z",
          "wordCount": 716,
          "title": "Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01203",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Feder Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrams_E/0/1/0/all/0/1\">Ellen Abrams</a>",
          "description": "Across machine learning (ML) sub-disciplines, researchers make explicit\nmathematical assumptions in order to facilitate proof-writing. We note that,\nspecifically in the area of fairness-accuracy trade-off optimization\nscholarship, similar attention is not paid to the normative assumptions that\nground this approach. Such assumptions presume that 1) accuracy and fairness\nare in inherent opposition to one another, 2) strict notions of mathematical\nequality can adequately model fairness, 3) it is possible to measure the\naccuracy and fairness of decisions independent from historical context, and 4)\ncollecting more data on marginalized individuals is a reasonable solution to\nmitigate the effects of the trade-off. We argue that such assumptions, which\nare often left implicit and unexamined, lead to inconsistent conclusions: While\nthe intended goal of this work may be to improve the fairness of machine\nlearning models, these unexamined, implicit assumptions can in fact result in\nemergent unfairness. We conclude by suggesting a concrete path forward toward a\npotential resolution.",
          "link": "http://arxiv.org/abs/2102.01203",
          "publishedOn": "2021-09-09T07:20:44.130Z",
          "wordCount": 644,
          "title": "Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research. (arXiv:2102.01203v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.07119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastianello_N/0/1/0/all/0/1\">Nicola Bastianello</a>",
          "description": "This paper introduces tvopt, a Python framework for prototyping and\nbenchmarking time-varying (or online) optimization algorithms. The paper first\ndescribes the theoretical approach that informed the development of tvopt. Then\nit discusses the different components of the framework and their use for\nmodeling and solving time-varying optimization problems. In particular, tvopt\nprovides functionalities for defining both centralized and distributed online\nproblems, and a collection of built-in algorithms to solve them, for example\ngradient-based methods, ADMM and other splitting methods. Moreover, the\nframework implements prediction strategies to improve the accuracy of the\nonline solvers. The paper then proposes some numerical results on a benchmark\nproblem and discusses their implementation using tvopt. The code for tvopt is\navailable at https://github.com/nicola-bastianello/tvopt.",
          "link": "http://arxiv.org/abs/2011.07119",
          "publishedOn": "2021-09-09T07:20:44.111Z",
          "wordCount": 601,
          "title": "tvopt: A Python Framework for Time-Varying Optimization. (arXiv:2011.07119v2 [cs.MS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.09507",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bradley_A/0/1/0/all/0/1\">Arwen V. Bradley</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Uribe_C/0/1/0/all/0/1\">Carlos Alberto Gomez-Uribe</a>",
          "description": "Recent works report that increasing the learning rate or decreasing the\nminibatch size in stochastic gradient descent (SGD) can improve test set\nperformance. We argue this is expected under some conditions in models with a\nloss function with multiple local minima. Our main contribution is an\napproximate but analytical approach inspired by methods in Physics to study the\nrole of the SGD learning rate and batch size in generalization. We characterize\ntest set performance under a shift between the training and test data\ndistributions for loss functions with multiple minima. The shift can simply be\ndue to sampling, and is therefore typically present in practical applications.\nWe show that the resulting shift in local minima worsens test performance by\npicking up curvature, implying that generalization improves by selecting wide\nand/or little-shifted local minima. We then specialize to SGD, and study its\ntest performance under stationarity. Because obtaining the exact stationary\ndistribution of SGD is intractable, we derive a Fokker-Planck approximation of\nSGD and obtain its stationary distribution instead. This process shows that the\nlearning rate divided by the minibatch size plays a role analogous to\ntemperature in statistical mechanics, and implies that SGD, including its\nstationary distribution, is largely invariant to changes in learning rate or\nbatch size that leave its temperature constant. We show that increasing SGD\ntemperature encourages the selection of local minima with lower curvature, and\ncan enable better generalization. We provide experiments on CIFAR10\ndemonstrating the temperature invariance of SGD, improvement of the test loss\nas SGD temperature increases, and quantifying the impact of sampling versus\ndomain shift in driving this effect. Finally, we present synthetic experiments\nshowing how our theory applies in a simplified loss with two local minima.",
          "link": "http://arxiv.org/abs/2108.09507",
          "publishedOn": "2021-09-09T07:20:44.103Z",
          "wordCount": 752,
          "title": "How Can Increased Randomness in Stochastic Gradient Descent Improve Generalization?. (arXiv:2108.09507v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03795",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Representation learning constructs low-dimensional representations to\nsummarize essential features of high-dimensional data. This learning problem is\noften approached by describing various desiderata associated with learned\nrepresentations; e.g., that they be non-spurious, efficient, or disentangled.\nIt can be challenging, however, to turn these intuitive desiderata into formal\ncriteria that can be measured and enhanced based on observed data. In this\npaper, we take a causal perspective on representation learning, formalizing\nnon-spuriousness and efficiency (in supervised representation learning) and\ndisentanglement (in unsupervised representation learning) using counterfactual\nquantities and observable consequences of causal assertions. This yields\ncomputable metrics that can be used to assess the degree to which\nrepresentations satisfy the desiderata of interest and learn non-spurious and\ndisentangled representations from single observational datasets.",
          "link": "http://arxiv.org/abs/2109.03795",
          "publishedOn": "2021-09-09T07:20:44.069Z",
          "wordCount": 570,
          "title": "Desiderata for Representation Learning: A Causal Perspective. (arXiv:2109.03795v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03769",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Shao_Y/0/1/0/all/0/1\">Yunqi Shao</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dietrich_F/0/1/0/all/0/1\">Florian M. Dietrich</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nettelblad_C/0/1/0/all/0/1\">Carl Nettelblad</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>",
          "description": "One hidden yet important issue for developing neural network potentials\n(NNPs) is the choice of training algorithm. Here we compare the performance of\ntwo popular training algorithms, the adaptive moment estimation algorithm\n(Adam) and the extended Kalman filter algorithm (EKF), using the\nBehler-Parrinello neural network (BPNN) and two publicly accessible datasets of\nliquid water. It is found that NNPs trained with EKF are more transferable and\nless sensitive to the value of the learning rate, as compared to Adam. In both\ncases, error metrics of the test set do not always serve as a good indicator\nfor the actual performance of NNPs. Instead, we show that their performance\ncorrelates well with a Fisher information based similarity measure.",
          "link": "http://arxiv.org/abs/2109.03769",
          "publishedOn": "2021-09-09T07:20:44.046Z",
          "wordCount": 575,
          "title": "Training Algorithm Matters for the Performance of Neural Network Potential. (arXiv:2109.03769v1 [physics.chem-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2108.11005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_Guan_Y/0/1/0/all/0/1\">Yue Shang-Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>",
          "description": "Online continual learning from data streams in dynamic environments is a\ncritical direction in the computer vision field. However, realistic benchmarks\nand fundamental studies in this line are still missing. To bridge the gap, we\npresent a new online continual object detection benchmark with an egocentric\nvideo dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,\nan ego-centric video stream collected over nine months by a graduate student.\nOAK provides exhaustive bounding box annotations of 80 video snippets (~17.5\nhours) for 105 object categories in outdoor scenes. The emergence of new object\ncategories in our benchmark follows a pattern similar to what a single person\nmight see in their day-to-day life. The dataset also captures the natural\ndistribution shifts as the person travels to different places. These egocentric\nlong-running videos provide a realistic playground for continual learning\nalgorithms, especially in online embodied settings. We also introduce new\nevaluation metrics to evaluate the model performance and catastrophic\nforgetting and provide baseline studies for online continual object detection.\nWe believe this benchmark will pose new exciting challenges for learning from\nnon-stationary data in continual learning. The OAK dataset and the associated\nbenchmark are released at https://oakdata.github.io/.",
          "link": "http://arxiv.org/abs/2108.11005",
          "publishedOn": "2021-09-09T07:20:44.010Z",
          "wordCount": 683,
          "title": "Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00802",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Onishi_Y/0/1/0/all/0/1\">Yuya Onishi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hashimoto_F/0/1/0/all/0/1\">Fumio Hashimoto</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ote_K/0/1/0/all/0/1\">Kibo Ote</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ohba_H/0/1/0/all/0/1\">Hiroyuki Ohba</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ota_R/0/1/0/all/0/1\">Ryosuke Ota</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoshikawa_E/0/1/0/all/0/1\">Etsuji Yoshikawa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ouchi_Y/0/1/0/all/0/1\">Yasuomi Ouchi</a>",
          "description": "Although supervised convolutional neural networks (CNNs) often outperform\nconventional alternatives for denoising positron emission tomography (PET)\nimages, they require many low- and high-quality reference PET image pairs.\nHerein, we propose an unsupervised 3D PET image denoising method based on an\nanatomical information-guided attention mechanism. The proposed magnetic\nresonance-guided deep decoder (MR-GDD) utilizes the spatial details and\nsemantic features of MR-guidance image more effectively by introducing\nencoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and\npatterns of the guidance image do not affect the denoised PET image, because\nthe guidance image is input to the network through an attention gate. In a\nMonte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the\nproposed method achieved the highest peak signal-to-noise ratio and structural\nsimilarity (27.92 $\\pm$ 0.44 dB/0.886 $\\pm$ 0.007), as compared with Gaussian\nfiltering (26.68 $\\pm$ 0.10 dB/0.807 $\\pm$ 0.004), image guided filtering\n(27.40 $\\pm$ 0.11 dB/0.849 $\\pm$ 0.003), deep image prior (DIP) (24.22 $\\pm$\n0.43 dB/0.737 $\\pm$ 0.017), and MR-DIP (27.65 $\\pm$ 0.42 dB/0.879 $\\pm$ 0.007).\nFurthermore, we experimentally visualized the behavior of the optimization\nprocess, which is often unknown in unsupervised CNN-based restoration problems.\nFor preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical\n(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates\nstate-of-the-art denoising performance while retaining spatial resolution and\nquantitative accuracy, despite using a common network architecture for various\nnoisy PET images with 1/10th of the full counts. These results suggest that the\nproposed MR-GDD can reduce PET scan times and PET tracer doses considerably\nwithout impacting patients.",
          "link": "http://arxiv.org/abs/2109.00802",
          "publishedOn": "2021-09-09T07:20:43.996Z",
          "wordCount": null,
          "title": "Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_B/0/1/0/all/0/1\">Bashir Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1\">Vishnu Boddeti</a>",
          "description": "Many applications of representation learning, such as privacy-preservation,\nalgorithmic fairness and domain adaptation, desire explicit control over\nsemantic information being discarded. This goal is often formulated as\nsatisfying two potentially competing objectives: maximizing utility for\npredicting a target attribute while simultaneously being independent or\ninvariant with respect to a known semantic attribute. In this paper, we\n\\emph{identify and determine} two fundamental trade-offs between utility and\nsemantic dependence induced by the statistical dependencies between the data\nand its corresponding target and semantic attributes. We derive closed-form\nsolutions for the global optima of the underlying optimization problems under\nmild assumptions, which in turn yields closed formulae for the exact\ntrade-offs. We also derive empirical estimates of the trade-offs and show their\nconvergence to the corresponding population counterparts. Finally, we\nnumerically quantify the trade-offs on representative problems and compare to\nthe solutions achieved by baseline representation learning algorithms.",
          "link": "http://arxiv.org/abs/2109.03386",
          "publishedOn": "2021-09-09T07:20:43.994Z",
          "wordCount": 585,
          "title": "On the Fundamental Trade-offs in Learning Invariant Representations. (arXiv:2109.03386v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malladi_S/0/1/0/all/0/1\">Sai Phani Kumar Malladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukhopadhyay_J/0/1/0/all/0/1\">Jayanta Mukhopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larabi_C/0/1/0/all/0/1\">Chaker Larabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhury_S/0/1/0/all/0/1\">Santanu Chaudhury</a>",
          "description": "Deep neural networks have shown their profound impact on achieving human\nlevel performance in visual saliency prediction. However, it is still unclear\nhow they learn the task and what it means in terms of understanding human\nvisual system. In this work, we develop a technique to derive explainable\nsaliency models from their corresponding deep neural architecture based\nsaliency models by applying human perception theories and the conventional\nconcepts of saliency. This technique helps us understand the learning pattern\nof the deep network at its intermediate layers through their activation maps.\nInitially, we consider two state-of-the-art deep saliency models, namely UNISAL\nand MSI-Net for our interpretation. We use a set of biologically plausible\nlog-gabor filters for identifying and reconstructing the activation maps of\nthem using our explainable saliency model. The final saliency map is generated\nusing these reconstructed activation maps. We also build our own deep saliency\nmodel named cross-concatenated multi-scale residual block based network\n(CMRNet) for saliency prediction. Then, we evaluate and compare the performance\nof the explainable models derived from UNISAL, MSI-Net and CMRNet on three\nbenchmark datasets with other state-of-the-art methods. Hence, we propose that\nthis approach of explainability can be applied to any deep visual saliency\nmodel for interpretation which makes it a generic one.",
          "link": "http://arxiv.org/abs/2109.03575",
          "publishedOn": "2021-09-09T07:20:43.751Z",
          "wordCount": null,
          "title": "Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.10411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oslandsbotn_A/0/1/0/all/0/1\">Andreas Oslandsbotn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kereta_Z/0/1/0/all/0/1\">Zeljko Kereta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumova_V/0/1/0/all/0/1\">Valeriya Naumova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1\">Yoav Freund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1\">Alexander Cloninger</a>",
          "description": "Kernel ridge regression (KRR) is a popular scheme for non-linear\nnon-parametric learning. However, existing implementations of KRR require that\nall the data is stored in the main memory, which severely limits the use of KRR\nin contexts where data size far exceeds the memory size. Such applications are\nincreasingly common in data mining, bioinformatics, and control. A powerful\nparadigm for computing on data sets that are too large for memory is the\nstreaming model of computation, where we process one data sample at a time,\ndiscarding each sample before moving on to the next one. In this paper, we\npropose StreaMRAK - a streaming version of KRR. StreaMRAK improves on existing\nKRR schemes by dividing the problem into several levels of resolution, which\nallows continual refinement to the predictions. The algorithm reduces the\nmemory requirement by continuously and efficiently integrating new samples into\nthe training model. With a novel sub-sampling scheme, StreaMRAK reduces memory\nand computational complexities by creating a sketch of the original data, where\nthe sub-sampling density is adapted to the bandwidth of the kernel and the\nlocal dimensionality of the data. We present a showcase study on two synthetic\nproblems and the prediction of the trajectory of a double pendulum. The results\nshow that the proposed algorithm is fast and accurate.",
          "link": "http://arxiv.org/abs/2108.10411",
          "publishedOn": "2021-09-09T07:20:43.744Z",
          "wordCount": null,
          "title": "StreaMRAK a Streaming Multi-Resolution Adaptive Kernel Algorithm. (arXiv:2108.10411v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03337",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyun Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Minwise hashing (MinHash) is an important and practical algorithm for\ngenerating random hashes to approximate the Jaccard (resemblance) similarity in\nmassive binary (0/1) data. The basic theory of MinHash requires applying\nhundreds or even thousands of independent random permutations to each data\nvector in the dataset, in order to obtain reliable results for (e.g.,) building\nlarge-scale learning models or approximate near neighbor search in massive\ndata. In this paper, we propose {\\bf Circulant MinHash (C-MinHash)} and provide\nthe surprising theoretical results that we just need \\textbf{two} independent\nrandom permutations. For C-MinHash, we first conduct an initial permutation on\nthe data vector, then we use a second permutation to generate hash values.\nBasically, the second permutation is re-used $K$ times via circulant shifting\nto produce $K$ hashes. Unlike classical MinHash, these $K$ hashes are obviously\ncorrelated, but we are able to provide rigorous proofs that we still obtain an\nunbiased estimate of the Jaccard similarity and the theoretical variance is\nuniformly smaller than that of the classical MinHash with $K$ independent\npermutations. The theoretical proofs of C-MinHash require some non-trivial\nefforts. Numerical experiments are conducted to justify the theory and\ndemonstrate the effectiveness of C-MinHash.",
          "link": "http://arxiv.org/abs/2109.03337",
          "publishedOn": "2021-09-09T07:20:43.739Z",
          "wordCount": null,
          "title": "C-MinHash: Rigorously Reducing $K$ Permutations to Two. (arXiv:2109.03337v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pfisterer_F/0/1/0/all/0/1\">Florian Pfisterer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1\">Lennart Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosbauer_J/0/1/0/all/0/1\">Julia Moosbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_M/0/1/0/all/0/1\">Martin Binder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1\">Bernd Bischl</a>",
          "description": "When developing and analyzing new hyperparameter optimization (HPO) methods,\nit is vital to empirically evaluate and compare them on well-curated benchmark\nsuites. In this work, we list desirable properties and requirements for such\nbenchmarks and propose a new set of challenging and relevant multifidelity HPO\nbenchmark problems motivated by these requirements. For this, we revisit the\nconcept of surrogate-based benchmarks and empirically compare them to more\nwidely-used tabular benchmarks, showing that the latter ones may induce bias in\nperformance estimation and ranking of HPO methods. We present a new\nsurrogate-based benchmark suite for multifidelity HPO methods consisting of 9\nbenchmark collections that constitute over 700 multifidelity HPO problems in\ntotal. All our benchmarks also allow for querying of multiple optimization\ntargets, enabling the benchmarking of multi-objective HPO. We examine and\ncompare our benchmark suite with respect to the defined requirements and show\nthat our benchmarks provide viable additions to existing suites.",
          "link": "http://arxiv.org/abs/2109.03670",
          "publishedOn": "2021-09-09T07:20:43.738Z",
          "wordCount": null,
          "title": "YAHPO Gym -- Design Criteria and a new Multifidelity Benchmark for Hyperparameter Optimization. (arXiv:2109.03670v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10698",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_P/0/1/0/all/0/1\">Pranesh Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karwande_A/0/1/0/all/0/1\">Atharva Karwande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolhe_T/0/1/0/all/0/1\">Tejas Kolhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamble_S/0/1/0/all/0/1\">Soham Kamble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1\">Akshay Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wyawahare_M/0/1/0/all/0/1\">Medha Wyawahare</a>",
          "description": "One of the important and tedious task in agricultural practices is the\ndetection of the disease on crops. It requires huge time as well as skilled\nlabor. This paper proposes a smart and efficient technique for detection of\ncrop disease which uses computer vision and machine learning techniques. The\nproposed system is able to detect 20 different diseases of 5 common plants with\n93% accuracy.",
          "link": "http://arxiv.org/abs/2106.10698",
          "publishedOn": "2021-09-09T07:20:43.737Z",
          "wordCount": null,
          "title": "Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chang-Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chia-Yi Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chia-Mu Yu</a>",
          "description": "Deep neural networks have developed rapidly and have achieved outstanding\nperformance in several tasks, such as image classification and natural language\nprocessing. However, recent studies have indicated that both digital and\nphysical adversarial examples can fool neural networks. Face-recognition\nsystems are used in various applications that involve security threats from\nphysical adversarial examples. Herein, we propose a physical adversarial attack\nwith the use of full-face makeup. The presence of makeup on the human face is a\nreasonable possibility, which possibly increases the imperceptibility of\nattacks. In our attack framework, we combine the cycle-adversarial generative\nnetwork (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to\ngenerate adversarial makeup, and the architecture of the victimized classifier\nis VGG 16. Our experimental results show that our attack can effectively\novercome manual errors in makeup application, such as color and\nposition-related errors. We also demonstrate that the approaches used to train\nthe models can influence physical attacks; the adversarial perturbations\ncrafted from the pre-trained model are affected by the corresponding training\ndata.",
          "link": "http://arxiv.org/abs/2109.03329",
          "publishedOn": "2021-09-09T07:20:43.734Z",
          "wordCount": null,
          "title": "Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>",
          "description": "Few-shot learning (FSL) aims to classify images under low-data regimes, where\nthe conventional pooled global feature is likely to lose useful local\ncharacteristics. Recent work has achieved promising performances by using deep\ndescriptors. They generally take all deep descriptors from neural networks into\nconsideration while ignoring that some of them are useless in classification\ndue to their limited receptive field, e.g., task-irrelevant descriptors could\nbe misleading and multiple aggregative descriptors from background clutter\ncould even overwhelm the object's presence. In this paper, we argue that a\nMutual Nearest Neighbor (MNN) relation should be established to explicitly\nselect the query descriptors that are most relevant to each task and discard\nless relevant ones from aggregative clutters in FSL. Specifically, we propose\nDiscriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive\nexperiments demonstrate that our method outperforms the existing\nstate-of-the-arts on both fine-grained and generalized datasets.",
          "link": "http://arxiv.org/abs/2103.08160",
          "publishedOn": "2021-09-09T07:20:43.655Z",
          "wordCount": null,
          "title": "DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chitnis_R/0/1/0/all/0/1\">Rohan Chitnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silver_T/0/1/0/all/0/1\">Tom Silver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "In robotic domains, learning and planning are complicated by continuous state\nspaces, continuous action spaces, and long task horizons. In this work, we\naddress these challenges with Neuro-Symbolic Relational Transition Models\n(NSRTs), a novel class of models that are data-efficient to learn, compatible\nwith powerful robotic planning methods, and generalizable over objects. NSRTs\nhave both symbolic and neural components, enabling a bilevel planning scheme\nwhere symbolic AI planning in an outer loop guides continuous planning with\nneural models in an inner loop. Experiments in four robotic planning domains\nshow that NSRTs can be learned after only tens or hundreds of training\nepisodes, and then used for fast planning in new tasks that require up to 60\nactions and involve many more objects than were seen during training. Video:\nhttps://tinyurl.com/chitnis-nsrts",
          "link": "http://arxiv.org/abs/2105.14074",
          "publishedOn": "2021-09-09T07:20:43.650Z",
          "wordCount": null,
          "title": "Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning. (arXiv:2105.14074v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.12803",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McMillan_A/0/1/0/all/0/1\">Audra McMillan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwar_K/0/1/0/all/0/1\">Kunal Talwar</a>",
          "description": "Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and\nThakurta [EFMRTT19] demonstrates that random shuffling amplifies differential\nprivacy guarantees of locally randomized data. Such amplification implies\nsubstantially stronger privacy guarantees for systems in which data is\ncontributed anonymously [BEMMRLRKTS17] and has lead to significant interest in\nthe shuffle model of privacy [CSUZZ19; EFMRTT19].\n\nWe show that random shuffling of $n$ data records that are input to\n$\\varepsilon_0$-differentially private local randomizers results in an\n$(O((1-e^{-\\varepsilon_0})\\sqrt{\\frac{e^{\\varepsilon_0}\\log(1/\\delta)}{n}}),\n\\delta)$-differentially private algorithm. This significantly improves over\nprevious work and achieves the asymptotically optimal dependence in\n$\\varepsilon_0$. Our result is based on a new approach that is simpler than\nprevious work and extends to approximate differential privacy with nearly the\nsame guarantees. Importantly, our work also yields an algorithm for deriving\ntighter bounds on the resulting $\\varepsilon$ and $\\delta$ as well as R\\'enyi\ndifferential privacy guarantees. We show numerically that our algorithm gets to\nwithin a small constant factor of the optimal bound. As a direct corollary of\nour analysis we derive a simple and nearly optimal algorithm for frequency\nestimation in the shuffle model of privacy. We also observe that our result\nimplies the first asymptotically optimal privacy analysis of noisy stochastic\ngradient descent that applies to sampling without replacement.",
          "link": "http://arxiv.org/abs/2012.12803",
          "publishedOn": "2021-09-09T07:20:43.648Z",
          "wordCount": null,
          "title": "Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. (arXiv:2012.12803v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-09-09T07:20:43.638Z",
          "wordCount": null,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00631",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruenstein_J/0/1/0/all/0/1\">Joshua Gruenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_N/0/1/0/all/0/1\">Neel Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>",
          "description": "A majority of microrobots are constructed using compliant materials that are\ndifficult to model analytically, limiting the utility of traditional\nmodel-based controllers. Challenges in data collection on microrobots and large\nerrors between simulated models and real robots make current model-based\nlearning and sim-to-real transfer methods difficult to apply. We propose a\nnovel framework residual model learning (RML) that leverages approximate models\nto substantially reduce the sample complexity associated with learning an\naccurate robot model. We show that using RML, we can learn a model of the\nHarvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively\ncollected interaction data. The learned model is accurate enough to be\nleveraged as \"proxy-simulator\" for learning walking and turning behaviors using\nmodel-free reinforcement learning algorithms. RML provides a general framework\nfor learning from extremely small amounts of interaction data, and our\nexperiments with HAMR clearly demonstrate that RML substantially outperforms\nexisting techniques.",
          "link": "http://arxiv.org/abs/2104.00631",
          "publishedOn": "2021-09-09T07:20:43.637Z",
          "wordCount": null,
          "title": "Residual Model Learning for Microrobot Control. (arXiv:2104.00631v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seitz_S/0/1/0/all/0/1\">Sarem Seitz</a>",
          "description": "Bayesian methods have become a popular way to incorporate prior knowledge and\na notion of uncertainty into machine learning models. At the same time, the\ncomplexity of modern machine learning makes it challenging to comprehend a\nmodel's reasoning process, let alone express specific prior assumptions in a\nrigorous manner. While primarily interested in the former issue, recent\ndevelopments intransparent machine learning could also broaden the range of\nprior information that we can provide to complex Bayesian models. Inspired by\nthe idea of self-explaining models, we introduce a corresponding concept for\nvariational GaussianProcesses. On the one hand, our contribution improves\ntransparency for these types of models. More importantly though, our proposed\nself-explaining variational posterior distribution allows to incorporate both\ngeneral prior knowledge about a target function as a whole and prior knowledge\nabout the contribution of individual features.",
          "link": "http://arxiv.org/abs/2109.03708",
          "publishedOn": "2021-09-09T07:20:43.634Z",
          "wordCount": null,
          "title": "Self-explaining variational posterior distributions for Gaussian Process models. (arXiv:2109.03708v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>",
          "description": "Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms most graph-based\nmodels. We further fine-tune DistilBERT for comparison and find that it\noutperforms all state-of-the-art models. We suggest that future studies use at\nleast an MLP baseline to contextualize the results. We provide recommendations\nfor the design and training of such a baseline.",
          "link": "http://arxiv.org/abs/2109.03777",
          "publishedOn": "2021-09-09T07:20:43.630Z",
          "wordCount": null,
          "title": "Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.10733",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waikhom_L/0/1/0/all/0/1\">Lilapati Waikhom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patgiri_R/0/1/0/all/0/1\">Ripon Patgiri</a>",
          "description": "In the last decade or so, we have witnessed deep learning reinvigorating the\nmachine learning field. It has solved many problems in the domains of computer\nvision, speech recognition, natural language processing, and various other\ntasks with state-of-the-art performance. The data is generally represented in\nthe Euclidean space in these domains. Various other domains conform to\nnon-Euclidean space, for which graph is an ideal representation. Graphs are\nsuitable for representing the dependencies and interrelationships between\nvarious entities. Traditionally, handcrafted features for graphs are incapable\nof providing the necessary inference for various tasks from this complex data\nrepresentation. Recently, there is an emergence of employing various advances\nin deep learning to graph data-based tasks. This article provides a\ncomprehensive survey of graph neural networks (GNNs) in each learning setting:\nsupervised, unsupervised, semi-supervised, and self-supervised learning.\nTaxonomy of each graph based learning setting is provided with logical\ndivisions of methods falling in the given learning setting. The approaches for\neach learning task are analyzed from both theoretical as well as empirical\nstandpoints. Further, we provide general architecture guidelines for building\nGNNs. Various applications and benchmark datasets are also provided, along with\nopen challenges still plaguing the general applicability of GNNs.",
          "link": "http://arxiv.org/abs/2108.10733",
          "publishedOn": "2021-09-09T07:20:43.630Z",
          "wordCount": null,
          "title": "Graph Neural Networks: Methods, Applications, and Opportunities. (arXiv:2108.10733v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tuan-Anh Nguyen Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat-Thanh Nguyen</a>",
          "description": "Information extraction from document images has received a lot of attention\nrecently, due to the need for digitizing a large volume of unstructured\ndocuments such as invoices, receipts, bank transfers, etc. In this paper, we\npropose a novel deep learning architecture for end-to-end information\nextraction on the 2D character-grid embedding of the document, namely the\n\\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and\nspatial relations between 2D elements, our model leverages a specialized\nmulti-stage encoder-decoders design, in conjunction with efficient uses of the\nself-attention mechanism and the box convolution. Experimental results on\ndifferent datasets show that our model outperforms the baseline U-Net\narchitecture by a large margin while using 40\\% fewer parameters. Moreover, it\nalso significantly improved the baseline in erroneous OCR and limited training\ndata scenario, thus becomes practical for real-world applications.",
          "link": "http://arxiv.org/abs/2106.00952",
          "publishedOn": "2021-09-09T07:20:43.627Z",
          "wordCount": null,
          "title": "End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03778",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_Mengin_M/0/1/0/all/0/1\">Marius Schmidt-Mengin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ricigliano_V/0/1/0/all/0/1\">Vito A.G. Ricigliano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bodini_B/0/1/0/all/0/1\">Benedetta Bodini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morena_E/0/1/0/all/0/1\">Emanuele Morena</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colombi_A/0/1/0/all/0/1\">Annalisa Colombi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamzaoui_M/0/1/0/all/0/1\">Mariem Hamzaoui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panah_A/0/1/0/all/0/1\">Arya Yazdan Panah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stankoff_B/0/1/0/all/0/1\">Bruno Stankoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Colliot_O/0/1/0/all/0/1\">Olivier Colliot</a>",
          "description": "Choroid plexuses (CP) are structures of the ventricles of the brain which\nproduce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo\nstudies have pointed towards their role in the inflammatory process in multiple\nsclerosis (MS). Automatic segmentation of CP from MRI thus has high value for\nstudying their characteristics in large cohorts of patients. To the best of our\nknowledge, the only freely available tool for CP segmentation is FreeSurfer but\nits accuracy for this specific structure is poor. In this paper, we propose to\nautomatically segment CP from non-contrast enhanced T1-weighted MRI. To that\nend, we introduce a new model called \"Axial-MLP\" based on an assembly of Axial\nmulti-layer perceptrons (MLPs). This is inspired by recent works which showed\nthat the self-attention layers of Transformers can be replaced with MLPs. This\napproach is systematically compared with a standard 3D U-Net, nnU-Net,\nFreesurfer and FastSurfer. For our experiments, we make use of a dataset of 141\nsubjects (44 controls and 97 patients with MS). We show that all the tested\ndeep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs\n0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is\nslightly less accurate. The conclusions of our paper are two-fold: 1) the\nstudied deep learning methods could be useful tools to study CP in large\ncohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to\nconvolutional neural networks for such tasks, although it could benefit from\nfurther improvements.",
          "link": "http://arxiv.org/abs/2109.03778",
          "publishedOn": "2021-09-09T07:20:43.626Z",
          "wordCount": null,
          "title": "Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ringsquandl_M/0/1/0/all/0/1\">Martin Ringsquandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1\">Daria Stepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamparter_S/0/1/0/all/0/1\">Steffen Lamparter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepratti_R/0/1/0/all/0/1\">Raffaello Lepratti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroger_P/0/1/0/all/0/1\">Peer Kr&#xf6;ger</a>",
          "description": "Smart factories are equipped with machines that can sense their manufacturing\nenvironments, interact with each other, and control production processes.\nSmooth operation of such factories requires that the machines and engineering\npersonnel that conduct their monitoring and diagnostics share a detailed common\nindustrial knowledge about the factory, e.g., in the form of knowledge graphs.\nCreation and maintenance of such knowledge is expensive and requires\nautomation. In this work we show how machine learning that is specifically\ntailored towards industrial applications can help in knowledge graph\ncompletion. In particular, we show how knowledge completion can benefit from\nevent logs that are common in smart factories. We evaluate this on the\nknowledge graph from a real world-inspired smart factory with encouraging\nresults.",
          "link": "http://arxiv.org/abs/2109.03655",
          "publishedOn": "2021-09-09T07:20:43.624Z",
          "wordCount": null,
          "title": "On Event-Driven Knowledge Graph Completion in Digital Factories. (arXiv:2109.03655v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.11114",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Farchi_A/0/1/0/all/0/1\">Alban Farchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bocquet_M/0/1/0/all/0/1\">Marc Bocquet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Laloyaux_P/0/1/0/all/0/1\">Patrick Laloyaux</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bonavita_M/0/1/0/all/0/1\">Massimo Bonavita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Malartic_Q/0/1/0/all/0/1\">Quentin Malartic</a>",
          "description": "Recent studies have shown that it is possible to combine machine learning\nmethods with data assimilation to reconstruct a dynamical system using only\nsparse and noisy observations of that system. The same approach can be used to\ncorrect the error of a knowledge-based model. The resulting surrogate model is\nhybrid, with a statistical part supplementing a physical part. In practice, the\ncorrection can be added as an integrated term (i.e. in the model resolvent) or\ndirectly inside the tendencies of the physical model. The resolvent correction\nis easy to implement. The tendency correction is more technical, in particular\nit requires the adjoint of the physical model, but also more flexible. We use\nthe two-scale Lorenz model to compare the two methods. The accuracy in\nlong-range forecast experiments is somewhat similar between the surrogate\nmodels using the resolvent correction and the tendency correction. By contrast,\nthe surrogate models using the tendency correction significantly outperform the\nsurrogate models using the resolvent correction in data assimilation\nexperiments. Finally, we show that the tendency correction opens the\npossibility to make online model error correction, i.e. improving the model\nprogressively as new observations become available. The resulting algorithm can\nbe seen as a new formulation of weak-constraint 4D-Var. We compare online and\noffline learning using the same framework with the two-scale Lorenz system, and\nshow that with online learning, it is possible to extract all the information\nfrom sparse and noisy observations.",
          "link": "http://arxiv.org/abs/2107.11114",
          "publishedOn": "2021-09-09T07:20:43.622Z",
          "wordCount": null,
          "title": "A comparison of combined data assimilation and machine learning methods for offline and online model error correction. (arXiv:2107.11114v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06291",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1\">Anum Talpur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1\">Mohan Gurusamy</a>",
          "description": "The growth of 5G and edge computing has enabled the emergence of Internet of\nVehicles. It supports different types of services with different resource and\nservice requirements. However, limited resources at the edge, high mobility of\nvehicles, increasing demand, and dynamicity in service request-types have made\nservice placement a challenging task. A typical static placement solution is\nnot effective as it does not consider the traffic mobility and service\ndynamics. Handling dynamics in IoV for service placement is an important and\nchallenging problem which is the primary focus of our work in this paper. We\npropose a Deep Reinforcement Learning-based Dynamic Service Placement (DRLD-SP)\nframework with the objective of minimizing the maximum edge resource usage and\nservice delay while considering the vehicle's mobility, varying demand, and\ndynamics in the requests for different types of services. We use SUMO and\nMATLAB to carry out simulation experiments. The experimental results show that\nthe proposed DRLD-SP approach is effective and outperforms other static and\ndynamic placement approaches.",
          "link": "http://arxiv.org/abs/2106.06291",
          "publishedOn": "2021-09-09T07:20:43.619Z",
          "wordCount": null,
          "title": "DRLD-SP: A Deep Reinforcement Learning-based Dynamic Service Placement in Edge-Enabled Internet of Vehicles. (arXiv:2106.06291v2 [cs.NI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1811.03179",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Liang_T/0/1/0/all/0/1\">Tengyuan Liang</a>",
          "description": "This paper studies the rates of convergence for learning distributions\nimplicitly with the adversarial framework and Generative Adversarial Networks\n(GANs), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated\nMethod of Moments (GMM/SMM) as special cases. We study a wide range of\nparametric and nonparametric target distributions under a host of objective\nevaluation metrics. We investigate how to obtain valid statistical guarantees\nfor GANs through the lens of regularization. On the nonparametric end, we\nderive the optimal minimax rates for distribution estimation under the\nadversarial framework. On the parametric end, we establish a theory for general\nneural network classes (including deep leaky ReLU networks) that characterizes\nthe interplay on the choice of generator and discriminator pair. We discover\nand isolate a new notion of regularization, called the\ngenerator-discriminator-pair regularization, that sheds light on the advantage\nof GANs compared to classical parametric and nonparametric approaches for\nexplicit distribution estimation. We develop novel oracle inequalities as the\nmain technical tools for analyzing GANs, which are of independent interest.",
          "link": "http://arxiv.org/abs/1811.03179",
          "publishedOn": "2021-09-09T07:20:43.616Z",
          "wordCount": null,
          "title": "How Well Generative Adversarial Networks Learn Distributions. (arXiv:1811.03179v4 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.13933",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rocks_J/0/1/0/all/0/1\">Jason W. Rocks</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mehta_P/0/1/0/all/0/1\">Pankaj Mehta</a>",
          "description": "The bias-variance trade-off is a central concept in supervised learning. In\nclassical statistics, increasing the complexity of a model (e.g., number of\nparameters) reduces bias but also increases variance. Until recently, it was\ncommonly believed that optimal performance is achieved at intermediate model\ncomplexities which strike a balance between bias and variance. Modern Deep\nLearning methods flout this dogma, achieving state-of-the-art performance using\n\"over-parameterized models\" where the number of fit parameters is large enough\nto perfectly fit the training data. As a result, understanding bias and\nvariance in over-parameterized models has emerged as a fundamental problem in\nmachine learning. Here, we use methods from statistical physics to derive\nanalytic expressions for bias and variance in two minimal models of\nover-parameterization (linear regression and two-layer neural networks with\nnonlinear data distributions), allowing us to disentangle properties stemming\nfrom the model architecture and random sampling of data. In both models,\nincreasing the number of fit parameters leads to a phase transition where the\ntraining error goes to zero and the test error diverges as a result of the\nvariance (while the bias remains finite). Beyond this threshold in the\ninterpolation regime, the training error remains zero while the test error\ndecreases. We also show that in contrast with classical intuition,\nover-parameterized models can overfit even in the absence of noise and exhibit\nbias even if the student and teacher models match. We synthesize these results\nto construct a holistic understanding of generalization error and the\nbias-variance trade-off in over-parameterized models and relate our results to\nrandom matrix theory.",
          "link": "http://arxiv.org/abs/2010.13933",
          "publishedOn": "2021-09-09T07:20:43.614Z",
          "wordCount": null,
          "title": "Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models. (arXiv:2010.13933v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">Daniel Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanford_C/0/1/0/all/0/1\">Clayton Sanford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Servedio_R/0/1/0/all/0/1\">Rocco A. Servedio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlatakis_Gkaragkounis_E/0/1/0/all/0/1\">Emmanouil-Vasileios Vlatakis-Gkaragkounis</a>",
          "description": "This paper considers the following question: how well can depth-two ReLU\nnetworks with randomly initialized bottom-level weights represent smooth\nfunctions? We give near-matching upper- and lower-bounds for\n$L_2$-approximation in terms of the Lipschitz constant, the desired accuracy,\nand the dimension of the problem, as well as similar results in terms of\nSobolev norms. Our positive results employ tools from harmonic analysis and\nridgelet representation theory, while our lower-bounds are based on (robust\nversions of) dimensionality arguments.",
          "link": "http://arxiv.org/abs/2102.02336",
          "publishedOn": "2021-09-09T07:20:43.608Z",
          "wordCount": null,
          "title": "On the Approximation Power of Two-Layer Networks of Random ReLUs. (arXiv:2102.02336v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rashid_S/0/1/0/all/0/1\">Syed Md. Mukit Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohammed Eunus Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_M/0/1/0/all/0/1\">Muhammad Aamir Cheema</a>",
          "description": "Trip itinerary recommendation finds an ordered sequence of Points-of-Interest\n(POIs) from a large number of candidate POIs in a city. In this paper, we\npropose a deep learning-based framework, called DeepAltTrip, that learns to\nrecommend top-k alternative itineraries for given source and destination POIs.\nThese alternative itineraries would be not only popular given the historical\nroutes adopted by past users but also dissimilar (or diverse) to each other.\nThe DeepAltTrip consists of two major components: (i) Itinerary Net (ITRNet)\nwhich estimates the likelihood of POIs on an itinerary by using graph\nautoencoders and two (forward and backward) LSTMs; and (ii) a route generation\nprocedure to generate k diverse itineraries passing through relevant POIs\nobtained using ITRNet. For the route generation step, we propose a novel\nsampling algorithm that can seamlessly handle a wide variety of user-defined\nconstraints. To the best of our knowledge, this is the first work that learns\nfrom historical trips to provide a set of alternative itineraries to the users.\nExtensive experiments conducted on eight popular real-world datasets show the\neffectiveness and efficacy of our approach over state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.03535",
          "publishedOn": "2021-09-09T07:20:43.605Z",
          "wordCount": null,
          "title": "DeepAltTrip: Top-k Alternative Itineraries for Trip Recommendation. (arXiv:2109.03535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>",
          "description": "Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.",
          "link": "http://arxiv.org/abs/2108.13990",
          "publishedOn": "2021-09-09T07:20:43.604Z",
          "wordCount": null,
          "title": "Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valluri_D/0/1/0/all/0/1\">Dinesh Valluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1\">Rory Campbell</a>",
          "description": "We define and establish the conditions for `equivalent neural networks' -\nneural networks with different weights, biases, and threshold functions that\nresult in the same associated function. We prove that given a neural network\n$\\mathcal{N}$ with piece-wise linear activation, the space of coefficients\ndescribing all equivalent neural networks is given by a semialgebraic set. This\nresult is obtained by studying different representations of a given piece-wise\nlinear function using the Tarski-Seidenberg theorem.",
          "link": "http://arxiv.org/abs/2109.03362",
          "publishedOn": "2021-09-09T07:20:43.602Z",
          "wordCount": null,
          "title": "On the space of coefficients of a Feed Forward Neural Network. (arXiv:2109.03362v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03560",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yuejia Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Contrastive Learning (CL) is one of the most popular self-supervised learning\nframeworks for graph representation learning, which trains a Graph Neural\nNetwork (GNN) by discriminating positive and negative node pairs. However,\nthere are two challenges for CL on graphs. On the one hand, traditional CL\nmethods will unavoidably introduce semantic errors since they will treat some\nsemantically similar nodes as negative pairs. On the other hand, most of the\nexisting CL methods ignore the multiplexity nature of the real-world graphs,\nwhere nodes are connected by various relations and each relation represents a\nview of the graph. To address these challenges, we propose a novel Graph\nMulti-View Prototypical (Graph-MVP) framework to extract node embeddings on\nmultiplex graphs. Firstly, we introduce a Graph Prototypical Contrastive\nLearning (Graph-PCL) framework to capture both node-level and semantic-level\ninformation for each view of multiplex graphs. Graph-PCL captures the\nnode-level information by a simple yet effective data transformation technique.\nIt captures the semantic-level information by an Expectation-Maximization (EM)\nalgorithm, which alternatively performs clustering over node embeddings and\nparameter updating for GNN. Next, we introduce Graph-MVP based on Graph-PCL to\njointly model different views of the multiplex graphs. Our key insight behind\nGraph-MVP is that different view-specific embeddings of the same node should\nhave similar underlying semantic, based on which we propose two versions of\nGraph-MVP: Graph-MVP_hard and Graph-MVP_soft to align embeddings across views.\nFinally, we evaluate the proposed Graph-PCL and Graph-MVP on a variety of\nreal-world datasets and downstream tasks. The experimental results demonstrate\nthe effectiveness of the proposed Graph-PCL and Graph-MVP frameworks.",
          "link": "http://arxiv.org/abs/2109.03560",
          "publishedOn": "2021-09-09T07:20:43.602Z",
          "wordCount": null,
          "title": "Graph-MVP: Multi-View Prototypical Contrastive Learning for Multiplex Graphs. (arXiv:2109.03560v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1\">Cristiane Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueira_G/0/1/0/all/0/1\">Gon&#xe7;alo Figueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amorim_P/0/1/0/all/0/1\">Pedro Amorim</a>",
          "description": "The emergence of Industry 4.0 is making production systems more flexible and\nalso more dynamic. In these settings, schedules often need to be adapted in\nreal-time by dispatching rules. Although substantial progress was made until\nthe '90s, the performance of these rules is still rather limited. The machine\nlearning literature is developing a variety of methods to improve them, but the\nresulting rules are difficult to interpret and do not generalise well for a\nwide range of settings. This paper is the first major attempt at combining\nmachine learning with domain problem reasoning for scheduling. The idea\nconsists of using the insights obtained with the latter to guide the empirical\nsearch of the former. Our hypothesis is that this guided empirical learning\nprocess should result in dispatching rules that are effective and interpretable\nand which generalise well to different instance classes. We test our approach\nin the classical dynamic job shop scheduling problem minimising tardiness,\nwhich is one of the most well-studied scheduling problems. Nonetheless, results\nsuggest that our approach was able to find new state-of-the-art rules, which\nsignificantly outperform the existing literature in the vast majority of\nsettings, from loose to tight due dates and from low utilisation conditions to\ncongested shops. Overall, the average improvement is 19%. Moreover, the rules\nare compact, interpretable, and generalise well to extreme, unseen scenarios.",
          "link": "http://arxiv.org/abs/2109.03323",
          "publishedOn": "2021-09-09T07:20:43.585Z",
          "wordCount": null,
          "title": "Effective and interpretable dispatching rules for dynamic job shops via guided empirical learning. (arXiv:2109.03323v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayad_R/0/1/0/all/0/1\">Raed Fayad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_A/0/1/0/all/0/1\">Adrian Taylor</a>",
          "description": "Given the success of reinforcement learning (RL) in various domains, it is\npromising to explore the application of its methods to the development of\nintelligent and autonomous cyber agents. Enabling this development requires a\nrepresentative RL training environment. To that end, this work presents CyGIL:\nan experimental testbed of an emulated RL training environment for network\ncyber operations. CyGIL uses a stateless environment architecture and\nincorporates the MITRE ATT&CK framework to establish a high fidelity training\nenvironment, while presenting a sufficiently abstracted interface to enable RL\ntraining. Its comprehensive action space and flexible game design allow the\nagent training to focus on particular advanced persistent threat (APT)\nprofiles, and to incorporate a broad range of potential threats and\nvulnerabilities. By striking a balance between fidelity and simplicity, it aims\nto leverage state of the art RL algorithms for application to real-world cyber\ndefence.",
          "link": "http://arxiv.org/abs/2109.03331",
          "publishedOn": "2021-09-09T07:20:43.583Z",
          "wordCount": null,
          "title": "CyGIL: A Cyber Gym for Training Autonomous Agents over Emulated Network Systems. (arXiv:2109.03331v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.09313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>",
          "description": "Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.",
          "link": "http://arxiv.org/abs/2010.09313",
          "publishedOn": "2021-09-09T07:20:43.573Z",
          "wordCount": null,
          "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liyan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yao Xie</a>",
          "description": "Given multiple source domains, domain generalization aims at learning a\nuniversal model that performs well on any unseen but related target domain. In\nthis work, we focus on the domain generalization scenario where domain shifts\noccur among class-conditional distributions of different domains. Existing\napproaches are not sufficiently robust when the variation of conditional\ndistributions given the same class is large. In this work, we extend the\nconcept of distributional robust optimization to solve the class-conditional\ndomain generalization problem. Our approach optimizes the worst-case\nperformance of a classifier over class-conditional distributions within a\nWasserstein ball centered around the barycenter of the source conditional\ndistributions. We also propose an iterative algorithm for learning the optimal\nradius of the Wasserstein balls automatically. Experiments show that the\nproposed framework has better performance on unseen target domain than\napproaches without domain generalization.",
          "link": "http://arxiv.org/abs/2109.03676",
          "publishedOn": "2021-09-09T07:20:43.563Z",
          "wordCount": null,
          "title": "Class-conditioned Domain Generalization via Wasserstein Distributional Robust Optimization. (arXiv:2109.03676v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03400",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Schatzki_L/0/1/0/all/0/1\">Louis Schatzki</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Arrasmith_A/0/1/0/all/0/1\">Andrew Arrasmith</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1\">Patrick J. Coles</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1\">M. Cerezo</a>",
          "description": "High-quality, large-scale datasets have played a crucial role in the\ndevelopment and success of classical machine learning. Quantum Machine Learning\n(QML) is a new field that aims to use quantum computers for data analysis, with\nthe hope of obtaining a quantum advantage of some sort. While most proposed QML\narchitectures are benchmarked using classical datasets, there is still doubt\nwhether QML on classical datasets will achieve such an advantage. In this work,\nwe argue that one should instead employ quantum datasets composed of quantum\nstates. For this purpose, we introduce the NTangled dataset composed of quantum\nstates with different amounts and types of multipartite entanglement. We first\nshow how a quantum neural network can be trained to generate the states in the\nNTangled dataset. Then, we use the NTangled dataset to benchmark QML models for\nsupervised learning classification tasks. We also consider an alternative\nentanglement-based dataset, which is scalable and is composed of states\nprepared by quantum circuits with different depths. As a byproduct of our\nresults, we introduce a novel method for generating multipartite entangled\nstates, providing a use-case of quantum neural networks for quantum\nentanglement theory.",
          "link": "http://arxiv.org/abs/2109.03400",
          "publishedOn": "2021-09-09T07:20:43.560Z",
          "wordCount": null,
          "title": "Entangled Datasets for Quantum Machine Learning. (arXiv:2109.03400v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Grumiaux_P/0/1/0/all/0/1\">Pierre-Amaury Grumiaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitic_S/0/1/0/all/0/1\">Sr&#x111;an Kiti&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girin_L/0/1/0/all/0/1\">Laurent Girin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerin_A/0/1/0/all/0/1\">Alexandre Gu&#xe9;rin</a>",
          "description": "This article is a review on deep learning methods for single and multiple\nsound source localization. We are particularly interested in sound source\nlocalization in indoor/domestic environment, where reverberation and diffuse\nnoise are present. We provide an exhaustive topography of the neural-based\nlocalization literature in this context, organized according to several\naspects: the neural network architecture, the type of input features, the\noutput strategy (classification or regression), the types of data used for\nmodel training and evaluation, and the model training strategy. This way, an\ninterested reader can easily comprehend the vast panorama of the deep\nlearning-based sound source localization methods. Tables summarizing the\nliterature review are provided at the end of the review for a quick search of\nmethods with a given set of target characteristics.",
          "link": "http://arxiv.org/abs/2109.03465",
          "publishedOn": "2021-09-09T07:20:43.548Z",
          "wordCount": null,
          "title": "A Review of Sound Source Localization with Deep Learning Methods. (arXiv:2109.03465v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.14483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1\">Tongzhou Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhan Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Derek Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuanlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_S/0/1/0/all/0/1\">Stone Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1\">Zhiwei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>",
          "description": "Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.",
          "link": "http://arxiv.org/abs/2107.14483",
          "publishedOn": "2021-09-09T07:20:43.547Z",
          "wordCount": null,
          "title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngjune Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kee-Eung Kim</a>",
          "description": "Knowledge Distillation (KD), which transfers the knowledge of a well-trained\nlarge model (teacher) to a small model (student), has become an important area\nof research for practical deployment of recommender systems. Recently, Relaxed\nRanking Distillation (RRD) has shown that distilling the ranking information in\nthe recommendation list significantly improves the performance. However, the\nmethod still has limitations in that 1) it does not fully utilize the\nprediction errors of the student model, which makes the training not fully\nefficient, and 2) it only distills the user-side ranking information, which\nprovides an insufficient view under the sparse implicit feedback. This paper\npresents Dual Correction strategy for Distillation (DCD), which transfers the\nranking information from the teacher model to the student model in a more\nefficient manner. Most importantly, DCD uses the discrepancy between the\nteacher model and the student model predictions to decide which knowledge to be\ndistilled. By doing so, DCD essentially provides the learning guidance tailored\nto \"correcting\" what the student model has failed to accurately predict. This\nprocess is applied for transferring the ranking information from the user-side\nas well as the item-side to address sparse implicit user feedback. Our\nexperiments show that the proposed method outperforms the state-of-the-art\nbaselines, and ablation studies validate the effectiveness of each component.",
          "link": "http://arxiv.org/abs/2109.03459",
          "publishedOn": "2021-09-09T07:20:43.543Z",
          "wordCount": null,
          "title": "Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03675",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangsibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>",
          "description": "Data auditing is a process to verify whether certain data have been removed\nfrom a trained model. A recently proposed method (Liu et al. 20) uses\nKolmogorov-Smirnov (KS) distance for such data auditing. However, it fails\nunder certain practical conditions. In this paper, we propose a new method\ncalled Ensembled Membership Auditing (EMA) for auditing data removal to\novercome these limitations. We compare both methods using benchmark datasets\n(MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP)\nand convolutional neural networks (CNN). Our experiments show that EMA is\nrobust under various conditions, including the failure cases of the previously\nproposed method. Our code is available at: https://github.com/Hazelsuko07/EMA.",
          "link": "http://arxiv.org/abs/2109.03675",
          "publishedOn": "2021-09-09T07:20:43.448Z",
          "wordCount": null,
          "title": "EMA: Auditing Data Removal from Trained Models. (arXiv:2109.03675v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03747",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abroshan_M/0/1/0/all/0/1\">Mahed Abroshan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_K/0/1/0/all/0/1\">Kai Hou Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekin_C/0/1/0/all/0/1\">Cem Tekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1\">Mihaela van der Schaar</a>",
          "description": "In high-stakes applications of data-driven decision making like healthcare,\nit is of paramount importance to learn a policy that maximizes the reward while\navoiding potentially dangerous actions when there is uncertainty. There are two\nmain challenges usually associated with this problem. Firstly, learning through\nonline exploration is not possible due to the critical nature of such\napplications. Therefore, we need to resort to observational datasets with no\ncounterfactuals. Secondly, such datasets are usually imperfect, additionally\ncursed with missing values in the attributes of features. In this paper, we\nconsider the problem of constructing personalized policies using logged data\nwhen there are missing values in the attributes of features in both training\nand test data. The goal is to recommend an action (treatment) when $\\Xt$, a\ndegraded version of $\\Xb$ with missing values, is observed. We consider three\nstrategies for dealing with missingness. In particular, we introduce the\n\\textit{conservative strategy} where the policy is designed to safely handle\nthe uncertainty due to missingness. In order to implement this strategy we need\nto estimate posterior distribution $p(\\Xb|\\Xt)$, we use variational autoencoder\nto achieve this. In particular, our method is based on partial variational\nautoencoders (PVAE) which are designed to capture the underlying structure of\nfeatures with missing values.",
          "link": "http://arxiv.org/abs/2109.03747",
          "publishedOn": "2021-09-09T07:20:43.418Z",
          "wordCount": null,
          "title": "Conservative Policy Construction Using Variational Autoencoders for Logged Data with Missing Values. (arXiv:2109.03747v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.01527",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingzhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaohui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Changchuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1\">Walid Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1\">H. Vincent Poor</a>",
          "description": "In this paper, the problem of minimizing the weighted sum of age of\ninformation (AoI) and total energy consumption of Internet of Things (IoT)\ndevices is studied. In the considered model, each IoT device monitors a\nphysical process that follows nonlinear dynamics. As the dynamics of the\nphysical process vary over time, each device must find an optimal sampling\nfrequency to sample the real-time dynamics of the physical system and send\nsampled information to a base station (BS). Due to limited wireless resources,\nthe BS can only select a subset of devices to transmit their sampled\ninformation. Thus, edge devices must cooperatively sample their monitored\ndynamics based on the local observations and the BS must collect the sampled\ninformation from the devices immediately, hence avoiding the additional time\nand energy used for sampling and information transmission. To this end, it is\nnecessary to jointly optimize the sampling policy of each device and the device\nselection scheme of the BS so as to accurately monitor the dynamics of the\nphysical process using minimum energy. This problem is formulated as an\noptimization problem whose goal is to minimize the weighted sum of AoI cost and\nenergy consumption. To solve this problem, we propose a novel distributed\nreinforcement learning (RL) approach for the sampling policy optimization. The\nproposed algorithm enables edge devices to cooperatively find the global\noptimal sampling policy using their own local observations. Given the sampling\npolicy, the device selection scheme can be optimized thus minimizing the\nweighted sum of AoI and energy consumption of all devices. Simulations with\nreal data of PM 2.5 pollution show that the proposed algorithm can reduce the\nsum of AoI by up to 17.8% and 33.9% and the total energy consumption by up to\n13.2% and 35.1%, compared to a conventional deep Q network method and a uniform\nsampling policy.",
          "link": "http://arxiv.org/abs/2104.01527",
          "publishedOn": "2021-09-09T07:20:43.416Z",
          "wordCount": null,
          "title": "Distributed Reinforcement Learning for Age of Information Minimization in Real-Time IoT Systems. (arXiv:2104.01527v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1\">Jessica Zosa Forde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1\">A. Feder Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwegyir_Aggrey_K/0/1/0/all/0/1\">Kweku Kwegyir-Aggrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1\">Chris De Sa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Littman_M/0/1/0/all/0/1\">Michael Littman</a>",
          "description": "Algorithmic fairness has emphasized the role of biased data in automated\ndecision outcomes. Recently, there has been a shift in attention to sources of\nbias that implicate fairness in other stages in the ML pipeline. We contend\nthat one source of such bias, human preferences in model selection, remains\nunder-explored in terms of its role in disparate impact across demographic\ngroups. Using a deep learning model trained on real-world medical imaging data,\nwe verify our claim empirically and argue that choice of metric for model\ncomparison, especially those that do not take variability into account, can\nsignificantly bias model selection outcomes.",
          "link": "http://arxiv.org/abs/2104.00606",
          "publishedOn": "2021-09-09T07:20:43.410Z",
          "wordCount": null,
          "title": "Model Selection's Disparate Impact in Real-World Deep Learning Applications. (arXiv:2104.00606v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_N/0/1/0/all/0/1\">Nadia Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samhi_J/0/1/0/all/0/1\">Jordan Samhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabore_A/0/1/0/all/0/1\">Abdoul Kader Kabore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allix_K/0/1/0/all/0/1\">Kevin Allix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1\">Tegawend&#xe9; F. Bissyand&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques Klein</a>",
          "description": "Computer vision has witnessed several advances in recent years, with\nunprecedented performance provided by deep representation learning research.\nImage formats thus appear attractive to other fields such as malware detection,\nwhere deep learning on images alleviates the need for comprehensively\nhand-crafted features generalising to different malware variants. We postulate\nthat this research direction could become the next frontier in Android malware\ndetection, and therefore requires a clear roadmap to ensure that new approaches\nindeed bring novel contributions. We contribute with a first building block by\ndeveloping and assessing a baseline pipeline for image-based malware detection\nwith straightforward steps. We propose DexRay, which converts the bytecode of\nthe app DEX files into grey-scale \"vector\" images and feeds them to a\n1-dimensional Convolutional Neural Network model. We view DexRay as\nfoundational due to the exceedingly basic nature of the design choices,\nallowing to infer what could be a minimal performance that can be obtained with\nimage-based learning in malware detection. The performance of DexRay evaluated\non over 158k apps demonstrates that, while simple, our approach is effective\nwith a high detection rate(F1-score= 0.96). Finally, we investigate the impact\nof time decay and image-resizing on the performance of DexRay and assess its\nresilience to obfuscation. This work-in-progress paper contributes to the\ndomain of Deep Learning based Malware detection by providing a sound, simple,\nyet effective approach (with available artefacts) that can be the basis to\nscope the many profound questions that will need to be investigated to fully\ndevelop this domain.",
          "link": "http://arxiv.org/abs/2109.03326",
          "publishedOn": "2021-09-09T07:20:43.390Z",
          "wordCount": null,
          "title": "DexRay: A Simple, yet Effective Deep Learning Approach to Android Malware Detection based on Image Representation of Bytecode. (arXiv:2109.03326v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Frank Po-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azam_S/0/1/0/all/0/1\">Sheikh Shams Azam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michelusi_N/0/1/0/all/0/1\">Nicol&#xf2; Michelusi</a>",
          "description": "Federated learning has emerged as a popular technique for distributing model\ntraining across the network edge. Its learning architecture is conventionally a\nstar topology between the devices and a central server. In this paper, we\npropose two timescale hybrid federated learning (TT-HF), which migrates to a\nmore distributed topology via device-to-device (D2D) communications. In TT-HF,\nlocal model training occurs at devices via successive gradient iterations, and\nthe synchronization process occurs at two timescales: (i) macro-scale, where\nglobal aggregations are carried out via device-server interactions, and (ii)\nmicro-scale, where local aggregations are carried out via D2D cooperative\nconsensus formation in different device clusters. Our theoretical analysis\nreveals how device, cluster, and network-level parameters affect the\nconvergence of TT-HF, and leads to a set of conditions under which a\nconvergence rate of O(1/t) is guaranteed. Experimental results demonstrate the\nimprovements in convergence and utilization that can be obtained by TT-HF over\nstate-of-the-art federated learning baselines.",
          "link": "http://arxiv.org/abs/2109.03350",
          "publishedOn": "2021-09-09T07:20:43.389Z",
          "wordCount": null,
          "title": "Federated Learning Beyond the Star: Local D2D Model Consensus with Global Cluster Sampling. (arXiv:2109.03350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03443",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kejiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hongsheng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>",
          "description": "Combining off-policy reinforcement learning methods with function\napproximators such as neural networks has been found to lead to overestimation\nof the value function and sub-optimal solutions. Improvement such as TD3 has\nbeen proposed to address this issue. However, we surprisingly find that its\nperformance lags behind the vanilla actor-critic methods (such as DDPG) in some\nprimitive environments. In this paper, we show that the failure of some cases\ncan be attributed to insufficient exploration. We reveal the culprit of\ninsufficient exploration in TD3, and propose a novel algorithm toward this\nproblem that ADapts between Exploration and Robustness, namely ADER. To enhance\nthe exploration ability while eliminating the overestimation bias, we introduce\na dynamic penalty term in value estimation calculated from estimated\nuncertainty, which takes into account different compositions of the uncertainty\nin different learning stages. Experiments in several challenging environments\ndemonstrate the supremacy of the proposed method in continuous control tasks.",
          "link": "http://arxiv.org/abs/2109.03443",
          "publishedOn": "2021-09-09T07:20:43.389Z",
          "wordCount": null,
          "title": "ADER:Adapting between Exploration and Robustness for Actor-Critic Methods. (arXiv:2109.03443v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03431",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takezawa_Y/0/1/0/all/0/1\">Yuki Takezawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1\">Ryoma Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozareva_Z/0/1/0/all/0/1\">Zornitsa Kozareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sujith Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>",
          "description": "The Wasserstein barycenter has been widely studied in various fields,\nincluding natural language processing, and computer vision. However, it\nrequires a high computational cost to solve the Wasserstein barycenter problem\nbecause the computation of the Wasserstein distance requires a quadratic time\nwith respect to the number of supports. By contrast, the Wasserstein distance\non a tree, called the tree-Wasserstein distance, can be computed in linear time\nand allows for the fast comparison of a large number of distributions. In this\nstudy, we propose a barycenter under the tree-Wasserstein distance, called the\nfixed support tree-Wasserstein barycenter (FS-TWB) and its extension, called\nthe fixed support tree-sliced Wasserstein barycenter (FS-TSWB). More\nspecifically, we first show that the FS-TWB and FS-TSWB problems are convex\noptimization problems and can be solved by using the projected subgradient\ndescent. Moreover, we propose a more efficient algorithm to compute the\nsubgradient and objective function value by using the properties of\ntree-Wasserstein barycenter problems. Through real-world experiments, we show\nthat, by using the proposed algorithm, the FS-TWB and FS-TSWB can be solved two\norders of magnitude faster than the original Wasserstein barycenter.",
          "link": "http://arxiv.org/abs/2109.03431",
          "publishedOn": "2021-09-09T07:20:43.387Z",
          "wordCount": null,
          "title": "Fixed Support Tree-Sliced Wasserstein Barycenter. (arXiv:2109.03431v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jafarnia_Jahromi_M/0/1/0/all/0/1\">Mehdi Jafarnia-Jahromi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rahul Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayyar_A/0/1/0/all/0/1\">Ashutosh Nayyar</a>",
          "description": "In this paper, we propose Posterior Sampling Reinforcement Learning for\nZero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that\nachieves Bayesian regret bound of $O(HS\\sqrt{AT})$ in the infinite-horizon\nzero-sum stochastic games with average-reward criterion. Here $H$ is an upper\nbound on the span of the bias function, $S$ is the number of states, $A$ is the\nnumber of joint actions and $T$ is the horizon. We consider the online setting\nwhere the opponent can not be controlled and can take any arbitrary\ntime-adaptive history-dependent strategy. This improves the best existing\nregret bound of $O(\\sqrt[3]{DS^2AT^2})$ by Wei et. al., 2017 under the same\nassumption and matches the theoretical lower bound in $A$ and $T$.",
          "link": "http://arxiv.org/abs/2109.03396",
          "publishedOn": "2021-09-09T07:20:43.376Z",
          "wordCount": null,
          "title": "Learning Zero-sum Stochastic Games with Posterior Sampling. (arXiv:2109.03396v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03430",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiding Liang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_Z/0/1/0/all/0/1\">Zhepeng Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_J/0/1/0/all/0/1\">Junhuan Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xiong_J/0/1/0/all/0/1\">Jinjun Xiong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Jiang_W/0/1/0/all/0/1\">Weiwen Jiang</a>",
          "description": "In the noisy intermediate-scale quantum (NISQ) era, one of the key questions\nis how to deal with the high noise level existing in physical quantum bits\n(qubits). Quantum error correction is promising but requires an extensive\nnumber (e.g., over 1,000) of physical qubits to create one \"perfect\" qubit,\nexceeding the capacity of the existing quantum computers. This paper aims to\ntackle the noise issue from another angle: instead of creating perfect qubits\nfor general quantum algorithms, we investigate the potential to mitigate the\nnoise issue for dedicate algorithms. Specifically, this paper targets quantum\nneural network (QNN), and proposes to learn the errors in the training phase,\nso that the identified QNN model can be resilient to noise. As a result, the\nimplementation of QNN needs no or a small number of additional physical qubits,\nwhich is more realistic for the near-term quantum computers. To achieve this\ngoal, an application-specific compiler is essential: on the one hand, the error\ncannot be learned if the mapping from logical qubits to physical qubits exists\nrandomness; on the other hand, the compiler needs to be efficient so that the\nlengthy training procedure can be completed in a reasonable time. In this\npaper, we utilize the recent QNN framework, QuantumFlow, as a case study.\nExperimental results show that the proposed approach can optimize QNN models\nfor different errors in qubits, achieving up to 28% accuracy improvement\ncompared with the model obtained by the error-agnostic training.",
          "link": "http://arxiv.org/abs/2109.03430",
          "publishedOn": "2021-09-09T07:20:43.284Z",
          "wordCount": null,
          "title": "Can Noise on Qubits Be Learned in Quantum Neural Network? A Case Study on QuantumFlow. (arXiv:2109.03430v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03275",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Grooby_E/0/1/0/all/0/1\">Ethan Grooby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jinyuan He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fattahi_D/0/1/0/all/0/1\">Davood Fattahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1\">Lindsay Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+King_A/0/1/0/all/0/1\">Arrabella King</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramanathan_A/0/1/0/all/0/1\">Ashwin Ramanathan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malhotra_A/0/1/0/all/0/1\">Atul Malhotra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumont_G/0/1/0/all/0/1\">Guy A. Dumont</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marzbanrad_F/0/1/0/all/0/1\">Faezeh Marzbanrad</a>",
          "description": "Obtaining high-quality heart and lung sounds enables clinicians to accurately\nassess a newborn's cardio-respiratory health and provide timely care. However,\nnoisy chest sound recordings are common, hindering timely and accurate\nassessment. A new Non-negative Matrix Co-Factorisation-based approach is\nproposed to separate noisy chest sound recordings into heart, lung, and noise\ncomponents to address this problem. This method is achieved through training\nwith 20 high-quality heart and lung sounds, in parallel with separating the\nsounds of the noisy recording. The method was tested on 68 10-second noisy\nrecordings containing both heart and lung sounds and compared to the current\nstate of the art Non-negative Matrix Factorisation methods. Results show\nsignificant improvements in heart and lung sound quality scores respectively,\nand improved accuracy of 3.6bpm and 1.2bpm in heart and breathing rate\nestimation respectively, when compared to existing methods.",
          "link": "http://arxiv.org/abs/2109.03275",
          "publishedOn": "2021-09-09T07:20:43.282Z",
          "wordCount": null,
          "title": "A New Non-Negative Matrix Co-Factorisation Approach for Noisy Neonatal Chest Sound Separation. (arXiv:2109.03275v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2102.04635",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuoning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhishuai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "Deep AUC (area under the ROC curve) Maximization (DAM) has attracted much\nattention recently due to its great potential for imbalanced data\nclassification. However, the research on Federated Deep AUC Maximization (FDAM)\nis still limited. Compared with standard federated learning (FL) approaches\nthat focus on decomposable minimization objectives, FDAM is more complicated\ndue to its minimization objective is non-decomposable over individual examples.\nIn this paper, we propose improved FDAM algorithms for heterogeneous data by\nsolving the popular non-convex strongly-concave min-max formulation of DAM in a\ndistributed fashion, which can also be applied to a class of non-convex\nstrongly-concave min-max problems. A striking result of this paper is that the\ncommunication complexity of the proposed algorithm is a constant independent of\nthe number of machines and also independent of the accuracy level, which\nimproves an existing result by orders of magnitude. The experiments have\ndemonstrated the effectiveness of our FDAM algorithm on benchmark datasets, and\non medical chest X-ray images from different organizations. Our experiment\nshows that the performance of FDAM using data from multiple hospitals can\nimprove the AUC score on testing data from a single hospital for detecting\nlife-threatening diseases based on chest radiographs.",
          "link": "http://arxiv.org/abs/2102.04635",
          "publishedOn": "2021-09-09T07:20:43.262Z",
          "wordCount": null,
          "title": "Federated Deep AUC Maximization for Heterogeneous Data with a Constant Communication Complexity. (arXiv:2102.04635v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03475",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kirmizis_A/0/1/0/all/0/1\">Athanasios Kirmizis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kyritsis_K/0/1/0/all/0/1\">Konstantinos Kyritsis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delopoulos_A/0/1/0/all/0/1\">Anastasios Delopoulos</a>",
          "description": "The consumption of tobacco has reached global epidemic proportions and is\ncharacterized as the leading cause of death and illness. Among the different\nways of consuming tobacco (e.g., smokeless, cigars), smoking cigarettes is the\nmost widespread. In this paper, we present a two-step, bottom-up algorithm\ntowards the automatic and objective monitoring of cigarette-based, smoking\nbehavior during the day, using the 3D acceleration and orientation velocity\nmeasurements from a commercial smartwatch. In the first step, our algorithm\nperforms the detection of individual smoking gestures (i.e., puffs) using an\nartificial neural network with both convolutional and recurrent layers. In the\nsecond step, we make use of the detected puff density to achieve the temporal\nlocalization of smoking sessions that occur throughout the day. In the\nexperimental section we provide extended evaluation regarding each step of the\nproposed algorithm, using our publicly available, realistic Smoking Event\nDetection (SED) and Free-living Smoking Event Detection (SED-FL) datasets\nrecorded under semi-controlled and free-living conditions, respectively. In\nparticular, leave-one-subject-out (LOSO) experiments reveal an F1-score of\n0.863 for the detection of puffs and an F1-score/Jaccard index equal to\n0.878/0.604 towards the temporal localization of smoking sessions during the\nday. Finally, to gain further insight, we also compare the puff detection part\nof our algorithm with a similar approach found in the recent literature.",
          "link": "http://arxiv.org/abs/2109.03475",
          "publishedOn": "2021-09-09T07:20:43.256Z",
          "wordCount": null,
          "title": "A Bottom-up method Towards the Automatic and Objective Monitoring of Smoking Behavior In-the-wild using Wrist-mounted Inertial Sensors. (arXiv:2109.03475v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Spoken question answering (SQA) requires fine-grained understanding of both\nspoken documents and questions for the optimal answer prediction. In this\npaper, we propose novel training schemes for spoken question answering with a\nself-supervised training stage and a contrastive representation learning stage.\nIn the self-supervised stage, we propose three auxiliary self-supervised tasks,\nincluding utterance restoration, utterance insertion, and question\ndiscrimination, and jointly train the model to capture consistency and\ncoherence among speech documents without any additional data or annotations. We\nthen propose to learn noise-invariant utterance representations in a\ncontrastive objective by adopting multiple augmentation strategies, including\nspan deletion and span substitution. Besides, we design a Temporal-Alignment\nattention to semantically align the speech-text clues in the learned common\nspace and benefit the SQA tasks. By this means, the training schemes can more\neffectively guide the generation model to predict more proper answers.\nExperimental results show that our model achieves state-of-the-art results on\nthree SQA benchmarks.",
          "link": "http://arxiv.org/abs/2109.03381",
          "publishedOn": "2021-09-09T07:20:43.248Z",
          "wordCount": null,
          "title": "Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03309",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Li_Y/0/1/0/all/0/1\">Yaqin Li</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjin Xu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yu_Y/0/1/0/all/0/1\">Yi Yu</a>",
          "description": "In this study, we propose the convolutional recurrent neural network and\ntransfer learning (CRNNTL) for QSAR modelling. The method was inspired by the\napplications of polyphonic sound detection and electrocardiogram\nclassification. Our strategy takes advantages of both convolutional and\nrecurrent neural networks for feature extraction, as well as the data\naugmentation method. Herein, CRNNTL is evaluated on 20 benchmark datasets in\ncomparison with baseline methods. In addition, one isomers based dataset is\nused to elucidate its ability for both local and global feature extraction.\nThen, knowledge transfer performance of CRNNTL is tested, especially for small\nbiological activity datasets. Finally, different latent representations from\nother type of AEs were used for versatility study of our model. The results\nshow the effectiveness of CRNNTL using different latent representation.\nMoreover, efficient knowledge transfer is achieved to overcome data scarcity\nconsidering binding site similarity between different targets.",
          "link": "http://arxiv.org/abs/2109.03309",
          "publishedOn": "2021-09-09T07:20:43.241Z",
          "wordCount": null,
          "title": "CRNNTL: convolutional recurrent neural network and transfer learning for QSAR modelling. (arXiv:2109.03309v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03327",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1\">Shengyu Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sammak_S/0/1/0/all/0/1\">Shervin Sammak</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Givi_P/0/1/0/all/0/1\">Peyman Givi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yurko1_J/0/1/0/all/0/1\">Joseph P.Yurko1</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jia_X/0/1/0/all/0/1\">Xiaowei Jia</a>",
          "description": "Direct numerical simulation (DNS) of turbulent flows is computationally\nexpensive and cannot be applied to flows with large Reynolds numbers. Large\neddy simulation (LES) is an alternative that is computationally less demanding,\nbut is unable to capture all of the scales of turbulent transport accurately.\nOur goal in this work is to build a new data-driven methodology based on\nsuper-resolution techniques to reconstruct DNS data from LES predictions. We\nleverage the underlying physical relationships to regularize the relationships\namongst different physical variables. We also introduce a hierarchical\ngenerative process and a reverse degradation process to fully explore the\ncorrespondence between DNS and LES data. We demonstrate the effectiveness of\nour method through a single-snapshot experiment and a cross-time experiment.\nThe results confirm that our method can better reconstruct high-resolution DNS\ndata over space and over time in terms of pixel-wise reconstruction error and\nstructural similarity. Visual comparisons show that our method performs much\nbetter in capturing fine-level flow dynamics.",
          "link": "http://arxiv.org/abs/2109.03327",
          "publishedOn": "2021-09-09T07:20:43.233Z",
          "wordCount": null,
          "title": "Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.09391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gulshad_S/0/1/0/all/0/1\">Sadaf Gulshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1\">Ivan Sosnovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1\">Arnold Smeulders</a>",
          "description": "We focus on building robustness in the convolutions of neural visual\nclassifiers, especially against natural perturbations like elastic\ndeformations, occlusions and Gaussian noise. Existing CNNs show outstanding\nperformance on clean images, but fail to tackle naturally occurring\nperturbations. In this paper, we start from elastic perturbations, which\napproximate (local) view-point changes of the object. We present\nelastically-augmented convolutions (EAConv) by parameterizing filters as a\ncombination of fixed elastically-perturbed bases functions and trainable\nweights for the purpose of integrating unseen viewpoints in the CNN. We show on\nCIFAR-10 and STL-10 datasets that the general robustness of our method on\nunseen occlusion, zoom, rotation, image cut and Gaussian perturbations\nimproves, while significantly improving the performance on clean images without\nany data augmentation.",
          "link": "http://arxiv.org/abs/2107.09391",
          "publishedOn": "2021-09-09T07:20:43.232Z",
          "wordCount": null,
          "title": "Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1\">Gyujoon Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1\">Ahmed Abdulkadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erus_G/0/1/0/all/0/1\">Guray Erus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habes_M/0/1/0/all/0/1\">Mohamad Habes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomponio_R/0/1/0/all/0/1\">Raymond Pomponio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_H/0/1/0/all/0/1\">Haochang Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doshi_J/0/1/0/all/0/1\">Jimit Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamourian_E/0/1/0/all/0/1\">Elizabeth Mamourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1\">Tanweer Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgel_M/0/1/0/all/0/1\">Murat Bilgel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sotiras_A/0/1/0/all/0/1\">Aristeidis Sotiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_D/0/1/0/all/0/1\">Dhivya Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_J/0/1/0/all/0/1\">John C. Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_D/0/1/0/all/0/1\">Daniel Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albert_M/0/1/0/all/0/1\">Marilyn S. Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_N/0/1/0/all/0/1\">Nick R. Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Resnick_S/0/1/0/all/0/1\">Susan M. Resnick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrallah_I/0/1/0/all/0/1\">Ilya M. Nasrallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davatzikos_C/0/1/0/all/0/1\">Christos Davatzikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a> (from the iSTAGING consortium, for the ADNI)",
          "description": "Neuroimaging biomarkers that distinguish between typical brain aging and\nAlzheimer's disease (AD) are valuable for determining how much each contributes\nto cognitive decline. Machine learning models can derive multi-variate brain\nchange patterns related to the two processes, including the SPARE-AD (Spatial\nPatterns of Atrophy for Recognition of Alzheimer's Disease) and SPARE-BA (of\nBrain Aging) investigated herein. However, substantial overlap between brain\nregions affected in the two processes confounds measuring them independently.\nWe present a methodology toward disentangling the two. T1-weighted MRI images\nof 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),\nor cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based\ncoordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were\nanalyzed. First, a subset of AD patients and CN adults were selected based\npurely on clinical diagnoses to train SPARE-BA1 (regression of age using CN\nindividuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous\ngroups were selected based on clinical and molecular markers to train SPARE-BA2\nand SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,\nA+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN\ngroup. Finally, the combined group of the AD continuum and A-/CN individuals\nwas used to train SPARE-BA3, with the intention to estimate brain age\nregardless of AD-related brain changes. Disentangled SPARE models derived brain\npatterns that were more specific to the two types of the brain changes.\nCorrelation between the SPARE-BA and SPARE-AD was significantly reduced.\nCorrelation of disentangled SPARE-AD was non-inferior to the molecular\nmeasurements and to the number of APOE4 alleles, but was less to AD-related\npsychometric test scores, suggesting contribution of advanced brain aging to\nthese scores.",
          "link": "http://arxiv.org/abs/2109.03723",
          "publishedOn": "2021-09-09T07:20:43.223Z",
          "wordCount": null,
          "title": "Disentangling Alzheimer's disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1\">Soung Chang Liew</a>",
          "description": "Over-the-air computation (OAC) is a promising technique to realize fast model\naggregation in the uplink of federated edge learning. OAC, however, hinges on\naccurate channel-gain precoding and strict synchronization among the edge\ndevices, which are challenging in practice. As such, how to design the maximum\nlikelihood (ML) estimator in the presence of residual channel-gain mismatch and\nasynchronies is an open problem. To fill this gap, this paper formulates the\nproblem of misaligned OAC for federated edge learning and puts forth a whitened\nmatched filtering and sampling scheme to obtain oversampled, but independent,\nsamples from the misaligned and overlapped signals. Given the whitened samples,\na sum-product ML estimator and an aligned-sample estimator are devised to\nestimate the arithmetic sum of the transmitted symbols. In particular, the\ncomputational complexity of our sum-product ML estimator is linear in the\npacket length and hence is significantly lower than the conventional ML\nestimator. Extensive simulations on the test accuracy versus the average\nreceived energy per symbol to noise power spectral density ratio (EsN0) yield\ntwo main results: 1) In the low EsN0 regime, the aligned-sample estimator can\nachieve superior test accuracy provided that the phase misalignment is\nnon-severe. In contrast, the ML estimator does not work well due to the error\npropagation and noise enhancement in the estimation process. 2) In the high\nEsN0 regime, the ML estimator attains the optimal learning performance\nregardless of the severity of phase misalignment. On the other hand, the\naligned-sample estimator suffers from a test-accuracy loss caused by phase\nmisalignment.",
          "link": "http://arxiv.org/abs/2102.13604",
          "publishedOn": "2021-09-09T07:20:43.210Z",
          "wordCount": null,
          "title": "Federated Edge Learning with Misaligned Over-The-Air Computation. (arXiv:2102.13604v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03812",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stewart_R/0/1/0/all/0/1\">Russell Stewart</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dixon_A/0/1/0/all/0/1\">Austin Dixon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knoll_F/0/1/0/all/0/1\">Florian Knoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhengnan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lui_Y/0/1/0/all/0/1\">Yvonne W. Lui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_M/0/1/0/all/0/1\">Michael S. Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>",
          "description": "Improving speed and image quality of Magnetic Resonance Imaging (MRI) via\nnovel reconstruction approaches remains one of the highest impact applications\nfor deep learning in medical imaging. The fastMRI dataset, unique in that it\ncontains large volumes of raw MRI data, has enabled significant advances in\naccelerating MRI using deep learning-based reconstruction methods. While the\nimpact of the fastMRI dataset on the field of medical imaging is unquestioned,\nthe dataset currently lacks clinical expert pathology annotations, critical to\naddressing clinically relevant reconstruction frameworks and exploring\nimportant questions regarding rendering of specific pathology using such novel\napproaches. This work introduces fastMRI+, which consists of 16154\nsubspecialist expert bounding box annotations and 13 study-level labels for 22\ndifferent pathology categories on the fastMRI knee dataset, and 7570\nsubspecialist expert bounding box annotations and 643 study-level labels for 30\ndifferent pathology categories for the fastMRI brain dataset. The fastMRI+\ndataset is open access and aims to support further research and advancement of\nmedical imaging in MRI reconstruction and beyond.",
          "link": "http://arxiv.org/abs/2109.03812",
          "publishedOn": "2021-09-09T07:20:43.199Z",
          "wordCount": null,
          "title": "fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1\">Pepa Atanasova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonsen_J/0/1/0/all/0/1\">Jakob Grue Simonsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioma_C/0/1/0/all/0/1\">Christina Lioma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>",
          "description": "Explanations shed light on a machine learning model's rationales and can aid\nin identifying deficiencies in its reasoning process. Explanation generation\nmodels are typically trained in a supervised way given human explanations. When\nsuch annotations are not available, explanations are often selected as those\nportions of the input that maximise a downstream task's performance, which\ncorresponds to optimising an explanation's Faithfulness to a given model.\nFaithfulness is one of several so-called diagnostic properties, which prior\nwork has identified as useful for gauging the quality of an explanation without\nrequiring annotations. Other diagnostic properties are Data Consistency, which\nmeasures how similar explanations are for similar input instances, and\nConfidence Indication, which shows whether the explanation reflects the\nconfidence of the model. In this work, we show how to directly optimise for\nthese diagnostic properties when training a model to generate sentence-level\nexplanations, which markedly improves explanation quality, agreement with human\nrationales, and downstream task performance on three complex reasoning tasks.",
          "link": "http://arxiv.org/abs/2109.03756",
          "publishedOn": "2021-09-09T07:20:43.196Z",
          "wordCount": null,
          "title": "Diagnostics-Guided Explanation Generation. (arXiv:2109.03756v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_O/0/1/0/all/0/1\">Omid Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagarkar_P/0/1/0/all/0/1\">Parth Nagarkar</a>",
          "description": "Machine learning has been utilized to perform tasks in many different domains\nsuch as classification, object detection, image segmentation and natural\nlanguage analysis. Data labeling has always been one of the most important\ntasks in machine learning. However, labeling large amounts of data increases\nthe monetary cost in machine learning. As a result, researchers started to\nfocus on reducing data annotation and labeling costs. Transfer learning was\ndesigned and widely used as an efficient approach that can reasonably reduce\nthe negative impact of limited data, which in turn, reduces the data\npreparation cost. Even transferring previous knowledge from a source domain\nreduces the amount of data needed in a target domain. However, large amounts of\nannotated data are still demanded to build robust models and improve the\nprediction accuracy of the model. Therefore, researchers started to pay more\nattention on auto annotation and labeling. In this survey paper, we provide a\nreview of previous techniques that focuses on optimized data annotation and\nlabeling for video, audio, and text data.",
          "link": "http://arxiv.org/abs/2109.03784",
          "publishedOn": "2021-09-09T07:20:43.155Z",
          "wordCount": null,
          "title": "A Survey on Machine Learning Techniques for Auto Labeling of Video, Audio, and Text Data. (arXiv:2109.03784v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiyun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jinjiang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Southon_A/0/1/0/all/0/1\">Anthony Southon</a>",
          "description": "With the rapid development of intelligent detection algorithms based on deep\nlearning, much progress has been made in automatic road defect recognition and\nroad marking parsing. This can effectively address the issue of an expensive\nand time-consuming process for professional inspectors to review the street\nmanually. Towards this goal, we present RoadAtlas, a novel end-to-end\nintegrated system that can support 1) road defect detection, 2) road marking\nparsing, 3) a web-based dashboard for presenting and inputting data by users,\nand 4) a backend containing a well-structured database and developed APIs.",
          "link": "http://arxiv.org/abs/2109.03385",
          "publishedOn": "2021-09-09T07:20:43.130Z",
          "wordCount": null,
          "title": "RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03264",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>",
          "description": "Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work\naddressing the generative aspects of speech pre-training, which replaces text\nwith discovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.",
          "link": "http://arxiv.org/abs/2109.03264",
          "publishedOn": "2021-09-09T07:20:43.115Z",
          "wordCount": null,
          "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seoh_R/0/1/0/all/0/1\">Ronald Seoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birle_I/0/1/0/all/0/1\">Ian Birle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tak_M/0/1/0/all/0/1\">Mrinal Tak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinette_B/0/1/0/all/0/1\">Brian Pinette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_A/0/1/0/all/0/1\">Alfred Hough</a>",
          "description": "For many business applications, we often seek to analyze sentiments\nassociated with any arbitrary aspects of commercial products, despite having a\nvery limited amount of labels or even without any labels at all. However,\nexisting aspect target sentiment classification (ATSC) models are not trainable\nif annotated datasets are not available. Even with labeled data, they fall\nshort of reaching satisfactory performance. To address this, we propose simple\napproaches that better solve ATSC with natural language prompts, enabling the\ntask under zero-shot cases and enhancing supervised settings, especially for\nfew-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop\ndomain, our method of reformulating ATSC as an NLI task outperforms supervised\nSOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.\nMoreover, we demonstrate that our prompts could handle implicitly stated\naspects as well: our models reach about 77% accuracy on detecting sentiments\nfor aspect categories (e.g., food), which do not necessarily appear within the\ntext, even though we trained the models only with explicitly mentioned aspect\nterms (e.g., fajitas) from just 16 reviews - while the accuracy of the\nno-prompt baseline is only around 65%.",
          "link": "http://arxiv.org/abs/2109.03685",
          "publishedOn": "2021-09-09T07:20:43.076Z",
          "wordCount": null,
          "title": "Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03378",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dai_M/0/1/0/all/0/1\">Mengyu Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1\">Haibin Hang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1\">Anuj Srivastava</a>",
          "description": "Empirically multidimensional discriminator (critic) output can be\nadvantageous, while a solid explanation for it has not been discussed. In this\npaper, (i) we rigorously prove that high-dimensional critic output has\nadvantage on distinguishing real and fake distributions; (ii) we also introduce\nan square-root velocity transformation (SRVT) block which further magnifies\nthis advantage. The proof is based on our proposed maximal p-centrality\ndiscrepancy which is bounded above by p-Wasserstein distance and perfectly fits\nthe Wasserstein GAN framework with high-dimensional critic output n. We have\nalso showed when n = 1, the proposed discrepancy is equivalent to 1-Wasserstein\ndistance. The SRVT block is applied to break the symmetric structure of\nhigh-dimensional critic output and improve the generalization capability of the\ndiscriminator network. In terms of implementation, the proposed framework does\nnot require additional hyper-parameter tuning, which largely facilitates its\nusage. Experiments on image generation tasks show performance improvement on\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2109.03378",
          "publishedOn": "2021-09-09T07:20:43.002Z",
          "wordCount": null,
          "title": "AWGAN: Empowering High-Dimensional Discriminator Output for Generative Adversarial Networks. (arXiv:2109.03378v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02810",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Lin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_B/0/1/0/all/0/1\">Bo-Hao Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1\">Y.-W. Peter Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1\">Chi-Chun Lee</a>",
          "description": "Advancement in speech technology has brought convenience to our life.\nHowever, the concern is on the rise as speech signal contains multiple personal\nattributes, which would lead to either sensitive information leakage or bias\ntoward decision. In this work, we propose an attribute-aligned learning\nstrategy to derive speech representation that can flexibly address these issues\nby attribute-selection mechanism. Specifically, we propose a\nlayered-representation variational autoencoder (LR-VAE), which factorizes\nspeech representation into attribute-sensitive nodes, to derive an\nidentity-free representation for speech emotion recognition (SER), and an\nemotionless representation for speaker verification (SV). Our proposed method\nachieves competitive performances on identity-free SER and a better performance\non emotionless SV, comparing to the current state-of-the-art method of using\nadversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,\nour proposed learning strategy reduces the model and training process needed to\nachieve multiple privacy-preserving tasks.",
          "link": "http://arxiv.org/abs/2106.02810",
          "publishedOn": "2021-09-09T07:20:42.937Z",
          "wordCount": null,
          "title": "An Attribute-Aligned Strategy for Learning Speech Representation. (arXiv:2106.02810v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1\">Roderick Murray-Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_J/0/1/0/all/0/1\">John H. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramsay_A/0/1/0/all/0/1\">Andrew Ramsay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonolini_F/0/1/0/all/0/1\">Francesco Tonolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_S/0/1/0/all/0/1\">Simon Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loriette_A/0/1/0/all/0/1\">Antoine Loriette</a>",
          "description": "We outline the role of forward and inverse modelling approaches in the design\nof human--computer interaction systems. Causal, forward models tend to be\neasier to specify and simulate, but HCI requires solutions of the inverse\nproblem. We infer finger 3D position $(x,y,z)$ and pose (pitch and yaw) on a\nmobile device using capacitive sensors which can sense the finger up to 5cm\nabove the screen. We use machine learning to develop data-driven models to\ninfer position, pose and sensor readings, based on training data from: 1. data\ngenerated by robots, 2. data from electrostatic simulators 3. human-generated\ndata. Machine learned emulation is used to accelerate the electrostatic\nsimulation performance by a factor of millions. We combine a Conditional\nVariational Autoencoder with domain expertise/models experimentally collected\ndata. We compare forward and inverse model approaches to direct inference of\nfinger pose. The combination gives the most accurate reported results on\ninferring 3D position and pose with a capacitive sensor on a mobile device.",
          "link": "http://arxiv.org/abs/2109.03366",
          "publishedOn": "2021-09-09T07:20:42.774Z",
          "wordCount": null,
          "title": "Forward and Inverse models in HCI:Physical simulation and deep learning for inferring 3D finger pose. (arXiv:2109.03366v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanaa_D/0/1/0/all/0/1\">David Kanaa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1\">Vikram Voleti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1\">Samira Ebrahimi Kahou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Despite having been studied to a great extent, the task of conditional\ngeneration of sequences of frames, or videos, remains extremely challenging. It\nis a common belief that a key step towards solving this task resides in\nmodelling accurately both spatial and temporal information in video signals. A\npromising direction to do so has been to learn latent variable models that\npredict the future in latent space and project back to pixels, as suggested in\nrecent literature. Following this line of work and building on top of a family\nof models introduced in prior work, Neural ODE, we investigate an approach that\nmodels time-continuous dynamics over a continuous latent space with a\ndifferential equation with respect to time. The intuition behind this approach\nis that these trajectories in latent space could then be extrapolated to\ngenerate video frames beyond the time steps for which the model is trained. We\nshow that our approach yields promising results in the task of future frame\nprediction on the Moving MNIST dataset with 1 and 2 digits.",
          "link": "http://arxiv.org/abs/2109.03292",
          "publishedOn": "2021-09-09T07:20:42.766Z",
          "wordCount": null,
          "title": "Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rizzi1_W/0/1/0/all/0/1\">Williams Rizzi1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francescomarino_C/0/1/0/all/0/1\">Chiara Di Francescomarino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1\">Fabrizio Maria Maggi</a>",
          "description": "Existing well investigated Predictive Process Monitoring techniques typically\nconstruct a predictive model based on past process executions, and then use it\nto predict the future of new ongoing cases, without the possibility of updating\nit with new cases when they complete their execution. This can make Predictive\nProcess Monitoring too rigid to deal with the variability of processes working\nin real environments that continuously evolve and/or exhibit new variant\nbehaviours over time. As a solution to this problem, we evaluate the use of\nthree different strategies that allow the periodic rediscovery or incremental\nconstruction of the predictive model so as to exploit new available data. The\nevaluation focuses on the performance of the new learned predictive models, in\nterms of accuracy and time, against the original one, and uses a number of real\nand synthetic datasets with and without explicit Concept Drift. The results\nprovide an evidence of the potential of incremental learning algorithms for\npredicting process monitoring in real environments.",
          "link": "http://arxiv.org/abs/2109.03501",
          "publishedOn": "2021-09-09T07:20:42.307Z",
          "wordCount": 626,
          "title": "How do I update my model? On the resilience of Predictive Process Monitoring models to change. (arXiv:2109.03501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03457",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Travelletti_C/0/1/0/all/0/1\">C&#xe9;dric Travelletti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ginsbourger_D/0/1/0/all/0/1\">David Ginsbourger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Linde_N/0/1/0/all/0/1\">Niklas Linde</a>",
          "description": "We consider the use of Gaussian process (GP) priors for solving inverse\nproblems in a Bayesian framework. As is well known, the computational\ncomplexity of GPs scales cubically in the number of datapoints. We here show\nthat in the context of inverse problems involving integral operators, one faces\nadditional difficulties that hinder inversion on large grids. Furthermore, in\nthat context, covariance matrices can become too large to be stored. By\nleveraging results about sequential disintegrations of Gaussian measures, we\nare able to introduce an implicit representation of posterior covariance\nmatrices that reduces the memory footprint by only storing low rank\nintermediate matrices, while allowing individual elements to be accessed\non-the-fly without needing to build full posterior covariance matrices.\nMoreover, it allows for fast sequential inclusion of new observations. These\nfeatures are crucial when considering sequential experimental design tasks. We\ndemonstrate our approach by computing sequential data collection plans for\nexcursion set recovery for a gravimetric inverse problem, where the goal is to\nprovide fine resolution estimates of high density regions inside the Stromboli\nvolcano, Italy. Sequential data collection plans are computed by extending the\nweighted integrated variance reduction (wIVR) criterion to inverse problems.\nOur results show that this criterion is able to significantly reduce the\nuncertainty on the excursion volume, reaching close to minimal levels of\nresidual uncertainty. Overall, our techniques allow the advantages of\nprobabilistic models to be brought to bear on large-scale inverse problems\narising in the natural sciences.",
          "link": "http://arxiv.org/abs/2109.03457",
          "publishedOn": "2021-09-09T07:20:42.300Z",
          "wordCount": 716,
          "title": "Uncertainty Quantification and Experimental Design for large-scale linear Inverse Problems under Gaussian Process Priors. (arXiv:2109.03457v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenyou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1\">Tin Lun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_Berthouze_N/0/1/0/all/0/1\">Nadia Bianchi-Berthouze</a>",
          "description": "The annotation of domain experts is important for some medical applications\nwhere the objective groundtruth is ambiguous to define, e.g., the\nrehabilitation for some chronic diseases, and the prescreening of some\nmusculoskeletal abnormalities without further medical examinations. However,\nimproper uses of the annotations may hinder developing reliable models. On one\nhand, forcing the use of a single groundtruth generated from multiple\nannotations is less informative for the modeling. On the other hand, feeding\nthe model with all the annotations without proper regularization is noisy given\nexisting disagreements. For such issues, we propose a novel agreement learning\nframework to tackle the challenge of learning from multiple annotators without\nobjective groundtruth. The framework has two streams, with one stream fitting\nwith the multiple annotators and the other stream learning agreement\ninformation between the annotators. In particular, the agreement learning\nstream produces regularization information to the classifier stream, tuning its\ndecision to be better in line with the agreement between the annotators. The\nproposed method can be easily plugged to existing backbones developed with\nmajority-voted groundtruth or multiple annotations. Thereon, experiments on two\nmedical datasets demonstrate improved agreement levels with annotators.",
          "link": "http://arxiv.org/abs/2109.03596",
          "publishedOn": "2021-09-09T07:20:42.291Z",
          "wordCount": 654,
          "title": "AgreementLearning: An End-to-End Framework for Learning with Multiple Annotators without Groundtruth. (arXiv:2109.03596v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03616",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pilikos_G/0/1/0/all/0/1\">Georgios Pilikos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horchens_L/0/1/0/all/0/1\">Lars Horchens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leeuwen_T/0/1/0/all/0/1\">Tristan van Leeuwen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>",
          "description": "Ultrasonic imaging is being used to obtain information about the acoustic\nproperties of a medium by emitting waves into it and recording their\ninteraction using ultrasonic transducer arrays. The Delay-And-Sum (DAS)\nalgorithm forms images using the main path on which reflected signals travel\nback to the transducers. In some applications, different insonification paths\ncan be considered, for instance by placing the transducers at different\nlocations or if strong reflectors inside the medium are known a-priori. These\ndifferent modes give rise to multiple DAS images reflecting different geometric\ninformation about the scatterers and the challenge is to either fuse them into\none image or to directly extract higher-level information regarding the\nmaterials of the medium, e.g., a segmentation map. Traditional image fusion\ntechniques typically use ad-hoc combinations of pre-defined image transforms,\npooling operations and thresholding. In this work, we propose a deep neural\nnetwork (DNN) architecture that directly maps all available data to a\nsegmentation map while explicitly incorporating the DAS image formation for the\ndifferent insonification paths as network layers. This enables information flow\nbetween data pre-processing and image post-processing DNNs, trained end-to-end.\nWe compare our proposed method to a traditional image fusion technique using\nsimulated data experiments, mimicking a non-destructive testing application\nwith four image modes, i.e., two transducer locations and two internal\nreflection boundaries. Using our approach, it is possible to obtain much more\naccurate segmentation of defects.",
          "link": "http://arxiv.org/abs/2109.03616",
          "publishedOn": "2021-09-09T07:20:42.280Z",
          "wordCount": 689,
          "title": "Deep Learning for Multi-View Ultrasonic Image Fusion. (arXiv:2109.03616v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allen_J/0/1/0/all/0/1\">Justin Allen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knapp_D/0/1/0/all/0/1\">David Knapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monteith_K/0/1/0/all/0/1\">Kristine Monteith</a>",
          "description": "The ability to identify applications based on the network data they generate\ncould be a valuable tool for cyber defense. We report on a machine learning\ntechnique capable of using netflow-like features to predict the application\nthat generated the traffic. In our experiments, we used ground-truth labels\nobtained from host-based sensors deployed in a large enterprise environment; we\napplied random forests and multilayer perceptrons to the tasks of browser vs.\nnon-browser identification, browser fingerprinting, and process name\nprediction. For each of these tasks, we demonstrate how machine learning models\ncan achieve high classification accuracy using only netflow-like features as\nthe basis for classification.",
          "link": "http://arxiv.org/abs/2109.03328",
          "publishedOn": "2021-09-09T07:20:42.263Z",
          "wordCount": 571,
          "title": "Predicting Process Name from Network Data. (arXiv:2109.03328v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03552",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">Saurabh Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>",
          "description": "The widespread presence of offensive language on social media motivated the\ndevelopment of systems capable of recognizing such content automatically. Apart\nfrom a few notable exceptions, most research on automatic offensive language\nidentification has dealt with English. To address this shortcoming, we\nintroduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first\ndataset of its kind compiled for Marathi, thus opening a new domain for\nresearch in low-resource Indo-Aryan languages. We present results from several\nmachine learning experiments on this dataset, including zero-short and other\ntransfer learning experiments on state-of-the-art cross-lingual transformers\nfrom existing data in Bengali, English, and Hindi.",
          "link": "http://arxiv.org/abs/2109.03552",
          "publishedOn": "2021-09-09T07:20:42.255Z",
          "wordCount": 581,
          "title": "Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03454",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Prang_M/0/1/0/all/0/1\">Mathieu Prang</a> (IRCAM), <a href=\"http://arxiv.org/find/cs/1/au:+Esling_P/0/1/0/all/0/1\">Philippe Esling</a>",
          "description": "A key aspect of machine learning models lies in their ability to learn\nefficient intermediate features. However, the input representation plays a\ncrucial role in this process, and polyphonic musical scores remain a\nparticularly complex type of information. In this paper, we introduce a novel\nrepresentation of symbolic music data, which transforms a polyphonic score into\na continuous signal. We evaluate the ability to learn meaningful features from\nthis representation from a musical point of view. Hence, we introduce an\nevaluation method relying on principled generation of synthetic data. Finally,\nto test our proposed representation we conduct an extensive benchmark against\nrecent polyphonic symbolic representations. We show that our signal-like\nrepresentation leads to better reconstruction and disentangled features. This\nimprovement is reflected in the metric properties and in the generation ability\nof the space learned from our signal-like representation according to music\ntheory properties.",
          "link": "http://arxiv.org/abs/2109.03454",
          "publishedOn": "2021-09-09T07:20:42.248Z",
          "wordCount": 614,
          "title": "Signal-domain representation of symbolic music for learning embedding spaces. (arXiv:2109.03454v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank\ntwice, reaD twice). The pipeline is composed of a retriever, passage reranker,\nextractive reader, generative reader and a mechanism that aggregates the final\nprediction from all system's components. We demonstrate its strength across\nthree open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,\nsurpassing state-of-the-art on the first two. Our analysis demonstrates that:\n(i) combining extractive and generative reader yields absolute improvements up\nto 5 exact match and it is at least twice as effective as the posterior\naveraging ensemble of the same models with different parameters, (ii) the\nextractive reader with fewer parameters can match the performance of the\ngenerative reader on extractive QA datasets.",
          "link": "http://arxiv.org/abs/2109.03502",
          "publishedOn": "2021-09-09T07:20:42.242Z",
          "wordCount": 582,
          "title": "R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03445",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Karandikar_R/0/1/0/all/0/1\">Rajeeva L. Karandikar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vidyasagar_M/0/1/0/all/0/1\">M. Vidyasagar</a>",
          "description": "The stochastic approximation (SA) algorithm is a widely used probabilistic\nmethod for finding a solution to an equation of the form\n$\\mathbf{f}(\\boldsymbol{\\theta}) = \\mathbf{0}$ where $\\mathbf{f} : \\mathbb{R}^d\n\\rightarrow \\mathbb{R}^d$, when only noisy measurements of $\\mathbf{f}(\\cdot)$\nare available. In the literature to date, one can make a distinction between\n\"synchronous\" updating, whereby the entire vector of the current guess\n$\\boldsymbol{\\theta}_t$ is updated at each time, and \"asynchronous\" updating,\nwhereby ony one component of $\\boldsymbol{\\theta}_t$ is updated. In convex and\nnonconvex optimization, there is also the notion of \"batch\" updating, whereby\nsome but not all components of $\\boldsymbol{\\theta}_t$ are updated at each time\n$t$. In addition, there is also a distinction between using a \"local\" clock\nversus a \"global\" clock. In the literature to date, convergence proofs when a\nlocal clock is used make the assumption that the measurement noise is an i.i.d\\\nsequence, an assumption that does not hold in Reinforcement Learning (RL).\n\nIn this note, we provide a general theory of convergence for batch\nasymchronous stochastic approximation (BASA), that works whether the updates\nuse a local clock or a global clock, for the case where the measurement noises\nform a martingale difference sequence. This is the most general result to date\nand encompasses all others.",
          "link": "http://arxiv.org/abs/2109.03445",
          "publishedOn": "2021-09-09T07:20:42.234Z",
          "wordCount": 675,
          "title": "Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03480",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonnefoy_A/0/1/0/all/0/1\">Antoine Bonnefoy</a>",
          "description": "Uncertainty in probabilistic classifiers predictions is a key concern when\nmodels are used to support human decision making, in broader probabilistic\npipelines or when sensitive automatic decisions have to be taken. Studies have\nshown that most models are not intrinsically well calibrated, meaning that\ntheir decision scores are not consistent with posterior probabilities. Hence\nbeing able to calibrate these models, or enforce calibration while learning\nthem, has regained interest in recent literature. In this context, properly\nassessing calibration is paramount to quantify new contributions tackling\ncalibration. However, there is room for improvement for commonly used metrics\nand evaluation of calibration could benefit from deeper analyses. Thus this\npaper focuses on the empirical evaluation of calibration metrics in the context\nof classification. More specifically it evaluates different estimators of the\nExpected Calibration Error ($ECE$), amongst which legacy estimators and some\nnovel ones, proposed in this paper. We build an empirical procedure to quantify\nthe quality of these $ECE$ estimators, and use it to decide which estimator\nshould be used in practice for different settings.",
          "link": "http://arxiv.org/abs/2109.03480",
          "publishedOn": "2021-09-09T07:20:42.212Z",
          "wordCount": 614,
          "title": "Estimating Expected Calibration Errors. (arXiv:2109.03480v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaojian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xilei Zhao</a>",
          "description": "Accurately forecasting ridesourcing demand is important for effective\ntransportation planning and policy-making. With the rise of Artificial\nIntelligence (AI), researchers have started to utilize machine learning models\nto forecast travel demand, which, in many cases, can produce higher prediction\naccuracy than statistical models. However, most existing machine-learning\nstudies used a global model to predict the demand and ignored the influence of\nspatial heterogeneity (i.e., the spatial variations in the impacts of\nexplanatory variables). Spatial heterogeneity can drive the parameter\nestimations varying over space; failing to consider the spatial variations may\nlimit the model's prediction performance. To account for spatial heterogeneity,\nthis study proposes a Clustering-aided Ensemble Method (CEM) to forecast the\nzone-to-zone (census-tract-to-census-tract) travel demand for ridesourcing\nservices. Specifically, we develop a clustering framework to split the\norigin-destination pairs into different clusters and ensemble the\ncluster-specific machine learning models for prediction. We implement and test\nthe proposed methodology by using the ridesourcing-trip data in Chicago. The\nresults show that, with a more transparent and flexible model structure, the\nCEM significantly improves the prediction accuracy than the benchmark models\n(i.e., global machine-learning and statistical models directly trained on all\nobservations). This study offers transportation researchers and practitioners a\nnew methodology of travel demand forecasting, especially for new travel modes\nlike ridesourcing and micromobility.",
          "link": "http://arxiv.org/abs/2109.03433",
          "publishedOn": "2021-09-09T07:20:42.206Z",
          "wordCount": 662,
          "title": "A Clustering-aided Ensemble Method for Predicting Ridesourcing Demand in Chicago. (arXiv:2109.03433v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hardt_M/0/1/0/all/0/1\">Michaela Hardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaoyi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelman_J/0/1/0/all/0/1\">Jason Gelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gollaprolu_S/0/1/0/all/0/1\">Satish Gollaprolu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">John He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larroy_P/0/1/0/all/0/1\">Pedro Larroy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_N/0/1/0/all/0/1\">Nick McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_A/0/1/0/all/0/1\">Ashish Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rees_S/0/1/0/all/0/1\">Scott Rees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siva_A/0/1/0/all/0/1\">Ankit Siva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_E/0/1/0/all/0/1\">ErhYuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasist_K/0/1/0/all/0/1\">Keerthan Vasist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_P/0/1/0/all/0/1\">Pinar Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sanjiv Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haas_K/0/1/0/all/0/1\">Kevin Haas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_T/0/1/0/all/0/1\">Tyler Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>",
          "description": "Understanding the predictions made by machine learning (ML) models and their\npotential biases remains a challenging and labor-intensive task that depends on\nthe application, the dataset, and the specific model. We present Amazon\nSageMaker Clarify, an explainability feature for Amazon SageMaker that launched\nin December 2020, providing insights into data and ML models by identifying\nbiases and explaining predictions. It is deeply integrated into Amazon\nSageMaker, a fully managed service that enables data scientists and developers\nto build, train, and deploy ML models at any scale. Clarify supports bias\ndetection and feature importance computation across the ML lifecycle, during\ndata preparation, model evaluation, and post-deployment monitoring. We outline\nthe desiderata derived from customer input, the modular architecture, and the\nmethodology for bias and explanation computations. Further, we describe the\ntechnical challenges encountered and the tradeoffs we had to make. For\nillustration, we discuss two customer use cases. We present our deployment\nresults including qualitative customer feedback and a quantitative evaluation.\nFinally, we summarize lessons learned, and discuss best practices for the\nsuccessful adoption of fairness and explanation tools in practice.",
          "link": "http://arxiv.org/abs/2109.03285",
          "publishedOn": "2021-09-09T07:20:42.199Z",
          "wordCount": 685,
          "title": "Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud. (arXiv:2109.03285v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03699",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Rongrong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shaofeng Zou</a>",
          "description": "Actor-critic (AC) algorithms have been widely adopted in decentralized\nmulti-agent systems to learn the optimal joint control policy. However,\nexisting decentralized AC algorithms either do not preserve the privacy of\nagents or are not sample and communication-efficient. In this work, we develop\ntwo decentralized AC and natural AC (NAC) algorithms that are private, and\nsample and communication-efficient. In both algorithms, agents share noisy\ninformation to preserve privacy and adopt mini-batch updates to improve sample\nand communication efficiency. Particularly for decentralized NAC, we develop a\ndecentralized Markovian SGD algorithm with an adaptive mini-batch size to\nefficiently compute the natural policy gradient. Under Markovian sampling and\nlinear function approximation, we prove the proposed decentralized AC and NAC\nalgorithms achieve the state-of-the-art sample complexities\n$\\mathcal{O}\\big(\\epsilon^{-2}\\ln(\\epsilon^{-1})\\big)$ and\n$\\mathcal{O}\\big(\\epsilon^{-3}\\ln(\\epsilon^{-1})\\big)$, respectively, and the\nsame small communication complexity\n$\\mathcal{O}\\big(\\epsilon^{-1}\\ln(\\epsilon^{-1})\\big)$. Numerical experiments\ndemonstrate that the proposed algorithms achieve lower sample and communication\ncomplexities than the existing decentralized AC algorithm.",
          "link": "http://arxiv.org/abs/2109.03699",
          "publishedOn": "2021-09-09T07:20:42.192Z",
          "wordCount": 602,
          "title": "Sample and Communication-Efficient Decentralized Actor-Critic Algorithms with Finite-Time Analysis. (arXiv:2109.03699v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangdong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dandan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kunpeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haoyuan Hu</a>",
          "description": "As online shopping prevails and e-commerce platforms emerge, there is a\ntremendous number of parcels being transported every day. Thus, it is crucial\nfor the logistics industry on how to assign a candidate logistics route for\neach shipping parcel properly as it leaves a significant impact on the total\nlogistics cost optimization and business constraints satisfaction such as\ntransit hub capacity and delivery proportion of delivery providers. This online\nroute-assignment problem can be viewed as a constrained online decision-making\nproblem. Notably, the large amount (beyond ${10^5}$) of daily parcels, the\nvariability and non-Markovian characteristics of parcel information impose\ndifficulties on attaining (near-) optimal solution without violating\nconstraints excessively. In this paper, we develop a model-free DRL approach\nnamed PPO-RA, in which Proximal Policy Optimization (PPO) is improved with\ndedicated techniques to address the challenges for route assignment (RA). The\nactor and critic networks use attention mechanism and parameter sharing to\naccommodate each incoming parcel with varying numbers and identities of\ncandidate routes, without modeling non-Markovian parcel arriving dynamics since\nwe make assumption of i.i.d. parcel arrival. We use recorded delivery parcel\ndata to evaluate the performance of PPO-RA by comparing it with widely-used\nbaselines via simulation. The results show the capability of the proposed\napproach to achieve considerable cost savings while satisfying most\nconstraints.",
          "link": "http://arxiv.org/abs/2109.03467",
          "publishedOn": "2021-09-09T07:20:42.184Z",
          "wordCount": 675,
          "title": "A Deep Reinforcement Learning Approach for Constrained Online Logistics Route Assignment. (arXiv:2109.03467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Holzinger_F/0/1/0/all/0/1\">Florian Holzinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>",
          "description": "Monitoring critical components of systems is a crucial step towards failure\nsafety. Affordable sensors are available and the industry is in the process of\nintroducing and extending monitoring solutions to improve product quality.\nOften, no expertise of how much data is required for a certain task (e.g.\nmonitoring) exists. Especially in vital machinery, a trend to exaggerated\nsensors may be noticed, both in quality and in quantity. This often results in\nan excessive generation of data, which should be transferred, processed and\nstored nonetheless. In a previous case study, several sensors have been mounted\non a healthy radial fan, which was later artificially damaged. The gathered\ndata was used for modeling (and therefore monitoring) a healthy state. The\nmodels were evaluated on a dataset created by using a faulty impeller. This\npaper focuses on the reduction of this data through downsampling and binning.\nDifferent models are created with linear regression and random forest\nregression and the resulting difference in quality is discussed.",
          "link": "http://arxiv.org/abs/2109.03468",
          "publishedOn": "2021-09-09T07:20:42.166Z",
          "wordCount": 650,
          "title": "Preprocessing and Modeling of Radial Fan Data for Health State Prediction. (arXiv:2109.03468v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03697",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Wen_G/0/1/0/all/0/1\">Gege Wen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Benson_S/0/1/0/all/0/1\">Sally M. Benson</a>",
          "description": "Numerical simulation of multiphase flow in porous media is essential for many\ngeoscience applications. However, due to the multi-physics, non-linear, and\nmulti-scale problem nature, these simulations are very expensive at desirable\ngrid resolutions, and the computational cost often impedes rigorous engineering\ndecision-making. Machine learning methods provide faster alternatives to\ntraditional simulators by training neural network models with numerical\nsimulation data mappings. Traditional convolutional neural network (CNN)-based\nmodels are accurate yet data-intensive and are prone to overfitting. Here we\npresent a new architecture, U-FNO, an enhanced Fourier neural operator for\nsolving the multiphase flow problem. The U-FNO is designed based on the Fourier\nneural operator (FNO) that learns an integral kernel in the Fourier space.\nThrough a systematic comparison among a CNN benchmark and three types of FNO\nvariations on a CO2-water multiphase problem in the context of CO2 geological\nstorage, we show that the U-FNO architecture has the advantages of both\ntraditional CNN and original FNO, providing significantly more accurate and\nefficient performance than previous architectures. The trained U-FNO provides\ngas saturation and pressure buildup predictions with a 10,000 times speedup\ncompared to traditional numerical simulators while maintaining similar\naccuracy.",
          "link": "http://arxiv.org/abs/2109.03697",
          "publishedOn": "2021-09-09T07:20:42.158Z",
          "wordCount": 648,
          "title": "U-FNO -- an enhanced Fourier neural operator based-deep learning model for multiphase flow. (arXiv:2109.03697v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03582",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Salvi_C/0/1/0/all/0/1\">Cristopher Salvi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lemercier_M/0/1/0/all/0/1\">Maud Lemercier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hovarth_B/0/1/0/all/0/1\">Blanka Hovarth</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1\">Theodoros Damoulas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lyons_T/0/1/0/all/0/1\">Terry Lyons</a>",
          "description": "Stochastic processes are random variables with values in some space of paths.\nHowever, reducing a stochastic process to a path-valued random variable ignores\nits filtration, i.e. the flow of information carried by the process through\ntime. By conditioning the process on its filtration, we introduce a family of\nhigher order kernel mean embeddings (KMEs) that generalizes the notion of KME\nand captures additional information related to the filtration. We derive\nempirical estimators for the associated higher order maximum mean discrepancies\n(MMDs) and prove consistency. We then construct a filtration-sensitive kernel\ntwo-sample test able to pick up information that gets missed by the standard\nMMD test. In addition, leveraging our higher order MMDs we construct a family\nof universal kernels on stochastic processes that allows to solve real-world\ncalibration and optimal stopping problems in quantitative finance (such as the\npricing of American options) via classical kernel-based regression methods.\nFinally, adapting existing tests for conditional independence to the case of\nstochastic processes, we design a causal-discovery algorithm to recover the\ncausal graph of structural dependencies among interacting bodies solely from\nobservations of their multidimensional trajectories.",
          "link": "http://arxiv.org/abs/2109.03582",
          "publishedOn": "2021-09-09T07:20:42.151Z",
          "wordCount": 645,
          "title": "Higher Order Kernel Mean Embeddings to Capture Filtrations of Stochastic Processes. (arXiv:2109.03582v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frady_E/0/1/0/all/0/1\">E. Paxon Frady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1\">Denis Kleyko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kymn_C/0/1/0/all/0/1\">Christopher J. Kymn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olshausen_B/0/1/0/all/0/1\">Bruno A. Olshausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sommer_F/0/1/0/all/0/1\">Friedrich T. Sommer</a>",
          "description": "Vector space models for symbolic processing that encode symbols by random\nvectors have been proposed in cognitive science and connectionist communities\nunder the names Vector Symbolic Architecture (VSA), and, synonymously,\nHyperdimensional (HD) computing. In this paper, we generalize VSAs to function\nspaces by mapping continuous-valued data into a vector space such that the\ninner product between the representations of any two data points represents a\nsimilarity kernel. By analogy to VSA, we call this new function encoding and\ncomputing framework Vector Function Architecture (VFA). In VFAs, vectors can\nrepresent individual data points as well as elements of a function space (a\nreproducing kernel Hilbert space). The algebraic vector operations, inherited\nfrom VSA, correspond to well-defined operations in function space. Furthermore,\nwe study a previously proposed method for encoding continuous data, fractional\npower encoding (FPE), which uses exponentiation of a random base vector to\nproduce randomized representations of data points and fulfills the kernel\nproperties for inducing a VFA. We show that the distribution from which\nelements of the base vector are sampled determines the shape of the FPE kernel,\nwhich in turn induces a VFA for computing with band-limited functions. In\nparticular, VFAs provide an algebraic framework for implementing large-scale\nkernel machines with random features, extending Rahimi and Recht, 2007.\nFinally, we demonstrate several applications of VFA models to problems in image\nrecognition, density estimation and nonlinear regression. Our analyses and\nresults suggest that VFAs constitute a powerful new framework for representing\nand manipulating functions in distributed neural systems, with myriad\napplications in artificial intelligence.",
          "link": "http://arxiv.org/abs/2109.03429",
          "publishedOn": "2021-09-09T07:20:42.142Z",
          "wordCount": 719,
          "title": "Computing on Functions Using Randomized Vector Representations. (arXiv:2109.03429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03661",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pilikos_G/0/1/0/all/0/1\">Georgios Pilikos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korte_C/0/1/0/all/0/1\">Chris L. de Korte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leeuwen_T/0/1/0/all/0/1\">Tristan van Leeuwen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lucka_F/0/1/0/all/0/1\">Felix Lucka</a>",
          "description": "In plane-wave imaging, multiple unfocused ultrasound waves are transmitted\ninto a medium of interest from different angles and an image is formed from the\nrecorded reflections. The number of plane waves used leads to a trade-off\nbetween frame-rate and image quality, with single-plane-wave (SPW) imaging\nbeing the fastest possible modality with the worst image quality. Recently,\ndeep learning methods have been proposed to improve ultrasound imaging. One\napproach is to use image-to-image networks that work on the formed image and\nanother is to directly learn a mapping from data to an image. Both approaches\nutilize purely data-driven models and require deep, expressive network\narchitectures, combined with large numbers of training samples to obtain good\nresults. Here, we propose a data-to-image architecture that incorporates a\nwave-physics-based image formation algorithm in-between deep convolutional\nneural networks. To achieve this, we implement the Fourier (FK) migration\nmethod as network layers and train the whole network end-to-end. We compare our\nproposed data-to-image network with an image-to-image network in simulated data\nexperiments, mimicking a medical ultrasound application. Experiments show that\nit is possible to obtain high-quality SPW images, almost similar to an image\nformed using 75 plane waves over an angular range of $\\pm$16$^\\circ$. This\nillustrates the great potential of combining deep neural networks with\nphysics-based image formation algorithms for SPW imaging.",
          "link": "http://arxiv.org/abs/2109.03661",
          "publishedOn": "2021-09-09T07:20:42.114Z",
          "wordCount": 677,
          "title": "Single Plane-Wave Imaging using Physics-Based Deep Learning. (arXiv:2109.03661v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03484",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dertli_H/0/1/0/all/0/1\">Hasan Dertli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>",
          "description": "Face anti-spoofing is essential to prevent false facial verification by using\na photo, video, mask, or a different substitute for an authorized person's\nface. Most of the state-of-the-art presentation attack detection (PAD) systems\nsuffer from overfitting, where they achieve near-perfect scores on a single\ndataset but fail on a different dataset with more realistic data. This problem\ndrives researchers to develop models that perform well under real-world\nconditions. This is an especially challenging problem for frame-based\npresentation attack detection systems that use convolutional neural networks\n(CNN). To this end, we propose a new PAD approach, which combines pixel-wise\nbinary supervision with patch-based CNN. We believe that training a CNN with\nface patches allows the model to distinguish spoofs without learning background\nor dataset-specific traces. We tested the proposed method both on the standard\nbenchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.\nThe proposed approach shows its superiority on challenging experimental setups.\nNamely, it achieves higher performance on OULU-NPU protocol 3, 4 and on\ninter-dataset real-world experiments.",
          "link": "http://arxiv.org/abs/2109.03484",
          "publishedOn": "2021-09-09T07:20:42.106Z",
          "wordCount": 641,
          "title": "Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anupam K. Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1\">Laurence Aitchison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1\">Nathan F. Lepora</a>",
          "description": "Robotic touch, particularly when using soft optical tactile sensors, suffers\nfrom distortion caused by motion-dependent shear. The manner in which the\nsensor contacts a stimulus is entangled with the tactile information about the\ngeometry of the stimulus. In this work, we propose a supervised convolutional\ndeep neural network model that learns to disentangle, in the latent space, the\ncomponents of sensor deformations caused by contact geometry from those due to\nsliding-induced shear. The approach is validated by reconstructing unsheared\ntactile images from sheared images and showing they match unsheared tactile\nimages collected with no sliding motion. In addition, the unsheared tactile\nimages give a faithful reconstruction of the contact geometry that is not\npossible from the sheared data, and robust estimation of the contact pose that\ncan be used for servo control sliding around various 2D shapes. Finally, the\ncontact geometry reconstruction in conjunction with servo control sliding were\nused for faithful full object reconstruction of various 2D shapes. The methods\nhave broad applicability to deep learning models for robots with a\nshear-sensitive sense of touch.",
          "link": "http://arxiv.org/abs/2109.03615",
          "publishedOn": "2021-09-09T07:20:42.098Z",
          "wordCount": 645,
          "title": "Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03604",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ringsquandl_M/0/1/0/all/0/1\">Martin Ringsquandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellami_H/0/1/0/all/0/1\">Houssem Sellami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_M/0/1/0/all/0/1\">Marcel Hildebrandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_D/0/1/0/all/0/1\">Dagmar Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henselmeyer_S/0/1/0/all/0/1\">Sylwia Henselmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Sebastian Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joblin_M/0/1/0/all/0/1\">Mitchell Joblin</a>",
          "description": "The application of graph neural networks (GNNs) to the domain of electrical\npower grids has high potential impact on smart grid monitoring. Even though\nthere is a natural correspondence of power flow to message-passing in GNNs,\ntheir performance on power grids is not well-understood. We argue that there is\na gap between GNN research driven by benchmarks which contain graphs that\ndiffer from power grids in several important aspects. Additionally, inductive\nlearning of GNNs across multiple power grid topologies has not been explored\nwith real-world data. We address this gap by means of (i) defining power grid\ngraph datasets in inductive settings, (ii) an exploratory analysis of graph\nproperties, and (iii) an empirical study of the concrete learning task of state\nestimation on real-world power grids. Our results show that GNNs are more\nrobust to noise with up to 400% lower error compared to baselines. Furthermore,\ndue to the unique properties of electrical grids, we do not observe the well\nknown over-smoothing phenomenon of GNNs and find the best performing models to\nbe exceptionally deep with up to 13 layers. This is in stark contrast to\nexisting benchmark datasets where the consensus is that 2 to 3 layer GNNs\nperform best. Our results demonstrate that a key challenge in this domain is to\neffectively handle long-range dependence.",
          "link": "http://arxiv.org/abs/2109.03604",
          "publishedOn": "2021-09-09T07:20:42.087Z",
          "wordCount": 683,
          "title": "Power to the Relational Inductive Bias: Graph Neural Networks in Electrical Power Grids. (arXiv:2109.03604v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03624",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Lajous_H/0/1/0/all/0/1\">H&#xe9;l&#xe8;ne Lajous</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Roy_C/0/1/0/all/0/1\">Christopher W. Roy</a> (1, +), <a href=\"http://arxiv.org/find/physics/1/au:+Hilbert_T/0/1/0/all/0/1\">Tom Hilbert</a> (1 and 3 and 4, +), <a href=\"http://arxiv.org/find/physics/1/au:+Dumast_P/0/1/0/all/0/1\">Priscille de Dumast</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Tourbier_S/0/1/0/all/0/1\">S&#xe9;bastien Tourbier</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Aleman_Gomez_Y/0/1/0/all/0/1\">Yasser Alem&#xe1;n-G&#xf3;mez</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Yerly_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Yerly</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Yu_T/0/1/0/all/0/1\">Thomas Yu</a> (4), <a href=\"http://arxiv.org/find/physics/1/au:+Kebiri_H/0/1/0/all/0/1\">Hamza Kebiri</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Payette_K/0/1/0/all/0/1\">Kelly Payette</a> (5 and 6), <a href=\"http://arxiv.org/find/physics/1/au:+Ledoux_J/0/1/0/all/0/1\">Jean-Baptiste Ledoux</a> (1 and 2), <a href=\"http://arxiv.org/find/physics/1/au:+Meuli_R/0/1/0/all/0/1\">Reto Meuli</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Hagmann_P/0/1/0/all/0/1\">Patric Hagmann</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Jakab_A/0/1/0/all/0/1\">Andras Jakab</a> (5 and 6), <a href=\"http://arxiv.org/find/physics/1/au:+Dunet_V/0/1/0/all/0/1\">Vincent Dunet</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Koob_M/0/1/0/all/0/1\">M&#xe9;riam Koob</a> (1), <a href=\"http://arxiv.org/find/physics/1/au:+Kober_T/0/1/0/all/0/1\">Tobias Kober</a> (1 and 3 and 4, &#xa7;), <a href=\"http://arxiv.org/find/physics/1/au:+Stuber_M/0/1/0/all/0/1\">Matthias Stuber</a> (1 and 2, &#xa7;), <a href=\"http://arxiv.org/find/physics/1/au:+Cuadra_M/0/1/0/all/0/1\">Meritxell Bach Cuadra</a> (2 and 1) ((1) Department of Radiology, Lausanne University Hospital (CHUV) and University of Lausanne (UNIL), Lausanne, Switzerland, (2) CIBM Center for Biomedical Imaging, Switzerland, (3) Advanced Clinical Imaging Technology (ACIT), Siemens Healthcare, Lausanne, Switzerland, (4) Signal Processing Laboratory 5 (LTS5), Ecole Polytechnique F&#xe9;d&#xe9;rale de Lausanne (EPFL), Lausanne, Switzerland, (5) Center for MR Research, University Children&#x27;s Hospital Zurich, University of Zurich, Zurich, Switzerland, (6) Neuroscience Center Zurich, University of Zurich, Zurich, Switzerland, (+, &#xa7;) These authors contributed equally to this work.)",
          "description": "Accurate characterization of in utero human brain maturation is critical as\nit involves complex and interconnected structural and functional processes that\nmay influence health later in life. Magnetic resonance imaging is a powerful\ntool to investigate equivocal neurological patterns during fetal development.\nHowever, the number of acquisitions of satisfactory quality available in this\ncohort of sensitive subjects remains scarce, thus hindering the validation of\nadvanced image processing techniques. Numerical phantoms can mitigate these\nlimitations by providing a controlled environment with a known ground truth. In\nthis work, we present FaBiAN, an open-source Fetal Brain magnetic resonance\nAcquisition Numerical phantom that simulates clinical T2-weighted fast spin\necho sequences of the fetal brain. This unique tool is based on a general,\nflexible and realistic setup that includes stochastic fetal movements, thus\nproviding images of the fetal brain throughout maturation comparable to\nclinical acquisitions. We demonstrate its value to evaluate the robustness and\noptimize the accuracy of an algorithm for super-resolution fetal brain magnetic\nresonance imaging from simulated motion-corrupted 2D low-resolution series as\ncompared to a synthetic high-resolution reference volume. We also show that the\nimages generated can complement clinical datasets to support data-intensive\ndeep learning methods for fetal brain tissue segmentation.",
          "link": "http://arxiv.org/abs/2109.03624",
          "publishedOn": "2021-09-09T07:20:42.051Z",
          "wordCount": 832,
          "title": "FaBiAN: A Fetal Brain magnetic resonance Acquisition Numerical phantom. (arXiv:2109.03624v1 [physics.med-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03508",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_J/0/1/0/all/0/1\">Jingtao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>",
          "description": "In the past years, significant improvements in the field of neural\narchitecture search(NAS) have been made. However, it is still challenging to\nsearch for efficient networks due to the gap between the searched constraint\nand real inference time exists. To search for a high-performance network with\nlow inference time, several previous works set a computational complexity\nconstraint for the search algorithm. However, many factors affect the speed of\ninference(e.g., FLOPs, MACs). The correlation between a single indicator and\nthe latency is not strong. Currently, some re-parameterization(Rep) techniques\nare proposed to convert multi-branch to single-path architecture which is\ninference-friendly. Nevertheless, multi-branch architectures are still\nhuman-defined and inefficient. In this work, we propose a new search space that\nis suitable for structural re-parameterization techniques. RepNAS, a one-stage\nNAS approach, is present to efficiently search the optimal diverse branch\nblock(ODBB) for each layer under the branch number constraint. Our experimental\nresults show the searched ODBB can easily surpass the manual diverse branch\nblock(DBB) with efficient training. Code and models will be available sooner.",
          "link": "http://arxiv.org/abs/2109.03508",
          "publishedOn": "2021-09-09T07:20:42.043Z",
          "wordCount": 618,
          "title": "RepNAS: Searching for Efficient Re-parameterizing Blocks. (arXiv:2109.03508v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03469",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fleck_P/0/1/0/all/0/1\">Philipp Fleck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugel_M/0/1/0/all/0/1\">Manfred K&#xfc;gel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1\">Michael Kommenda</a>",
          "description": "Industrial applications of machine learning face unique challenges due to the\nnature of raw industry data. Preprocessing and preparing raw industrial data\nfor machine learning applications is a demanding task that often takes more\ntime and work than the actual modeling process itself and poses additional\nchallenges. This paper addresses one of those challenges, specifically, the\nchallenge of missing values due to sensor unavailability at different\nproduction units of nonlinear production lines. In cases where only a small\nproportion of the data is missing, those missing values can often be imputed.\nIn cases of large proportions of missing data, imputing is often not feasible,\nand removing observations containing missing values is often the only option.\nThis paper presents a technique, that allows to utilize all of the available\ndata without the need of removing large amounts of observations where data is\nonly partially available. We do not only discuss the principal idea of the\npresented method, but also show different possible implementations that can be\napplied depending on the data at hand. Finally, we demonstrate the application\nof the presented method with data from a steel production plant.",
          "link": "http://arxiv.org/abs/2109.03469",
          "publishedOn": "2021-09-09T07:20:42.035Z",
          "wordCount": 677,
          "title": "Understanding and Preparing Data of Industrial Processes for Machine Learning Applications. (arXiv:2109.03469v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Ling Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>",
          "description": "We introduce in this work the normalizing field flows (NFF) for learning\nrandom fields from scattered measurements. More precisely, we construct a\nbijective transformation (a normalizing flow characterizing by neural networks)\nbetween a Gaussian random field with the Karhunen-Lo\\`eve (KL) expansion\nstructure and the target stochastic field, where the KL expansion coefficients\nand the invertible networks are trained by maximizing the sum of the\nlog-likelihood on scattered measurements. This NFF model can be used to solve\ndata-driven forward, inverse, and mixed forward/inverse stochastic partial\ndifferential equations in a unified framework. We demonstrate the capability of\nthe proposed NFF model for learning Non Gaussian processes and different types\nof stochastic partial differential equations.",
          "link": "http://arxiv.org/abs/2108.12956",
          "publishedOn": "2021-09-08T07:20:13.007Z",
          "wordCount": 584,
          "title": "Normalizing field flows: Solving forward and inverse stochastic differential equations using physics-informed flow models. (arXiv:2108.12956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14038",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kashefi_A/0/1/0/all/0/1\">Ali Kashefi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukerji_T/0/1/0/all/0/1\">Tapan Mukerji</a>",
          "description": "We propose a novel deep learning framework for predicting permeability of\nporous media from their digital images. Unlike convolutional neural networks,\ninstead of feeding the whole image volume as inputs to the network, we model\nthe boundary between solid matrix and pore spaces as point clouds and feed them\nas inputs to a neural network based on the PointNet architecture. This approach\novercomes the challenge of memory restriction of graphics processing units and\nits consequences on the choice of batch size, and convergence. Compared to\nconvolutional neural networks, the proposed deep learning methodology provides\nfreedom to select larger batch sizes, due to reducing significantly the size of\nnetwork inputs. Specifically, we use the classification branch of PointNet and\nadjust it for a regression task. As a test case, two and three dimensional\nsynthetic digital rock images are considered. We investigate the effect of\ndifferent components of our neural network on its performance. We compare our\ndeep learning strategy with a convolutional neural network from various\nperspectives, specifically for maximum possible batch size. We inspect the\ngeneralizability of our network by predicting the permeability of real-world\nrock samples as well as synthetic digital rocks that are statistically\ndifferent from the samples used during training. The network predicts the\npermeability of digital rocks a few thousand times faster than a Lattice\nBoltzmann solver with a high level of prediction accuracy.",
          "link": "http://arxiv.org/abs/2107.14038",
          "publishedOn": "2021-09-08T07:20:12.997Z",
          "wordCount": 694,
          "title": "Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.00784",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1\">Natalia Khanzhina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1\">Alexey Lapenok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1\">Andrey Filchenkov</a>",
          "description": "According to recent studies, commonly used computer vision datasets contain\nabout 4% of label errors. For example, the COCO dataset is known for its high\nlevel of noise in data labels, which limits its use for training robust neural\ndeep architectures in a real-world scenario. To model such a noise, in this\npaper we have proposed the homoscedastic aleatoric uncertainty estimation, and\npresent a series of novel loss functions to address the problem of image object\ndetection at scale. Specifically, the proposed functions are based on Bayesian\ninference and we have incorporated them into the common community-adopted\nobject detection deep learning architecture RetinaNet. We have also shown that\nmodeling of homoscedastic aleatoric uncertainty using our novel functions\nallows to increase the model interpretability and to improve the object\ndetection performance being evaluated on the COCO dataset.",
          "link": "http://arxiv.org/abs/2108.00784",
          "publishedOn": "2021-09-08T07:20:12.990Z",
          "wordCount": 622,
          "title": "Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12954",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhang_S/0/1/0/all/0/1\">Shengjun Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Dong_Y/0/1/0/all/0/1\">Yunlong Dong</a>, <a href=\"http://arxiv.org/find/math/1/au:+Xie_D/0/1/0/all/0/1\">Dong Xie</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yao_L/0/1/0/all/0/1\">Lisha Yao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bailey_C/0/1/0/all/0/1\">Colleen P. Bailey</a>, <a href=\"http://arxiv.org/find/math/1/au:+Fu_S/0/1/0/all/0/1\">Shengli Fu</a>",
          "description": "This paper investigates the stochastic distributed nonconvex optimization\nproblem of minimizing a global cost function formed by the summation of $n$\nlocal cost functions. We solve such a problem by involving zeroth-order (ZO)\ninformation exchange. In this paper, we propose a ZO distributed primal-dual\ncoordinate method (ZODIAC) to solve the stochastic optimization problem. Agents\napproximate their own local stochastic ZO oracle along with coordinates with an\nadaptive smoothing parameter. We show that the proposed algorithm achieves the\nconvergence rate of $\\mathcal{O}(\\sqrt{p}/\\sqrt{T})$ for general nonconvex cost\nfunctions. We demonstrate the efficiency of proposed algorithms through a\nnumerical example in comparison with the existing state-of-the-art centralized\nand distributed ZO algorithms.",
          "link": "http://arxiv.org/abs/2103.12954",
          "publishedOn": "2021-09-08T07:20:12.984Z",
          "wordCount": 585,
          "title": "Convergence Analysis of Nonconvex Distributed Stochastic Zeroth-order Coordinate Method. (arXiv:2103.12954v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.09262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murillo_R/0/1/0/all/0/1\">Raul Murillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrio_A/0/1/0/all/0/1\">Alberto A. Del Barrio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botella_G/0/1/0/all/0/1\">Guillermo Botella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min Soo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyunJin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagherzadeh_N/0/1/0/all/0/1\">Nader Bagherzadeh</a>",
          "description": "The Posit Number System was introduced in 2017 as a replacement for\nfloating-point numbers. Since then, the community has explored its application\nin Neural Network related tasks and produced some unit designs which are still\nfar from being competitive with their floating-point counterparts. This paper\nproposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to\nsignificantly reduce the complexity of posit multipliers, the most power-hungry\nunits within Deep Neural Network architectures. When comparing with\nstate-of-the-art posit multipliers, experiments show that the proposed\ntechnique reduces the area, power, and delay of hardware multipliers up to\n72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.",
          "link": "http://arxiv.org/abs/2102.09262",
          "publishedOn": "2021-09-08T07:20:12.962Z",
          "wordCount": 579,
          "title": "PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kopuklu_O/0/1/0/all/0/1\">Okan K&#xf6;p&#xfc;kl&#xfc;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taseska_M/0/1/0/all/0/1\">Maja Taseska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigoll_G/0/1/0/all/0/1\">Gerhard Rigoll</a>",
          "description": "Successful active speaker detection requires a three-stage pipeline: (i)\naudio-visual encoding for all speakers in the clip, (ii) inter-speaker relation\nmodeling between a reference speaker and the background speakers within each\nframe, and (iii) temporal modeling for the reference speaker. Each stage of\nthis pipeline plays an important role for the final performance of the created\narchitecture. Based on a series of controlled experiments, this work presents\nseveral practical guidelines for audio-visual active speaker detection.\nCorrespondingly, we present a new architecture called ASDNet, which achieves a\nnew state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%\noutperforming the second best with a large margin of 4.7%. Our code and\npretrained models are publicly available.",
          "link": "http://arxiv.org/abs/2106.03932",
          "publishedOn": "2021-09-08T07:20:12.948Z",
          "wordCount": 615,
          "title": "How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1\">David Atienza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielza_C/0/1/0/all/0/1\">Concha Bielza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larranaga_P/0/1/0/all/0/1\">Pedro Larra&#xf1;aga</a>",
          "description": "We introduce semiparametric Bayesian networks that combine parametric and\nnonparametric conditional probability distributions. Their aim is to\nincorporate the advantages of both components: the bounded complexity of\nparametric models and the flexibility of nonparametric ones. We demonstrate\nthat semiparametric Bayesian networks generalize two well-known types of\nBayesian networks: Gaussian Bayesian networks and kernel density estimation\nBayesian networks. For this purpose, we consider two different conditional\nprobability distributions required in a semiparametric Bayesian network. In\naddition, we present modifications of two well-known algorithms (greedy\nhill-climbing and PC) to learn the structure of a semiparametric Bayesian\nnetwork from data. To realize this, we employ a score function based on\ncross-validation. In addition, using a validation dataset, we apply an\nearly-stopping criterion to avoid overfitting. To evaluate the applicability of\nthe proposed algorithm, we conduct an exhaustive experiment on synthetic data\nsampled by mixing linear and nonlinear functions, multivariate normal data\nsampled from Gaussian Bayesian networks, real data from the UCI repository, and\nbearings degradation data. As a result of this experiment, we conclude that the\nproposed algorithm accurately learns the combination of parametric and\nnonparametric components, while achieving a performance comparable with those\nprovided by state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.03008",
          "publishedOn": "2021-09-08T07:20:12.941Z",
          "wordCount": 645,
          "title": "Semiparametric Bayesian Networks. (arXiv:2109.03008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.06986",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hurley_C/0/1/0/all/0/1\">Catherine B. Hurley</a>, <a href=\"http://arxiv.org/find/stat/1/au:+OConnell_M/0/1/0/all/0/1\">Mark O&#x27;Connell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Domijan_K/0/1/0/all/0/1\">Katarina Domijan</a>",
          "description": "Machine learning models fit complex algorithms to arbitrarily large datasets.\nThese algorithms are well-known to be high on performance and low on\ninterpretability. We use interactive visualization of slices of predictor space\nto address the interpretability deficit; in effect opening up the black-box of\nmachine learning algorithms, for the purpose of interrogating, explaining,\nvalidating and comparing model fits. Slices are specified directly through\ninteraction, or using various touring algorithms designed to visit\nhigh-occupancy sections or regions where the model fits have interesting\nproperties. The methods presented here are implemented in the R package\n\\pkg{condvis2}.",
          "link": "http://arxiv.org/abs/2101.06986",
          "publishedOn": "2021-09-08T07:20:12.925Z",
          "wordCount": 558,
          "title": "Interactive slice visualization for exploring machine learning models. (arXiv:2101.06986v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.07858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jarne_C/0/1/0/all/0/1\">Cecilia Jarne</a>",
          "description": "Training neural networks to perform different tasks is relevant across\nvarious disciplines that go beyond Machine Learning. In particular, Recurrent\nNeural Networks (RNN) are of great interest to different scientific\ncommunities, for example, Computational Neuroscience research and Dynamical\nSystems among others. Open-source frameworks dedicated to Machine Learning such\nas Tensorflow and Keras has produced significant changes in the development of\ntechnologies that we currently use. One relevant problem that can be approached\nis how to build the models for the study of dynamical systems, and how to\nextract the relevant information to be able to answer the scientific questions\nof interest. The purpose of the present work is to contribute to this aim by\nusing a temporal processing task, in this case, a 3-bit Flip Flop memory, to\nshow the modeling procedure in every step: from equations to the software code\nusing Tensorflow and Keras. The obtained networks are analyzed to describe the\ndynamics and to show different visualization and analysis tools. The code\ndeveloped in this work is provided to be used as a base for model other\nsystems.",
          "link": "http://arxiv.org/abs/2010.07858",
          "publishedOn": "2021-09-08T07:20:12.907Z",
          "wordCount": 663,
          "title": "What you need to know to train recurrent neural networks to make Flip Flops memories and more. (arXiv:2010.07858v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00596",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1\">Fan Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1\">Karl Rohe</a>",
          "description": "Previous versions of sparse principal component analysis (PCA) have presumed\nthat the eigen-basis (a $p \\times k$ matrix) is approximately sparse. We\npropose a method that presumes the $p \\times k$ matrix becomes approximately\nsparse after a $k \\times k$ rotation. The simplest version of the algorithm\ninitializes with the leading $k$ principal components. Then, the principal\ncomponents are rotated with an $k \\times k$ orthogonal rotation to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nprincipal components. This approach differs from prior approaches because it\nuses an orthogonal rotation to approximate a sparse basis. One consequence is\nthat a sparse component need not to be a leading eigenvector, but rather a\nmixture of them. In this way, we propose a new (rotated) basis for sparse PCA.\nIn addition, our approach avoids \"deflation\" and multiple tuning parameters\nrequired for that. Our sparse PCA framework is versatile; for example, it\nextends naturally to a two-way analysis of a data matrix for simultaneous\ndimensionality reduction of rows and columns. We provide evidence showing that\nfor the same level of sparsity, the proposed sparse PCA method is more stable\nand can explain more variance compared to alternative methods. Through three\napplications -- sparse coding of images, analysis of transcriptome sequencing\ndata, and large-scale clustering of social networks, we demonstrate the modern\nusefulness of sparse PCA in exploring multivariate data.",
          "link": "http://arxiv.org/abs/2007.00596",
          "publishedOn": "2021-09-08T07:20:12.894Z",
          "wordCount": 702,
          "title": "A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>",
          "description": "We tackle the Online 3D Bin Packing Problem, a challenging yet practically\nuseful variant of the classical Bin Packing Problem. In this problem, the items\nare delivered to the agent without informing the full sequence information.\nAgent must directly pack these items into the target bin stably without\nchanging their arrival order, and no further adjustment is permitted. Online\n3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt\ndeep reinforcement learning, in particular, the on-policy actor-critic\nframework, to solve this MDP with constrained action space. To learn a\npractically feasible packing policy, we propose three critical designs. First,\nwe propose an online analysis of packing stability based on a novel stacking\ntree. It attains a high analysis accuracy while reducing the computational\ncomplexity from $O(N^2)$ to $O(N \\log N)$, making it especially suited for RL\ntraining. Second, we propose a decoupled packing policy learning for different\ndimensions of placement which enables high-resolution spatial discretization\nand hence high packing precision. Third, we introduce a reward function that\ndictates the robot to place items in a far-to-near order and therefore\nsimplifies the collision avoidance in movement planning of the robotic arm.\nFurthermore, we provide a comprehensive discussion on several key implemental\nissues. The extensive evaluation demonstrates that our learned policy\noutperforms the state-of-the-art methods significantly and is practically\nusable for real-world applications.",
          "link": "http://arxiv.org/abs/2108.13680",
          "publishedOn": "2021-09-08T07:20:12.888Z",
          "wordCount": 684,
          "title": "Learning Practically Feasible Policies for Online 3D Bin Packing. (arXiv:2108.13680v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1\">Alexandre Rame</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>",
          "description": "Learning robust models that generalize well under changes in the data\ndistribution is critical for real-world applications. To this end, there has\nbeen a growing surge of interest to learn simultaneously from multiple training\ndomains - while enforcing different types of invariance across those domains.\nYet, all existing approaches fail to show systematic benefits under fair\nevaluation protocols. In this paper, we propose a new learning scheme to\nenforce domain invariance in the space of the gradients of the loss function:\nspecifically, we introduce a regularization term that matches the domain-level\nvariances of gradients across training domains. Critically, our strategy, named\nFishr, exhibits close relations with the Fisher Information and the Hessian of\nthe loss. We show that forcing domain-level gradient covariances to be similar\nduring the learning procedure eventually aligns the domain-level loss\nlandscapes locally around the final weights. Extensive experiments demonstrate\nthe effectiveness of Fishr for out-of-distribution generalization. In\nparticular, Fishr improves the state of the art on the DomainBed benchmark and\nperforms significantly better than Empirical Risk Minimization. The code is\nreleased at https://github.com/alexrame/fishr.",
          "link": "http://arxiv.org/abs/2109.02934",
          "publishedOn": "2021-09-08T07:20:12.881Z",
          "wordCount": 631,
          "title": "Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08889",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Sharma_A/0/1/0/all/0/1\">Arun K. Sharma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Verma_N/0/1/0/all/0/1\">Nishchal K. Verma</a>",
          "description": "The fault diagnostic model trained for a laboratory case machine fails to\nperform well on the industrial machines running under variable operating\nconditions. For every new operating condition of such machines, a new\ndiagnostic model has to be trained which is a time-consuming and uneconomical\nprocess. Therefore, we propose a quick learning mechanism that can transform\nthe existing diagnostic model into a new model suitable for industrial machines\noperating in different conditions. The proposed method uses the Net2Net\ntransformation followed by a fine-tuning to cancel/minimize the maximum mean\ndiscrepancy between the new data and the previous one. The fine-tuning of the\nmodel requires a very less amount of labelled target samples and very few\niterations of training. Therefore, the proposed method is capable of learning\nthe new target data pattern quickly. The effectiveness of the proposed fault\ndiagnosis method has been demonstrated on the Case Western Reserve University\ndataset, Intelligent Maintenance Systems bearing dataset, and Paderborn\nuniversity dataset under the wide variations of the operating conditions. It\nhas been validated that the diagnostic model trained on artificially damaged\nfault datasets can be used to quickly train another model for a real damage\ndataset.",
          "link": "http://arxiv.org/abs/2103.08889",
          "publishedOn": "2021-09-08T07:20:12.864Z",
          "wordCount": 661,
          "title": "Quick Learning Mechanism with Cross-Domain Adaptation for Intelligent Fault Diagnosis. (arXiv:2103.08889v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.12551",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Cao_J/0/1/0/all/0/1\">Jay Cao</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Chen_J/0/1/0/all/0/1\">Jacky Chen</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Hull_J/0/1/0/all/0/1\">John Hull</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Poulos_Z/0/1/0/all/0/1\">Zissis Poulos</a>",
          "description": "A common approach to valuing exotic options involves choosing a model and\nthen determining its parameters to fit the volatility surface as closely as\npossible. We refer to this as the model calibration approach (MCA). A\ndisadvantage of MCA is that some information in the volatility surface is lost\nduring the calibration process and the prices of exotic options will not in\ngeneral be consistent with those of plain vanilla options. We consider an\nalternative approach where the structure of the user's preferred model is\npreserved but points on the volatility are features input to a neural network.\nWe refer to this as the volatility feature approach (VFA) model. We conduct\nexperiments showing that VFA can be expected to outperform MCA for the\nvolatility surfaces encountered in practice. Once the upfront computational\ntime has been invested in developing the neural network, the valuation of\nexotic options using VFA is very fast.",
          "link": "http://arxiv.org/abs/2103.12551",
          "publishedOn": "2021-09-08T07:20:12.856Z",
          "wordCount": 611,
          "title": "Deep Learning for Exotic Option Valuation. (arXiv:2103.12551v2 [q-fin.CP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.02394",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yulin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiayi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>",
          "description": "Deep learning based semi-supervised learning (SSL) algorithms have led to\npromising results in recent years. However, they tend to introduce multiple\ntunable hyper-parameters, making them less practical in real SSL scenarios\nwhere the labeled data is scarce for extensive hyper-parameter search. In this\npaper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that\nrequires tuning only one additional hyper-parameter, compared with a standard\nsupervised deep learning algorithm, to achieve competitive performance under\nvarious conditions of SSL. We start by defining a meta optimization problem\nthat minimizes the loss on labeled data through dynamically reweighting the\nloss on unlabeled samples, which are associated with soft pseudo labels during\ntraining. As the meta problem is computationally intensive to solve directly,\nwe propose an efficient algorithm to dynamically obtain the approximate\nsolutions. We show theoretically that Meta-Semi converges to the stationary\npoint of the loss function on labeled data under mild conditions. Empirically,\nMeta-Semi outperforms state-of-the-art SSL algorithms significantly on the\nchallenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves\ncompetitive performance on CIFAR-10 and SVHN.",
          "link": "http://arxiv.org/abs/2007.02394",
          "publishedOn": "2021-09-08T07:20:12.849Z",
          "wordCount": 685,
          "title": "Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00234",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>",
          "description": "Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.",
          "link": "http://arxiv.org/abs/2101.00234",
          "publishedOn": "2021-09-08T07:20:12.842Z",
          "wordCount": 587,
          "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.03831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Joon Kwon</a>",
          "description": "Blackwell's approachability is a framework where two players, the Decision\nMaker and the Environment, play a repeated game with vector-valued payoffs. The\ngoal of the Decision Maker is to make the average payoff converge to a given\nset called the target. When this is indeed possible, simple algorithms which\nguarantee the convergence are known. This abstract tool was successfully used\nfor the construction of optimal strategies in various repeated games, but also\nfound several applications in online learning. By extending an approach\nproposed by (Abernethy et al., 2011), we construct and analyze a class of\nFollow the Regularized Leader algorithms (FTRL) for Blackwell's approachability\nwhich are able to minimize not only the Euclidean distance to the target set\n(as it is often the case in the context of Blackwell's approachability) but a\nwide range of distance-like quantities. This flexibility enables us to apply\nthese algorithms to closely minimize the quantity of interest in various online\nlearning problems. In particular, for regret minimization with $\\ell_p$ global\ncosts, we obtain the first bounds with explicit dependence in $p$ and the\ndimension $d$.",
          "link": "http://arxiv.org/abs/2009.03831",
          "publishedOn": "2021-09-08T07:20:12.836Z",
          "wordCount": 675,
          "title": "Refined approachability algorithms and application to regret minimization with global costs. (arXiv:2009.03831v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1\">Mohammadreza Armandpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kidd_B/0/1/0/all/0/1\">Brian Kidd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianhua Z. Huang</a>",
          "description": "In this paper, we study the problem of blood glucose forecasting and provide\na deep personalized solution. Predicting blood glucose level in people with\ndiabetes has significant value because health complications of abnormal glucose\nlevel are serious, sometimes even leading to death. Therefore, having a model\nthat can accurately and quickly warn patients of potential problems is\nessential. To develop a better deep model for blood glucose forecasting, we\nanalyze the data and detect important patterns. These observations helped us to\npropose a method that has several key advantages over existing methods: 1- it\nlearns a personalized model for each patient as well as a global model; 2- it\nuses an attention mechanism and extracted time features to better learn\nlong-term dependencies in the data; 3- it introduces a new, robust training\nprocedure for time series data. We empirically show the efficacy of our model\non a real dataset.",
          "link": "http://arxiv.org/abs/2106.00884",
          "publishedOn": "2021-09-08T07:20:12.828Z",
          "wordCount": 632,
          "title": "Deep Personalized Glucose Level Forecasting Using Attention-based Recurrent Neural Networks. (arXiv:2106.00884v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>",
          "description": "Training sample re-weighting is an effective approach for tackling data\nbiases such as imbalanced and corrupted labels. Recent methods develop\nlearning-based algorithms to learn sample re-weighting strategies jointly with\nmodel training based on the frameworks of reinforcement learning and meta\nlearning. However, depending on additional unbiased reward data is limiting\ntheir general applicability. Furthermore, existing learning-based sample\nre-weighting methods require nested optimizations of models and weighting\nparameters, which requires expensive second-order computation. This paper\naddresses these two problems and presents a novel learning-based fast sample\nre-weighting (FSR) method that does not require additional reward data. The\nmethod is based on two key ideas: learning from history to build proxy reward\ndata and feature sharing to reduce the optimization cost. Our experiments show\nthe proposed method achieves competitive results compared to state of the arts\non label noise robustness and long-tailed recognition, and does so while\nachieving significantly improved training efficiency. The source code is\npublicly available at\nhttps://github.com/google-research/google-research/tree/master/ieg.",
          "link": "http://arxiv.org/abs/2109.03216",
          "publishedOn": "2021-09-08T07:20:12.807Z",
          "wordCount": 602,
          "title": "Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.05901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>",
          "description": "The Jeffreys divergence is a renown symmetrization of the oriented\nKullback-Leibler divergence broadly used in information sciences. Since the\nJeffreys divergence between Gaussian mixture models is not available in\nclosed-form, various techniques with pros and cons have been proposed in the\nliterature to either estimate, approximate, or lower and upper bound this\ndivergence. In this paper, we propose a simple yet fast heuristic to\napproximate the Jeffreys divergence between two univariate Gaussian mixtures\nwith arbitrary number of components. Our heuristic relies on converting the\nmixtures into pairs of dually parameterized probability densities belonging to\nan exponential family. In particular, we consider the versatile polynomial\nexponential family densities, and design a divergence to measure in closed-form\nthe goodness of fit between a Gaussian mixture and its polynomial exponential\ndensity approximation. This goodness-of-fit divergence is a generalization of\nthe Hyv\\\"arinen divergence used to estimate models with computationally\nintractable normalizers. It allows us to perform model selection by choosing\nthe orders of the polynomial exponential densities used to approximate the\nmixtures. We demonstrate experimentally that our heuristic to approximate the\nJeffreys divergence improves by several orders of magnitude the computational\ntime of stochastic Monte Carlo estimations while approximating reasonably well\nthe Jeffreys divergence, specially when the mixtures have a very small number\nof modes. Besides, our mixture-to-exponential family conversion techniques may\nprove useful in other settings.",
          "link": "http://arxiv.org/abs/2107.05901",
          "publishedOn": "2021-09-08T07:20:12.800Z",
          "wordCount": 714,
          "title": "Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v3 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tifrea_A/0/1/0/all/0/1\">Alexandru &#x162;ifrea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavarache_E/0/1/0/all/0/1\">Eric Stavarache</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fanny Yang</a>",
          "description": "Despite their excellent performance on in-distribution (ID) data, machine\nlearning-based prediction systems often predict out-of-distribution (OOD)\nsamples incorrectly while indicating high confidence. Instead, they should flag\nsamples that are not similar to the training data, for example, when new\nclasses emerge over time. Even though current OOD detection algorithms can\nsuccessfully distinguish completely different data sets, they fail to reliably\nidentify samples from novel classes. We develop a new ensemble-based procedure\nthat promotes model diversity and exploits regularization to limit disagreement\nto only OOD samples, using a batch containing an unknown mixture of ID and OOD\ndata. We show that our procedure significantly outperforms state-of-the-art\nmethods, including those that have access, during training, to data that is\nknown to be OOD. We run extensive comparisons of our approach on a variety of\nnovel-class detection scenarios, on standard image data sets such as\nSVHN/CIFAR-10/CIFAR-100, as well as on new disease detection on medical image\ndata sets.",
          "link": "http://arxiv.org/abs/2012.05825",
          "publishedOn": "2021-09-08T07:20:12.793Z",
          "wordCount": 617,
          "title": "Novelty detection using ensembles with regularized disagreement. (arXiv:2012.05825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xingen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Changle Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zhen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chih-Min Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longzhi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_C/0/1/0/all/0/1\">Changjing Shang</a>",
          "description": "On error of value function inevitably causes an overestimation phenomenon and\nhas a negative impact on the convergence of the algorithms. To mitigate the\nnegative effects of the approximation error, we propose Error Controlled\nActor-critic which ensures confining the approximation error in value function.\nWe present an analysis of how the approximation error can hinder the\noptimization process of actor-critic methods.Then, we derive an upper boundary\nof the approximation error of Q function approximator and find that the error\ncan be lowered by restricting on the KL-divergence between every two\nconsecutive policies when training the policy. The results of experiments on a\nrange of continuous control tasks demonstrate that the proposed actor-critic\nalgorithm apparently reduces the approximation error and significantly\noutperforms other model-free RL algorithms.",
          "link": "http://arxiv.org/abs/2109.02517",
          "publishedOn": "2021-09-08T07:20:12.786Z",
          "wordCount": 580,
          "title": "Error Controlled Actor-Critic. (arXiv:2109.02517v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tangjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zehao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenglong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zuoqiang Shi</a>",
          "description": "Interpreting deep neural networks from the ordinary differential equations\n(ODEs) perspective has inspired many efficient and robust network\narchitectures. However, existing ODE based approaches ignore the relationship\namong data points, which is a critical component in many problems including\nfew-shot learning and semi-supervised learning. In this paper, inspired by the\ndiffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to\nstrengthen the interactions among data points. Under the structured data\nassumption, it is proved that the diffusion mechanism can decrease the\ndistance-diameter ratio that improves the separability of inter-class points\nand reduces the distance among local intra-class points. This property can be\neasily adopted by the residual networks for constructing the separable\nhyperplanes. The synthetic binary classification experiments demonstrate the\neffectiveness of the proposed diffusion mechanism. Moreover, extensive\nexperiments of few-shot image classification and semi-supervised graph node\nclassification in various datasets validate the advantages of the proposed\nDiff-ResNet over existing few-shot learning methods.",
          "link": "http://arxiv.org/abs/2105.03155",
          "publishedOn": "2021-09-08T07:20:12.767Z",
          "wordCount": 620,
          "title": "Diff-ResNets for Few-shot Learning -- an ODE Perspective. (arXiv:2105.03155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_E/0/1/0/all/0/1\">Enda Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Dezun Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yemao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Shuo Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1\">Xiangke Liao</a>",
          "description": "Communication overhead is the key challenge for distributed training.\nGradient compression is a widely used approach to reduce communication traffic.\nWhen combining with parallel communication mechanism method like pipeline,\ngradient compression technique can greatly alleviate the impact of\ncommunication overhead. However, there exists two problems of gradient\ncompression technique to be solved. Firstly, gradient compression brings in\nextra computation cost, which will delay the next training iteration. Secondly,\ngradient compression usually leads to the decrease of convergence accuracy.",
          "link": "http://arxiv.org/abs/2106.10796",
          "publishedOn": "2021-09-08T07:20:12.758Z",
          "wordCount": 558,
          "title": "CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation. (arXiv:2106.10796v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Ahi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1\">Sheng-hua Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>",
          "description": "The data scarcity problem in Electroencephalography (EEG) based affective\ncomputing results into difficulty in building an effective model with high\naccuracy and stability using machine learning algorithms especially deep\nlearning models. Data augmentation has recently achieved considerable\nperformance improvement for deep learning models: increased accuracy,\nstability, and reduced over-fitting. In this paper, we propose a novel data\naugmentation framework, namely Generative Adversarial Network-based\nSelf-supervised Data Augmentation (GANSER). As the first to combine adversarial\ntraining with self-supervised learning for EEG-based emotion recognition, the\nproposed framework can generate high-quality and high-diversity simulated EEG\nsamples. In particular, we utilize adversarial training to learn an EEG\ngenerator and force the generated EEG signals to approximate the distribution\nof real samples, ensuring the quality of augmented samples. A transformation\nfunction is employed to mask parts of EEG signals and force the generator to\nsynthesize potential EEG signals based on the remaining parts, to produce a\nwide variety of samples. The masking possibility during transformation is\nintroduced as prior knowledge to guide to extract distinguishable features for\nsimulated EEG signals and generalize the classifier to the augmented sample\nspace. Finally, extensive experiments demonstrate our proposed method can help\nemotion recognition for performance gain and achieve state-of-the-art results.",
          "link": "http://arxiv.org/abs/2109.03124",
          "publishedOn": "2021-09-08T07:20:12.739Z",
          "wordCount": 650,
          "title": "GANSER: A Self-supervised Data Augmentation Framework for EEG-based Emotion Recognition. (arXiv:2109.03124v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.06201",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Huang_H/0/1/0/all/0/1\">He-Liang Huang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Gong_M/0/1/0/all/0/1\">Ming Gong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhao_Y/0/1/0/all/0/1\">Youwei Zhao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1\">Yulin Wu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_S/0/1/0/all/0/1\">Shaowei Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liang_F/0/1/0/all/0/1\">Futian Liang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lin_J/0/1/0/all/0/1\">Jin Lin</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Xu_Y/0/1/0/all/0/1\">Yu Xu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Hsieh_M/0/1/0/all/0/1\">Min-Hsiu Hsieh</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rong_H/0/1/0/all/0/1\">Hao Rong</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Peng_C/0/1/0/all/0/1\">Cheng-Zhi Peng</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lu_C/0/1/0/all/0/1\">Chao-Yang Lu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Ao Chen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobo Zhu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Pan_J/0/1/0/all/0/1\">Jian-Wei Pan</a>",
          "description": "Quantum machine learning is expected to be one of the first practical\napplications of near-term quantum devices. Pioneer theoretical works suggest\nthat quantum generative adversarial networks (GANs) may exhibit a potential\nexponential advantage over classical GANs, thus attracting widespread\nattention. However, it remains elusive whether quantum GANs implemented on\nnear-term quantum devices can actually solve real-world learning tasks. Here,\nwe devise a flexible quantum GAN scheme to narrow this knowledge gap, which\ncould accomplish image generation with arbitrarily high-dimensional features,\nand could also take advantage of quantum superposition to train multiple\nexamples in parallel. For the first time, we experimentally achieve the\nlearning and generation of real-world hand-written digit images on a\nsuperconducting quantum processor. Moreover, we utilize a gray-scale bar\ndataset to exhibit the competitive performance between quantum GANs and the\nclassical GANs based on multilayer perceptron and convolutional neural network\narchitectures, respectively, benchmarked by the Fr\\'echet Distance score. Our\nwork provides guidance for developing advanced quantum generative models on\nnear-term quantum devices and opens up an avenue for exploring quantum\nadvantages in various GAN-related learning tasks.",
          "link": "http://arxiv.org/abs/2010.06201",
          "publishedOn": "2021-09-08T07:20:12.739Z",
          "wordCount": null,
          "title": "Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hailiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xiaoji Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tisheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">You Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingnan Liu</a>",
          "description": "Odometer has been proven to significantly improve the accuracy of the Global\nNavigation Satellite System / Inertial Navigation System (GNSS/INS) integrated\nvehicle navigation in GNSS-challenged environments. However, the odometer is\ninaccessible in many applications, especially for aftermarket devices. To apply\nforward speed aiding without hardware wheeled odometer, we propose OdoNet, an\nuntethered one-dimensional Convolution Neural Network (CNN)-based\npseudo-odometer model learning from a single Inertial Measurement Unit (IMU),\nwhich can act as an alternative to the wheeled odometer. Dedicated experiments\nhave been conducted to verify the feasibility and robustness of the OdoNet. The\nresults indicate that the IMU individuality, the vehicle loads, and the road\nconditions have little impact on the robustness and precision of the OdoNet,\nwhile the IMU biases and the mounting angles may notably ruin the OdoNet. Thus,\na data-cleaning procedure is added to effectively mitigate the impacts of the\nIMU biases and the mounting angles. Compared to the process using only\nnon-holonomic constraint (NHC), after employing the pseudo-odometer, the\npositioning error is reduced by around 68%, while the percentage is around 74%\nfor the hardware wheeled odometer. In conclusion, the proposed OdoNet can be\nemployed as an untethered pseudo-odometer for vehicle navigation, which can\nefficiently improve the accuracy and reliability of the positioning in\nGNSS-denied environments.",
          "link": "http://arxiv.org/abs/2109.03091",
          "publishedOn": "2021-09-08T07:20:12.732Z",
          "wordCount": 666,
          "title": "OdoNet: Untethered Speed Aiding for Vehicle Navigation Without Hardware Wheeled Odometer. (arXiv:2109.03091v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03534",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whalen_E/0/1/0/all/0/1\">Eamon Whalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyene_A/0/1/0/all/0/1\">Azariah Beyene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_C/0/1/0/all/0/1\">Caitlin Mueller</a>",
          "description": "This paper introduces the Simulated Jet Engine Bracket Dataset (SimJEB): a\nnew, public collection of crowdsourced mechanical brackets and accompanying\nstructural simulations. SimJEB is applicable to a wide range of geometry\nprocessing tasks; the complexity of the shapes in SimJEB offer a challenge to\nautomated geometry cleaning and meshing, while categorical labels and\nstructural simulations facilitate classification and regression (i.e.\nengineering surrogate modeling). In contrast to existing shape collections,\nSimJEB's models are all designed for the same engineering function and thus\nhave consistent structural loads and support conditions. On the other hand,\nSimJEB models are more complex, diverse, and realistic than the synthetically\ngenerated datasets commonly used in parametric surrogate model evaluation. The\ndesigns in SimJEB were derived from submissions to the GrabCAD Jet Engine\nBracket Challenge: an open engineering design competition with over 700\nhand-designed CAD entries from 320 designers representing 56 countries. Each\nmodel has been cleaned, categorized, meshed, and simulated with finite element\nanalysis according to the original competition specifications. The result is a\ncollection of 381 diverse, high-quality and application-focused designs for\nadvancing geometric deep learning, engineering surrogate modeling, automated\ncleaning and related geometry processing tasks.",
          "link": "http://arxiv.org/abs/2105.03534",
          "publishedOn": "2021-09-08T07:20:12.716Z",
          "wordCount": null,
          "title": "SimJEB: Simulated Jet Engine Bracket Dataset. (arXiv:2105.03534v2 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.10316",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhu_W/0/1/0/all/0/1\">Weijun Zhu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lu_F/0/1/0/all/0/1\">Fengyuan Lu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Li_E/0/1/0/all/0/1\">En Li</a>",
          "description": "How to accurately classify and diagnose whether an individual has Coronary\nStenosis (CS) without invasive physical examination? This problem has not been\nsolved satisfactorily. To this end, the four machine learning (ML) algorithms,\ni.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and\nRandom Forest (RF) are employed in this paper. First, eleven features including\nbasic information of an individual, symptoms and results of routine physical\nexamination are selected, as well as one label is specified, indicating whether\nan individual suffers from different severity of coronary artery stenosis or\nnot. On the basis of it, a sample set is constructed. Second, each of these\nfour ML algorithms learns from the sample set to obtain the corresponding\noptimal classified results, respectively. The experimental results show that:\nRF performs better than other three algorithms, and the former algorithm\nclassifies whether an individual has CS with an accuracy of 95.7% (=90/94).",
          "link": "http://arxiv.org/abs/2007.10316",
          "publishedOn": "2021-09-08T07:20:12.715Z",
          "wordCount": 642,
          "title": "Auxiliary Diagnosing Coronary Stenosis Using Machine Learning. (arXiv:2007.10316v4 [q-bio.TO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00632",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hawley_S/0/1/0/all/0/1\">Scott H. Hawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_A/0/1/0/all/0/1\">Andrew C. Morrison</a>",
          "description": "We train an object detector built from convolutional neural networks to count\ninterference fringes in elliptical antinode regions in frames of high-speed\nvideo recordings of transient oscillations in Caribbean steelpan drums\nilluminated by electronic speckle pattern interferometry (ESPI). The\nannotations provided by our model aim to contribute to the understanding of\ntime-dependent behavior in such drums by tracking the development of\nsympathetic vibration modes. The system is trained on a dataset of crowdsourced\nhuman-annotated images obtained from the Zooniverse Steelpan Vibrations\nProject. Due to the small number of human-annotated images and the ambiguity of\nthe annotation task, we also evaluate the model on a large corpus of synthetic\nimages whose properties have been matched to the real images by style transfer\nusing a Generative Adversarial Network. Applying the model to thousands of\nunlabeled video frames, we measure oscillations consistent with audio\nrecordings of these drum strikes. One unanticipated result is that sympathetic\noscillations of higher-octave notes significantly precede the rise in sound\nintensity of the corresponding second harmonic tones; the mechanism responsible\nfor this remains unidentified. This paper primarily concerns the development of\nthe predictive model; further exploration of the steelpan images and deeper\nphysical insights await its further application.",
          "link": "http://arxiv.org/abs/2102.00632",
          "publishedOn": "2021-09-08T07:20:12.700Z",
          "wordCount": null,
          "title": "ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03115",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Dahan_S/0/1/0/all/0/1\">Simon Dahan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Williams_L/0/1/0/all/0/1\">Logan Z. J. Williams</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>",
          "description": "The study of functional brain connectivity (FC) is important for\nunderstanding the underlying mechanisms of many psychiatric disorders. Many\nrecent analyses adopt graph convolutional networks, to study non-linear\ninteractions between functionally-correlated states. However, although patterns\nof brain activation are known to be hierarchically organised in both space and\ntime, many methods have failed to extract powerful spatio-temporal features. To\novercome those challenges, and improve understanding of long-range functional\ndynamics, we translate an approach, from the domain of skeleton-based action\nrecognition, designed to model interactions across space and time. We evaluate\nthis approach using the Human Connectome Project (HCP) dataset on sex\nclassification and fluid intelligence prediction. To account for subject\ntopographic variability of functional organisation, we modelled functional\nconnectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.\nResults show a prediction accuracy of 94.4% for sex classification (an increase\nof 6.2% compared to other methods), and an improvement of correlation with\nfluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes\nspace and time separately. Results suggest that explicit encoding of\nspatio-temporal dynamics of brain functional activity may improve the precision\nwith which behavioural and cognitive phenotypes may be predicted in the future.",
          "link": "http://arxiv.org/abs/2109.03115",
          "publishedOn": "2021-09-08T07:20:12.694Z",
          "wordCount": null,
          "title": "Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])"
        },
        {
          "id": "http://arxiv.org/abs/1911.12990",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged as one of the key ingredients to reduce\nthe size of neural networks for their deployments to resource-limited devices.\nIn order to overcome the nature of transforming continuous activations and\nweights to discrete ones, recent study called Relaxed Quantization (RQ)\n[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that\nallows this transformation with efficient gradient-based optimization. However,\nRQ with this Gumbel-Softmax relaxation still suffers from bias-variance\ntrade-off depending on the temperature parameter of Gumbel-Softmax. To resolve\nthe issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses\nmulti-class straight-through estimator to effectively reduce the bias and\nvariance, along with a new regularization technique, DropBits that replaces\ndropout regularization to randomly drop the bits instead of neurons to further\nreduce the bias of the multi-class straight-through estimator in SRQ. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer\nusing DropBits. We experimentally validate our method on various benchmark\ndatasets and network architectures, and also support the quantized lottery\nticket hypothesis: learning heterogeneous quantization levels outperforms the\ncase using the same but fixed quantization levels from scratch.",
          "link": "http://arxiv.org/abs/1911.12990",
          "publishedOn": "2021-09-08T07:20:12.694Z",
          "wordCount": null,
          "title": "Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03622",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1\">Hana Chockler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1\">Daniel Kroening</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Youcheng Sun</a>",
          "description": "Existing algorithms for explaining the output of image classifiers perform\npoorly on inputs where the object of interest is partially occluded. We present\na novel, black-box algorithm for computing explanations that uses a principled\napproach based on causal theory. We have implemented the method in the\nDEEPCOVER tool. We obtain explanations that are much more accurate than those\ngenerated by the existing explanation tools on images with occlusions and\nobserve a level of performance comparable to the state of the art when\nexplaining images without occlusions.",
          "link": "http://arxiv.org/abs/2103.03622",
          "publishedOn": "2021-09-08T07:20:12.694Z",
          "wordCount": null,
          "title": "Explanations for Occluded Images. (arXiv:2103.03622v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.14580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passananti_J/0/1/0/all/0/1\">Josephine Passananti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1\">Arjun Bhagoji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuanshun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>",
          "description": "Backdoor attacks embed hidden malicious behaviors into deep learning models,\nwhich only activate and cause misclassifications on model inputs containing a\nspecific trigger. Existing works on backdoor attacks and defenses, however,\nmostly focus on digital attacks that use digitally generated patterns as\ntriggers. A critical question remains unanswered: can backdoor attacks succeed\nusing physical objects as triggers, thus making them a credible threat against\ndeep learning systems in the real world? We conduct a detailed empirical study\nto explore this question for facial recognition, a critical deep learning task.\nUsing seven physical objects as triggers, we collect a custom dataset of 3205\nimages of ten volunteers and use it to study the feasibility of physical\nbackdoor attacks under a variety of real-world conditions. Our study reveals\ntwo key findings. First, physical backdoor attacks can be highly successful if\nthey are carefully configured to overcome the constraints imposed by physical\nobjects. In particular, the placement of successful triggers is largely\nconstrained by the target model's dependence on key facial features. Second,\nfour of today's state-of-the-art defenses against (digital) backdoors are\nineffective against physical backdoors, because the use of physical objects\nbreaks core assumptions used to construct these defenses. Our study confirms\nthat (physical) backdoor attacks are not a hypothetical phenomenon but rather\npose a serious real-world threat to critical classification tasks. We need new\nand more robust defenses against backdoors in the physical world.",
          "link": "http://arxiv.org/abs/2006.14580",
          "publishedOn": "2021-09-08T07:20:12.693Z",
          "wordCount": null,
          "title": "Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>",
          "description": "Online hate speech has caught everyone's attention from the news related to\nthe COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -\nan umbrella term for online hateful behavior, manifests itself in forms such as\nonline hate speech. Hate speech is a deliberate attack directed towards an\nindividual or a group motivated by the targeted entity's identity or opinions.\nThe rising mass communication through social media further exacerbates the\nharmful consequences of online hate speech. While there has been significant\nresearch on hate-speech identification using Natural Language Processing (NLP),\nthe work on utilizing NLP for prevention and intervention of online hate speech\nlacks relatively. This paper presents a holistic conceptual framework on\nhate-speech NLP countering methods along with a thorough survey on the current\nprogress of NLP for countering online hate speech. It classifies the countering\ntechniques based on their time of action, and identifies potential future\nresearch areas on this topic.",
          "link": "http://arxiv.org/abs/2109.02941",
          "publishedOn": "2021-09-08T07:20:12.692Z",
          "wordCount": null,
          "title": "Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1809.03048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Druskin_V/0/1/0/all/0/1\">Vladimir Druskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamonov_A/0/1/0/all/0/1\">Alexander V. Mamonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaslavsky_M/0/1/0/all/0/1\">Mikhail Zaslavsky</a>",
          "description": "Graph-Laplacians and their spectral embeddings play an important role in\nmultiple areas of machine learning. This paper is focused on graph-Laplacian\ndimension reduction for the spectral clustering of data as a primary\napplication. Spectral embedding provides a low-dimensional parametrization of\nthe data manifold which makes the subsequent task (e.g., clustering) much\neasier. However, despite reducing the dimensionality of data, the overall\ncomputational cost may still be prohibitive for large data sets due to two\nfactors. First, computing the partial eigendecomposition of the graph-Laplacian\ntypically requires a large Krylov subspace. Second, after the spectral\nembedding is complete, one still has to operate with the same number of data\npoints. For example, clustering of the embedded data is typically performed\nwith various relaxations of k-means which computational cost scales poorly with\nrespect to the size of data set. In this work, we switch the focus from the\nentire data set to a subset of graph vertices (target subset). We develop two\nnovel algorithms for such low-dimensional representation of the original graph\nthat preserves important global distances between the nodes of the target\nsubset. In particular, it allows to ensure that target subset clustering is\nconsistent with the spectral clustering of the full data set if one would\nperform such. That is achieved by a properly parametrized reduced-order model\n(ROM) of the graph-Laplacian that approximates accurately the diffusion\ntransfer function of the original graph for inputs and outputs restricted to\nthe target subset. Working with a small target subset reduces greatly the\nrequired dimension of Krylov subspace and allows to exploit the conventional\nalgorithms (like approximations of k-means) in the regimes when they are most\nrobust and efficient.",
          "link": "http://arxiv.org/abs/1809.03048",
          "publishedOn": "2021-09-08T07:20:12.691Z",
          "wordCount": null,
          "title": "Distance preserving model order reduction of graph-Laplacians and cluster analysis. (arXiv:1809.03048v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rutwik Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Astuto_B/0/1/0/all/0/1\">Bruno Astuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_T/0/1/0/all/0/1\">Tyler Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_W/0/1/0/all/0/1\">Will Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaga_J/0/1/0/all/0/1\">Justin Banaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweetwood_K/0/1/0/all/0/1\">Kevin Sweetwood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_A/0/1/0/all/0/1\">Allen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Rina Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGill_K/0/1/0/all/0/1\">Kevin McGill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Link_T/0/1/0/all/0/1\">Thomas Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crane_J/0/1/0/all/0/1\">Jason Crane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedoia_V/0/1/0/all/0/1\">Valentina Pedoia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Sharmila Majumdar</a>",
          "description": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.",
          "link": "http://arxiv.org/abs/2107.07341",
          "publishedOn": "2021-09-08T07:20:12.688Z",
          "wordCount": null,
          "title": "Utilizing a digital swarm intelligence platform to improve consensus among radiologists and exploring its applications. (arXiv:2107.07341v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02785",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Banerjee_S/0/1/0/all/0/1\">Subhashis Banerjee</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mitra_S/0/1/0/all/0/1\">Sushmita Mitra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hall_L/0/1/0/all/0/1\">Lawrence O. Hall</a>",
          "description": "Prediction of Overall Survival (OS) of brain cancer patients from multi-modal\nMRI is a challenging field of research. Most of the existing literature on\nsurvival prediction is based on Radiomic features, which does not consider\neither non-biological factors or the functional neurological status of the\npatient(s). Besides, the selection of an appropriate cut-off for survival and\nthe presence of censored data create further problems. Application of deep\nlearning models for OS prediction is also limited due to the lack of large\nannotated publicly available datasets. In this scenario we analyse the\npotential of two novel neuroimaging feature families, extracted from brain\nparcellation atlases and spatial habitats, along with classical radiomic and\ngeometric features; to study their combined predictive power for analysing\noverall survival. A cross validation strategy with grid search is proposed to\nsimultaneously select and evaluate the most predictive feature subset based on\nits predictive power. A Cox Proportional Hazard (CoxPH) model is employed for\nunivariate feature selection, followed by the prediction of patient-specific\nsurvival functions by three multivariate parsimonious models viz. Coxnet,\nRandom survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI\ndata used for this research was taken from two open-access collections TCGA-GBM\nand TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding\nsurvival data for each patient was downloaded from The Cancer Genome Atlas\n(TCGA). A high cross validation $C-index$ score of $0.82\\pm.10$ was achieved\nusing RSF with the best $24$ selected features. Age was found to be the most\nimportant biological predictor. There were $9$, $6$, $6$ and $2$ features\nselected from the parcellation, habitat, radiomic and region-based feature\ngroups respectively.",
          "link": "http://arxiv.org/abs/2109.02785",
          "publishedOn": "2021-09-08T07:20:12.687Z",
          "wordCount": null,
          "title": "Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02894",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shoush_M/0/1/0/all/0/1\">Mahmoud Shoush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumas_M/0/1/0/all/0/1\">Marlon Dumas</a>",
          "description": "Prescriptive process monitoring is a family of techniques to optimize the\nperformance of a business process by triggering interventions at runtime.\nExisting prescriptive process monitoring techniques assume that the number of\ninterventions that may be triggered is unbounded. In practice, though, specific\ninterventions consume resources with finite capacity. For example, in a loan\norigination process, an intervention may consist of preparing an alternative\nloan offer to increase the applicant's chances of taking a loan. This\nintervention requires a certain amount of time from a credit officer, and thus,\nit is not possible to trigger this intervention in all cases. This paper\nproposes a prescriptive process monitoring technique that triggers\ninterventions to optimize a cost function under fixed resource constraints. The\nproposed technique relies on predictive modeling to identify cases that are\nlikely to lead to a negative outcome, in combination with causal inference to\nestimate the effect of an intervention on the outcome of the case. These\noutputs are then used to allocate resources to interventions to maximize a cost\nfunction. A preliminary empirical evaluation suggests that the proposed\napproach produces a higher net gain than a purely predictive (non-causal)\nbaseline.",
          "link": "http://arxiv.org/abs/2109.02894",
          "publishedOn": "2021-09-08T07:20:12.672Z",
          "wordCount": null,
          "title": "Prescriptive Process Monitoring Under Resource Constraints: A Causal Inference Approach. (arXiv:2109.02894v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Long H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1\">Nhat Truong Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Van Huong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Liu Tai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh Tin Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_V/0/1/0/all/0/1\">Van Dung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Duy Nguyen</a>",
          "description": "SARS-CoV-2 is colloquially known as COVID-19 that had an initial outbreak in\nDecember 2019. The deadly virus has spread across the world, taking part in the\nglobal pandemic disease since March 2020. In addition, a recent variant of\nSARS-CoV-2 named Delta is intractably contagious and responsible for more than\nfour million deaths over the world. Therefore, it is vital to possess a\nself-testing service of SARS-CoV-2 at home. In this study, we introduce\nFruit-CoV, a two-stage vision framework, which is capable of detecting\nSARS-CoV-2 infections through recorded cough sounds. Specifically, we convert\nsounds into Log-Mel Spectrograms and use the EfficientNet-V2 network to extract\nits visual features in the first stage. In the second stage, we use 14\nconvolutional layers extracted from the large-scale Pretrained Audio Neural\nNetworks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN to\naggregate feature representations of the Log-Mel Spectrograms. Finally, we use\nthe combined features to train a binary classifier. In this study, we use a\ndataset provided by the AICovidVN 115M Challenge, which includes a total of\n7371 recorded cough sounds collected throughout Vietnam, India, and\nSwitzerland. Experimental results show that our proposed model achieves an AUC\nscore of 92.8% and ranks the 1st place on the leaderboard of the AICovidVN\nChallenge. More importantly, our proposed framework can be integrated into a\ncall center or a VoIP system to speed up detecting SARS-CoV-2 infections\nthrough online/recorded cough sounds.",
          "link": "http://arxiv.org/abs/2109.03219",
          "publishedOn": "2021-09-08T07:20:12.672Z",
          "wordCount": null,
          "title": "Fruit-CoV: An Efficient Vision-based Framework for Speedy Detection and Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds. (arXiv:2109.03219v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03159",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liren Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qi Ye</a>",
          "description": "This article presents a different way to study the theory of regularized\nlearning for generalized data including representer theorems and convergence\ntheorems. The generalized data are composed of linear functionals and real\nscalars to represent the discrete information of the local models. By the\nextension of the classical machine learning, the empirical risks are computed\nby the generalized data and the loss functions. According to the techniques of\nregularization, the global solutions are approximated by minimizing the\nregularized empirical risks over the Banach spaces. The Banach spaces are\nadaptively chosen to endow the generalized input data with compactness such\nthat the existence and convergence of the approximate solutions are guaranteed\nby the weak* topology.",
          "link": "http://arxiv.org/abs/2109.03159",
          "publishedOn": "2021-09-08T07:20:12.670Z",
          "wordCount": null,
          "title": "Regularized Learning in Banach Spaces. (arXiv:2109.03159v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhihua Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Huamin Qu</a>",
          "description": "Existing generative pre-trained language models (e.g., GPT) focus on modeling\nthe language structure and semantics of general texts. However, those models do\nnot consider the numerical properties of numbers and cannot perform robustly on\nnumerical reasoning tasks (e.g., math word problems and measurement\nestimation). In this paper, we propose NumGPT, a generative pre-trained model\nthat explicitly models the numerical properties of numbers in texts.\nSpecifically, it leverages a prototype-based numeral embedding to encode the\nmantissa of the number and an individual embedding to encode the exponent of\nthe number. A numeral-aware loss function is designed to integrate numerals\ninto the pre-training objective of NumGPT. We conduct extensive experiments on\nfour different datasets to evaluate the numeracy ability of NumGPT. The\nexperiment results show that NumGPT outperforms baseline models (e.g., GPT and\nGPT with DICE) on a range of numerical reasoning tasks such as measurement\nestimation, number comparison, math word problems, and magnitude\nclassification. Ablation studies are also conducted to evaluate the impact of\npre-training and model hyperparameters on the performance.",
          "link": "http://arxiv.org/abs/2109.03137",
          "publishedOn": "2021-09-08T07:20:12.668Z",
          "wordCount": null,
          "title": "NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.08597",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Uppal_A/0/1/0/all/0/1\">Ananya Uppal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Singh_S/0/1/0/all/0/1\">Shashank Singh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Poczos_B/0/1/0/all/0/1\">Barnabas Poczos</a>",
          "description": "We study minimax convergence rates of nonparametric density estimation in the\nHuber contamination model, in which a proportion of the data comes from an\nunknown outlier distribution. We provide the first results for this problem\nunder a large family of losses, called Besov integral probability metrics\n(IPMs), that includes $\\mathcal{L}^p$, Wasserstein, Kolmogorov-Smirnov, and\nother common distances between probability distributions. Specifically, under a\nrange of smoothness assumptions on the population and outlier distributions, we\nshow that a re-scaled thresholding wavelet series estimator achieves minimax\noptimal convergence rates under a wide variety of losses. Finally, based on\nconnections that have recently been shown between nonparametric density\nestimation under IPM losses and generative adversarial networks (GANs), we show\nthat certain GAN architectures also achieve these minimax rates.",
          "link": "http://arxiv.org/abs/2004.08597",
          "publishedOn": "2021-09-08T07:20:12.667Z",
          "wordCount": null,
          "title": "Robust Density Estimation under Besov IPM Losses. (arXiv:2004.08597v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.01026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengqi Zhang</a>",
          "description": "Federated learning has received great attention for its capability to train a\nlarge-scale model in a decentralized manner without needing to access user data\ndirectly. It helps protect the users' private data from centralized collecting.\nUnlike distributed machine learning, federated learning aims to tackle non-IID\ndata from heterogeneous sources in various real-world applications, such as\nthose on smartphones. Existing federated learning approaches usually adopt a\nsingle global model to capture the shared knowledge of all users by aggregating\ntheir gradients, regardless of the discrepancy between their data\ndistributions. However, due to the diverse nature of user behaviors, assigning\nusers' gradients to different global models (i.e., centers) can better capture\nthe heterogeneity of data distributions across users. Our paper proposes a\nnovel multi-center aggregation mechanism for federated learning, which learns\nmultiple global models from the non-IID user data and simultaneously derives\nthe optimal matching between users and centers. We formulate the problem as a\njoint optimization that can be efficiently solved by a stochastic expectation\nmaximization (EM) algorithm. Our experimental results on benchmark datasets\nshow that our method outperforms several popular federated learning methods.",
          "link": "http://arxiv.org/abs/2005.01026",
          "publishedOn": "2021-09-08T07:20:12.666Z",
          "wordCount": null,
          "title": "Multi-Center Federated Learning. (arXiv:2005.01026v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03020",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Eidel_B/0/1/0/all/0/1\">Bernhard Eidel</a>",
          "description": "In the present work, 3D convolutional neural networks (CNNs) are trained to\nlink random heterogeneous, two-phase materials of arbitrary phase fractions to\ntheir elastic macroscale stiffness thus replacing explicit homogenization\nsimulations. In order to reduce the uncertainty of the true stiffness of the\nsynthetic composites due to unknown boundary conditions (BCs), the CNNs predict\nbeyond the stiffness for periodic BC the upper bound through kinematically\nuniform BC, and the lower bound through stress uniform BC. This work describes\nthe workflow of the homogenization-CNN, from microstructure generation over the\nCNN design, the operations of convolution, nonlinear activation and pooling as\nwell as training and validation along with backpropagation up to performance\nmeasurements in tests. Therein the CNNs demonstrate the predictive accuracy not\nonly for the standard test set but also for samples of the real, two-phase\nmicrostructure of a diamond-based coating. The CNN that covers all three\nboundary types is virtually as accurate as the separate treatment in three\ndifferent nets. The CNNs of this contribution provide through stiffness bounds\nan indicator of the proper RVE size for individual snapshot samples. Moreover,\nthey enable statistical analyses for the effective elastic stiffness on\nensembles of synthetical microstructures without costly simulations.",
          "link": "http://arxiv.org/abs/2109.03020",
          "publishedOn": "2021-09-08T07:20:12.665Z",
          "wordCount": null,
          "title": "Deep Convolutional Neural Networks Predict Elasticity Tensors and their Bounds in Homogenization. (arXiv:2109.03020v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02862",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mahabubul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1\">Satwik Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaloglu_R/0/1/0/all/0/1\">Rasit Onur Topaloglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Swaroop Ghosh</a>",
          "description": "Image classification is a major application domain for conventional deep\nlearning (DL). Quantum machine learning (QML) has the potential to\nrevolutionize image classification. In any typical DL-based image\nclassification, we use convolutional neural network (CNN) to extract features\nfrom the image and multi-layer perceptron network (MLP) to create the actual\ndecision boundaries. On one hand, QML models can be useful in both of these\ntasks. Convolution with parameterized quantum circuits (Quanvolution) can\nextract rich features from the images. On the other hand, quantum neural\nnetwork (QNN) models can create complex decision boundaries. Therefore,\nQuanvolution and QNN can be used to create an end-to-end QML model for image\nclassification. Alternatively, we can extract image features separately using\nclassical dimension reduction techniques such as, Principal Components Analysis\n(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to\ntrain a QNN. We review two proposals on quantum-classical hybrid ML models for\nimage classification namely, Quanvolutional Neural Network and dimension\nreduction using a classical algorithm followed by QNN. Particularly, we make a\ncase for trainable filters in Quanvolution and CAE-based feature extraction for\nimage datasets (instead of dimension reduction using linear transformations\nsuch as, PCA). We discuss various design choices, potential opportunities, and\ndrawbacks of these models. We also release a Python-based framework to create\nand explore these hybrid models with a variety of design choices.",
          "link": "http://arxiv.org/abs/2109.02862",
          "publishedOn": "2021-09-08T07:20:12.657Z",
          "wordCount": null,
          "title": "ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02909",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Prabakaran_B/0/1/0/all/0/1\">Bharath Srinivas Prabakaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akhtar_A/0/1/0/all/0/1\">Asima Akhtar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rehman_S/0/1/0/all/0/1\">Semeen Rehman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_O/0/1/0/all/0/1\">Osman Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "In this work, we propose the BioNetExplorer framework to systematically\ngenerate and explore multiple DNN architectures for bio-signal processing in\nwearables. Our framework adapts key neural architecture parameters to search\nfor an embedded DNN with a low hardware overhead, which can be deployed in\nwearable edge devices to analyse the bio-signal data and to extract the\nrelevant information, such as arrhythmia and seizure. Our framework also\nenables hardware-aware DNN architecture search using genetic algorithms by\nimposing user requirements and hardware constraints (storage, FLOPs, etc.)\nduring the exploration stage, thereby limiting the number of networks explored.\nMoreover, BioNetExplorer can also be used to search for DNNs based on the\nuser-required output classes; for instance, a user might require a specific\noutput class due to genetic predisposition or a pre-existing heart condition.\nThe use of genetic algorithms reduces the exploration time, on average, by 9x,\ncompared to exhaustive exploration. We are successful in identifying\nPareto-optimal designs, which can reduce the storage overhead of the DNN by\n~30MB for a quality loss of less than 0.5%. To enable low-cost embedded DNNs,\nBioNetExplorer also employs different model compression techniques to further\nreduce the storage overhead of the network by up to 53x for a quality loss of\n<0.2%.",
          "link": "http://arxiv.org/abs/2109.02909",
          "publishedOn": "2021-09-08T07:20:12.652Z",
          "wordCount": null,
          "title": "BioNetExplorer: Architecture-Space Exploration of Bio-Signal Processing Deep Neural Networks for Wearables. (arXiv:2109.02909v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shinan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bronzino_F/0/1/0/all/0/1\">Francesco Bronzino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_P/0/1/0/all/0/1\">Paul Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feamster_N/0/1/0/all/0/1\">Nick Feamster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_R/0/1/0/all/0/1\">Ricardo Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crespo_H/0/1/0/all/0/1\">Hector Garcia Crespo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_B/0/1/0/all/0/1\">Brian Ward</a>",
          "description": "Operational networks are increasingly using machine learning models for a\nvariety of tasks, including detecting anomalies, inferring application\nperformance, and forecasting demand. Accurate models are important, yet\naccuracy can degrade over time due to concept drift, whereby either the\ncharacteristics of the data change over time (data drift) or the relationship\nbetween the features and the target predictor change over time (model drift).\nDrift is important to detect because changes in properties of the underlying\ndata or relationships to the target prediction can require model retraining,\nwhich can be time-consuming and expensive. Concept drift occurs in operational\nnetworks for a variety of reasons, ranging from software upgrades to\nseasonality to changes in user behavior. Yet, despite the prevalence of drift\nin networks, its extent and effects on prediction accuracy have not been\nextensively studied. This paper presents an initial exploration into concept\ndrift in a large cellular network in the United States for a major metropolitan\narea in the context of demand forecasting. We find that concept drift arises\nlargely due to data drift, and it appears across different key performance\nindicators (KPIs), models, training set sizes, and time intervals. We identify\nthe sources of concept drift for the particular problem of forecasting downlink\nvolume. Weekly and seasonal patterns introduce both high and low-frequency\nmodel drift, while disasters and upgrades result in sudden drift due to\nexogenous shocks. Regions with high population density, lower traffic volumes,\nand higher speeds also tend to correlate with more concept drift. The features\nthat contribute most significantly to concept drift are User Equipment (UE)\ndownlink packets, UE uplink packets, and Real-time Transport Protocol (RTP)\ntotal received packets.",
          "link": "http://arxiv.org/abs/2109.03011",
          "publishedOn": "2021-09-08T07:20:12.503Z",
          "wordCount": 729,
          "title": "Understanding Model Drift in a Large Cellular Network. (arXiv:2109.03011v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1\">Daniel Berleant</a>",
          "description": "The accuracy of DL classifiers is unstable in that it often changes\nsignificantly when retested on adversarial images, imperfect images, or\nperturbed images. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on defective images. Unlike\nexisted single-factor digital perturbation work, we provide state-of-the-art\ntwo-factor perturbation that provides two natural perturbations on images\napplied in different sequences. The two-factor perturbation includes (1) two\ndigital perturbations (Salt & pepper noise and Gaussian noise) applied in both\nsequences. (2) one digital perturbation (salt & pepper noise) and a geometric\nperturbation (rotation) applied in different sequences. To measure robust DL\nclassifiers, previous scientists provided 15 types of single-factor corruption.\nWe created 69 benchmarking image sets, including a clean set, sets with single\nfactor perturbations, and sets with two-factor perturbation conditions. To be\nbest of our knowledge, this is the first report that two-factor perturbed\nimages improves both robustness and accuracy of DL classifiers. Previous\nresearch evaluating deep learning (DL) classifiers has often used top-1/top-5\naccuracy, so researchers have usually offered tables, line diagrams, and bar\ncharts to display accuracy of DL classifiers. But these existed approaches\ncannot quantitively evaluate robustness of DL classifiers. We innovate a new\ntwo-dimensional, statistical visualization tool, including mean accuracy and\ncoefficient of variation (CV), to benchmark the robustness of DL classifiers.\nAll source codes and related image sets are shared on websites\n(this http URL or\nhttps://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to\nsupport future academic research and industry projects.",
          "link": "http://arxiv.org/abs/2103.03102",
          "publishedOn": "2021-09-08T07:20:12.474Z",
          "wordCount": 740,
          "title": "Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1\">Aws Albarghouthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAntoni_L/0/1/0/all/0/1\">Loris D&#x27;Antoni</a>",
          "description": "Deep neural networks for natural language processing are fragile in the face\nof adversarial examples -- small input perturbations, like synonym substitution\nor word duplication, which cause a neural network to change its prediction. We\npresent an approach to certifying the robustness of LSTMs (and extensions of\nLSTMs) and training models that can be efficiently certified. Our approach can\ncertify robustness to intractably large perturbation spaces defined\nprogrammatically in a language of string transformations. Our evaluation shows\nthat (1) our approach can train models that are more robust to combinations of\nstring transformations than those produced using existing techniques; (2) our\napproach can show high certification accuracy of the resulting models.",
          "link": "http://arxiv.org/abs/2102.07818",
          "publishedOn": "2021-09-08T07:20:12.444Z",
          "wordCount": 576,
          "title": "Certified Robustness to Programmable Transformations in LSTMs. (arXiv:2102.07818v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Ruixuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>",
          "description": "Previous studies demonstrate DNNs' vulnerability to adversarial examples and\nadversarial training can establish a defense to adversarial examples. In\naddition, recent studies show that deep neural networks also exhibit\nvulnerability to parameter corruptions. The vulnerability of model parameters\nis of crucial value to the study of model robustness and generalization. In\nthis work, we introduce the concept of parameter corruption and propose to\nleverage the loss change indicators for measuring the flatness of the loss\nbasin and the parameter robustness of neural network parameters. On such basis,\nwe analyze parameter corruptions and propose the multi-step adversarial\ncorruption algorithm. To enhance neural networks, we propose the adversarial\nparameter defense algorithm that minimizes the average risk of multiple\nadversarial parameter corruptions. Experimental results show that the proposed\nalgorithm can improve both the parameter robustness and accuracy of neural\nnetworks.",
          "link": "http://arxiv.org/abs/2109.02889",
          "publishedOn": "2021-09-08T07:20:12.413Z",
          "wordCount": null,
          "title": "Adversarial Parameter Defense by Multi-Step Risk Minimization. (arXiv:2109.02889v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Junghyo Jo</a>",
          "description": "The success of machine learning stems from its structured data\nrepresentation. Similar data have close representation as compressed codes for\nclassification or emerged labels for clustering. We observe that the frequency\nof the internal representation follows power laws in both supervised and\nunsupervised learning. The scale-invariant distribution implies that machine\nlearning largely compresses frequent typical data, and at the same time,\ndifferentiates many atypical data as outliers. In this study, we derive how the\npower laws can naturally arise in machine learning. In terms of information\ntheory, the scale-invariant representation corresponds to a maximally uncertain\ndata grouping among possible representations that guarantee pre-specified\nlearning accuracy.",
          "link": "http://arxiv.org/abs/2109.02914",
          "publishedOn": "2021-09-08T07:20:12.413Z",
          "wordCount": null,
          "title": "Scale-invariant representation of machine learning. (arXiv:2109.02914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03220",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1\">Qi Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yigui Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hongyao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Laurence Tianruo Yang</a>",
          "description": "Recursive least squares (RLS) algorithms were once widely used for training\nsmall-scale neural networks, due to their fast convergence. However, previous\nRLS algorithms are unsuitable for training deep neural networks (DNNs), since\nthey have high computational complexity and too many preconditions. In this\npaper, to overcome these drawbacks, we propose three novel RLS optimization\nalgorithms for training feedforward neural networks, convolutional neural\nnetworks and recurrent neural networks (including long short-term memory\nnetworks), by using the error backpropagation and our average-approximation RLS\nmethod, together with the equivalent gradients of the linear least squares loss\nfunction with respect to the linear outputs of hidden layers. Compared with\nprevious RLS optimization algorithms, our algorithms are simple and elegant.\nThey can be viewed as an improved stochastic gradient descent (SGD) algorithm,\nwhich uses the inverse autocorrelation matrix of each layer as the adaptive\nlearning rate. Their time and space complexities are only several times those\nof SGD. They only require the loss function to be the mean squared error and\nthe activation function of the output layer to be invertible. In fact, our\nalgorithms can be also used in combination with other first-order optimization\nalgorithms without requiring these two preconditions. In addition, we present\ntwo improved methods for our algorithms. Finally, we demonstrate their\neffectiveness compared to the Adam algorithm on MNIST, CIFAR-10 and IMDB\ndatasets, and investigate the influences of their hyperparameters\nexperimentally.",
          "link": "http://arxiv.org/abs/2109.03220",
          "publishedOn": "2021-09-08T07:20:12.390Z",
          "wordCount": 696,
          "title": "Revisiting Recursive Least Squares for Training Deep Neural Networks. (arXiv:2109.03220v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiangyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>",
          "description": "Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.",
          "link": "http://arxiv.org/abs/2009.13401",
          "publishedOn": "2021-09-08T07:20:12.381Z",
          "wordCount": 600,
          "title": "Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vardhan_A/0/1/0/all/0/1\">Aleti Vardhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_S/0/1/0/all/0/1\">Sudarshan Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vijay_S/0/1/0/all/0/1\">Supriti Vijay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhabra_N/0/1/0/all/0/1\">Nipuna Chhabra</a>",
          "description": "The increasing use of social media sites in countries like India has given\nrise to large volumes of code-mixed data. Sentiment analysis of this data can\nprovide integral insights into people's perspectives and opinions. Developing\nrobust explainability techniques which explain why models make their\npredictions becomes essential. In this paper, we propose an adequate\nmethodology to integrate explainable approaches into code-mixed sentiment\nanalysis.",
          "link": "http://arxiv.org/abs/2109.03200",
          "publishedOn": "2021-09-08T07:20:12.352Z",
          "wordCount": 525,
          "title": "ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Mingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasile_C/0/1/0/all/0/1\">Cristian-Ioan Vasile</a>",
          "description": "Reinforcement learning (RL) is a promising approach and has limited success\ntowards real-world applications, because ensuring safe exploration or\nfacilitating adequate exploitation is a challenges for controlling robotic\nsystems with unknown models and measurement uncertainties. Such a learning\nproblem becomes even more intractable for complex tasks over continuous space\n(state-space and action-space). In this paper, we propose a learning-based\ncontrol framework consisting of several aspects: (1) linear temporal logic\n(LTL) is leveraged to facilitate complex tasks over an infinite horizons which\ncan be translated to a novel automaton structure; (2) we propose an innovative\nreward scheme for RL-agent with the formal guarantee such that global optimal\npolicies maximize the probability of satisfying the LTL specifications; (3)\nbased on a reward shaping technique, we develop a modular policy-gradient\narchitecture utilizing the benefits of automaton structures to decompose\noverall tasks and facilitate the performance of learned controllers; (4) by\nincorporating Gaussian Processes (GPs) to estimate the uncertain dynamic\nsystems, we synthesize a model-based safeguard using Exponential Control\nBarrier Functions (ECBFs) to address problems with high-order relative degrees.\nIn addition, we utilize the properties of LTL automatons and ECBFs to construct\na guiding process to further improve the efficiency of exploration. Finally, we\ndemonstrate the effectiveness of the framework via several robotic\nenvironments. And we show such an ECBF-based modular deep RL algorithm achieves\nnear-perfect success rates and guard safety with a high probability confidence\nduring training.",
          "link": "http://arxiv.org/abs/2109.02791",
          "publishedOn": "2021-09-08T07:20:12.346Z",
          "wordCount": 695,
          "title": "Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions. (arXiv:2109.02791v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wijeratne_S/0/1/0/all/0/1\">Sasindu Wijeratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayaweera_S/0/1/0/all/0/1\">Sandaruwan Jayaweera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dananjaya_M/0/1/0/all/0/1\">Mahesh Dananjaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasqual_A/0/1/0/all/0/1\">Ajith Pasqual</a>",
          "description": "Convolutional Neural Networks (CNNs) are widely used in deep learning\napplications, e.g. visual systems, robotics etc. However, existing software\nsolutions are not efficient. Therefore, many hardware accelerators have been\nproposed optimizing performance, power and resource utilization of the\nimplementation. Amongst existing solutions, Field Programmable Gate Array\n(FPGA) based architecture provides better cost-energy-performance trade-offs as\nwell as scalability and minimizing development time. In this paper, we present\na model-independent reconfigurable co-processing architecture to accelerate\nCNNs. Our architecture consists of parallel Multiply and Accumulate (MAC) units\nwith caching techniques and interconnection networks to exploit maximum data\nparallelism. In contrast to existing solutions, we introduce limited precision\n32 bit Q-format fixed point quantization for arithmetic representations and\noperations. As a result, our architecture achieved significant reduction in\nresource utilization with competitive accuracy. Furthermore, we developed an\nassembly-type microinstructions to access the co-processing fabric to manage\nlayer-wise parallelism, thereby making re-use of limited resources. Finally, we\nhave tested our architecture up to 9x9 kernel size on Xilinx Virtex 7 FPGA,\nachieving a throughput of up to 226.2 GOp/S for 3x3 kernel size.",
          "link": "http://arxiv.org/abs/2109.03040",
          "publishedOn": "2021-09-08T07:20:12.330Z",
          "wordCount": 643,
          "title": "Reconfigurable co-processor architecture with limited numerical precision to accelerate deep convolutional neural networks. (arXiv:2109.03040v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benato_B/0/1/0/all/0/1\">Barbara C Benato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telea_A/0/1/0/all/0/1\">Alexandru C Telea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1\">Alexandre X Falc&#xe3;o</a>",
          "description": "Training deep neural networks is challenging when large and annotated\ndatasets are unavailable. Extensive manual annotation of data samples is\ntime-consuming, expensive, and error-prone, notably when it needs to be done by\nexperts. To address this issue, increased attention has been devoted to\ntechniques that propagate uncertain labels (also called pseudo labels) to large\namounts of unsupervised samples and use them for training the model. However,\nthese techniques still need hundreds of supervised samples per class in the\ntraining set and a validation set with extra supervised samples to tune the\nmodel. We improve a recent iterative pseudo-labeling technique, Deep Feature\nAnnotation (DeepFA), by selecting the most confident unsupervised samples to\niteratively train a deep neural network. Our confidence-based sampling strategy\nrelies on only dozens of annotated training samples per class with no\nvalidation set, considerably reducing user effort in data annotation. We first\nascertain the best configuration for the baseline -- a self-trained deep neural\nnetwork -- and then evaluate our confidence DeepFA for different confidence\nthresholds. Experiments on six datasets show that DeepFA already outperforms\nthe self-trained baseline, but confidence DeepFA can considerably outperform\nthe original DeepFA and the baseline.",
          "link": "http://arxiv.org/abs/2109.02717",
          "publishedOn": "2021-09-08T07:20:12.320Z",
          "wordCount": 634,
          "title": "Iterative Pseudo-Labeling with Deep Feature Annotation and Confidence-Based Sampling. (arXiv:2109.02717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02703",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Anderson_J/0/1/0/all/0/1\">James Anderson</a>",
          "description": "Learning a dynamical system from input/output data is a fundamental task in\nthe control design pipeline. In the partially observed setting there are two\ncomponents to identification: parameter estimation to learn the Markov\nparameters, and system realization to obtain a state space model. In both\nsub-problems it is implicitly assumed that standard numerical algorithms such\nas the singular value decomposition (SVD) can be easily and reliably computed.\nWhen trying to fit a high-dimensional model to data, for example in the\ncyber-physical system setting, even computing an SVD is intractable. In this\nwork we show that an approximate matrix factorization obtained using randomized\nmethods can replace the standard SVD in the realization algorithm while\nmaintaining the non-asymptotic (in data-set size) performance and robustness\nguarantees of classical methods. Numerical examples illustrate that for large\nsystem models, this is the only method capable of producing a model.",
          "link": "http://arxiv.org/abs/2109.02703",
          "publishedOn": "2021-09-08T07:20:12.311Z",
          "wordCount": 595,
          "title": "Large-Scale System Identification Using a Randomized SVD. (arXiv:2109.02703v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02749",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1\">Georgios Albanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1\">Nikolaos Zioulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drakoulis_P/0/1/0/all/0/1\">Petros Drakoulis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkitsas_V/0/1/0/all/0/1\">Vasileios Gkitsas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterzentsenko_V/0/1/0/all/0/1\">Vladimiros Sterzentsenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_F/0/1/0/all/0/1\">Federico Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1\">Dimitrios Zarpalas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daras_P/0/1/0/all/0/1\">Petros Daras</a>",
          "description": "Pano3D is a new benchmark for depth estimation from spherical panoramas. It\naims to assess performance across all depth estimation traits, the primary\ndirect depth estimation performance targeting precision and accuracy, and also\nthe secondary traits, boundary preservation, and smoothness. Moreover, Pano3D\nmoves beyond typical intra-dataset evaluation to inter-dataset performance\nassessment. By disentangling the capacity to generalize to unseen data into\ndifferent test splits, Pano3D represents a holistic benchmark for $360^o$ depth\nestimation. We use it as a basis for an extended analysis seeking to offer\ninsights into classical choices for depth estimation. This results in a solid\nbaseline for panoramic depth that follow-up works can build upon to steer\nfuture progress.",
          "link": "http://arxiv.org/abs/2109.02749",
          "publishedOn": "2021-09-08T07:20:12.290Z",
          "wordCount": 597,
          "title": "Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitas_I/0/1/0/all/0/1\">Ioannis Pitas</a>",
          "description": "Existing road pothole detection approaches can be classified as computer\nvision-based or machine learning-based. The former approaches typically employ\n2-D image analysis/understanding or 3-D point cloud modeling and segmentation\nalgorithms to detect road potholes from vision sensor data. The latter\napproaches generally address road pothole detection using convolutional neural\nnetworks (CNNs) in an end-to-end manner. However, road potholes are not\nnecessarily ubiquitous and it is challenging to prepare a large well-annotated\ndataset for CNN training. In this regard, while computer vision-based methods\nwere the mainstream research trend in the past decade, machine learning-based\nmethods were merely discussed. Recently, we published the first stereo\nvision-based road pothole detection dataset and a novel disparity\ntransformation algorithm, whereby the damaged and undamaged road areas can be\nhighly distinguished. However, there are no benchmarks currently available for\nstate-of-the-art (SoTA) CNNs trained using either disparity images or\ntransformed disparity images. Therefore, in this paper, we first discuss the\nSoTA CNNs designed for semantic segmentation and evaluate their performance for\nroad pothole detection with extensive experiments. Additionally, inspired by\ngraph neural network (GNN), we propose a novel CNN layer, referred to as graph\nattention layer (GAL), which can be easily deployed in any existing CNN to\noptimize image feature representations for semantic segmentation. Our\nexperiments compare GAL-DeepLabv3+, our best-performing implementation, with\nnine SoTA CNNs on three modalities of training data: RGB images, disparity\nimages, and transformed disparity images. The experimental results suggest that\nour proposed GAL-DeepLabv3+ achieves the best overall pothole detection\naccuracy on all training data modalities.",
          "link": "http://arxiv.org/abs/2109.02711",
          "publishedOn": "2021-09-08T07:20:12.283Z",
          "wordCount": 734,
          "title": "Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1\">Manish Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aditya Jain</a>",
          "description": "Recommendation engines are integral to the modern e-commerce experience, both\nfor the seller and the end user. Accurate recommendations lead to higher\nrevenue and better user experience. In this paper, we are presenting our\nsolution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of\nthis challenge is to maximize the chances of a click when the users are\npresented with set of fashion items. We have approached this problem as a\nbinary classification problem. Our winning solution utilizes Catboost as the\nclassifier and Bayesian Optimization for hyper parameter tuning. Our baseline\nmodel achieved MRR of 0.5153 on the validation set. Bayesian optimization of\nhyper parameters improved the MRR to 0.5240 on the validation set. Our final\nsubmission on the test set achieved a MRR of 0.5257.",
          "link": "http://arxiv.org/abs/2108.01314",
          "publishedOn": "2021-09-08T07:20:12.221Z",
          "wordCount": 599,
          "title": "Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1\">Martin Milenkoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "In this paper, we describe a method to tackle data sparsity and create\nrecommendations in domains with limited knowledge about user preferences. We\nexpand the variational autoencoder collaborative filtering from a single-domain\nto a multi-domain setting. The intuition is that user-item interactions in a\nsource domain can augment the recommendation quality in a target domain. The\nintuition can be taken to its extreme, where, in a cross-domain setup, the user\nhistory in a source domain is enough to generate high-quality recommendations\nin a target one. We thus create a Product-of-Experts (POE) architecture for\nrecommendations that jointly models user-item interactions across multiple\ndomains. The method is resilient to missing data for one or more of the\ndomains, which is a situation often found in real life. We present results on\ntwo widely-used datasets - Amazon and Yelp, which support the claim that\nholistic user preference knowledge leads to better recommendations.\nSurprisingly, we find that in some cases, a POE recommender that does not\naccess the target domain user representation can surpass a strong VAE\nrecommender baseline trained on the target domain.",
          "link": "http://arxiv.org/abs/2104.12822",
          "publishedOn": "2021-09-08T07:20:12.201Z",
          "wordCount": 686,
          "title": "Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02890",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Ratledge_N/0/1/0/all/0/1\">Nathan Ratledge</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Cadamuro_G/0/1/0/all/0/1\">Gabe Cadamuro</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Cuesta_B/0/1/0/all/0/1\">Brandon de la Cuesta</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Stigler_M/0/1/0/all/0/1\">Matthieu Stigler</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Burke_M/0/1/0/all/0/1\">Marshall Burke</a>",
          "description": "In many regions of the world, sparse data on key economic outcomes inhibits\nthe development, targeting, and evaluation of public policy. We demonstrate how\nadvancements in satellite imagery and machine learning can help ameliorate\nthese data and inference challenges. In the context of an expansion of the\nelectrical grid across Uganda, we show how a combination of satellite imagery\nand computer vision can be used to develop local-level livelihood measurements\nappropriate for inferring the causal impact of electricity access on\nlivelihoods. We then show how ML-based inference techniques deliver more\nreliable estimates of the causal impact of electrification than traditional\nalternatives when applied to these data. We estimate that grid access improves\nvillage-level asset wealth in rural Uganda by 0.17 standard deviations, more\nthan doubling the growth rate over our study period relative to untreated\nareas. Our results provide country-scale evidence on the impact of a key\ninfrastructure investment, and provide a low-cost, generalizable approach to\nfuture policy evaluation in data sparse environments.",
          "link": "http://arxiv.org/abs/2109.02890",
          "publishedOn": "2021-09-08T07:20:12.129Z",
          "wordCount": 629,
          "title": "Using Satellite Imagery and Machine Learning to Estimate the Livelihood Impact of Electricity Access. (arXiv:2109.02890v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11627",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiqiang Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jingshuang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>",
          "description": "We introduced the least-squares ReLU neural network (LSNN) method for solving\nthe linear advection-reaction problem with discontinuous solution and showed\nthat the method outperforms mesh-based numerical methods in terms of the number\nof degrees of freedom. This paper studies the LSNN method for scalar nonlinear\nhyperbolic conservation law. The method is a discretization of an equivalent\nleast-squares (LS) formulation in the set of neural network functions with the\nReLU activation function. Evaluation of the LS functional is done by using\nnumerical integration and conservative finite volume scheme. Numerical results\nof some test problems show that the method is capable of approximating the\ndiscontinuous interface of the underlying problem automatically through the\nfree breaking lines of the ReLU neural network. Moreover, the method does not\nexhibit the common Gibbs phenomena along the discontinuous interface.",
          "link": "http://arxiv.org/abs/2105.11627",
          "publishedOn": "2021-09-08T07:20:12.078Z",
          "wordCount": 600,
          "title": "Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02639",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>",
          "description": "Out-of-distribution (OOD) detection and lossless compression constitute two\nproblems that can be solved by the training of probabilistic models on a first\ndataset with subsequent likelihood evaluation on a second dataset, where data\ndistributions differ. By defining the generalization of probabilistic models in\nterms of likelihood we show that, in the case of image models, the OOD\ngeneralization ability is dominated by local features. This motivates our\nproposal of a Local Autoregressive model that exclusively models local image\nfeatures towards improving OOD performance. We apply the proposed model to OOD\ndetection tasks and achieve state-of-the-art unsupervised OOD detection\nperformance without the introduction of additional data. Additionally, we\nemploy our model to build a new lossless image compressor: NeLLoC (Neural Local\nLossless Compressor) and report state-of-the-art compression rates and model\nsize.",
          "link": "http://arxiv.org/abs/2109.02639",
          "publishedOn": "2021-09-08T07:20:11.981Z",
          "wordCount": 581,
          "title": "On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Erlansari_A/0/1/0/all/0/1\">Aan Erlansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Effendi_R/0/1/0/all/0/1\">Rusdi Effendi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+C_F/0/1/0/all/0/1\">Funny Farady C</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijanarko_A/0/1/0/all/0/1\">Andang Wijanarko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susilo_B/0/1/0/all/0/1\">Boko Susilo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardiansyah_R/0/1/0/all/0/1\">Reza Hardiansyah</a>",
          "description": "Bloodstock shortages and its uncertain demand has become a major problem for\nall countries worldwide. Therefore, this study aims to provide solution to the\nissues of blood distribution during the Covid-19 Pandemic at Bengkulu,\nIndonesia. The Backpropagation algorithm was used to improve the possibility of\ndiscovering available and potential donors. Furthermore, the distances, age,\nand length of donation were measured to obtain the right person to donate blood\nwhen it needed. The Backpropagation uses three input layers to classify\neligible donors, namely age, body, weight, and bias. In addition, the system\nthrough its query automatically counts the variables via the Fuzzy Tahani and\nsimultaneously access the vast database.",
          "link": "http://arxiv.org/abs/2109.02645",
          "publishedOn": "2021-09-08T07:20:11.945Z",
          "wordCount": 626,
          "title": "Backpropagation and fuzzy algorithm Modelling to Resolve Blood Supply Chain Issues in the Covid-19 Pandemic. (arXiv:2109.02645v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Driven by the need to capture users' evolving interests and optimize their\nlong-term experiences, more and more recommender systems have started to model\nrecommendation as a Markov decision process and employ reinforcement learning\nto address the problem. Shouldn't research on the fairness of recommender\nsystems follow the same trend from static evaluation and one-shot intervention\nto dynamic monitoring and non-stop control? In this paper, we portray the\nrecent developments in recommender systems first and then discuss how fairness\ncould be baked into the reinforcement learning techniques for recommendation.\nMoreover, we argue that in order to make further progress in recommendation\nfairness, we may want to consider multi-agent (game-theoretic) optimization,\nmulti-objective (Pareto) optimization, and simulation-based optimization, in\nthe general framework of stochastic games.",
          "link": "http://arxiv.org/abs/2109.03150",
          "publishedOn": "2021-09-08T07:20:11.837Z",
          "wordCount": 570,
          "title": "Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02839",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_Z/0/1/0/all/0/1\">Zhiqiang Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jingshuang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_M/0/1/0/all/0/1\">Min Liu</a>",
          "description": "Designing an optimal deep neural network for a given task is important and\nchallenging in many machine learning applications. To address this issue, we\nintroduce a self-adaptive algorithm: the adaptive network enhancement (ANE)\nmethod, written as loops of the form train, estimate and enhance. Starting with\na small two-layer neural network (NN), the step train is to solve the\noptimization problem at the current NN; the step estimate is to compute a\nposteriori estimator/indicators using the solution at the current NN; the step\nenhance is to add new neurons to the current NN.\n\nNovel network enhancement strategies based on the computed\nestimator/indicators are developed in this paper to determine how many new\nneurons and when a new layer should be added to the current NN. The ANE method\nprovides a natural process for obtaining a good initialization in training the\ncurrent NN; in addition, we introduce an advanced procedure on how to\ninitialize newly added neurons for a better approximation. We demonstrate that\nthe ANE method can automatically design a nearly minimal NN for learning\nfunctions exhibiting sharp transitional layers as well as discontinuous\nsolutions of hyperbolic partial differential equations.",
          "link": "http://arxiv.org/abs/2109.02839",
          "publishedOn": "2021-09-08T07:20:11.830Z",
          "wordCount": 643,
          "title": "Self-adaptive deep neural network: Numerical approximation to functions and PDEs. (arXiv:2109.02839v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lockwood_O/0/1/0/all/0/1\">Owen Lockwood</a>",
          "description": "Quantum Machine Learning (QML) is considered to be one of the most promising\napplications of near term quantum devices. However, the optimization of quantum\nmachine learning models presents numerous challenges arising from the\nimperfections of hardware and the fundamental obstacles in navigating an\nexponentially scaling Hilbert space. In this work, we evaluate the potential of\ncontemporary methods in deep reinforcement learning to augment gradient based\noptimization routines in quantum variational circuits. We find that\nreinforcement learning augmented optimizers consistently outperform gradient\ndescent in noisy environments. All code and pretrained weights are available to\nreplicate the results or deploy the models at\nhttps://github.com/lockwo/rl_qvc_opt.",
          "link": "http://arxiv.org/abs/2109.03188",
          "publishedOn": "2021-09-08T07:20:11.790Z",
          "wordCount": 542,
          "title": "Optimizing Quantum Variational Circuits with Deep Reinforcement Learning. (arXiv:2109.03188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eysenbach_B/0/1/0/all/0/1\">Benjamin Eysenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>",
          "description": "Many of the challenges facing today's reinforcement learning (RL) algorithms,\nsuch as robustness, generalization, transfer, and computational efficiency are\nclosely related to compression. Prior work has convincingly argued why\nminimizing information is useful in the supervised learning setting, but\nstandard RL algorithms lack an explicit mechanism for compression. The RL\nsetting is unique because (1) its sequential nature allows an agent to use past\ninformation to avoid looking at future observations and (2) the agent can\noptimize its behavior to prefer states where decision making requires few bits.\nWe take advantage of these properties to propose a method (RPC) for learning\nsimple policies. This method brings together ideas from information\nbottlenecks, model-based RL, and bits-back coding into a simple and\ntheoretically-justified algorithm. Our method jointly optimizes a latent-space\nmodel and policy to be self-consistent, such that the policy avoids states\nwhere the model is inaccurate. We demonstrate that our method achieves much\ntighter compression than prior methods, achieving up to 5x higher reward than a\nstandard information bottleneck. We also demonstrate that our method learns\npolicies that are more robust and generalize better to new tasks.",
          "link": "http://arxiv.org/abs/2109.03214",
          "publishedOn": "2021-09-08T07:20:11.784Z",
          "wordCount": 626,
          "title": "Robust Predictable Control. (arXiv:2109.03214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.03714",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>",
          "description": "Nonnegative matrix factorization is usually powerful for learning the\n\"shallow\" parts-based representation, but it clearly fails to discover deep\nhierarchical information within both the basis and representation spaces. In\nthis paper, we technically propose a new enriched prior based Dual-constrained\nDeep Semi-Supervised Coupled Factorization Network, called DS2CF-Net, for\nlearning the hierarchical coupled representations. To ex-tract hidden deep\nfeatures, DS2CF-Net is modeled as a deep-structure and geometrical\nstructure-constrained neural network. Specifically, DS2CF-Net designs a deep\ncoupled factorization architecture using multi-layers of linear\ntransformations, which coupled updates the bases and new representations in\neach layer. To improve the discriminating ability of learned deep\nrepresentations and deep coefficients, our network clearly considers enriching\nthe supervised prior by the joint deep coefficients-regularized label\nprediction, and incorporates enriched prior information as additional label and\nstructure constraints. The label constraint can enable the samples of the same\nlabel to have the same coordinate in the new feature space, while the structure\nconstraint forces the coefficient matrices in each layer to be block-diagonal\nso that the enhanced prior using the self-expressive label propagation are more\naccurate. Our network also integrates the adaptive dual-graph learning to\nretain the local manifold structures of both the data manifold and feature\nmanifold by minimizing the reconstruction errors in each layer. Extensive\nexperiments on several real databases demonstrate that our DS2CF-Net can obtain\nstate-of-the-art performance for representation learning and clustering.",
          "link": "http://arxiv.org/abs/2009.03714",
          "publishedOn": "2021-09-08T07:20:11.778Z",
          "wordCount": 719,
          "title": "Dual-constrained Deep Semi-Supervised Coupled Factorization Network with Enriched Prior. (arXiv:2009.03714v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02986",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1\">Yu Yao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>",
          "description": "Label noise will degenerate the performance of deep learning algorithms\nbecause deep neural networks easily overfit label errors. Let X and Y denote\nthe instance and clean label, respectively. When Y is a cause of X, according\nto which many datasets have been constructed, e.g., SVHN and CIFAR, the\ndistributions of P(X) and P(Y|X) are entangled. This means that the\nunsupervised instances are helpful to learn the classifier and thus reduce the\nside effect of label noise. However, it remains elusive on how to exploit the\ncausal information to handle the label noise problem. In this paper, by\nleveraging a structural causal model, we propose a novel generative approach\nfor instance-dependent label-noise learning. In particular, we show that\nproperly modeling the instances will contribute to the identifiability of the\nlabel noise transition matrix and thus lead to a better classifier.\nEmpirically, our method outperforms all state-of-the-art methods on both\nsynthetic and real-world label-noise datasets.",
          "link": "http://arxiv.org/abs/2109.02986",
          "publishedOn": "2021-09-08T07:20:11.758Z",
          "wordCount": 601,
          "title": "Instance-dependent Label-noise Learning under a Structural Causal Model. (arXiv:2109.02986v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1\">Sahan Bulathwela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1\">Maria Perez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_E/0/1/0/all/0/1\">Erik Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>",
          "description": "Educational recommenders have received much less attention in comparison to\ne-commerce and entertainment-related recommenders, even though efficient\nintelligent tutors have great potential to improve learning gains. One of the\nmain challenges in advancing this research direction is the scarcity of large,\npublicly available datasets. In this work, we release a large, novel dataset of\nlearners engaging with educational videos in-the-wild. The dataset, named\nPersonalised Educational Engagement with Knowledge Topics PEEK, is the first\npublicly available dataset of this nature. The video lectures have been\nassociated with Wikipedia concepts related to the material of the lecture, thus\nproviding a humanly intuitive taxonomy. We believe that granular learner\nengagement signals in unison with rich content representations will pave the\nway to building powerful personalization algorithms that will revolutionise\neducational and informational recommendation systems. Towards this goal, we 1)\nconstruct a novel dataset from a popular video lecture repository, 2) identify\na set of benchmark algorithms to model engagement, and 3) run extensive\nexperimentation on the PEEK dataset to demonstrate its value. Our experiments\nwith the dataset show promise in building powerful informational recommender\nsystems. The dataset and the support code is available publicly.",
          "link": "http://arxiv.org/abs/2109.03154",
          "publishedOn": "2021-09-08T07:20:11.751Z",
          "wordCount": 674,
          "title": "PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02969",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Veshki_F/0/1/0/all/0/1\">Farshad G. Veshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobyov_S/0/1/0/all/0/1\">Sergiy A. Vorobyov</a>",
          "description": "Convolutional sparse coding improves on the standard sparse approximation by\nincorporating a global shift-invariant model. The most efficient convolutional\nsparse coding methods are based on the alternating direction method of\nmultipliers and the convolution theorem. The only major difference between\nthese methods is how they approach a convolutional least-squares fitting\nsubproblem. This letter presents a solution to this subproblem, which improves\nthe efficiency of the state-of-the-art algorithms. We also use the same\napproach for developing an efficient convolutional dictionary learning method.\nFurthermore, we propose a novel algorithm for convolutional sparse coding with\na constraint on the approximation error.",
          "link": "http://arxiv.org/abs/2109.02969",
          "publishedOn": "2021-09-08T07:20:11.743Z",
          "wordCount": 541,
          "title": "Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_E/0/1/0/all/0/1\">Emil Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrenheim_V/0/1/0/all/0/1\">Vilhelm von Ehrenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_D/0/1/0/all/0/1\">Dhiana Deva Cavalcanti Rocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Anna Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horn_S/0/1/0/all/0/1\">Sonja Horn</a>",
          "description": "Sentence embedding refers to a set of effective and versatile techniques for\nconverting raw text into numerical vector representations that can be used in a\nwide range of natural language processing (NLP) applications. The majority of\nthese techniques are either supervised or unsupervised. Compared to the\nunsupervised methods, the supervised ones make less assumptions about\noptimization objectives and usually achieve better results. However, the\ntraining requires a large amount of labeled sentence pairs, which is not\navailable in many industrial scenarios. To that end, we propose a generic and\nend-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence\nEmbedding), capable of learning high-quality sentence embeddings from a\npartially labeled dataset. We experimentally show that PAUSE achieves, and\nsometimes surpasses, state-of-the-art results using only a small fraction of\nlabeled sentence pairs on various benchmark tasks. When applied to a real\nindustrial use case where labeled samples are scarce, PAUSE encourages us to\nextend our dataset without the liability of extensive manual annotation work.",
          "link": "http://arxiv.org/abs/2109.03155",
          "publishedOn": "2021-09-08T07:20:11.738Z",
          "wordCount": 638,
          "title": "PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madeira_M/0/1/0/all/0/1\">Manuel Madeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negrinho_R/0/1/0/all/0/1\">Renato Negrinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xavier_J/0/1/0/all/0/1\">Jo&#xe3;o Xavier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguiar_P/0/1/0/all/0/1\">Pedro M. Q. Aguiar</a>",
          "description": "First-order methods for stochastic optimization have undeniable relevance, in\npart due to their pivotal role in machine learning. Variance reduction for\nthese algorithms has become an important research topic. In contrast to common\napproaches, which rarely leverage global models of the objective function, we\nexploit convexity and L-smoothness to improve the noisy estimates outputted by\nthe stochastic gradient oracle. Our method, named COCO denoiser, is the joint\nmaximum likelihood estimator of multiple function gradients from their noisy\nobservations, subject to co-coercivity constraints between them. The resulting\nestimate is the solution of a convex Quadratically Constrained Quadratic\nProblem. Although this problem is expensive to solve by interior point methods,\nwe exploit its structure to apply an accelerated first-order algorithm, the\nFast Dual Proximal Gradient method. Besides analytically characterizing the\nproposed estimator, we show empirically that increasing the number and\nproximity of the queried points leads to better gradient estimates. We also\napply COCO in stochastic settings by plugging it in existing algorithms, such\nas SGD, Adam or STRSAGA, outperforming their vanilla versions, even in\nscenarios where our modelling assumptions are mismatched.",
          "link": "http://arxiv.org/abs/2109.03207",
          "publishedOn": "2021-09-08T07:20:11.730Z",
          "wordCount": 638,
          "title": "COCO Denoiser: Using Co-Coercivity for Variance Reduction in Stochastic Convex Optimization. (arXiv:2109.03207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badanidiyuru_A/0/1/0/all/0/1\">Ashwinkumar Badanidiyuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guruganesh_G/0/1/0/all/0/1\">Guru Guruganesh</a>",
          "description": "In this paper, we investigate the problem about how to bid in repeated\ncontextual first price auctions. We consider a single bidder (learner) who\nrepeatedly bids in the first price auctions: at each time $t$, the learner\nobserves a context $x_t\\in \\mathbb{R}^d$ and decides the bid based on\nhistorical information and $x_t$. We assume a structured linear model of the\nmaximum bid of all the others $m_t = \\alpha_0\\cdot x_t + z_t$, where\n$\\alpha_0\\in \\mathbb{R}^d$ is unknown to the learner and $z_t$ is randomly\nsampled from a noise distribution $\\mathcal{F}$ with log-concave density\nfunction $f$. We consider both \\emph{binary feedback} (the learner can only\nobserve whether she wins or not) and \\emph{full information feedback} (the\nlearner can observe $m_t$) at the end of each time $t$. For binary feedback,\nwhen the noise distribution $\\mathcal{F}$ is known, we propose a bidding\nalgorithm, by using maximum likelihood estimation (MLE) method to achieve at\nmost $\\widetilde{O}(\\sqrt{\\log(d) T})$ regret. Moreover, we generalize this\nalgorithm to the setting with binary feedback and the noise distribution is\nunknown but belongs to a parametrized family of distributions. For the full\ninformation feedback with \\emph{unknown} noise distribution, we provide an\nalgorithm that achieves regret at most $\\widetilde{O}(\\sqrt{dT})$. Our approach\ncombines an estimator for log-concave density functions and then MLE method to\nlearn the noise distribution $\\mathcal{F}$ and linear weight $\\alpha_0$\nsimultaneously. We also provide a lower bound result such that any bidding\npolicy in a broad class must achieve regret at least $\\Omega(\\sqrt{T})$, even\nwhen the learner receives the full information feedback and $\\mathcal{F}$ is\nknown.",
          "link": "http://arxiv.org/abs/2109.03173",
          "publishedOn": "2021-09-08T07:20:11.710Z",
          "wordCount": 703,
          "title": "Learning to Bid in Contextual First Price Auctions. (arXiv:2109.03173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.02921",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Malik_M/0/1/0/all/0/1\">Muhammad Ammar Malik</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Michoel_T/0/1/0/all/0/1\">Tom Michoel</a>",
          "description": "Random effect models are popular statistical models for detecting and\ncorrecting spurious sample correlations due to hidden confounders in\ngenome-wide gene expression data. In applications where some confounding\nfactors are known, estimating simultaneously the contribution of known and\nlatent variance components in random effect models is a challenge that has so\nfar relied on numerical gradient-based optimizers to maximize the likelihood\nfunction. This is unsatisfactory because the resulting solution is poorly\ncharacterized and the efficiency of the method may be suboptimal. Here we prove\nanalytically that maximum-likelihood latent variables can always be chosen\northogonal to the known confounding factors, in other words, that\nmaximum-likelihood latent variables explain sample covariances not already\nexplained by known factors. Based on this result we propose a restricted\nmaximum-likelihood method which estimates the latent variables by maximizing\nthe likelihood on the restricted subspace orthogonal to the known confounding\nfactors, and show that this reduces to probabilistic PCA on that subspace. The\nmethod then estimates the variance-covariance parameters by maximizing the\nremaining terms in the likelihood function given the latent variables, using a\nnewly derived analytic solution for this problem. Compared to gradient-based\noptimizers, our method attains greater or equal likelihood values, can be\ncomputed using standard matrix operations, results in latent factors that don't\noverlap with any known factors, and has a runtime reduced by several orders of\nmagnitude. Hence the restricted maximum-likelihood method facilitates the\napplication of random effect modelling strategies for learning latent variance\ncomponents to much larger gene expression datasets than possible with current\nmethods.",
          "link": "http://arxiv.org/abs/2005.02921",
          "publishedOn": "2021-09-08T07:20:11.700Z",
          "wordCount": 750,
          "title": "Restricted maximum-likelihood method for learning latent variance components in gene expression data with known and unknown confounders. (arXiv:2005.02921v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karimi_B/0/1/0/all/0/1\">Belhal Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weijie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>",
          "description": "Adaptive gradient methods including Adam, AdaGrad, and their variants have\nbeen very successful for training deep learning models, such as neural\nnetworks. Meanwhile, given the need for distributed computing, distributed\noptimization algorithms are rapidly becoming a focal point. With the growth of\ncomputing power and the need for using machine learning models on mobile\ndevices, the communication cost of distributed training algorithms needs\ncareful consideration. In this paper, we introduce novel convergent\ndecentralized adaptive gradient methods and rigorously incorporate adaptive\ngradient methods into decentralized training procedures. Specifically, we\npropose a general algorithmic framework that can convert existing adaptive\ngradient methods to their decentralized counterparts. In addition, we\nthoroughly analyze the convergence behavior of the proposed algorithmic\nframework and show that if a given adaptive gradient method converges, under\nsome specific conditions, then its decentralized counterpart is also\nconvergent. We illustrate the benefit of our generic decentralized framework on\na prototype method, i.e., AMSGrad, both theoretically and numerically.",
          "link": "http://arxiv.org/abs/2109.03194",
          "publishedOn": "2021-09-08T07:20:11.694Z",
          "wordCount": 596,
          "title": "On the Convergence of Decentralized Adaptive Gradient Methods. (arXiv:2109.03194v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "Recent studies on compression of pretrained language models (e.g., BERT)\nusually use preserved accuracy as the metric for evaluation. In this paper, we\npropose two new metrics, label loyalty and probability loyalty that measure how\nclosely a compressed model (i.e., student) mimics the original model (i.e.,\nteacher). We also explore the effect of compression with regard to robustness\nunder adversarial attacks. We benchmark quantization, pruning, knowledge\ndistillation and progressive module replacing with loyalty and robustness. By\ncombining multiple compression techniques, we provide a practical strategy to\nachieve better accuracy, loyalty and robustness.",
          "link": "http://arxiv.org/abs/2109.03228",
          "publishedOn": "2021-09-08T07:20:11.684Z",
          "wordCount": 556,
          "title": "Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.",
          "link": "http://arxiv.org/abs/2004.12835",
          "publishedOn": "2021-09-08T07:20:11.673Z",
          "wordCount": 571,
          "title": "Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_C/0/1/0/all/0/1\">Chaitanya K. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1\">Quentin Cappart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1\">Louis-Martin Rousseau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurent_T/0/1/0/all/0/1\">Thomas Laurent</a>",
          "description": "End-to-end training of neural network solvers for combinatorial optimization\nproblems such as the Travelling Salesman Problem is intractable and inefficient\nbeyond a few hundreds of nodes. While state-of-the-art Machine Learning\napproaches perform closely to classical solvers when trained on trivially small\nsizes, they are unable to generalize the learnt policy to larger instances of\npractical scales. Towards leveraging transfer learning to solve large-scale\nTSPs, this paper identifies inductive biases, model architectures and learning\nalgorithms that promote generalization to instances larger than those seen in\ntraining. Our controlled experiments provide the first principled investigation\ninto such zero-shot generalization, revealing that extrapolating beyond\ntraining data requires rethinking the neural combinatorial optimization\npipeline, from network layers and learning paradigms to evaluation protocols.",
          "link": "http://arxiv.org/abs/2006.07054",
          "publishedOn": "2021-09-08T07:20:11.655Z",
          "wordCount": 622,
          "title": "Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02975",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anggrainingsih_R/0/1/0/all/0/1\">Rini Anggrainingsih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1\">Ghulam Mubashar Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1\">Amitava Datta</a>",
          "description": "The role of social media in opinion formation has far-reaching implications\nin all spheres of society. Though social media provide platforms for expressing\nnews and views, it is hard to control the quality of posts due to the sheer\nvolumes of posts on platforms like Twitter and Facebook. Misinformation and\nrumours have lasting effects on society, as they tend to influence people's\nopinions and also may motivate people to act irrationally. It is therefore very\nimportant to detect and remove rumours from these platforms. The only way to\nprevent the spread of rumours is through automatic detection and classification\nof social media posts. Our focus in this paper is the Twitter social medium, as\nit is relatively easy to collect data from Twitter. The majority of previous\nstudies used supervised learning approaches to classify rumours on Twitter.\nThese approaches rely on feature extraction to obtain both content and context\nfeatures from the text of tweets to distinguish rumours and non-rumours.\nManually extracting features however is time-consuming considering the volume\nof tweets. We propose a novel approach to deal with this problem by utilising\nsentence embedding using BERT to identify rumours on Twitter, rather than the\nusual feature extraction techniques. We use sentence embedding using BERT to\nrepresent each tweet's sentences into a vector according to the contextual\nmeaning of the tweet. We classify those vectors into rumours or non-rumours by\nusing various supervised learning techniques. Our BERT based models improved\nthe accuracy by approximately 10% as compared to previous methods.",
          "link": "http://arxiv.org/abs/2109.02975",
          "publishedOn": "2021-09-08T07:20:11.643Z",
          "wordCount": 714,
          "title": "BERT based classification system for detecting rumours on Twitter. (arXiv:2109.02975v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Maria Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Douglas C. Schmidt</a>",
          "description": "An emerging trend in audio processing is capturing low-level speech\nrepresentations from raw waveforms. These representations have shown promising\nresults on a variety of tasks, such as speech recognition and speech\nseparation. Compared to handcrafted features, learning speech features via\nbackpropagation provides the model greater flexibility in how it represents\ndata for different tasks theoretically. However, results from empirical study\nshows that, in some tasks, such as voice spoof detection, handcrafted features\nare more competitive than learned features. Instead of evaluating handcrafted\nfeatures and raw waveforms independently, this paper proposes an Auxiliary\nRawnet model to complement handcrafted features with features learned from raw\nwaveforms. A key benefit of the approach is that it can improve accuracy at a\nrelatively low computational cost. The proposed Auxiliary Rawnet model is\ntested using the ASVspoof 2019 dataset and the results from this dataset\nindicate that a light-weight waveform encoder can potentially boost the\nperformance of handcrafted-features-based encoders in exchange for a small\namount of additional computational work.",
          "link": "http://arxiv.org/abs/2109.02773",
          "publishedOn": "2021-09-08T07:20:11.600Z",
          "wordCount": 626,
          "title": "Complementing Handcrafted Features with Raw Waveform Using a Light-weight Auxiliary Model. (arXiv:2109.02773v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sapkota_S/0/1/0/all/0/1\">Suman Sapkota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juneja_M/0/1/0/all/0/1\">Manish Juneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keleras_L/0/1/0/all/0/1\">Laurynas Keleras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotwal_P/0/1/0/all/0/1\">Pranav Kotwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattarai_B/0/1/0/all/0/1\">Binod Bhattarai</a>",
          "description": "In this paper we present our solution to extract albedo of branded labels for\ne-commerce products. To this end, we generate a large-scale photo-realistic\nsynthetic data set for albedo extraction followed by training a generative\nmodel to translate images with diverse lighting conditions to albedo. We\nperformed an extensive evaluation to test the generalisation of our method to\nin-the-wild images. From the experimental results, we observe that our solution\ngeneralises well compared to the existing method both in the unseen rendered\nimages as well as in the wild image.",
          "link": "http://arxiv.org/abs/2109.02929",
          "publishedOn": "2021-09-08T07:20:11.588Z",
          "wordCount": 551,
          "title": "Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Cheng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deore_U/0/1/0/all/0/1\">Uday Deore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Myah Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalil_I/0/1/0/all/0/1\">Iya Khalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devarakonda_M/0/1/0/all/0/1\">Murthy Devarakonda</a>",
          "description": "FDA has been promoting enrollment practices that could enhance the diversity\nof clinical trial populations, through broadening eligibility criteria.\nHowever, how to broaden eligibility remains a significant challenge. We propose\nan AI approach to Cohort Optimization (AICO) through transformer-based natural\nlanguage processing of the eligibility criteria and evaluation of the criteria\nusing real-world data. The method can extract common eligibility criteria\nvariables from a large set of relevant trials and measure the generalizability\nof trial designs to real-world patients. It overcomes the scalability limits of\nexisting manual methods and enables rapid simulation of eligibility criteria\ndesign for a disease of interest. A case study on breast cancer trial design\ndemonstrates the utility of the method in improving trial generalizability.",
          "link": "http://arxiv.org/abs/2109.02808",
          "publishedOn": "2021-09-08T07:20:11.582Z",
          "wordCount": 615,
          "title": "A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhixue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopfgartner_F/0/1/0/all/0/1\">Frank Hopfgartner</a>",
          "description": "Toxic comment classification models are often found biased toward identity\nterms which are terms characterizing a specific group of people such as\n\"Muslim\" and \"black\". Such bias is commonly reflected in false-positive\npredictions, i.e. non-toxic comments with identity terms. In this work, we\npropose a novel approach to tackle such bias in toxic comment classification,\nleveraging the notion of subjectivity level of a comment and the presence of\nidentity terms. We hypothesize that when a comment is made about a group of\npeople that is characterized by an identity term, the likelihood of that\ncomment being toxic is associated with the subjectivity level of the comment,\ni.e. the extent to which the comment conveys personal feelings and opinions.\nBuilding upon the BERT model, we propose a new structure that is able to\nleverage these features, and thoroughly evaluate our model on 4 datasets of\nvarying sizes and representing different social media platforms. The results\nshow that our model can consistently outperform BERT and a SOTA model devised\nto address identity term bias in a different way, with a maximum improvement in\nF1 of 2.43% and 1.91% respectively.",
          "link": "http://arxiv.org/abs/2109.02691",
          "publishedOn": "2021-09-08T07:20:11.563Z",
          "wordCount": 656,
          "title": "SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of \"Subjectivity\" and \"Identity Terms\". (arXiv:2109.02691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meng-Chieh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shubhranshu Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faloutsos_C/0/1/0/all/0/1\">Christos Faloutsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutson_T/0/1/0/all/0/1\">T. Noah Hutson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iasemidis_L/0/1/0/all/0/1\">Leon Iasemidis</a>",
          "description": "In a cloud of m-dimensional data points, how would we spot, as well as rank,\nboth single-point- as well as group- anomalies? We are the first to generalize\nanomaly detection in two dimensions: The first dimension is that we handle both\npoint-anomalies, as well as group-anomalies, under a unified view -- we shall\nrefer to them as generalized anomalies. The second dimension is that gen2Out\nnot only detects, but also ranks, anomalies in suspiciousness order. Detection,\nand ranking, of anomalies has numerous applications: For example, in EEG\nrecordings of an epileptic patient, an anomaly may indicate a seizure; in\ncomputer network traffic data, it may signify a power failure, or a DoS/DDoS\nattack. We start by setting some reasonable axioms; surprisingly, none of the\nearlier methods pass all the axioms. Our main contribution is the gen2Out\nalgorithm, that has the following desirable properties: (a) Principled and\nSound anomaly scoring that obeys the axioms for detectors, (b) Doubly-general\nin that it detects, as well as ranks generalized anomaly -- both point- and\ngroup-anomalies, (c) Scalable, it is fast and scalable, linear on input size.\n(d) Effective, experiments on real-world epileptic recordings (200GB)\ndemonstrate effectiveness of gen2Out as confirmed by clinicians. Experiments on\n27 real-world benchmark datasets show that gen2Out detects ground truth groups,\nmatches or outperforms point-anomaly baseline algorithms on accuracy, with no\ncompetition for group-anomalies and requires about 2 minutes for 1 million data\npoints on a stock machine.",
          "link": "http://arxiv.org/abs/2109.02704",
          "publishedOn": "2021-09-08T07:20:11.545Z",
          "wordCount": 691,
          "title": "gen2Out: Detecting and Ranking Generalized Anomalies. (arXiv:2109.02704v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02723",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Laat_L/0/1/0/all/0/1\">Leonardo van der Laat</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Baldares_R/0/1/0/all/0/1\">Ronald J.L. Baldares</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chaves_E/0/1/0/all/0/1\">Esteban J. Chaves</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Meneses_E/0/1/0/all/0/1\">Esteban Meneses</a>",
          "description": "Small magnitude earthquakes are the most abundant but the most difficult to\nlocate robustly and well due to their low amplitudes and high frequencies\nusually obscured by heterogeneous noise sources. They highlight crucial\ninformation about the stress state and the spatio-temporal behavior of fault\nsystems during the earthquake cycle, therefore, its full characterization is\nthen crucial for improving earthquake hazard assessment. Modern DL algorithms\nalong with the increasing computational power are exploiting the continuously\ngrowing seismological databases, allowing scientists to improve the\ncompleteness for earthquake catalogs, systematically detecting smaller\nmagnitude earthquakes and reducing the errors introduced mainly by human\nintervention. In this work, we introduce OKSP, a novel automatic earthquake\ndetection pipeline for seismic monitoring in Costa Rica. Using Kabre\nsupercomputer from the Costa Rica High Technology Center, we applied OKSP to\nthe day before and the first 5 days following the Puerto Armuelles, M6.5,\nearthquake that occurred on 26 June, 2019, along the Costa Rica-Panama border\nand found 1100 more earthquakes previously unidentified by the Volcanological\nand Seismological Observatory of Costa Rica. From these events, a total of 23\nearthquakes with magnitudes below 1.0 occurred a day to hours prior to the\nmainshock, shedding light about the rupture initiation and earthquake\ninteraction leading to the occurrence of this productive seismic sequence. Our\nobservations show that for the study period, the model was 100% exhaustive and\n82% precise, resulting in an F1 score of 0.90. This effort represents the very\nfirst attempt for automatically detecting earthquakes in Costa Rica using deep\nlearning methods and demonstrates that, in the near future, earthquake\nmonitoring routines will be carried out entirely by AI algorithms.",
          "link": "http://arxiv.org/abs/2109.02723",
          "publishedOn": "2021-09-08T07:20:11.537Z",
          "wordCount": 734,
          "title": "OKSP: A Novel Deep Learning Automatic Event Detection Pipeline for Seismic Monitoringin Costa Rica. (arXiv:2109.02723v1 [physics.geo-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burdick_R/0/1/0/all/0/1\">Ryerson Burdick</a>",
          "description": "The application of Generative Pre-trained Transformer (GPT-2) to learn\ntext-archived game notation provides a model environment for exploring sparse\nreward gameplay. The transformer architecture proves amenable to training on\nsolved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The\nmethod benefits from fine-tuning the transformer architecture to visualize\nplausible strategies derived outside any guidance from human heuristics or\ndomain expertise. The large search space ($>10^{19}$) for the games provides a\npuzzle environment in which the solution has few intermediate rewards and a\nfinal move that solves the challenge.",
          "link": "http://arxiv.org/abs/2109.02797",
          "publishedOn": "2021-09-08T07:20:11.527Z",
          "wordCount": 537,
          "title": "Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Quchen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1\">Zhongwei Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jules White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_M/0/1/0/all/0/1\">Maria Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Douglas C. Schmidt</a>",
          "description": "Voice assistants, such as smart speakers, have exploded in popularity. It is\ncurrently estimated that the smart speaker adoption rate has exceeded 35% in\nthe US adult population. Manufacturers have integrated speaker identification\ntechnology, which attempts to determine the identity of the person speaking, to\nprovide personalized services to different members of the same family. Speaker\nidentification can also play an important role in controlling how the smart\nspeaker is used. For example, it is not critical to correctly identify the user\nwhen playing music. However, when reading the user's email out loud, it is\ncritical to correctly verify the speaker that making the request is the\nauthorized user. Speaker verification systems, which authenticate the speaker\nidentity, are therefore needed as a gatekeeper to protect against various\nspoofing attacks that aim to impersonate the enrolled user. This paper compares\npopular learnable front-ends which learn the representations of audio by joint\ntraining with downstream tasks (End-to-End). We categorize the front-ends by\ndefining two generic architectures and then analyze the filtering stages of\nboth types in terms of learning constraints. We propose replacing fixed\nfilterbanks with a learnable layer that can better adapt to anti-spoofing\ntasks. The proposed FastAudio front-end is then tested with two popular\nback-ends to measure the performance on the LA track of the ASVspoof 2019\ndataset. The FastAudio front-end achieves a relative improvement of 27% when\ncompared with fixed front-ends, outperforming all other learnable front-ends on\nthis task.",
          "link": "http://arxiv.org/abs/2109.02774",
          "publishedOn": "2021-09-08T07:20:11.506Z",
          "wordCount": 698,
          "title": "FastAudio: A Learnable Audio Front-End for Spoof Speech Detection. (arXiv:2109.02774v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02801",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berlioz_L/0/1/0/all/0/1\">Luis Berlioz</a> (University of Pittsburgh)",
          "description": "We introduce ArGoT, a data set of mathematical terms extracted from the\narticles hosted on the arXiv website. A term is any mathematical concept\ndefined in an article. Using labels in the article's source code and examples\nfrom other popular math websites, we mine all the terms in the arXiv data and\ncompile a comprehensive vocabulary of mathematical terms. Each term can be then\norganized in a dependency graph by using the term's definitions and the arXiv's\nmetadata. Using both hyperbolic and standard word embeddings, we demonstrate\nhow this structure is reflected in the text's vector representation and how\nthey capture relations of entailment in mathematical concepts. This data set is\npart of an ongoing effort to align natural mathematical text with existing\nInteractive Theorem Prover Libraries (ITPs) of formally verified statements.",
          "link": "http://arxiv.org/abs/2109.02801",
          "publishedOn": "2021-09-08T07:20:11.499Z",
          "wordCount": 590,
          "title": "ArGoT: A Glossary of Terms extracted from the arXiv. (arXiv:2109.02801v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03029",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_T/0/1/0/all/0/1\">Tathagata Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollada_M/0/1/0/all/0/1\">Matthew Kollada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gersberg_P/0/1/0/all/0/1\">Pablo Gersberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_O/0/1/0/all/0/1\">Oscar Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiller_J/0/1/0/all/0/1\">Jane Tiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaffe_A/0/1/0/all/0/1\">Andrew E Jaffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynders_J/0/1/0/all/0/1\">John Reynders</a>",
          "description": "We developed a novel, interpretable multimodal classification method to\nidentify symptoms of mood disorders viz. depression, anxiety and anhedonia\nusing audio, video and text collected from a smartphone application. We used\nCNN-based unimodal encoders to learn dynamic embeddings for each modality and\nthen combined these through a transformer encoder. We applied these methods to\na novel dataset - collected by a smartphone application - on 3002 participants\nacross up to three recording sessions. Our method demonstrated better\nmultimodal classification performance compared to existing methods that\nemployed static embeddings. Lastly, we used SHapley Additive exPlanations\n(SHAP) to prioritize important features in our model that could serve as\npotential digital markers.",
          "link": "http://arxiv.org/abs/2109.03029",
          "publishedOn": "2021-09-08T07:20:11.492Z",
          "wordCount": 596,
          "title": "Predicting Mood Disorder Symptoms with Remotely Collected Videos Using an Interpretable Multimodal Dynamic Attention Fusion Network. (arXiv:2109.03029v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02832",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_M/0/1/0/all/0/1\">Minshuo Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Liao_W/0/1/0/all/0/1\">Wenjing Liao</a>",
          "description": "Most of existing statistical theories on deep neural networks have sample\ncomplexities cursed by the data dimension and therefore cannot well explain the\nempirical success of deep learning on high-dimensional data. To bridge this\ngap, we propose to exploit low-dimensional geometric structures of the real\nworld data sets. We establish theoretical guarantees of convolutional residual\nnetworks (ConvResNet) in terms of function approximation and statistical\nestimation for binary classification. Specifically, given the data lying on a\n$d$-dimensional manifold isometrically embedded in $\\mathbb{R}^D$, we prove\nthat if the network architecture is properly chosen, ConvResNets can (1)\napproximate Besov functions on manifolds with arbitrary accuracy, and (2) learn\na classifier by minimizing the empirical logistic risk, which gives an excess\nrisk in the order of $n^{-\\frac{s}{2s+2(s\\vee d)}}$, where $s$ is a smoothness\nparameter. This implies that the sample complexity depends on the intrinsic\ndimension $d$, instead of the data dimension $D$. Our results demonstrate that\nConvResNets are adaptive to low-dimensional structures of data sets.",
          "link": "http://arxiv.org/abs/2109.02832",
          "publishedOn": "2021-09-08T07:20:11.486Z",
          "wordCount": 618,
          "title": "Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks. (arXiv:2109.02832v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03099",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_Thu_V/0/1/0/all/0/1\">V&#xe2;n Anh Huynh-Thu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geurts_P/0/1/0/all/0/1\">Pierre Geurts</a>",
          "description": "This paper presents a model-agnostic ensemble approach for supervised\nlearning. The proposed approach alternates between (1) learning an ensemble of\nmodels using a parametric version of the Random Subspace approach, in which\nfeature subsets are sampled according to Bernoulli distributions, and (2)\nidentifying the parameters of the Bernoulli distributions that minimize the\ngeneralization error of the ensemble model. Parameter optimization is rendered\ntractable by using an importance sampling approach able to estimate the\nexpected model output for any given parameter set, without the need to learn\nnew models. While the degree of randomization is controlled by a\nhyper-parameter in standard Random Subspace, it has the advantage to be\nautomatically tuned in our parametric version. Furthermore, model-agnostic\nfeature importance scores can be easily derived from the trained ensemble\nmodel. We show the good performance of the proposed approach, both in terms of\nprediction and feature ranking, on simulated and real-world datasets. We also\nshow that our approach can be successfully used for the reconstruction of gene\nregulatory networks.",
          "link": "http://arxiv.org/abs/2109.03099",
          "publishedOn": "2021-09-08T07:20:11.479Z",
          "wordCount": 597,
          "title": "Optimizing model-agnostic Random Subspace ensembles. (arXiv:2109.03099v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_A/0/1/0/all/0/1\">Andrew Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_A/0/1/0/all/0/1\">Anhthy Ngo</a>",
          "description": "As machine learning systems become more ubiquitous, methods for understanding\nand interpreting these models become increasingly important. In particular,\npractitioners are often interested both in what features the model relies on\nand how the model relies on them--the feature's impact on model predictions.\nPrior work on feature impact including partial dependence plots (PDPs) and\nIndividual Conditional Expectation (ICE) plots has focused on a visual\ninterpretation of feature impact. We propose a natural extension to ICE plots\nwith ICE feature impact, a model-agnostic, performance-agnostic feature impact\nmetric drawn out from ICE plots that can be interpreted as a close analogy to\nlinear regression coefficients. Additionally, we introduce an in-distribution\nvariant of ICE feature impact to vary the influence of out-of-distribution\npoints as well as heterogeneity and non-linearity measures to characterize\nfeature impact. Lastly, we demonstrate ICE feature impact's utility in several\ntasks using real-world data.",
          "link": "http://arxiv.org/abs/2109.02724",
          "publishedOn": "2021-09-08T07:20:11.465Z",
          "wordCount": 609,
          "title": "Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots. (arXiv:2109.02724v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02836",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fields_G/0/1/0/all/0/1\">Greg Fields</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samragh_M/0/1/0/all/0/1\">Mohammad Samragh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaheripi_M/0/1/0/all/0/1\">Mojan Javaheripi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1\">Farinaz Koushanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javidi_T/0/1/0/all/0/1\">Tara Javidi</a>",
          "description": "Deep neural networks have been shown to be vulnerable to backdoor, or trojan,\nattacks where an adversary has embedded a trigger in the network at training\ntime such that the model correctly classifies all standard inputs, but\ngenerates a targeted, incorrect classification on any input which contains the\ntrigger. In this paper, we present the first ultra light-weight and highly\neffective trojan detection method that does not require access to the\ntraining/test data, does not involve any expensive computations, and makes no\nassumptions on the nature of the trojan trigger. Our approach focuses on\nanalysis of the weights of the final, linear layer of the network. We\nempirically demonstrate several characteristics of these weights that occur\nfrequently in trojaned networks, but not in benign networks. In particular, we\nshow that the distribution of the weights associated with the trojan target\nclass is clearly distinguishable from the weights associated with other\nclasses. Using this, we demonstrate the effectiveness of our proposed detection\nmethod against state-of-the-art attacks across a variety of architectures,\ndatasets, and trigger types.",
          "link": "http://arxiv.org/abs/2109.02836",
          "publishedOn": "2021-09-08T07:20:11.429Z",
          "wordCount": 613,
          "title": "Trojan Signatures in DNN Weights. (arXiv:2109.02836v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02715",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuankai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanhong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lijun Sun</a>",
          "description": "Individual mobility prediction is an essential task for transportation demand\nmanagement and traffic system operation. There exist a large body of works on\nmodeling location sequence and predicting the next location of users; however,\nlittle attention is paid to the prediction of the next trip, which is governed\nby the strong spatiotemporal dependencies between diverse attributes, including\ntrip start time $t$, origin $o$, and destination $d$. To fill this gap, in this\npaper we propose a novel point process-based model -- Attentive Marked temporal\npoint processes (AMTPP) -- to model human mobility and predict the whole trip\n$(t,o,d)$ in a joint manner. To encode the influence of history trips, AMTPP\nemploys the self-attention mechanism with a carefully designed positional\nembedding to capture the daily/weekly periodicity and regularity in individual\ntravel behavior. Given the unique peaked nature of inter-event time in human\nbehavior, we use an asymmetric log-Laplace mixture distribution to precisely\nmodel the distribution of trip start time $t$. Furthermore, an\norigin-destination (OD) matrix learning block is developed to model the\nrelationship between every origin and destination pair. Experimental results on\ntwo large metro trip datasets demonstrate the superior performance of AMTPP.",
          "link": "http://arxiv.org/abs/2109.02715",
          "publishedOn": "2021-09-08T07:20:11.421Z",
          "wordCount": 632,
          "title": "Individual Mobility Prediction via Attentive Marked Temporal Point Processes. (arXiv:2109.02715v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1\">Tomohiko Mizutani</a>",
          "description": "Hottopixx, proposed by Bittorf et al. at NIPS 2012, is an algorithm for\nsolving nonnegative matrix factorization (NMF) problems under the separability\nassumption. Separable NMFs have important applications, such as topic\nextraction from documents and unmixing of hyperspectral images. In such\napplications, the robustness of the algorithm to noise is the key to the\nsuccess. Hottopixx has been shown to be robust to noise, and its robustness can\nbe further enhanced through postprocessing. However, there is a drawback.\nHottopixx and its postprocessing require us to estimate the noise level\ninvolved in the matrix we want to factorize before running, since they use it\nas part of the input data. The noise-level estimation is not an easy task. In\nthis paper, we overcome this drawback. We present a refinement of Hottopixx and\nits postprocessing that runs without prior knowledge of the noise level. We\nshow that the refinement has almost the same robustness to noise as the\noriginal algorithm.",
          "link": "http://arxiv.org/abs/2109.02863",
          "publishedOn": "2021-09-08T07:20:11.390Z",
          "wordCount": 599,
          "title": "Refinement of Hottopixx and its Postprocessing. (arXiv:2109.02863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02752",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ponti_M/0/1/0/all/0/1\">Moacir Antonelli Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_F/0/1/0/all/0/1\">Fernando Pereira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leo Sampaio Ferraz Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_G/0/1/0/all/0/1\">Gabriel Biscaro Cavallari</a>",
          "description": "Training deep neural networks may be challenging in real world data. Using\nmodels as black-boxes, even with transfer learning, can result in poor\ngeneralization or inconclusive results when it comes to small datasets or\nspecific applications. This tutorial covers the basic steps as well as more\nrecent options to improve models, in particular, but not restricted to,\nsupervised learning. It can be particularly useful in datasets that are not as\nwell-prepared as those in challenges, and also under scarce annotation and/or\nsmall data. We describe basic procedures: as data preparation, optimization and\ntransfer learning, but also recent architectural choices such as use of\ntransformer modules, alternative convolutional layers, activation functions,\nwide and deep networks, as well as training procedures including as curriculum,\ncontrastive and self-supervised learning.",
          "link": "http://arxiv.org/abs/2109.02752",
          "publishedOn": "2021-09-08T07:20:11.365Z",
          "wordCount": 590,
          "title": "Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xueping Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1\">Guodong Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jing Jiang</a>",
          "description": "Sequential diagnosis prediction on the Electronic Health Record (EHR) has\nbeen proven crucial for predictive analytics in the medical domain. EHR data,\nsequential records of a patient's interactions with healthcare systems, has\nnumerous inherent characteristics of temporality, irregularity and data\ninsufficiency. Some recent works train healthcare predictive models by making\nuse of sequential information in EHR data, but they are vulnerable to\nirregular, temporal EHR data with the states of admission/discharge from\nhospital, and insufficient data. To mitigate this, we propose an end-to-end\nrobust transformer-based model called SETOR, which exploits neural ordinary\ndifferential equation to handle both irregular intervals between a patient's\nvisits with admitted timestamps and length of stay in each visit, to alleviate\nthe limitation of insufficient data by integrating medical ontology, and to\ncapture the dependencies between the patient's visits by employing multi-layer\ntransformer blocks. Experiments conducted on two real-world healthcare datasets\nshow that, our sequential diagnoses prediction model SETOR not only achieves\nbetter predictive results than previous state-of-the-art approaches,\nirrespective of sufficient or insufficient training data, but also derives more\ninterpretable embeddings of medical codes. The experimental codes are available\nat the GitHub repository (https://github.com/Xueping/SETOR).",
          "link": "http://arxiv.org/abs/2109.03069",
          "publishedOn": "2021-09-08T07:20:11.335Z",
          "wordCount": 652,
          "title": "Sequential Diagnosis Prediction with Transformer and Ontological Representation. (arXiv:2109.03069v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalantzi_M/0/1/0/all/0/1\">Maria Kalantzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Graph Neural Networks (GNNs) bring the power of deep representation learning\nto graph and relational data and achieve state-of-the-art performance in many\napplications. GNNs compute node representations by taking into account the\ntopology of the node's ego-network and the features of the ego-network's nodes.\nWhen the nodes do not have high-quality features, GNNs learn an embedding layer\nto compute node embeddings and use them as input features. However, the size of\nthe embedding layer is linear to the product of the number of nodes in the\ngraph and the dimensionality of the embedding and does not scale to big data\nand graphs with hundreds of millions of nodes. To reduce the memory associated\nwith this embedding layer, hashing-based approaches, commonly used in\napplications like NLP and recommender systems, can potentially be used.\nHowever, a direct application of these ideas fails to exploit the fact that in\nmany real-world graphs, nodes that are topologically close will tend to be\nrelated to each other (homophily) and as such their representations will be\nsimilar.\n\nIn this work, we present approaches that take advantage of the nodes'\nposition in the graph to dramatically reduce the memory required, with minimal\nif any degradation in the quality of the resulting GNN model. Our approaches\ndecompose a node's embedding into two components: a position-specific component\nand a node-specific component. The position-specific component models homophily\nand the node-specific component models the node-to-node variation. Extensive\nexperiments using different datasets and GNN models show that our methods are\nable to reduce the memory requirements by 88% to 97% while achieving, in nearly\nall cases, better classification accuracy than other competing approaches,\nincluding the full embeddings.",
          "link": "http://arxiv.org/abs/2109.00101",
          "publishedOn": "2021-09-08T07:20:11.311Z",
          "wordCount": 736,
          "title": "Position-based Hash Embeddings For Scaling Graph Neural Networks. (arXiv:2109.00101v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.11817",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Ziji Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Futao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "More transformer blocks with residual connections have recently achieved\nimpressive results on various tasks. To achieve better performance with fewer\ntrainable parameters, recent methods are proposed to go shallower by parameter\nsharing or model compressing along with the depth. However, weak modeling\ncapacity limits their performance. Contrastively, going wider by inducing more\ntrainable matrixes and parameters would produce a huge model requiring advanced\nparallelism to train and inference.\n\nIn this paper, we propose a parameter-efficient framework, going wider\ninstead of deeper. Specially, following existing works, we adapt parameter\nsharing to compress along depth. But, such deployment would limit the\nperformance. To maximize modeling capacity, we scale along model width by\nreplacing feed-forward network (FFN) with mixture-of-experts (MoE). Across\ntransformer blocks, instead of sharing normalization layers, we propose to use\nindividual layernorms to transform various semantic representations in a more\nparameter-efficient way. To evaluate our plug-and-run framework, we design\nWideNet and conduct comprehensive experiments on popular computer vision and\nnatural language processing benchmarks. On ImageNet-1K, our best model\noutperforms Vision Transformer (ViT) by $1.5\\%$ with $0.72 \\times$ trainable\nparameters. Using $0.46 \\times$ and $0.13 \\times$ parameters, our WideNet can\nstill surpass ViT and ViT-MoE by $0.8\\%$ and $2.1\\%$, respectively. On four\nnatural language processing datasets, WideNet outperforms ALBERT by $1.8\\%$ on\naverage and surpass BERT using factorized embedding parameterization by $0.8\\%$\nwith fewer parameters.",
          "link": "http://arxiv.org/abs/2107.11817",
          "publishedOn": "2021-09-08T07:20:11.265Z",
          "wordCount": 709,
          "title": "Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "User purchasing prediction with multi-behavior information remains a\nchallenging problem for current recommendation systems. Various methods have\nbeen proposed to address it via leveraging the advantages of graph neural\nnetworks (GNNs) or multi-task learning. However, most existing works do not\ntake the complex dependencies among different behaviors of users into\nconsideration. They utilize simple and fixed schemes, like neighborhood\ninformation aggregation or mathematical calculation of vectors, to fuse the\nembeddings of different user behaviors to obtain a unified embedding to\nrepresent a user's behavioral patterns which will be used in downstream\nrecommendation tasks. To tackle the challenge, in this paper, we first propose\nthe concept of hyper meta-path to construct hyper meta-paths or hyper\nmeta-graphs to explicitly illustrate the dependencies among different behaviors\nof a user. How to obtain a unified embedding for a user from hyper meta-paths\nand avoid the previously mentioned limitations simultaneously is critical.\nThanks to the recent success of graph contrastive learning, we leverage it to\nlearn embeddings of user behavior patterns adaptively instead of assigning a\nfixed scheme to understand the dependencies among different behaviors. A new\ngraph contrastive learning based framework is proposed by coupling with hyper\nmeta-paths, namely HMG-CR, which consistently and significantly outperforms all\nbaselines in extensive comparison experiments.",
          "link": "http://arxiv.org/abs/2109.02859",
          "publishedOn": "2021-09-08T07:20:11.246Z",
          "wordCount": 660,
          "title": "Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.13045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walke_H/0/1/0/all/0/1\">Homer Walke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">R. Kenny Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>",
          "description": "Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, editing, and more. Training such inference models is challenging\ndue to the lack of paired (shape, program) data in most domains. A popular\napproach is to pre-train a model on synthetic data and then fine-tune on real\nshapes using slow, unstable reinforcement learning. In this paper, we argue\nthat self-training is a viable alternative for fine-tuning such models.\nSelf-training is a semi-supervised learning paradigm where a model assigns\npseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)\npairs as the new ground truth. We show that for constructive solid geometry and\nassembly-based modeling, self-training outperforms state-of-the-art\nreinforcement learning approaches. Additionally, shape program inference has a\nunique property that circumvents a potential downside of self-training\n(incorrect pseudo-label assignment): inferred programs are executable. For a\ngiven shape from our distribution of interest $\\mathbf{x}^*$ and its predicted\nprogram $\\mathbf{z}$, one can execute $\\mathbf{z}$ to obtain a shape\n$\\mathbf{x}$ and train on $(\\mathbf{z}, \\mathbf{x})$ pairs, rather than\n$(\\mathbf{z}, \\mathbf{x}^*)$ pairs. We term this procedure latent execution\nself training (LEST). We demonstrate that self training infers shape programs\nwith higher shape reconstruction accuracy and converges significantly faster\nthan reinforcement learning approaches, and in some domains, LEST can further\nimprove this performance.",
          "link": "http://arxiv.org/abs/2011.13045",
          "publishedOn": "2021-09-08T07:20:11.232Z",
          "wordCount": 692,
          "title": "Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.07791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharaee_Z/0/1/0/all/0/1\">Zahra Gharaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1\">Shreyas Kowshik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stromann_O/0/1/0/all/0/1\">Oliver Stromann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>",
          "description": "We present a novel learning-based approach to graph representations of road\nnetworks employing state-of-the-art graph convolutional neural networks. Our\napproach is applied to realistic road networks of 17 cities from Open Street\nMap. While edge features are crucial to generate descriptive graph\nrepresentations of road networks, graph convolutional networks usually rely on\nnode features only. We show that the highly representative edge features can\nstill be integrated into such networks by applying a line graph transformation.\nWe also propose a method for neighborhood sampling based on a topological\nneighborhood composed of both local and global neighbors. We compare the\nperformance of learning representations using different types of neighborhood\naggregation functions in transductive and inductive tasks and in supervised and\nunsupervised learning. Furthermore, we propose a novel aggregation approach,\nGraph Attention Isomorphism Network, GAIN. Our results show that GAIN\noutperforms state-of-the-art methods on the road type classification problem.",
          "link": "http://arxiv.org/abs/2107.07791",
          "publishedOn": "2021-09-08T07:20:11.225Z",
          "wordCount": 631,
          "title": "Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03048",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilin Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1\">Chun-An Chou</a>",
          "description": "Respiratory failure is the one of major causes of death in critical care\nunit. During the outbreak of COVID-19, critical care units experienced an\nextreme shortage of mechanical ventilation because of respiratory failure\nrelated syndromes. To help this, the early mortality risk prediction in\npatients who suffer respiratory failure can provide timely support for clinical\ntreatment and resource management. In the study, we propose a dynamic modeling\napproach for early mortality risk prediction of the respiratory failure\npatients based on the first 24 hours ICU physiological data. Our proposed model\nis validated on the eICU collaborate database. We achieved a high AUROC\nperformance (80-83%) and significantly improved AUCPR 4% on Day 5 since ICU\nadmission, compared to the state-of-art prediction models. In addition, we\nillustrated that the survival curve includes the time-varying information for\nthe early ICU admission survival analysis.",
          "link": "http://arxiv.org/abs/2109.03048",
          "publishedOn": "2021-09-08T07:20:11.188Z",
          "wordCount": 624,
          "title": "Early ICU Mortality Prediction and Survival Analysis for Respiratory Failure. (arXiv:2109.03048v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinjun Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiaxing Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_F/0/1/0/all/0/1\">Fei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dajiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Linjiang Zheng</a>",
          "description": "Many real-world data can be represented as heterogeneous graphs with\ndifferent types of nodes and connections. Heterogeneous graph neural network\nmodel aims to embed nodes or subgraphs into low-dimensional vector space for\nvarious downstream tasks such as node classification, link prediction, etc.\nAlthough several models were proposed recently, they either only aggregate\ninformation from the same type of neighbors, or just indiscriminately treat\nhomogeneous and heterogeneous neighbors in the same way. Based on these\nobservations, we propose a new heterogeneous graph neural network model named\nHMSG to comprehensively capture structural, semantic and attribute information\nfrom both homogeneous and heterogeneous neighbors. Specifically, we first\ndecompose the heterogeneous graph into multiple metapath-based homogeneous and\nheterogeneous subgraphs, and each subgraph associates specific semantic and\nstructural information. Then message aggregation methods are applied to each\nsubgraph independently, so that information can be learned in a more targeted\nand efficient manner. Through a type-specific attribute transformation, node\nattributes can also be transferred among different types of nodes. Finally, we\nfuse information from subgraphs together to get the complete representation.\nExtensive experiments on several datasets for node classification, node\nclustering and link prediction tasks show that HMSG achieves the best\nperformance in all evaluation metrics than state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2109.02868",
          "publishedOn": "2021-09-08T07:20:11.182Z",
          "wordCount": 670,
          "title": "HMSG: Heterogeneous Graph Neural Network based on Metapath Subgraph Learning. (arXiv:2109.02868v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02755",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tweardy_M/0/1/0/all/0/1\">Mackenzie Tweardy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wegerich_S/0/1/0/all/0/1\">Stephan W. Wegerich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goergen_C/0/1/0/all/0/1\">Craig J. Goergen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wodicka_G/0/1/0/all/0/1\">George R. Wodicka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>",
          "description": "Photoplethysmography (PPG) is a non-invasive and economical technique to\nextract vital signs of the human body. Although it has been widely used in\nconsumer and research grade wrist devices to track a user's physiology, the PPG\nsignal is very sensitive to motion which can corrupt the signal's quality.\nExisting Motion Artifact (MA) reduction techniques have been developed and\nevaluated using either synthetic noisy signals or signals collected during\nhigh-intensity activities - both of which are difficult to generalize for\nreal-life scenarios. Therefore, it is valuable to collect realistic PPG signals\nwhile performing Activities of Daily Living (ADL) to develop practical signal\ndenoising and analysis methods. In this work, we propose an automatic pseudo\nclean PPG generation process for reliable PPG signal selection. For each noisy\nPPG segment, the corresponding pseudo clean PPG reduces the MAs and contains\nrich temporal details depicting cardiac features. Our experimental results show\nthat 71% of the pseudo clean PPG collected from ADL can be considered as high\nquality segment where the derived MAE of heart rate and respiration rate are\n1.46 BPM and 3.93 BrPM, respectively. Therefore, our proposed method can\ndetermine the reliability of the raw noisy PPG by considering quality of the\ncorresponding pseudo clean PPG signal.",
          "link": "http://arxiv.org/abs/2109.02755",
          "publishedOn": "2021-09-08T07:20:11.173Z",
          "wordCount": 660,
          "title": "Motion Artifact Reduction In Photoplethysmography For Reliable Signal Selection. (arXiv:2109.02755v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02748",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_S/0/1/0/all/0/1\">Sepideh Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_E/0/1/0/all/0/1\">Eric Robertson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>",
          "description": "In a regular open set detection problem, samples of known classes (also\ncalled closed set classes) are used to train a special classifier. In testing,\nthe classifier can (1) classify the test samples of known classes to their\nrespective classes and (2) also detect samples that do not belong to any of the\nknown classes (we say they belong to some unknown or open set classes). This\npaper studies the problem of zero-shot open-set detection, which still performs\nthe same two tasks in testing but has no training except using the given known\nclass names. This paper proposes a novel and yet simple method (called ZO-CLIP)\nto solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot\nclassification through multi-modal representation learning. It first extends\nthe pre-trained multi-modal model CLIP by training a text-based image\ndescription generator on top of CLIP. In testing, it uses the extended model to\ngenerate some candidate unknown class names for each test sample and computes a\nconfidence score based on both the known class names and candidate unknown\nclass names for zero-shot open set detection. Experimental results on 5\nbenchmark datasets for open set detection confirm that ZO-CLIP outperforms the\nbaselines by a large margin.",
          "link": "http://arxiv.org/abs/2109.02748",
          "publishedOn": "2021-09-08T07:20:11.154Z",
          "wordCount": 651,
          "title": "Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Casey_O/0/1/0/all/0/1\">Owen Casey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seliya_N/0/1/0/all/0/1\">Naeem Seliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boone_E/0/1/0/all/0/1\">Evelyn R Sowells Boone</a>",
          "description": "In this paper machine learning networks are explored for their use in\nrestoring degraded and compressed speech audio. The project intent is to build\na new trained model from voice data to learn features of compression\nartifacting distortion introduced by data loss from lossy compression and\nresolution loss with an existing algorithm presented in SEGAN: Speech\nEnhancement Generative Adversarial Network. The resulting generator from the\nmodel was then to be used to restore degraded speech audio. This paper details\nan examination of the subsequent compatibility and operational issues presented\nby working with deprecated code, which obstructed the trained model from\nsuccessfully being developed. This paper further serves as an examination of\nthe challenges, limitations, and compatibility in the current state of machine\nlearning.",
          "link": "http://arxiv.org/abs/2109.02692",
          "publishedOn": "2021-09-08T07:20:11.146Z",
          "wordCount": 580,
          "title": "Machine Learning: Challenges, Limitations, and Compatibility for Audio Restoration Processes. (arXiv:2109.02692v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1\">Kexin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaspari_T/0/1/0/all/0/1\">Theodora Chaspari</a>",
          "description": "Speech-based machine learning (ML) has been heralded as a promising solution\nfor tracking prosodic and spectrotemporal patterns in real-life that are\nindicative of emotional changes, providing a valuable window into one's\ncognitive and mental state. Yet, the scarcity of labelled data in ambulatory\nstudies prevents the reliable training of ML models, which usually rely on\n\"data-hungry\" distribution-based learning. Leveraging the abundance of labelled\nspeech data from acted emotions, this paper proposes a few-shot learning\napproach for automatically recognizing emotion in spontaneous speech from a\nsmall number of labelled samples. Few-shot learning is implemented via a metric\nlearning approach through a siamese neural network, which models the relative\ndistance between samples rather than relying on learning absolute patterns of\nthe corresponding distributions of each emotion. Results indicate the\nfeasibility of the proposed metric learning in recognizing emotions from\nspontaneous speech in four datasets, even with a small amount of labelled\nsamples. They further demonstrate superior performance of the proposed metric\nlearning compared to commonly used adaptation methods, including network\nfine-tuning and adversarial learning. Findings from this work provide a\nfoundation for the ambulatory tracking of human emotion in spontaneous speech\ncontributing to the real-life assessment of mental health degradation.",
          "link": "http://arxiv.org/abs/2109.02915",
          "publishedOn": "2021-09-08T07:20:11.140Z",
          "wordCount": 666,
          "title": "Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a Siamese Neural Network with Adaptive Sample Pair Formation. (arXiv:2109.02915v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zejiang Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kung_S/0/1/0/all/0/1\">Sun-Yuan Kung</a>",
          "description": "We study the few-shot learning (FSL) problem, where a model learns to\nrecognize new objects with extremely few labeled training data per category.\nMost of previous FSL approaches resort to the meta-learning paradigm, where the\nmodel accumulates inductive bias through learning many training tasks so as to\nsolve a new unseen few-shot task. In contrast, we propose a simple approach to\nexploit unlabeled data accompanying the few-shot task for improving few-shot\nperformance. Firstly, we propose a Dependency Maximization method based on the\nHilbert-Schmidt norm of the cross-covariance operator, which maximizes the\nstatistical dependency between the embedded feature of those unlabeled data and\ntheir label predictions, together with the supervised loss over the support\nset. We then use the obtained model to infer the pseudo-labels for those\nunlabeled data. Furthermore, we propose anInstance Discriminant Analysis to\nevaluate the credibility of each pseudo-labeled example and select the most\nfaithful ones into an augmented support set to retrain the model as in the\nfirst step. We iterate the above process until the pseudo-labels for the\nunlabeled data becomes stable. Following the standard transductive and\nsemi-supervised FSL setting, our experiments show that the proposed method\nout-performs previous state-of-the-art methods on four widely used benchmarks,\nincluding mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.",
          "link": "http://arxiv.org/abs/2109.02820",
          "publishedOn": "2021-09-08T07:20:11.126Z",
          "wordCount": 656,
          "title": "Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02765",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1\">Omid Poursaeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianxing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">SerNam Lim</a>",
          "description": "While deep neural networks have achieved remarkable success in various\ncomputer vision tasks, they often fail to generalize to new domains and subtle\nvariations of input images. Several defenses have been proposed to improve the\nrobustness against these variations. However, current defenses can only\nwithstand the specific attack used in training, and the models often remain\nvulnerable to other input variations. Moreover, these methods often degrade\nperformance of the model on clean images and do not generalize to out-of-domain\nsamples. In this paper we present Generative Adversarial Training, an approach\nto simultaneously improve the model's generalization to the test set and\nout-of-domain samples as well as its robustness to unseen adversarial attacks.\nInstead of altering a low-level pre-defined aspect of images, we generate a\nspectrum of low-level, mid-level and high-level changes using generative models\nwith a disentangled latent space. Adversarial training with these examples\nenable the model to withstand a wide range of attacks by observing a variety of\ninput alterations during training. We show that our approach not only improves\nperformance of the model on clean images and out-of-domain samples but also\nmakes it robust against unforeseen attacks and outperforms prior work. We\nvalidate effectiveness of our method by demonstrating results on various tasks\nsuch as classification, segmentation and object detection.",
          "link": "http://arxiv.org/abs/2109.02765",
          "publishedOn": "2021-09-08T07:20:11.120Z",
          "wordCount": 676,
          "title": "Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.08888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingqi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1\">Mon-on Pun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haojun Li</a>",
          "description": "Maintaining the long-term exploration capability of the agent remains one of\nthe critical challenges in deep reinforcement learning. A representative\nsolution is to leverage reward shaping to provide intrinsic rewards for the\nagent to encourage exploration. However, most existing methods suffer from\nvanishing intrinsic rewards, which cannot provide sustainable exploration\nincentives. Moreover, they rely heavily on complex models and additional memory\nto record learning procedures, resulting in high computational complexity and\nlow robustness. To tackle this problem, entropy-based methods are proposed to\nevaluate the global exploration performance, encouraging the agent to visit the\nstate space more equitably. However, the sample complexity of estimating the\nstate visitation entropy is prohibitive when handling environments with\nhigh-dimensional observations. In this paper, we introduce a novel metric\nentitled Jain's fairness index (JFI) to replace the entropy regularizer, which\nsolves the exploration problem from a brand new perspective. In sharp contrast\nto the entropy regularizer, JFI is more computable and robust and can be easily\napplied generalized into arbitrary tasks. Furthermore, we leverage a\nvariational auto-encoder (VAE) model to capture the life-long novelty of\nstates, which is combined with the global JFI score to form multimodal\nintrinsic rewards. Finally, extensive simulation results demonstrate that our\nmultimodal reward shaping (MMRS) method can achieve higher performance than\nother benchmark schemes.",
          "link": "http://arxiv.org/abs/2107.08888",
          "publishedOn": "2021-09-07T20:22:05.433Z",
          "wordCount": 692,
          "title": "Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-09-07T20:22:05.411Z",
          "wordCount": 782,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12996",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhro Ghosh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rigollet_P/0/1/0/all/0/1\">Philippe Rigollet</a>",
          "description": "Motivated by cutting-edge applications like cryo-electron microscopy\n(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an\nunknown signal from repeated measurements of its images under the latent action\nof a group of isometries and additive noise of magnitude $\\sigma$. Despite\nsignificant interest, a clear picture for understanding rates of estimation in\nthis model has emerged only recently, particularly in the high-noise regime\n$\\sigma \\gg 1$ that is highly relevant in applications. Recent investigations\nhave revealed a remarkable asymptotic sample complexity of order $\\sigma^6$ for\ncertain signals whose Fourier transforms have full support, in stark contrast\nto the traditional $\\sigma^2$ that arise in regular models. Often prohibitively\nlarge in practice, these results have prompted the investigation of variations\naround the MRA model where better sample complexity may be achieved. In this\npaper, we show that \\emph{sparse} signals exhibit an intermediate $\\sigma^4$\nsample complexity even in the classical MRA model. Our results explore and\nexploit connections of the MRA estimation problem with two classical topics in\napplied mathematics: the \\textit{beltway problem} from combinatorial\noptimization, and \\textit{uniform uncertainty principles} from harmonic\nanalysis.",
          "link": "http://arxiv.org/abs/2106.12996",
          "publishedOn": "2021-09-07T20:22:05.403Z",
          "wordCount": 655,
          "title": "Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08399",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1\">Andrei Velichko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1\">Hanif Heidari</a>",
          "description": "Measuring the predictability and complexity of time series is an essential\ntool in designing and controlling the nonlinear system. Different entropy\nmeasures exist in the literature to analyze the predictability and complexity\nof time series. However, the existed methods have some drawbacks related to a\nstrong dependence of entropy on the parameters of the methods, as well as on\nthe length and amplitude of the time series. To overcome these difficulties,\nthis study proposes a new method for estimating the entropy of a time series\nusing the LogNNet neural network model. The LogNNet reservoir matrix is filled\nwith the time series elements according to our algorithm. The network is\ntrained on MNIST-10 dataset and the classification accuracy is calculated. The\naccuracy is considered as the entropy measure and denoted by NNetEn. The\nnovelty of entropy calculation is that the time series is involved in mixing\nthe input information in the reservoir. The greater complexity of the time\nseries leads to the better ability of the neural network to learn, and to the\nhigher classification accuracy and NNetEn values. The epochs number in the\ntraining process of LogNNet is considered as the control parameter. We\nintroduce a new time series characteristic, called time series learning\ninertia, that determines the learning rate of the neural network. The\nrobustness and efficiency of the method is verified on chaotic, periodic,\nrandom, binary and constant time series. The comparison of NNetEn with other\nmethods of entropy estimation demonstrates that our method is more robust and\naccurate and can be widely used in practice.",
          "link": "http://arxiv.org/abs/2107.08399",
          "publishedOn": "2021-09-07T20:22:05.394Z",
          "wordCount": 752,
          "title": "A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.03869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nor_A/0/1/0/all/0/1\">Ahmad Kamal Bin Mohd Nor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedapait_S/0/1/0/all/0/1\">Srinivasa Rao Pedapait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_M/0/1/0/all/0/1\">Masdi Muhammad</a>",
          "description": "A state-of-the-art systematic review on XAI applied to Prognostic and Health\nManagement (PHM) of industrial asset is presented. This work provides an\noverview of the general trend of XAI in PHM, answers the question of accuracy\nversus explainability, the extent of human involvement, the explanation\nassessment and uncertainty quantification in PHM-XAI domain. Research articles\nassociated with the subject, from 2015 to 2021 were selected from five known\ndatabases following PRISMA guidelines. Data was then extracted from the\nselected articles and examined. Several findings were synthesized. Firstly,\nwhile the discipline is still young, the analysis indicated the growing\nacceptance of XAI in PHM domain. Secondly, XAI functions as a double edge\nsword, where it is assimilated as a tool to execute PHM tasks as well as a mean\nof explanation, particularly in diagnostic and anomaly detection activities,\nimplying a real need for XAI in PHM. Thirdly, the review showed that PHM-XAI\npapers produce either good or excellent result in general, suggesting that PHM\nperformance is unaffected by XAI. Fourthly, human role, evaluation metrics and\nuncertainty management are areas requiring further attention by the PHM\ncommunity. Adequate assessment metrics to cater for PHM need are urgently\nneeded.Finally, most case study featured on the accepted articles are based on\nreal, industrial data, indicating that the available PHM-XAI blends are fit to\nsolve complex,real-world challenges, increasing the confidence in AI adoption\nin the industry.",
          "link": "http://arxiv.org/abs/2107.03869",
          "publishedOn": "2021-09-07T20:22:05.387Z",
          "wordCount": 710,
          "title": "Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.08962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kaza_K/0/1/0/all/0/1\">Kesav Kaza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meshram_R/0/1/0/all/0/1\">Rahul Meshram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Varun Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merchant_S/0/1/0/all/0/1\">S.N.Merchant</a>",
          "description": "This paper studies a class of constrained restless multi-armed bandits\n(CRMAB). The constraints are in the form of time varying set of actions (set of\navailable arms). This variation can be either stochastic or semi-deterministic.\nGiven a set of arms, a fixed number of them can be chosen to be played in each\ndecision interval. The play of each arm yields a state dependent reward. The\ncurrent states of arms are partially observable through binary feedback signals\nfrom arms that are played. The current availability of arms is fully\nobservable. The objective is to maximize long term cumulative reward. The\nuncertainty about future availability of arms along with partial state\ninformation makes this objective challenging. Applications for CRMAB can be\nfound in resource allocation in cyber-physical systems involving components\nwith time varying availability.\n\nFirst, this optimization problem is analyzed using Whittle's index policy. To\nthis end, a constrained restless single-armed bandit is studied. It is shown to\nadmit a threshold-type optimal policy and is also indexable. An algorithm to\ncompute Whittle's index is presented. An alternate solution method with lower\ncomplexity is also presented in the form of an online rollout policy. A\ndetailed discussion on the complexity of both these schemes is also presented,\nwhich suggests that online rollout policy with short look ahead is simpler to\nimplement than Whittle's index computation. Further, upper bounds on the value\nfunction are derived in order to estimate the degree of sub-optimality of\nvarious solutions. The simulation study compares the performance of Whittle's\nindex, online rollout, myopic and modified Whittle's index policies.",
          "link": "http://arxiv.org/abs/1904.08962",
          "publishedOn": "2021-09-07T20:22:05.368Z",
          "wordCount": 770,
          "title": "Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v5 [cs.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen-Yu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "We propose a black-box reduction that turns a certain reinforcement learning\nalgorithm with optimal regret in a (near-)stationary environment into another\nalgorithm with optimal dynamic regret in a non-stationary environment,\nimportantly without any prior knowledge on the degree of non-stationarity. By\nplugging different algorithms into our black-box, we provide a list of examples\nshowing that our approach not only recovers recent results for (contextual)\nmulti-armed bandits achieved by very specialized algorithms, but also\nsignificantly improves the state of the art for (generalized) linear bandits,\nepisodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most\ncases our algorithm achieves the optimal dynamic regret\n$\\widetilde{\\mathcal{O}}(\\min\\{\\sqrt{LT}, \\Delta^{1/3}T^{2/3}\\})$ where $T$ is\nthe number of rounds and $L$ and $\\Delta$ are the number and amount of changes\nof the world respectively, while previous works only obtain suboptimal bounds\nand/or require the knowledge of $L$ and $\\Delta$.",
          "link": "http://arxiv.org/abs/2102.05406",
          "publishedOn": "2021-09-07T20:22:05.354Z",
          "wordCount": 625,
          "title": "Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01034",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>",
          "description": "The Intergovernmental Panel on Climate Change proposes different mitigation\nstrategies to achieve the net emissions reductions that would be required to\nfollow a pathway that limits global warming to 1.5{\\deg}C with no or limited\novershoot. The transition towards a carbon-free society goes through an\ninevitable increase in the share of renewable generation in the energy mix and\na drastic decrease in the total consumption of fossil fuels. Therefore, this\nthesis studies the integration of renewables in power systems by investigating\nforecasting and decision-making tools. Indeed, in contrast to conventional\npower plants, renewable energy is subject to uncertainty. Most of the\ngeneration technologies based on renewable sources are non-dispatchable, and\ntheir production is stochastic and complex to predict in advance. A high share\nof renewables is challenging for power systems that have been designed and\nsized for dispatchable units. In this context, probabilistic forecasts, which\naim at modeling the distribution of all possible future realizations, have\nbecome a vital tool to equip decision-makers, hopefully leading to better\ndecisions in energy applications. This thesis focuses on two main research\nquestions: (1) How to produce reliable probabilistic renewable generation\nforecasts, consumption, and electricity prices? (2) How to make decisions with\nuncertainty using probabilistic forecasts? The thesis perimeter is the energy\nmanagement of \"small\" systems such as microgrids at a residential scale on a\nday-ahead basis. It is divided into two main parts to propose directions to\naddress both research questions (1) a forecasting part; (2) a planning and\ncontrol part.",
          "link": "http://arxiv.org/abs/2107.01034",
          "publishedOn": "2021-09-07T20:22:05.314Z",
          "wordCount": 746,
          "title": "Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v4 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1602.03822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Motivated by a $2$-dimensional (unsupervised) image segmentation task whereby\nlocal regions of pixels are clustered via edge detection methods, a more\ngeneral probabilistic mathematical framework is devised. Critical thresholds\nare calculated that indicate strong correlation between randomly-generated,\nhigh dimensional data points that have been projected into structures in a\npartition of a bounded, $2$-dimensional area, of which, an image is a special\ncase. A neighbor concept for structures in the partition is defined and a\ncritical radius is uncovered. Measured from a central structure in localized\nregions of the partition, the radius indicates strong, long and short range\ncorrelation in the count of occupied structures. The size of a short interval\nof radii is estimated upon which the transition from short-to-long range\ncorrelation is virtually assured, which defines a demarcation of when an image\nceases to be \"interesting\".",
          "link": "http://arxiv.org/abs/1602.03822",
          "publishedOn": "2021-09-07T20:22:05.176Z",
          "wordCount": 717,
          "title": "A Critical Connectivity Radius for Segmenting Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v8 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10785",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Godaz_R/0/1/0/all/0/1\">Reza Godaz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Monsefi_R/0/1/0/all/0/1\">Reza Monsefi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Toutounian_F/0/1/0/all/0/1\">Faezeh Toutounian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>",
          "description": "In this paper, we tackle two important problems in low-rank learning, which\nare partial singular value decomposition and numerical rank estimation of huge\nmatrices. By using the concepts of Krylov subspaces such as Golub-Kahan\nbidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose\ntwo methods for solving these problems in a fast and accurate way. Our\nexperiments show the advantages of the proposed methods compared to the\ntraditional and randomized singular value decomposition methods. The proposed\nmethods are appropriate for applications involving huge matrices where the\naccuracy of the desired singular values and also all of their corresponding\nsingular vectors are essential. As a real application, we evaluate the\nperformance of our methods on the problem of Riemannian similarity learning\nbetween two various image datasets of MNIST and USPS.",
          "link": "http://arxiv.org/abs/2104.10785",
          "publishedOn": "2021-09-07T20:22:05.099Z",
          "wordCount": 614,
          "title": "Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Horiguchi_A/0/1/0/all/0/1\">Akira Horiguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santner_T/0/1/0/all/0/1\">Thomas J. Santner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Ying Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratola_M/0/1/0/all/0/1\">Matthew T. Pratola</a>",
          "description": "Techniques to reduce the energy burden of an industrial ecosystem often\nrequire solving a multiobjective optimization problem. However, collecting\nexperimental data can often be either expensive or time-consuming. In such\ncases, statistical methods can be helpful. This article proposes Pareto Front\n(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression\nTrees (BART), which is a non-parametric model whose assumptions are typically\nless restrictive than popular alternatives, such as Gaussian Processes (GPs).\nThese less restrictive assumptions allow BART to handle scenarios (e.g.\nhigh-dimensional input spaces, nonsmooth responses, large datasets) that GPs\nfind difficult. The performance of our BART-based method is compared to a\nGP-based method using analytic test functions, demonstrating convincing\nadvantages. Finally, our BART-based methodology is applied to a motivating\nengineering problem. Supplementary materials, which include a theorem proof,\nalgorithms, and R code, for this article are available online.",
          "link": "http://arxiv.org/abs/2101.02558",
          "publishedOn": "2021-09-07T07:20:15.609Z",
          "wordCount": 629,
          "title": "Using BART to Perform Pareto Optimization and Quantify its Uncertainties. (arXiv:2101.02558v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Ashwinkumar Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1\">Sunil Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1\">Tim Oates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holt_J/0/1/0/all/0/1\">James Holt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLean_M/0/1/0/all/0/1\">Mark McLean</a>",
          "description": "Holographic Reduced Representations (HRR) are a method for performing\nsymbolic AI on top of real-valued vectors \\cite{Plate1995} by associating each\nvector with an abstract concept, and providing mathematical operations to\nmanipulate vectors as if they were classic symbolic objects. This method has\nseen little use outside of older symbolic AI work and cognitive science. Our\ngoal is to revisit this approach to understand if it is viable for enabling a\nhybrid neural-symbolic approach to learning as a differentiable component of a\ndeep learning architecture. HRRs today are not effective in a differentiable\nsolution due to numerical instability, a problem we solve by introducing a\nprojection step that forces the vectors to exist in a well behaved point in\nspace. In doing so we improve the concept retrieval efficacy of HRRs by over\n$100\\times$. Using multi-label classification we demonstrate how to leverage\nthe symbolic HRR properties to develop an output layer and loss function that\nis able to learn effectively, and allows us to investigate some of the pros and\ncons of an HRR neuro-symbolic learning approach.",
          "link": "http://arxiv.org/abs/2109.02157",
          "publishedOn": "2021-09-07T07:20:15.342Z",
          "wordCount": 625,
          "title": "Learning with Holographic Reduced Representations. (arXiv:2109.02157v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2002.03614",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Aisha Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuoda_G/0/1/0/all/0/1\">Ghadeer Abuoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1\">Abdurrahman Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaoudi_Z/0/1/0/all/0/1\">Zoi Kaoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboulnaga_A/0/1/0/all/0/1\">Ashraf Aboulnaga</a>",
          "description": "Knowledge graphs represented as RDF datasets are integral to many machine\nlearning applications. RDF is supported by a rich ecosystem of data management\nsystems and tools, most notably RDF database systems that provide a SPARQL\nquery interface. Surprisingly, machine learning tools for knowledge graphs do\nnot use SPARQL, despite the obvious advantages of using a database system. This\nis due to the mismatch between SPARQL and machine learning tools in terms of\ndata model and programming style. Machine learning tools work on data in\ntabular format and process it using an imperative programming style, while\nSPARQL is declarative and has as its basic operation matching graph patterns to\nRDF triples. We posit that a good interface to knowledge graphs from a machine\nlearning software stack should use an imperative, navigational programming\nparadigm based on graph traversal rather than the SPARQL query paradigm based\non graph patterns. In this paper, we present RDFFrames, a framework that\nprovides such an interface. RDFFrames provides an imperative Python API that\ngets internally translated to SPARQL, and it is integrated with the PyData\nmachine learning software stack. RDFFrames enables the user to make a sequence\nof Python calls to define the data to be extracted from a knowledge graph\nstored in an RDF database system, and it translates these calls into a compact\nSPQARL query, executes it on the database system, and returns the results in a\nstandard tabular format. Thus, RDFFrames is a useful tool for data preparation\nthat combines the usability of PyData with the flexibility and performance of\nRDF database systems.",
          "link": "http://arxiv.org/abs/2002.03614",
          "publishedOn": "2021-09-07T07:20:15.334Z",
          "wordCount": 764,
          "title": "RDFFrames: Knowledge Graph Access for Machine Learning Tools. (arXiv:2002.03614v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01658",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Storaas_A/0/1/0/all/0/1\">Andrea M. Stor&#xe5;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauslund_J/0/1/0/all/0/1\">Jakob Grauslund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L. Hammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1\">Anis Yazidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundersen_K/0/1/0/all/0/1\">Kjell G. Gundersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utheim_T/0/1/0/all/0/1\">Tor P. Utheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_C/0/1/0/all/0/1\">Catherine Jackson</a>",
          "description": "Dry eye disease (DED) has a prevalence of between 5 and 50\\%, depending on\nthe diagnostic criteria used and population under study. However, it remains\none of the most underdiagnosed and undertreated conditions in ophthalmology.\nMany tests used in the diagnosis of DED rely on an experienced observer for\nimage interpretation, which may be considered subjective and result in\nvariation in diagnosis. Since artificial intelligence (AI) systems are capable\nof advanced problem solving, use of such techniques could lead to more\nobjective diagnosis. Although the term `AI' is commonly used, recent success in\nits applications to medicine is mainly due to advancements in the sub-field of\nmachine learning, which has been used to automatically classify images and\npredict medical outcomes. Powerful machine learning techniques have been\nharnessed to understand nuances in patient data and medical images, aiming for\nconsistent diagnosis and stratification of disease severity. This is the first\nliterature review on the use of AI in DED. We provide a brief introduction to\nAI, report its current use in DED research and its potential for application in\nthe clinic. Our review found that AI has been employed in a wide range of DED\nclinical tests and research applications, primarily for interpretation of\ninterferometry, slit-lamp and meibography images. While initial results are\npromising, much work is still needed on model development, clinical testing and\nstandardisation.",
          "link": "http://arxiv.org/abs/2109.01658",
          "publishedOn": "2021-09-07T07:20:15.324Z",
          "wordCount": 688,
          "title": "Artificial Intelligence in Dry Eye Disease. (arXiv:2109.01658v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1\">Atsuki Yamaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>",
          "description": "Masked language modeling (MLM), a self-supervised pretraining objective, is\nwidely used in natural language processing for learning text representations.\nMLM trains a model to predict a random sample of input tokens that have been\nreplaced by a [MASK] placeholder in a multi-class setting over the entire\nvocabulary. When pretraining, it is common to use alongside MLM other auxiliary\nobjectives on the token or sequence level to improve downstream performance\n(e.g. next sentence prediction). However, no previous work so far has attempted\nin examining whether other simpler linguistically intuitive or not objectives\ncan be used standalone as main pretraining objectives. In this paper, we\nexplore five simple pretraining objectives based on token-level classification\ntasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our\nproposed methods achieve comparable or better performance to MLM using a\nBERT-BASE architecture. We further validate our methods using smaller models,\nshowing that pretraining a model with 41% of the BERT-BASE's parameters,\nBERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.",
          "link": "http://arxiv.org/abs/2109.01819",
          "publishedOn": "2021-09-07T07:20:15.317Z",
          "wordCount": 626,
          "title": "Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2002.10940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Richeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yufan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>",
          "description": "Federated learning (FL) has emerged as a prominent distributed learning\nparadigm. FL entails some pressing needs for developing novel parameter\nestimation approaches with theoretical guarantees of convergence, which are\nalso communication efficient, differentially private and Byzantine resilient in\nthe heterogeneous data distribution settings. Quantization-based SGD solvers\nhave been widely adopted in FL and the recently proposed SIGNSGD with majority\nvote shows a promising direction. However, no existing methods enjoy all the\naforementioned properties. In this paper, we propose an intuitively-simple yet\ntheoretically-sound method based on SIGNSGD to bridge the gap. We present\nStochastic-Sign SGD which utilizes novel stochastic-sign based gradient\ncompressors enabling the aforementioned properties in a unified framework. We\nalso present an error-feedback variant of the proposed Stochastic-Sign SGD\nwhich further improves the learning performance in FL. We test the proposed\nmethod with extensive experiments using deep neural networks on the MNIST\ndataset and the CIFAR-10 dataset. The experimental results corroborate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2002.10940",
          "publishedOn": "2021-09-07T07:20:15.285Z",
          "wordCount": 652,
          "title": "Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees. (arXiv:2002.10940v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masuyama_N/0/1/0/all/0/1\">Naoki Masuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nojima_Y/0/1/0/all/0/1\">Yusuke Nojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishibuchi_H/0/1/0/all/0/1\">Hisao Ishibuchi</a>",
          "description": "This paper proposes a multi-label classification algorithm capable of\ncontinual learning by applying an Adaptive Resonance Theory (ART)-based\nclustering algorithm and the Bayesian approach for label probability\ncomputation. The ART-based clustering algorithm adaptively and continually\ngenerates prototype nodes corresponding to given data, and the generated nodes\nare used as classifiers. The label probability computation independently counts\nthe number of label appearances for each class and calculates the Bayesian\nprobabilities. Thus, the label probability computation can cope with an\nincrease in the number of labels. Experimental results with synthetic and\nreal-world multi-label datasets show that the proposed algorithm has\ncompetitive classification performance to other well-known algorithms while\nrealizing continual learning.",
          "link": "http://arxiv.org/abs/2103.01511",
          "publishedOn": "2021-09-07T07:20:15.278Z",
          "wordCount": 584,
          "title": "Multi-label Classification via Adaptive Resonance Theory-based Clustering. (arXiv:2103.01511v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-09-07T07:20:15.268Z",
          "wordCount": 750,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ayodeji_A/0/1/0/all/0/1\">Abiodun Ayodeji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianzhong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jianquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinggao Liu</a>",
          "description": "A single unit (head) is the conventional input feature extractor in deep\nlearning architectures trained on multivariate time series signals. The\nimportance of the fixed-dimensional vector representation generated by the\nsingle-head network has been demonstrated for industrial machinery condition\nmonitoring and predictive maintenance. However, processing heterogeneous sensor\nsignals with a single head may result in a model that cannot explicitly account\nfor the diversity in time-varying multivariate inputs. This work extends the\nconventional single-head deep learning models to a more robust form by\ndeveloping context-specific heads to independently capture the inherent pattern\nof each sensor reading in multivariate time series signals. Using the turbofan\naircraft engine benchmark dataset (CMAPSS), an extensive experiment is\nperformed to verify the effectiveness and benefits of multi-head fully\nconnected neurons, recurrent networks, convolution network, the\ntransformer-style stand-alone attention network, and their variants for\nremaining useful life estimation. Moreover, the effect of different attention\nmechanisms on the multi-head models is also evaluated. In addition, each\narchitecture's relative advantage and computational overhead are analyzed.\nResults show that utilizing the attention layer is task-sensitive and\nmodel-dependent, as it does not provide consistent improvement across the\nmodels investigated. The result is further compared with five state-of-the-art\nmodels, and the comparison shows that a relatively simple multi-head\narchitecture performs better than the state-of-the-art models. The results\npresented in this study demonstrate the importance of multi-head models and\nattention mechanisms to improved understanding of the remaining useful life of\nindustrial assets.",
          "link": "http://arxiv.org/abs/2109.01761",
          "publishedOn": "2021-09-07T07:20:15.250Z",
          "wordCount": 705,
          "title": "An empirical evaluation of attention-based multi-head models for improved turbofan engine remaining useful life prediction. (arXiv:2109.01761v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01824",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jia_Z/0/1/0/all/0/1\">Ziyu Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1\">Youfang Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ning_X/0/1/0/all/0/1\">Xiaojun Ning</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yuanlai He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Ronghao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuhan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lehman_L/0/1/0/all/0/1\">Li-wei H. Lehman</a>",
          "description": "Sleep stage classification is essential for sleep assessment and disease\ndiagnosis. Although previous attempts to classify sleep stages have achieved\nhigh classification performance, several challenges remain open: 1) How to\neffectively utilize time-varying spatial and temporal features from\nmulti-channel brain signals remains challenging. Prior works have not been able\nto fully utilize the spatial topological information among brain regions. 2)\nDue to the many differences found in individual biological signals, how to\novercome the differences of subjects and improve the generalization of deep\nneural networks is important. 3) Most deep learning methods ignore the\ninterpretability of the model to the brain. To address the above challenges, we\npropose a multi-view spatial-temporal graph convolutional networks (MSTGCN)\nwith domain generalization for sleep stage classification. Specifically, we\nconstruct two brain view graphs for MSTGCN based on the functional connectivity\nand physical distance proximity of the brain regions. The MSTGCN consists of\ngraph convolutions for extracting spatial features and temporal convolutions\nfor capturing the transition rules among sleep stages. In addition, attention\nmechanism is employed for capturing the most relevant spatial-temporal\ninformation for sleep stage classification. Finally, domain generalization and\nMSTGCN are integrated into a unified framework to extract subject-invariant\nsleep features. Experiments on two public datasets demonstrate that the\nproposed model outperforms the state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2109.01824",
          "publishedOn": "2021-09-07T07:20:15.241Z",
          "wordCount": 698,
          "title": "Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cho_B/0/1/0/all/0/1\">Byungjin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yu Xiao</a>",
          "description": "Vehicular fog computing (VFC) pushes the cloud computing capability to the\ndistributed fog nodes at the edge of the Internet, enabling compute-intensive\nand latency-sensitive computing services for vehicles through task offloading.\nHowever, a heterogeneous mobility environment introduces uncertainties in terms\nof resource supply and demand, which are inevitable bottlenecks for the optimal\noffloading decision. Also, these uncertainties bring extra challenges to task\noffloading under the oblivious adversary attack and data privacy risks. In this\narticle, we develop a new adversarial online learning algorithm with bandit\nfeedback based on the adversarial multi-armed bandit theory, to enable scalable\nand low-complexity offloading decision making. Specifically, we focus on\noptimizing fog node selection with the aim of minimizing the offloading service\ncosts in terms of delay and energy. The key is to implicitly tune the\nexploration bonus in the selection process and the assessment rules of the\ndesigned algorithm, taking into account volatile resource supply and demand. We\ntheoretically prove that the input-size dependent selection rule allows to\nchoose a suitable fog node without exploring the sub-optimal actions, and also\nan appropriate score patching rule allows to quickly adapt to evolving\ncircumstances, which reduce variance and bias simultaneously, thereby achieving\na better exploitation-exploration balance. Simulation results verify the\neffectiveness and robustness of the proposed algorithm.",
          "link": "http://arxiv.org/abs/2104.12827",
          "publishedOn": "2021-09-07T07:20:15.221Z",
          "wordCount": 698,
          "title": "Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haoyue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fengwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1\">Nanyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1\">S.-H. Gary Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>",
          "description": "Recent advances on Out-of-Distribution (OoD) generalization reveal the\nrobustness of deep learning models against distribution shifts. However,\nexisting works focus on OoD algorithms, such as invariant risk minimization,\ndomain generalization, or stable learning, without considering the influence of\ndeep model architectures on OoD generalization, which may lead to sub-optimal\nperformance. Neural Architecture Search (NAS) methods search for architecture\nbased on its performance on the training data, which may result in poor\ngeneralization for OoD tasks. In this work, we propose robust Neural\nArchitecture Search for OoD generalization (NAS-OoD), which optimizes the\narchitecture with respect to its performance on generated OoD data by gradient\ndescent. Specifically, a data generator is learned to synthesize OoD data by\nmaximizing losses computed by different neural architectures, while the goal\nfor architecture search is to find the optimal architecture parameters that\nminimize the synthetic OoD data losses. The data generator and the neural\narchitecture are jointly optimized in an end-to-end manner, and the minimax\ntraining process effectively discovers robust architectures that generalize\nwell for different distribution shifts. Extensive experimental results show\nthat NAS-OoD achieves superior performance on various OoD generalization\nbenchmarks with deep models having a much fewer number of parameters. In\naddition, on a real industry dataset, the proposed NAS-OoD method reduces the\nerror rate by more than 70% compared with the state-of-the-art method,\ndemonstrating the proposed method's practicality for real applications.",
          "link": "http://arxiv.org/abs/2109.02038",
          "publishedOn": "2021-09-07T07:20:15.214Z",
          "wordCount": 674,
          "title": "NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization. (arXiv:2109.02038v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2005.04563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fagbohungbe_O/0/1/0/all/0/1\">Omobayode Fagbohungbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_S/0/1/0/all/0/1\">Sheikh Rufsan Reza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xishuang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lijun Qian</a>",
          "description": "In order to extract knowledge from the large data collected by edge devices,\ntraditional cloud based approach that requires data upload may not be feasible\ndue to communication bandwidth limitation as well as privacy and security\nconcerns of end users. To address these challenges, a novel privacy preserving\nedge computing framework is proposed in this paper for image classification.\nSpecifically, autoencoder will be trained unsupervised at each edge device\nindividually, then the obtained latent vectors will be transmitted to the edge\nserver for the training of a classifier. This framework would reduce the\ncommunications overhead and protect the data of the end users. Comparing to\nfederated learning, the training of the classifier in the proposed framework\ndoes not subject to the constraints of the edge devices, and the autoencoder\ncan be trained independently at each edge device without any server\ninvolvement. Furthermore, the privacy of the end users' data is protected by\ntransmitting latent vectors without additional cost of encryption. Experimental\nresults provide insights on the image classification performance vs. various\ndesign parameters such as the data compression ratio of the autoencoder and the\nmodel complexity.",
          "link": "http://arxiv.org/abs/2005.04563",
          "publishedOn": "2021-09-07T07:20:15.205Z",
          "wordCount": 679,
          "title": "Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.10243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Behera_M/0/1/0/all/0/1\">Monik Raj Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sudhir Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1\">Suresh Shetty</a>",
          "description": "Over the recent years, Federated machine learning continues to gain interest\nand momentum where there is a need to draw insights from data while preserving\nthe data provider's privacy. However, one among other existing challenges in\nthe adoption of federated learning has been the lack of fair, transparent and\nuniversally agreed incentivization schemes for rewarding the federated learning\ncontributors. Smart contracts on a blockchain network provide transparent,\nimmutable and independently verifiable proofs by all participants of the\nnetwork. We leverage this open and transparent nature of smart contracts on a\nblockchain to define incentivization rules for the contributors, which is based\non a novel scalar quantity - federated contribution. Such a smart contract\nbased reward-driven model has the potential to revolutionize the federated\nlearning adoption in enterprises. Our contribution is two-fold: first is to\nshow how smart contract based blockchain can be a very natural communication\nchannel for federated learning. Second, leveraging this infrastructure, we can\nshow how an intuitive measure of each agents' contribution can be built and\nintegrated with the life cycle of the training and reward process.",
          "link": "http://arxiv.org/abs/2107.10243",
          "publishedOn": "2021-09-07T07:20:15.198Z",
          "wordCount": 665,
          "title": "Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Hang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzak_F/0/1/0/all/0/1\">Farid Razzak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>",
          "description": "Multi-view representation learning captures comprehensive information from\nmultiple views of a shared context. Recent works intuitively apply contrastive\nlearning (CL) to learn representations, regarded as a pairwise manner, which is\nstill scalable: view-specific noise is not filtered in learning view-shared\nrepresentations; the fake negative pairs, where the negative terms are actually\nwithin the same class as the positive, and the real negative pairs are\ncoequally treated; and evenly measuring the similarities between terms might\ninterfere with optimization. Importantly, few works research the theoretical\nframework of generalized self-supervised multi-view learning, especially for\nmore than two views. To this end, we rethink the existing multi-view learning\nparadigm from the information theoretical perspective and then propose a novel\ninformation theoretical framework for generalized multi-view learning. Guided\nby it, we build a multi-view coding method with a three-tier progressive\narchitecture, namely Information theory-guided heuristic Progressive Multi-view\nCoding (IPMC). In the distribution-tier, IPMC aligns the distribution between\nviews to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted\npools for contrasting, which utilizes a view filter to adaptively modify the\npools. Lastly, in the instance-tier, we adopt a designed unified loss to learn\ndiscriminative representations and reduce the gradient interference.\nTheoretically and empirically, we demonstrate the superiority of IPMC over\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.02344",
          "publishedOn": "2021-09-07T07:20:15.190Z",
          "wordCount": 664,
          "title": "Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01880",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yexing Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lafci_B/0/1/0/all/0/1\">Berkan Lafci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luzgin_A/0/1/0/all/0/1\">Artur Luzgin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klohs_J/0/1/0/all/0/1\">Jan Klohs</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dean_Ben_X/0/1/0/all/0/1\">Xose Luis Dean-Ben</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_R/0/1/0/all/0/1\">Ruiqing Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Razansky_D/0/1/0/all/0/1\">Daniel Razansky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wuwei Ren</a>",
          "description": "Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging\nmethod providing multiplex molecular and functional information from the rodent\nbrain. It can be greatly augmented by magnetic resonance imaging (MRI) that\noffers excellent soft-tissue contrast and high-resolution brain anatomy.\nNevertheless, registration of multi-modal images remains challenging, chiefly\ndue to the entirely different image contrast rendered by these modalities.\nPreviously reported registration algorithms mostly relied on manual\nuser-dependent brain segmentation, which compromised data interpretation and\naccurate quantification. Here we propose a fully automated registration method\nfor MSOT-MRI multimodal imaging empowered by deep learning. The automated\nworkflow includes neural network-based image segmentation to generate suitable\nmasks, which are subsequently registered using an additional neural network.\nPerformance of the algorithm is showcased with datasets acquired by\ncross-sectional MSOT and high-field MRI preclinical scanners. The automated\nregistration method is further validated with manual and half-automated\nregistration, demonstrating its robustness and accuracy.",
          "link": "http://arxiv.org/abs/2109.01880",
          "publishedOn": "2021-09-07T07:20:15.183Z",
          "wordCount": 635,
          "title": "Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01904",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilligan_Lee_C/0/1/0/all/0/1\">Ciaran M. Gilligan-Lee</a>",
          "description": "There has been much recent work using machine learning to answer causal\nqueries. Most focus on interventional queries, such as the conditional average\ntreatment effect. However, as noted by Pearl, interventional queries only form\npart of a larger hierarchy of causal queries, with counterfactuals sitting at\nthe top. Despite this, our community has not fully succeeded in adapting\nmachine learning tools to answer counterfactual queries. This work addresses\nthis challenge by showing how to implement twin network counterfactual\ninference -- an alternative to abduction, action, & prediction counterfactual\ninference -- with deep learning to estimate counterfactual queries. We show how\nthe graphical nature of twin networks makes them particularly amenable to deep\nlearning, yielding simple neural network architectures that, when trained, are\ncapable of counterfactual inference. Importantly, we show how to enforce known\nidentifiability constraints during training, ensuring the answer to each\ncounterfactual query is uniquely determined. We demonstrate our approach by\nusing it to accurately estimate the probabilities of causation -- important\ncounterfactual queries that quantify the degree to which one event was a\nnecessary or sufficient cause of another -- on both synthetic and real data.",
          "link": "http://arxiv.org/abs/2109.01904",
          "publishedOn": "2021-09-07T07:20:15.164Z",
          "wordCount": 638,
          "title": "Estimating the probabilities of causation via deep monotonic twin networks. (arXiv:2109.01904v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.05722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hackenberg_M/0/1/0/all/0/1\">Maren Hackenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grodd_M/0/1/0/all/0/1\">Marlon Grodd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutz_C/0/1/0/all/0/1\">Clemens Kreutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Martina Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esins_J/0/1/0/all/0/1\">Janina Esins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grabenhenrich_L/0/1/0/all/0/1\">Linus Grabenhenrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karagiannidis_C/0/1/0/all/0/1\">Christian Karagiannidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_H/0/1/0/all/0/1\">Harald Binder</a>",
          "description": "Differentiable programming has recently received much interest as a paradigm\nthat facilitates taking gradients of computer programs. While the corresponding\nflexible gradient-based optimization approaches so far have been used\npredominantly for deep learning or enriching the latter with modeling\ncomponents, we want to demonstrate that they can also be useful for statistical\nmodeling per se, e.g., for quick prototyping when classical maximum likelihood\napproaches are challenging or not feasible. In an application from a COVID-19\nsetting, we utilize differentiable programming to quickly build and optimize a\nflexible prediction model adapted to the data quality challenges at hand.\nSpecifically, we develop a regression model, inspired by delay differential\nequations, that can bridge temporal gaps of observations in the central German\nregistry of COVID-19 intensive care cases for predicting future demand. With\nthis exemplary modeling challenge, we illustrate how differentiable programming\ncan enable simple gradient-based optimization of the model by automatic\ndifferentiation. This allowed us to quickly prototype a model under time\npressure that outperforms simpler benchmark models. We thus exemplify the\npotential of differentiable programming also outside deep learning\napplications, to provide more options for flexible applied statistical\nmodeling.",
          "link": "http://arxiv.org/abs/2012.05722",
          "publishedOn": "2021-09-07T07:20:15.158Z",
          "wordCount": 719,
          "title": "Using Differentiable Programming for Flexible Statistical Modeling. (arXiv:2012.05722v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03746",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Shang_J/0/1/0/all/0/1\">Jiayu Shang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jiang_J/0/1/0/all/0/1\">Jingzhe Jiang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sun_Y/0/1/0/all/0/1\">Yanni Sun</a>",
          "description": "Motivation: Bacteriophages (aka phages), which mainly infect bacteria, play\nkey roles in the biology of microbes. As the most abundant biological entities\non the planet, the number of discovered phages is only the tip of the iceberg.\nRecently, many new phages have been revealed using high throughput sequencing,\nparticularly metagenomic sequencing. Compared to the fast accumulation of\nphage-like sequences, there is a serious lag in taxonomic classification of\nphages. High diversity, abundance, and limited known phages pose great\nchallenges for taxonomic analysis. In particular, alignment-based tools have\ndifficulty in classifying fast accumulating contigs assembled from metagenomic\ndata. Results: In this work, we present a novel semi-supervised learning model,\nnamed PhaGCN, to conduct taxonomic classification for phage contigs. In this\nlearning model, we construct a knowledge graph by combining the DNA sequence\nfeatures learned by convolutional neural network (CNN) and protein sequence\nsimilarity gained from gene-sharing network. Then we apply graph convolutional\nnetwork (GCN) to utilize both the labeled and unlabeled samples in training to\nenhance the learning ability. We tested PhaGCN on both simulated and real\nsequencing data. The results clearly show that our method competes favorably\nagainst available phage classification tools.",
          "link": "http://arxiv.org/abs/2102.03746",
          "publishedOn": "2021-09-07T07:20:15.151Z",
          "wordCount": 668,
          "title": "Bacteriophage classification for assembled contigs using Graph Convolutional Network. (arXiv:2102.03746v2 [q-bio.GN] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09415",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Thumwanit_N/0/1/0/all/0/1\">Napat Thumwanit</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lortaraprasert_C/0/1/0/all/0/1\">Chayaphol Lortaraprasert</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Yano_H/0/1/0/all/0/1\">Hiroshi Yano</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Raymond_R/0/1/0/all/0/1\">Rudy Raymond</a>",
          "description": "Quantum classifiers provide sophisticated embeddings of input data in Hilbert\nspace promising quantum advantage. The advantage stems from quantum feature\nmaps encoding the inputs into quantum states with variational quantum circuits.\nA recent work shows how to map discrete features with fewer quantum bits using\nQuantum Random Access Coding (QRAC), an important primitive to encode binary\nstrings into quantum states. We propose a new method to embed discrete features\nwith trainable quantum circuits by combining QRAC and a recently proposed\nstrategy for training quantum feature map called quantum metric learning. We\nshow that the proposed trainable embedding requires not only as few qubits as\nQRAC but also overcomes the limitations of QRAC to classify inputs whose\nclasses are based on hard Boolean functions. We numerically demonstrate its use\nin variational quantum classifiers to achieve better performances in\nclassifying real-world datasets, and thus its possibility to leverage near-term\nquantum computers for quantum machine learning.",
          "link": "http://arxiv.org/abs/2106.09415",
          "publishedOn": "2021-09-07T07:20:15.143Z",
          "wordCount": 615,
          "title": "Trainable Discrete Feature Embeddings for Variational Quantum Classifier. (arXiv:2106.09415v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01784",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ezaki_T/0/1/0/all/0/1\">Takahiro Ezaki</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Imura_N/0/1/0/all/0/1\">Naoto Imura</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nishinari_K/0/1/0/all/0/1\">Katsuhiro Nishinari</a>",
          "description": "Demand forecasting based on empirical data is a viable approach for\noptimizing a supply chain. However, in this approach, a model constructed from\npast data occasionally becomes outdated due to long-term changes in the\nenvironment, in which case the model should be updated (i.e., retrained) using\nthe latest data. In this study, we examine the effects of updating models in a\nsupply chain using a minimal setting. We demonstrate that when each party in\nthe supply chain has its own forecasting model, uncoordinated model retraining\ncauses the bullwhip effect even if a very simple replenishment policy is\napplied. Our results also indicate that sharing the forecasting model among the\nparties involved significantly reduces the bullwhip effect.",
          "link": "http://arxiv.org/abs/2109.01784",
          "publishedOn": "2021-09-07T07:20:15.126Z",
          "wordCount": 574,
          "title": "Model retraining and information sharing in a supply chain with long-term fluctuating demands. (arXiv:2109.01784v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Satpathy_A/0/1/0/all/0/1\">Asish Satpathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behari_S/0/1/0/all/0/1\">Satyajit Behari</a>",
          "description": "Chronic diseases such as diabetes are quite prevalent in the world and are\nresponsible for a significant number of deaths per year. In addition,\ntreatments for such chronic diseases account for a high healthcare cost.\nHowever, research has shown that diabetes can be proactively managed and\nprevented while lowering these healthcare costs. We have mined a sample of ten\nmillion customers' 360-degree data representing the state of Texas, USA, with\nattributes current as of late 2018. The sample received from a market research\ndata vendor has over 1000 customer attributes consisting of demography,\nlifestyle, and in some cases self-reported chronic conditions. In this study,\nwe have developed a classification model to predict chronic diabetes with an\naccuracy of 80%. We demonstrate a use case where a large volume of 360-degree\ncustomer data can be useful to predict and hence proactively prevent chronic\ndiseases such as diabetes.",
          "link": "http://arxiv.org/abs/2109.01863",
          "publishedOn": "2021-09-07T07:20:15.119Z",
          "wordCount": 583,
          "title": "Customer 360-degree Insights in Predicting Chronic Diabetes. (arXiv:2109.01863v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jong Wook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>",
          "description": "Large pre-trained models such as CLIP offer consistent accuracy across a\nrange of data distributions when performing zero-shot inference (i.e., without\nfine-tuning on a specific dataset). Although existing fine-tuning approaches\nsubstantially improve accuracy in-distribution, they also reduce\nout-of-distribution robustness. We address this tension by introducing a simple\nand effective method for improving robustness: ensembling the weights of the\nzero-shot and fine-tuned models. Compared to standard fine-tuning, the\nresulting weight-space ensembles provide large accuracy improvements\nout-of-distribution, while matching or improving in-distribution accuracy. On\nImageNet and five derived distribution shifts, weight-space ensembles improve\nout-of-distribution accuracy by 2 to 10 percentage points while increasing\nin-distribution accuracy by nearly 1 percentage point relative to standard\nfine-tuning. These improvements come at no additional computational cost during\nfine-tuning or inference.",
          "link": "http://arxiv.org/abs/2109.01903",
          "publishedOn": "2021-09-07T07:20:15.112Z",
          "wordCount": 576,
          "title": "Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.11148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenghao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1\">Zhoufeng Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ray T. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>",
          "description": "Optical neural networks (ONNs) have demonstrated record-breaking potential in\nhigh-performance neuromorphic computing due to their ultra-high execution speed\nand low energy consumption. However, current learning protocols fail to provide\nscalable and efficient solutions to photonic circuit optimization in practical\napplications. In this work, we propose a novel on-chip learning framework to\nrelease the full potential of ONNs for power-efficient in situ training.\nInstead of deploying implementation-costly back-propagation, we directly\noptimize the device configurations with computation budgets and power\nconstraints. We are the first to model the ONN on-chip learning as a\nresource-constrained stochastic noisy zeroth-order optimization problem, and\npropose a novel mixed-training strategy with two-level sparsity and power-aware\ndynamic pruning to offer a scalable on-chip training solution in practical ONN\ndeployment. Compared with previous methods, we are the first to optimize over\n2,500 optical components on chip. We can achieve much better optimization\nstability, 3.7x-7.6x higher efficiency, and save >90% power under practical\ndevice variations and thermal crosstalk.",
          "link": "http://arxiv.org/abs/2012.11148",
          "publishedOn": "2021-09-07T07:20:15.101Z",
          "wordCount": null,
          "title": "Efficient On-Chip Learning for Optical Neural Networks Through Power-Aware Sparse Zeroth-Order Optimization. (arXiv:2012.11148v3 [cs.ET] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02178",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Tena_F/0/1/0/all/0/1\">Felix Tena</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Garnica_O/0/1/0/all/0/1\">Oscar Garnica</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lanchares_J/0/1/0/all/0/1\">Juan Lanchares</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hidalgo_J/0/1/0/all/0/1\">J. Ignacio Hidalgo</a>",
          "description": "This article compares ten recently proposed neural networks and proposes two\nensemble neural network-based models for blood glucose prediction. All of them\nare tested under the same dataset, preprocessing workflow, and tools using the\nOhioT1DM Dataset at three different prediction horizons: 30, 60, and 120\nminutes. We compare their performance using the most common metrics in blood\nglucose prediction and rank the best-performing ones using three methods\ndevised for the statistical comparison of the performance of multiple\nalgorithms: scmamp, model confidence set, and superior predictive ability. Our\nanalysis highlights those models with the highest probability of being the best\npredictors, estimates the increase in error of the models that perform more\npoorly with respect to the best ones, and provides a guide for their use in\nclinical practice.",
          "link": "http://arxiv.org/abs/2109.02178",
          "publishedOn": "2021-09-07T07:20:15.099Z",
          "wordCount": null,
          "title": "A Critical Review of the state-of-the-art on Deep Neural Networks for Blood Glucose Prediction in Patients with Diabetes. (arXiv:2109.02178v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2004.14724",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gruttemeier_N/0/1/0/all/0/1\">Niels Gr&#xfc;ttemeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komusiewicz_C/0/1/0/all/0/1\">Christian Komusiewicz</a>",
          "description": "We study the problem of learning the structure of an optimal Bayesian network\nwhen additional constraints are posed on the network or on its moralized graph.\nMore precisely, we consider the constraint that the network or its moralized\ngraph are close, in terms of vertex or edge deletions, to a sparse graph class\n$\\Pi$. For example, we show that learning an optimal network whose moralized\ngraph has vertex deletion distance at most $k$ from a graph with maximum degree\n1 can be computed in polynomial time when $k$ is constant. This extends\nprevious work that gave an algorithm with such a running time for the vertex\ndeletion distance to edgeless graphs [Korhonen & Parviainen, NIPS 2015]. We\nthen show that further extensions or improvements are presumably impossible.\nFor example, we show that learning optimal networks where the network or its\nmoralized graph have maximum degree $2$ or connected components of size at most\n$c$, $c\\ge 3$, is NP-hard. Finally, we show that learning an optimal network\nwith at most $k$ edges in the moralized graph presumably has no $f(k)\\cdot\n|I|^{O(1)}$-time algorithm and that, in contrast, an optimal network with at\nmost $k$ arcs can be computed in $2^{O(k)}\\cdot |I|^{O(1)}$ time where $|I|$ is\nthe total input size.",
          "link": "http://arxiv.org/abs/2004.14724",
          "publishedOn": "2021-09-07T07:20:15.098Z",
          "wordCount": null,
          "title": "Learning Bayesian Networks Under Sparsity Constraints: A Parameterized Complexity Analysis. (arXiv:2004.14724v3 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1710.03113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chang-Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jian-Huang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee-Keong Kwoh</a>",
          "description": "The rapid emergence of high-dimensional data in various areas has brought new\nchallenges to current ensemble clustering research. To deal with the curse of\ndimensionality, recently considerable efforts in ensemble clustering have been\nmade by means of different subspace-based techniques. However, besides the\nemphasis on subspaces, rather limited attention has been paid to the potential\ndiversity in similarity/dissimilarity metrics. It remains a surprisingly open\nproblem in ensemble clustering how to create and aggregate a large population\nof diversified metrics, and furthermore, how to jointly investigate the\nmulti-level diversity in the large populations of metrics, subspaces, and\nclusters in a unified framework. To tackle this problem, this paper proposes a\nnovel multidiversified ensemble clustering approach. In particular, we create a\nlarge number of diversified metrics by randomizing a scaled exponential\nsimilarity kernel, which are then coupled with random subspaces to form a large\nset of metric-subspace pairs. Based on the similarity matrices derived from\nthese metric-subspace pairs, an ensemble of diversified base clusterings can\nthereby be constructed. Further, an entropy-based criterion is utilized to\nexplore the cluster-wise diversity in ensembles, based on which three specific\nensemble clustering algorithms are presented by incorporating three types of\nconsensus functions. Extensive experiments are conducted on 30 high-dimensional\ndatasets, including 18 cancer gene expression datasets and 12 image/speech\ndatasets, which demonstrate the superiority of our algorithms over the\nstate-of-the-art. The source code is available at\nhttps://github.com/huangdonghere/MDEC.",
          "link": "http://arxiv.org/abs/1710.03113",
          "publishedOn": "2021-09-07T07:20:15.094Z",
          "wordCount": null,
          "title": "Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naranjo_Alcazar_J/0/1/0/all/0/1\">Javier Naranjo-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Castanos_S/0/1/0/all/0/1\">Sergi Perez-Castanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccarrello_P/0/1/0/all/0/1\">Pedro Zuccarrello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_A/0/1/0/all/0/1\">Ana M. Torres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1\">Jose J. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferri_F/0/1/0/all/0/1\">Franscesc J. Ferri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobos_M/0/1/0/all/0/1\">Maximo Cobos</a>",
          "description": "The problem of training with a small set of positive samples is known as\nfew-shot learning (FSL). It is widely known that traditional deep learning (DL)\nalgorithms usually show very good performance when trained with large datasets.\nHowever, in many applications, it is not possible to obtain such a high number\nof samples. In the image domain, typical FSL applications include those related\nto face recognition. In the audio domain, music fraud or speaker recognition\ncan be clearly benefited from FSL methods. This paper deals with the\napplication of FSL to the detection of specific and intentional acoustic events\ngiven by different types of sound alarms, such as door bells or fire alarms,\nusing a limited number of samples. These sounds typically occur in domestic\nenvironments where many events corresponding to a wide variety of sound classes\ntake place. Therefore, the detection of such alarms in a practical scenario can\nbe considered an open-set recognition (OSR) problem. To address the lack of a\ndedicated public dataset for audio FSL, researchers usually make modifications\non other available datasets. This paper is aimed at poviding the audio\nrecognition community with a carefully annotated dataset\n(https://zenodo.org/record/3689288) for FSL in an OSR context comprised of 1360\nclips from 34 classes divided into pattern sounds} and unwanted sounds. To\nfacilitate and promote research on this area, results with state-of-the-art\nbaseline systems based on transfer learning are also presented.",
          "link": "http://arxiv.org/abs/2002.11561",
          "publishedOn": "2021-09-07T07:20:15.094Z",
          "wordCount": null,
          "title": "An Open-set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments. (arXiv:2002.11561v7 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pingfan Xu</a>",
          "description": "Phishing attacks are among emerging security issues that recently draws\nsignificant attention in the cyber security community. There are numerous\nexisting approaches for phishing URL detection. However, malicious URL\ndetection is still a research hotspot because attackers can bypass newly\nintroduced detection mechanisms by changing their tactics. This paper will\nintroduce a transformer-based malicious URL detection model, which has\nsignificant accuracy and outperforms current detection methods. We conduct\nexperiments and compare them with six existing classical detection models.\nExperiments demonstrate that our transformer-based model is the best performing\nmodel from all perspectives among the seven models and achieves 97.3 % of\ndetection accuracy.",
          "link": "http://arxiv.org/abs/2109.02138",
          "publishedOn": "2021-09-07T07:20:15.093Z",
          "wordCount": null,
          "title": "A Transformer-based Model to Detect Phishing URLs. (arXiv:2109.02138v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02183",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Thuerey_N/0/1/0/all/0/1\">Nils Thuerey</a>",
          "description": "The present study investigates the accurate inference of Reynolds-averaged\nNavier-Stokes solutions for the compressible flow over aerofoils in two\ndimensions with a deep neural network. Our approach yields networks that learn\nto generate precise flow fields for varying body-fitted, structured grids by\nproviding them with an encoding of the corresponding mapping to a canonical\nspace for the solutions. We apply the deep neural network model to a benchmark\ncase of incompressible flow at randomly given angles of attack and Reynolds\nnumbers and achieve an improvement of more than an order of magnitude compared\nto previous work. Further, for transonic flow cases, the deep neural network\nmodel accurately predicts complex flow behaviour at high Reynolds numbers, such\nas shock wave/boundary layer interaction, and quantitative distributions like\npressure coefficient, skin friction coefficient as well as wake total pressure\nprofiles downstream of aerofoils. The proposed deep learning method\nsignificantly speeds up the predictions of flow fields and shows promise for\nenabling fast aerodynamic designs.",
          "link": "http://arxiv.org/abs/2109.02183",
          "publishedOn": "2021-09-07T07:20:15.093Z",
          "wordCount": null,
          "title": "Towards high-accuracy deep learning inference of compressible turbulent flows over aerofoils. (arXiv:2109.02183v1 [physics.flu-dyn])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01417",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ornia_D/0/1/0/all/0/1\">Daniel Jarne Ornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazo_M/0/1/0/all/0/1\">Manuel Mazo Jr</a>",
          "description": "We present in this work an approach to reduce the communication of\ninformation needed on a multi-agent learning system inspired by Event Triggered\nControl (ETC) techniques. We consider a baseline scenario of a distributed\nQ-learning problem on a Markov Decision Process (MDP). Following an event-based\napproach, N agents explore the MDP and communicate experiences to a central\nlearner only when necessary, which performs updates of the actor Q functions.\nWe analyse the convergence guarantees retained with respect to a regular\nQ-learning algorithm, and present experimental results showing that event-based\ncommunication results in a substantial reduction of data transmission rates in\nsuch distributed systems. Additionally, we discuss what effects (desired and\nundesired) these event-based approaches have on the learning processes studied,\nand how they can be applied to more complex multi-agent learning systems.",
          "link": "http://arxiv.org/abs/2109.01417",
          "publishedOn": "2021-09-07T07:20:15.093Z",
          "wordCount": null,
          "title": "Event-Based Communication in Multi-Agent Distributed Q-Learning. (arXiv:2109.01417v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01991",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dunipace_E/0/1/0/all/0/1\">Eric Dunipace</a>",
          "description": "Weighting methods are a common tool to de-bias estimates of causal effects.\nAnd though there are an increasing number of seemingly disparate methods, many\nof them can be folded into one unifying regime: causal optimal transport. This\nnew method directly targets distributional balance by minimizing optimal\ntransport distances between treatment and control groups or, more generally,\nbetween a source and target population. Our approach is model-free but can also\nincorporate moments or any other important functions of covariates that the\nresearcher desires to balance. We find that the causal optimal transport\noutperforms competitor methods when both the propensity score and outcome\nmodels are misspecified, indicating it is a robust alternative to common\nweighting methods. Finally, we demonstrate the utility of our method in an\nexternal control study examining the effect of misoprostol versus oxytocin for\ntreatment of post-partum hemorrhage.",
          "link": "http://arxiv.org/abs/2109.01991",
          "publishedOn": "2021-09-07T07:20:15.092Z",
          "wordCount": null,
          "title": "Optimal transport weights for causal inference. (arXiv:2109.01991v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02035",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Berrone_S/0/1/0/all/0/1\">Stefano Berrone</a>, <a href=\"http://arxiv.org/find/math/1/au:+Canuto_C/0/1/0/all/0/1\">Claudio Canuto</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pintore_M/0/1/0/all/0/1\">Moreno Pintore</a>",
          "description": "In this work we analyze how Gaussian or Newton-Cotes quadrature rules of\ndifferent precisions and piecewise polynomial test functions of different\ndegrees affect the convergence rate of Variational Physics Informed Neural\nNetworks (VPINN) with respect to mesh refinement, while solving elliptic\nboundary-value problems. Using a Petrov-Galerkin framework relying on an\ninf-sup condition, we derive an a priori error estimate in the energy norm\nbetween the exact solution and a suitable high-order piecewise interpolant of a\ncomputed neural network. Numerical experiments confirm the theoretical\npredictions, and also indicate that the error decay follows the same behavior\nwhen the neural network is not interpolated. Our results suggest, somehow\ncounterintuitively, that for smooth solutions the best strategy to achieve a\nhigh decay rate of the error consists in choosing test functions of the lowest\npolynomial degree, while using quadrature formulas of suitably high precision.",
          "link": "http://arxiv.org/abs/2109.02035",
          "publishedOn": "2021-09-07T07:20:15.092Z",
          "wordCount": null,
          "title": "Variational Physics Informed Neural Networks: the role of quadratures and test functions. (arXiv:2109.02035v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanaparthy_S/0/1/0/all/0/1\">Samhita Kanaparthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padala_M/0/1/0/all/0/1\">Manisha Padala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damle_S/0/1/0/all/0/1\">Sankarshan Damle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1\">Sujit Gujar</a>",
          "description": "We consider the problem of achieving fair classification in Federated\nLearning (FL) under data heterogeneity. Most of the approaches proposed for\nfair classification require diverse data that represent the different\ndemographic groups involved. In contrast, it is common for each client to own\ndata that represents only a single demographic group. Hence the existing\napproaches cannot be adopted for fair classification models at the client\nlevel. To resolve this challenge, we propose several aggregation techniques. We\nempirically validate these techniques by comparing the resulting fairness\nmetrics and accuracy on CelebA, UTK, and FairFace datasets.",
          "link": "http://arxiv.org/abs/2109.02351",
          "publishedOn": "2021-09-07T07:20:15.092Z",
          "wordCount": null,
          "title": "Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1\">Shi-Ying Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Run-Qing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>",
          "description": "Anomaly detection on time series is a fundamental task in monitoring the Key\nPerformance Indicators (KPIs) of IT systems. Many of the existing approaches in\nthe literature show good performance while requiring a lot of training\nresources. In this paper, the online matrix profile, which requires no\ntraining, is proposed to address this issue. The anomalies are detected by\nreferring to the past subsequence that is the closest to the current one. The\ndistance significance is introduced based on the online matrix profile, which\ndemonstrates a prominent pattern when an anomaly occurs. Another training-free\napproach spectral residual is integrated into our approach to further enhance\nthe detection accuracy. Moreover, the proposed approach is sped up by at least\nfour times for long time series by the introduced cache strategy. In comparison\nto the existing approaches, the online matrix profile makes a good trade-off\nbetween accuracy and efficiency. More importantly, it is generic to various\ntypes of time series in the sense that it works without the constraint from any\ntrained model.",
          "link": "http://arxiv.org/abs/2108.12093",
          "publishedOn": "2021-09-07T07:20:15.089Z",
          "wordCount": 640,
          "title": "Anomaly Detection on IT Operation Series via Online Matrix Profile. (arXiv:2108.12093v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_S/0/1/0/all/0/1\">Sandeep Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1\">Alexander Hoelzemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1\">Kristof Van Laerhoven</a>",
          "description": "Data augmentation is a widely used technique in classification to increase\ndata used in training. It improves generalization and reduces amount of\nannotated human activity data needed for training which reduces labour and time\nneeded with the dataset. Sensor time-series data, unlike images, cannot be\naugmented by computationally simple transformation algorithms. State of the art\nmodels like Recurrent Generative Adversarial Networks (RGAN) are used to\ngenerate realistic synthetic data. In this paper, transformer based generative\nadversarial networks which have global attention on data, are compared on\nPAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer\napproach provides improvements in time and savings in computational resources\nneeded for data augmentation than previous approach.",
          "link": "http://arxiv.org/abs/2109.01081",
          "publishedOn": "2021-09-07T07:20:15.089Z",
          "wordCount": null,
          "title": "Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_N/0/1/0/all/0/1\">Ning Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiahua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Di Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>",
          "description": "Designing optimal reward functions has been desired but extremely difficult\nin reinforcement learning (RL). When it comes to modern complex tasks,\nsophisticated reward functions are widely used to simplify policy learning yet\neven a tiny adjustment on them is expensive to evaluate due to the drastically\nincreasing cost of training. To this end, we propose a hindsight reward\ntweaking approach by designing a novel paradigm for deep reinforcement learning\nto model the influences of reward functions within a near-optimal space. We\nsimply extend the input observation with a condition vector linearly correlated\nwith the effective environment reward parameters and train the model in a\nconventional manner except for randomizing reward configurations, obtaining a\nhyper-policy whose characteristics are sensitively regulated over the condition\nspace. We demonstrate the feasibility of this approach and study one of its\npotential application in policy performance boosting with multiple MuJoCo\ntasks.",
          "link": "http://arxiv.org/abs/2109.02332",
          "publishedOn": "2021-09-07T07:20:15.080Z",
          "wordCount": null,
          "title": "Hindsight Reward Tweaking via Conditional Deep Reinforcement Learning. (arXiv:2109.02332v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Recommender systems research tends to evaluate model performance offline and\non randomly sampled targets, yet the same systems are later used to predict\nuser behavior sequentially from a fixed point in time. Simulating online\nrecommender system performance is notoriously difficult and the discrepancy\nbetween online and offline behaviors is typically not accounted for in offline\nevaluations. This disparity permits weaknesses to go unnoticed until the model\nis deployed in a production setting. In this paper, we first demonstrate how\nomitting temporal context when evaluating recommender system performance leads\nto false confidence. To overcome this, we postulate that offline evaluation\nprotocols can only model real-life use-cases if they account for temporal\ncontext. Next, we propose a training procedure to further embed the temporal\ncontext in existing models. We use a multi-objective approach to introduce\ntemporal context into traditionally time-unaware recommender systems and\nconfirm its advantage via the proposed evaluation protocol. Finally, we\nvalidate that the Pareto Fronts obtained with the added objective dominate\nthose produced by state-of-the-art models that are only optimized for accuracy\non three real-world publicly available datasets. The results show that\nincluding our temporal objective can improve recall@20 by up to 20%.",
          "link": "http://arxiv.org/abs/2009.08978",
          "publishedOn": "2021-09-07T07:20:15.079Z",
          "wordCount": null,
          "title": "Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Johnsen_P/0/1/0/all/0/1\">P&#xe5;l Vegard Johnsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strumke_I/0/1/0/all/0/1\">Inga Str&#xfc;mke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_Sorensen_S/0/1/0/all/0/1\">Signe Riemer-S&#xf8;rensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeWan_A/0/1/0/all/0/1\">Andrew Thomas DeWan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langaas_M/0/1/0/all/0/1\">Mette Langaas</a>",
          "description": "Estimating feature importance is a significant aspect of explaining\ndata-based models. Besides explaining the model itself, an equally relevant\nquestion is which features are important in the underlying data generating\nprocess. We present a Shapley value based framework for inferring the\nimportance of individual features, including uncertainty in the estimator. We\nbuild upon the recently published feature importance measure of SAGE (Shapley\nadditive global importance) and introduce sub-SAGE which can be estimated\nwithout resampling for tree-based models. We argue that the uncertainties can\nbe estimated from bootstrapping and demonstrate the approach for tree ensemble\nmethods. The framework is exemplified on synthetic data as well as\nhigh-dimensional genomics data.",
          "link": "http://arxiv.org/abs/2109.00855",
          "publishedOn": "2021-09-07T07:20:15.079Z",
          "wordCount": null,
          "title": "Inferring feature importance with uncertainties in high-dimensional data. (arXiv:2109.00855v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02084",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saini_S/0/1/0/all/0/1\">Shreshth Saini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agrawal_G/0/1/0/all/0/1\">Geetika Agrawal</a>",
          "description": "Segmentation plays a crucial role in diagnosis. Studying the retinal\nvasculatures from fundus images help identify early signs of many crucial\nillnesses such as diabetic retinopathy. Due to the varying shape, size, and\npatterns of retinal vessels, along with artefacts and noises in fundus images,\nno one-stage method can accurately segment retinal vessels. In this work, we\npropose a multi-scale, multi-level attention embedded CNN architecture\n((M)SLAe-Net) to address the issue of multi-stage processing for robust and\nprecise segmentation of retinal vessels. We do this by extracting features at\nmultiple scales and multiple levels of the network, enabling our model to\nholistically extracts the local and global features. Multi-scale features are\nextracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We\nalso aggregate the features from all the network levels. These effectively\nresolved the issues of varying shapes and artefacts and hence the need for\nmultiple stages. To assist in better pixel-level classification, we use the\nSqueeze and Attention(SA) module, a smartly adapted version of the Squeeze and\nExcitation(SE) module for segmentation tasks in our network to facilitate\npixel-group attention. Our unique network design and novel D-DPP module with\nefficient task-specific loss function for thin vessels enabled our model for\nbetter cross data performance. Exhaustive experimental results on DRIVE, STARE,\nHRF, and CHASE-DB1 show the superiority of our method.",
          "link": "http://arxiv.org/abs/2109.02084",
          "publishedOn": "2021-09-07T07:20:15.078Z",
          "wordCount": null,
          "title": "(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dori_Hacohen_S/0/1/0/all/0/1\">Shiri Dori-Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_R/0/1/0/all/0/1\">Roberto Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_K/0/1/0/all/0/1\">Keen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_M/0/1/0/all/0/1\">Michela Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_Johnson_J/0/1/0/all/0/1\">Jennifer Edwards-Johnson</a>",
          "description": "Most Fairness in AI research focuses on exposing biases in AI systems. A\nbroader lens on fairness reveals that AI can serve a greater aspiration:\nrooting out societal inequities from their source. Specifically, we focus on\ninequities in health information, and aim to reduce bias in that domain using\nAI. The AI algorithms under the hood of search engines and social media, many\nof which are based on recommender systems, have an outsized impact on the\nquality of medical and health information online. Therefore, embedding bias\ndetection and reduction into these recommender systems serving up medical and\nhealth content online could have an outsized positive impact on patient\noutcomes and wellbeing.\n\nIn this position paper, we offer the following contributions: (1) we propose\na novel framework of Fairness via AI, inspired by insights from medical\neducation, sociology and antiracism; (2) we define a new term, bisinformation,\nwhich is related to, but distinct from, misinformation, and encourage\nresearchers to study it; (3) we propose using AI to study, detect and mitigate\nbiased, harmful, and/or false health information that disproportionately hurts\nminority groups in society; and (4) we suggest several pillars and pose several\nopen problems in order to seed inquiry in this new space. While part (3) of\nthis work specifically focuses on the health domain, the fundamental computer\nscience advances and contributions stemming from research efforts in bias\nreduction and Fairness via AI have broad implications in all areas of society.",
          "link": "http://arxiv.org/abs/2109.02202",
          "publishedOn": "2021-09-07T07:20:15.078Z",
          "wordCount": null,
          "title": "Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2009.08392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garland_J/0/1/0/all/0/1\">Joshua Garland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghazi_Zahedi_K/0/1/0/all/0/1\">Keyan Ghazi-Zahedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1\">Jean-Gabriel Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_Dufresne_L/0/1/0/all/0/1\">Laurent H&#xe9;bert-Dufresne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galesic_M/0/1/0/all/0/1\">Mirta Galesic</a>",
          "description": "Citizen-generated counter speech is a promising way to fight hate speech and\npromote peaceful, non-polarized discourse. However, there is a lack of\nlarge-scale longitudinal studies of its effectiveness for reducing hate speech.\nTo this end, we perform an exploratory analysis of the effectiveness of counter\nspeech using several different macro- and micro-level measures to analyze\n180,000 political conversations that took place on German Twitter over four\nyears. We report on the dynamic interactions of hate and counter speech over\ntime and provide insights into whether, as in `classic' bullying situations,\norganized efforts are more effective than independent individuals in steering\nonline discourse. Taken together, our results build a multifaceted picture of\nthe dynamics of hate and counter speech online. While we make no causal claims\ndue to the complexity of discourse dynamics, our findings suggest that\norganized hate speech is associated with changes in public discourse and that\ncounter speech -- especially when organized -- may help curb hateful rhetoric\nin online discourse.",
          "link": "http://arxiv.org/abs/2009.08392",
          "publishedOn": "2021-09-07T07:20:15.078Z",
          "wordCount": null,
          "title": "Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.04020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Hao_B/0/1/0/all/0/1\">Botao Hao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1\">Tor Lattimore</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1\">Mengdi Wang</a>",
          "description": "Stochastic linear bandits with high-dimensional sparse features are a\npractical model for a variety of domains, including personalized medicine and\nonline advertising. We derive a novel $\\Omega(n^{2/3})$ dimension-free minimax\nregret lower bound for sparse linear bandits in the data-poor regime where the\nhorizon is smaller than the ambient dimension and where the feature vectors\nadmit a well-conditioned exploration distribution. This is complemented by a\nnearly matching upper bound for an explore-then-commit algorithm showing that\nthat $\\Theta(n^{2/3})$ is the optimal rate in the data-poor regime. The results\ncomplement existing bounds for the data-rich regime and provide another example\nwhere carefully balancing the trade-off between information and regret is\nnecessary. Finally, we prove a dimension-free $O(\\sqrt{n})$ regret upper bound\nunder an additional assumption on the magnitude of the signal for relevant\nfeatures.",
          "link": "http://arxiv.org/abs/2011.04020",
          "publishedOn": "2021-09-07T07:20:15.078Z",
          "wordCount": null,
          "title": "High-Dimensional Sparse Linear Bandits. (arXiv:2011.04020v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00674",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhigen Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_W/0/1/0/all/0/1\">Wen Zhou</a>",
          "description": "We study nonparametric dependence detection with the proposed binary\nexpansion approximation of uniformity (BEAUTY) approach, which generalizes the\ncelebrated Euler's formula, and approximates the characteristic function of any\ncopula with a linear combination of expectations of binary interactions from\nmarginal binary expansions. This novel theory enables a unification of many\nimportant tests through approximations from some quadratic forms of symmetry\nstatistics, where the deterministic weight matrix characterizes the power\nproperties of each test. To achieve a robust power, we study test statistics\nwith data-adaptive weights, referred to as the binary expansion adaptive\nsymmetry test (BEAST). By utilizing the properties of the binary expansion\nfiltration, we show that the Neyman-Pearson test of uniformity can be\napproximated by an oracle weighted sum of symmetry statistics. The BEAST with\nthis oracle provides a benchmark of feasible power against any alternative by\nleading all existing tests with a substantial margin. To approach this oracle\npower, we develop the BEAST through a regularized resampling approximation of\nthe oracle test. The BEAST improves the empirical power of many existing tests\nagainst a wide spectrum of common alternatives while providing clear\ninterpretation of the form of dependency upon rejection.",
          "link": "http://arxiv.org/abs/2103.00674",
          "publishedOn": "2021-09-07T07:20:15.077Z",
          "wordCount": null,
          "title": "BEAUTY Powered BEAST. (arXiv:2103.00674v3 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.02585",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harvey_N/0/1/0/all/0/1\">Nicholas J. A. Harvey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portella_V/0/1/0/all/0/1\">Victor S. Portella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>",
          "description": "Online mirror descent (OMD) and dual averaging (DA) -- two fundamental\nalgorithms for online convex optimization -- are known to have very similar\n(and sometimes identical) performance guarantees when used with a fixed\nlearning rate. Under dynamic learning rates, however, OMD is provably inferior\nto DA and suffers a linear regret, even in common settings such as prediction\nwith expert advice. We modify the OMD algorithm through a simple technique that\nwe call stabilization. We give essentially the same abstract regret bound for\nOMD with stabilization and for DA by modifying the classical OMD convergence\nanalysis in a careful and modular way that allows for straightforward and\nflexible proofs. Simple corollaries of these bounds show that OMD with\nstabilization and DA enjoy the same performance guarantees in many applications\n-- even under dynamic learning rates. We also shed light on the similarities\nbetween OMD and DA and show simple conditions under which stabilized-OMD and DA\ngenerate the same iterates.",
          "link": "http://arxiv.org/abs/2006.02585",
          "publishedOn": "2021-09-07T07:20:15.076Z",
          "wordCount": null,
          "title": "Online mirror descent and dual averaging: keeping pace in the dynamic case. (arXiv:2006.02585v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10901",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tanqiu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendre_S/0/1/0/all/0/1\">Sidhant K. Bendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Hanjia Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>",
          "description": "Wildfire is one of the biggest disasters that frequently occurs on the west\ncoast of the United States. Many efforts have been made to understand the\ncauses of the increases in wildfire intensity and frequency in recent years. In\nthis work, we propose static and dynamic prediction models to analyze and\nassess the areas with high wildfire risks in California by utilizing a\nmultitude of environmental data including population density, Normalized\nDifference Vegetation Index (NDVI), Palmer Drought Severity Index (PDSI), tree\nmortality area, tree mortality number, and altitude. Moreover, we focus on a\nbetter understanding of the impacts of different factors so as to inform\npreventive actions. To validate our models and findings, we divide the land of\nCalifornia into 4,242 grids of 0.1 degrees $\\times$ 0.1 degrees in latitude and\nlongitude, and compute the risk of each grid based on spatial and temporal\nconditions. To verify the generalizability of our models, we further expand the\nscope of wildfire risk assessment from California to Washington without any\nfine tuning. By performing counterfactual analysis, we uncover the effects of\nseveral possible methods on reducing the number of high risk wildfires. Taken\ntogether, our study has the potential to estimate, monitor, and reduce the\nrisks of wildfires across diverse areas provided that such environment data is\navailable.",
          "link": "http://arxiv.org/abs/2103.10901",
          "publishedOn": "2021-09-07T07:20:15.076Z",
          "wordCount": null,
          "title": "From Static to Dynamic Prediction: Wildfire Risk Assessment Based on Multiple Environmental Factors. (arXiv:2103.10901v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vogel_R/0/1/0/all/0/1\">Robin Vogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Cl&#xe9;men&#xe7;on</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1\">Pierre Laforgue</a>",
          "description": "In practice, and more especially when training deep neural networks, visual\nrecognition rules are often learned based on various sources of information. On\nthe other hand, the recent deployment of facial recognition systems with uneven\npredictive performances on different population segments highlights the\nrepresentativeness issues possibly induced by a naive aggregation of image\ndatasets. Indeed, sampling bias does not vanish simply by considering larger\ndatasets, and ignoring its impact may completely jeopardize the generalization\ncapacity of the learned prediction rules. In this paper, we show how biasing\nmodels, originally introduced for nonparametric estimation in (Gill et al.,\n1988), and recently revisited from the perspective of statistical learning\ntheory in (Laforgue and Cl\\'emen\\c{c}on, 2019), can be applied to remedy these\nproblems in the context of visual recognition. Based on the (approximate)\nknowledge of the biasing mechanisms at work, our approach consists in\nreweighting the observations, so as to form a nearly debiased estimator of the\ntarget distribution. One key condition for our method to be theoretically valid\nis that the supports of the distributions generating the biased datasets at\ndisposal must overlap, and cover the support of the target distribution. In\norder to meet this requirement in practice, we propose to use a low dimensional\nimage representation, shared across the image databases. Finally, we provide\nnumerical experiments highlighting the relevance of our approach whenever the\nbiasing functions are appropriately chosen.",
          "link": "http://arxiv.org/abs/2109.02357",
          "publishedOn": "2021-09-07T07:20:15.075Z",
          "wordCount": null,
          "title": "Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10059",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishtiaq_A/0/1/0/all/0/1\">Arhum Ishtiaq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_S/0/1/0/all/0/1\">Sara Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anees_M/0/1/0/all/0/1\">Maheen Anees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mumtaz_N/0/1/0/all/0/1\">Neha Mumtaz</a>",
          "description": "With time, machine learning models have increased in their scope,\nfunctionality and size. Consequently, the increased functionality and size of\nsuch models requires high-end hardware to both train and provide inference\nafter the fact. This paper aims to explore the possibilities within the domain\nof model compression, discuss the efficiency of combining various levels of\npruning and quantization, while proposing a quality measurement metric to\nobjectively decide which combination is best in terms of minimizing the\naccuracy delta and maximizing the size reduction factor.",
          "link": "http://arxiv.org/abs/2105.10059",
          "publishedOn": "2021-09-07T07:20:15.075Z",
          "wordCount": null,
          "title": "Model Compression. (arXiv:2105.10059v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02150",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Maddouri_O/0/1/0/all/0/1\">Omar Maddouri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qian_X/0/1/0/all/0/1\">Xiaoning Qian</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Alexander_F/0/1/0/all/0/1\">Francis J. Alexander</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dougherty_E/0/1/0/all/0/1\">Edward R. Dougherty</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yoon_B/0/1/0/all/0/1\">Byung-Jun Yoon</a>",
          "description": "Classification has been a major task for building intelligent systems as it\nenables decision-making under uncertainty. Classifier design aims at building\nmodels from training data for representing feature-label distributions--either\nexplicitly or implicitly. In many scientific or clinical settings, training\ndata are typically limited, which makes designing accurate classifiers and\nevaluating their classification error extremely challenging. While transfer\nlearning (TL) can alleviate this issue by incorporating data from relevant\nsource domains to improve learning in a different target domain, it has\nreceived little attention for performance assessment, notably in error\nestimation. In this paper, we fill this gap by investigating knowledge\ntransferability in the context of classification error estimation within a\nBayesian paradigm. We introduce a novel class of Bayesian minimum mean-square\nerror (MMSE) estimators for optimal Bayesian transfer learning (OBTL), which\nenables rigorous evaluation of classification error under uncertainty in a\nsmall-sample setting. Using Monte Carlo importance sampling, we employ the\nproposed estimator to evaluate the classification accuracy of a broad family of\nclassifiers that span diverse learning capabilities. Experimental results based\non both synthetic data as well as real-world RNA sequencing (RNA-seq) data show\nthat our proposed OBTL error estimation scheme clearly outperforms standard\nerror estimators, especially in a small-sample setting, by tapping into the\ndata from other relevant domains.",
          "link": "http://arxiv.org/abs/2109.02150",
          "publishedOn": "2021-09-07T07:20:15.074Z",
          "wordCount": null,
          "title": "Robust Importance Sampling for Error Estimation in the Context of Optimal Bayesian Transfer Learning. (arXiv:2109.02150v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.11430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaqi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hanqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chenghao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ray T. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">David Z. Pan</a>",
          "description": "Deep neural networks (DNN) have shown superior performance in a variety of\ntasks. As they rapidly evolve, their escalating computation and memory demands\nmake it challenging to deploy them on resource-constrained edge devices. Though\nextensive efficient accelerator designs, from traditional electronics to\nemerging photonics, have been successfully demonstrated, they are still\nbottlenecked by expensive memory accesses due to tremendous gaps between the\nbandwidth/power/latency of electrical memory and computing cores. Previous\nsolutions fail to fully-leverage the ultra-fast computational speed of emerging\nDNN accelerators to break through the critical memory bound. In this work, we\npropose a general and unified framework to trade expensive memory transactions\nwith ultra-fast on-chip computations, directly translating to performance\nimprovement. We are the first to jointly explore the intrinsic correlations and\nbit-level redundancy within DNN kernels and propose a multi-level in situ\ngeneration mechanism with mixed-precision bases to achieve on-the-fly recovery\nof high-resolution parameters with minimum hardware overhead. Extensive\nexperiments demonstrate that our proposed joint method can boost the memory\nefficiency by 10-20x with comparable accuracy over four state-of-the-art\ndesigns, when benchmarked on ResNet-18/DenseNet-121/MobileNetV2/V3 with various\ntasks.",
          "link": "http://arxiv.org/abs/2108.11430",
          "publishedOn": "2021-09-07T07:20:15.074Z",
          "wordCount": null,
          "title": "Towards Memory-Efficient Neural Networks via Multi-Level in situ Generation. (arXiv:2108.11430v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.09381",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1\">Julia Lust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1\">Alexandru Paul Condurache</a>",
          "description": "Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous\napplications. However, it is difficult to tell beforehand if a DNN receiving an\ninput will deliver the correct output since their decision criteria are usually\nnontransparent. A DNN delivers the correct output if the input is within the\narea enclosed by its generalization envelope. In this case, the information\ncontained in the input sample is processed reasonably by the network. It is of\nlarge practical importance to assess at inference time if a DNN generalizes\ncorrectly. Currently, the approaches to achieve this goal are investigated in\ndifferent problem set-ups rather independently from one another, leading to\nthree main research and literature fields: predictive uncertainty,\nout-of-distribution detection and adversarial example detection. This survey\nconnects the three fields within the larger framework of investigating the\ngeneralization performance of machine learning methods and in particular DNNs.\nWe underline the common ground, point at the most promising approaches and give\na structured overview of the methods that provide at inference time means to\nestablish if the current input is within the generalization envelope of a DNN.",
          "link": "http://arxiv.org/abs/2008.09381",
          "publishedOn": "2021-09-07T07:20:15.073Z",
          "wordCount": null,
          "title": "A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01661",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barua_H/0/1/0/all/0/1\">Hrishav Bakul Barua</a>",
          "description": "As we are fast approaching the beginning of a paradigm shift in the field of\nscience, Data driven science (the so called fourth science paradigm) is going\nto be the driving force in research and innovation. From medicine to\nbiodiversity and astronomy to geology, all these terms are somehow going to be\naffected by this paradigm shift. The huge amount of data to be processed under\nthis new paradigm will be a major concern in the future and one will strongly\nrequire cloud based services in all the aspects of these computations (from\nstorage to compute and other services). Another aspect will be energy\nconsumption and performance of prediction jobs and tasks within such a\nscientific paradigm which will change the way one sees computation. Data\nscience has heavily impacted or rather triggered the emergence of Machine\nLearning, Signal/Image/Video processing related algorithms, Artificial\nintelligence, Robotics, health informatics, geoinformatics, and many more such\nareas of interest. Hence, we envisage an era where Data science can deliver its\npromises with the help of the existing cloud based platforms and services with\nthe addition of new services. In this article, we discuss about data driven\nscience and Machine learning and how they are going to be linked through cloud\nbased services in the future. It also discusses the rise of paradigms like\napproximate computing, quantum computing and many more in recent times and\ntheir applicability in big data processing, data science, analytics, prediction\nand machine learning in the cloud environments.",
          "link": "http://arxiv.org/abs/2109.01661",
          "publishedOn": "2021-09-07T07:20:15.072Z",
          "wordCount": 734,
          "title": "Data science and Machine learning in the Clouds: A Perspective for the Future. (arXiv:2109.01661v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01983",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanhao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>",
          "description": "We consider adversarial attacks to a black-box model when no queries are\nallowed. In this setting, many methods directly attack surrogate models and\ntransfer the obtained adversarial examples to fool the target model. Plenty of\nprevious works investigated what kind of attacks to the surrogate model can\ngenerate more transferable adversarial examples, but their performances are\nstill limited due to the mismatches between surrogate models and the target\nmodel. In this paper, we tackle this problem from a novel angle -- instead of\nusing the original surrogate models, can we obtain a Meta-Surrogate Model (MSM)\nsuch that attacks to this model can be easier transferred to other models? We\nshow that this goal can be mathematically formulated as a well-posed\n(bi-level-like) optimization problem and design a differentiable attacker to\nmake training feasible. Given one or a set of surrogate models, our method can\nthus obtain an MSM such that adversarial examples generated on MSM enjoy\neximious transferability. Comprehensive experiments on Cifar-10 and ImageNet\ndemonstrate that by attacking the MSM, we can obtain stronger transferable\nadversarial examples to fool black-box models including adversarially trained\nones, with much higher success rates than existing methods. The proposed method\nreveals significant security challenges of deep models and is promising to be\nserved as a state-of-the-art benchmark for evaluating the robustness of deep\nmodels in the black-box setting.",
          "link": "http://arxiv.org/abs/2109.01983",
          "publishedOn": "2021-09-07T07:20:15.064Z",
          "wordCount": 674,
          "title": "Training Meta-Surrogate Model for Transferable Adversarial Attack. (arXiv:2109.01983v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01668",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sanner_A/0/1/0/all/0/1\">Antoine Sanner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1\">Camila Gonzalez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mukhopadhyay_A/0/1/0/all/0/1\">Anirban Mukhopadhyay</a>",
          "description": "The recent achievements of Deep Learning rely on the test data being similar\nin distribution to the training data. In an ideal case, Deep Learning models\nwould achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make\npredictions on out-of-distribution data. Yet in practice, models usually fail\nto generalize well when facing a shift in distribution. Several methods were\nthereby designed to improve the robustness of the features learned by a model\nthrough Regularization- or Domain-Prediction-based schemes. Segmenting medical\nimages such as MRIs of the hippocampus is essential for the diagnosis and\ntreatment of neuropsychiatric disorders. But these brain images often suffer\nfrom distribution shift due to the patient's age and various pathologies\naffecting the shape of the organ. In this work, we evaluate OoD Generalization\nsolutions for the problem of hippocampus segmentation in MR data using both\nfully- and semi-supervised training. We find that no method performs reliably\nin all experiments. Only the V-REx loss stands out as it remains easy to tune,\nwhile it outperforms a standard U-Net in most cases.",
          "link": "http://arxiv.org/abs/2109.01668",
          "publishedOn": "2021-09-07T07:20:15.051Z",
          "wordCount": 629,
          "title": "How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Poiitis_M/0/1/0/all/0/1\">Marinos Poiitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sermpezis_P/0/1/0/all/0/1\">Pavlos Sermpezis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakali_A/0/1/0/all/0/1\">Athena Vakali</a>",
          "description": "Graph Representation Learning (GRL) has become essential for modern graph\ndata mining and learning tasks. GRL aims to capture the graph's structural\ninformation and exploit it in combination with node and edge attributes to\ncompute low-dimensional representations. While Graph Neural Networks (GNNs)\nhave been used in state-of-the-art GRL architectures, they have been shown to\nsuffer from over smoothing when many GNN layers need to be stacked. In a\ndifferent GRL approach, spectral methods based on graph filtering have emerged\naddressing over smoothing; however, up to now, they employ traditional neural\nnetworks that cannot efficiently exploit the structure of graph data. Motivated\nby this, we propose PointSpectrum, a spectral method that incorporates a set\nequivariant network to account for a graph's structure. PointSpectrum enhances\nthe efficiency and expressiveness of spectral methods, while it outperforms or\ncompetes with state-of-the-art GRL methods. Overall, PointSpectrum addresses\nover smoothing by employing a graph filter and captures a graph's structure\nthrough set equivariance, lying on the intersection of GNNs and spectral\nmethods. Our findings are promising for the benefits and applicability of this\narchitectural shift for spectral methods and GRL.",
          "link": "http://arxiv.org/abs/2109.02358",
          "publishedOn": "2021-09-07T07:20:14.954Z",
          "wordCount": null,
          "title": "Pointspectrum: Equivariance Meets Laplacian Filtering for Graph Representation Learning. (arXiv:2109.02358v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>",
          "description": "We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications.\n\nOur algorithm produces primal solutions and dual lower bounds that estimate\nthe distance to optimum. We implement our algorithm on GPUs and show resulting\none to two order-of-magnitudes improvements in execution speed without\nsacrificing solution quality compared to traditional serial algorithms that run\non CPUs. We can solve very large scale benchmark problems with up to\n$\\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We\nmake our code available at https://github.com/pawelswoboda/RAMA.",
          "link": "http://arxiv.org/abs/2109.01838",
          "publishedOn": "2021-09-07T07:20:14.953Z",
          "wordCount": null,
          "title": "RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yusen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1\">Yelena Yesha</a>",
          "description": "Adversarial attacks attempt to disrupt the training, retraining and utilizing\nof artificial intelligence and machine learning models in large-scale\ndistributed machine learning systems. This causes security risks on its\nprediction outcome. For example, attackers attempt to poison the model by\neither presenting inaccurate misrepresentative data or altering the models'\nparameters. In addition, Byzantine faults including software, hardware, network\nissues occur in distributed systems which also lead to a negative impact on the\nprediction outcome. In this paper, we propose a novel distributed training\nalgorithm, partial synchronous stochastic gradient descent (ParSGD), which\ndefends adversarial attacks and/or tolerates Byzantine faults. We demonstrate\nthe effectiveness of our algorithm under three common adversarial attacks again\nthe ML models and a Byzantine fault during the training phase. Our results show\nthat using ParSGD, ML models can still produce accurate predictions as if it is\nnot being attacked nor having failures at all when almost half of the nodes are\nbeing compromised or failed. We will report the experimental evaluations of\nParSGD in comparison with other algorithms.",
          "link": "http://arxiv.org/abs/2109.02018",
          "publishedOn": "2021-09-07T07:20:14.951Z",
          "wordCount": null,
          "title": "Tolerating Adversarial Attacks and Byzantine Faults in Distributed Machine Learning. (arXiv:2109.02018v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on Spark AR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on the enhanced data sets and showing that they outperform\nequivalent systems trained on the original data sets (containing faces without\nmasks), when the test benchmark contains masked faces.",
          "link": "http://arxiv.org/abs/2109.01745",
          "publishedOn": "2021-09-07T07:20:14.948Z",
          "wordCount": null,
          "title": "A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1\">Renchunzi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>",
          "description": "Knowledge transfer across several streaming processes remain challenging\nproblem not only because of different distributions of each stream but also\nbecause of rapidly changing and never-ending environments of data streams.\nAlbeit growing research achievements in this area, most of existing works are\ndeveloped for a single source domain which limits its resilience to exploit\nmulti-source domains being beneficial to recover from concept drifts quickly\nand to avoid the negative transfer problem. An online domain adaptation\ntechnique under multisource streaming processes, namely automatic online\nmulti-source domain adaptation (AOMSDA), is proposed in this paper. The online\ndomain adaptation strategy of AOMSDA is formulated under a coupled generative\nand discriminative approach of denoising autoencoder (DAE) where the central\nmoment discrepancy (CMD)-based regularizer is integrated to handle the\nexistence of multi-source domains thereby taking advantage of complementary\ninformation sources. The asynchronous concept drifts taking place at different\ntime periods are addressed by a self-organizing structure and a node\nre-weighting strategy. Our numerical study demonstrates that AOMSDA is capable\nof outperforming its counterparts in 5 of 8 study cases while the ablation\nstudy depicts the advantage of each learning component. In addition, AOMSDA is\ngeneral for any number of source streams. The source code of AOMSDA is shared\npublicly in https://github.com/Renchunzi-Xie/AOMSDA.git.",
          "link": "http://arxiv.org/abs/2109.01996",
          "publishedOn": "2021-09-07T07:20:14.939Z",
          "wordCount": null,
          "title": "Automatic Online Multi-Source Domain Adaptation. (arXiv:2109.01996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Effati_M/0/1/0/all/0/1\">Meysam Effati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu-Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naguib_H/0/1/0/all/0/1\">Hani E. Naguib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejat_G/0/1/0/all/0/1\">Goldie Nejat</a>",
          "description": "The COVID-19 pandemic is one of the most challenging healthcare crises during\nthe 21st century. As the virus continues to spread on a global scale, the\nmajority of efforts have been on the development of vaccines and the mass\nimmunization of the public. While the daily case numbers were following a\ndecreasing trend, the emergent of new virus mutations and variants still pose a\nsignificant threat. As economies start recovering and societies start opening\nup with people going back into office buildings, schools, and malls, we still\nneed to have the ability to detect and minimize the spread of COVID-19.\nIndividuals with COVID-19 may show multiple symptoms such as cough, fever, and\nshortness of breath. Many of the existing detection techniques focus on\nsymptoms having the same equal importance. However, it has been shown that some\nsymptoms are more prevalent than others. In this paper, we present a multimodal\nmethod to predict COVID-19 by incorporating existing deep learning classifiers\nusing convolutional neural networks and our novel probability-based weighting\nfunction that considers the prevalence of each symptom. The experiments were\nperformed on an existing dataset with respect to the three considered modes of\ncoughs, fever, and shortness of breath. The results show considerable\nimprovements in the detection of COVID-19 using our weighting function when\ncompared to an equal weighting function.",
          "link": "http://arxiv.org/abs/2109.01669",
          "publishedOn": "2021-09-07T07:20:14.938Z",
          "wordCount": null,
          "title": "Multimodal Detection of COVID-19 Symptoms using Deep Learning & Probability-based Weighting of Modes. (arXiv:2109.01669v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadjafari_S/0/1/0/all/0/1\">Sanaz Mohammadjafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cevik_M/0/1/0/all/0/1\">Mucahit Cevik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basar_A/0/1/0/all/0/1\">Ayse Basar</a>",
          "description": "Generative adversarial networks (GANs) are one of the most widely used\ngenerative models. GANs can learn complex multi-modal distributions, and\ngenerate real-like samples. Despite the major success of GANs in generating\nsynthetic data, they might suffer from unstable training process, and mode\ncollapse. In this paper, we introduce a new GAN architecture called variance\nenforcing GAN (VARGAN), which incorporates a third network to introduce\ndiversity in the generated samples. The third network measures the diversity of\nthe generated samples, which is used to penalize the generator's loss for low\ndiversity samples. The network is trained on the available training data and\nundesired distributions with limited modality. On a set of synthetic and\nreal-world image data, VARGAN generates a more diverse set of samples compared\nto the recent state-of-the-art models. High diversity and low computational\ncomplexity, as well as fast convergence, make VARGAN a promising model to\nalleviate mode collapse.",
          "link": "http://arxiv.org/abs/2109.02117",
          "publishedOn": "2021-09-07T07:20:14.938Z",
          "wordCount": null,
          "title": "VARGAN: Variance Enforcing Network Enhanced GAN. (arXiv:2109.02117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.05248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_G/0/1/0/all/0/1\">Gregory Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1\">Yann Dauphin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frome_A/0/1/0/all/0/1\">Andrea Frome</a>",
          "description": "Deep neural network pruning and quantization techniques have demonstrated it\nis possible to achieve high levels of compression with surprisingly little\ndegradation to test set accuracy. However, this measure of performance conceals\nsignificant differences in how different classes and images are impacted by\nmodel compression techniques. We find that models with radically different\nnumbers of weights have comparable top-line performance metrics but diverge\nconsiderably in behavior on a narrow subset of the dataset. This small subset\nof data points, which we term Pruning Identified Exemplars (PIEs) are\nsystematically more impacted by the introduction of sparsity. Compression\ndisproportionately impacts model performance on the underrepresented long-tail\nof the data distribution. PIEs over-index on atypical or noisy images that are\nfar more challenging for both humans and algorithms to classify. Our work\nprovides intuition into the role of capacity in deep neural networks and the\ntrade-offs incurred by compression. An understanding of this disparate impact\nis critical given the widespread deployment of compressed models in the wild.",
          "link": "http://arxiv.org/abs/1911.05248",
          "publishedOn": "2021-09-07T07:20:14.938Z",
          "wordCount": null,
          "title": "What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yanming Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiliang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fengru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jerry Zhijian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuanyuan Yang</a>",
          "description": "In this paper, we construct neural networks with ReLU, sine and $2^x$ as\nactivation functions. For general continuous $f$ defined on $[0,1]^d$ with\ncontinuity modulus $\\omega_f(\\cdot)$, we construct ReLU-sine-$2^x$ networks\nthat enjoy an approximation rate\n$\\mathcal{O}(\\omega_f(\\sqrt{d})\\cdot2^{-M}+\\omega_{f}\\left(\\frac{\\sqrt{d}}{N}\\right))$,\nwhere $M,N\\in \\mathbb{N}^{+}$ denote the hyperparameters related to widths of\nthe networks. As a consequence, we can construct ReLU-sine-$2^x$ network with\nthe depth $5$ and width\n$\\max\\left\\{\\left\\lceil2d^{3/2}\\left(\\frac{3\\mu}{\\epsilon}\\right)^{1/{\\alpha}}\\right\\rceil,2\\left\\lceil\\log_2\\frac{3\\mu\nd^{\\alpha/2}}{2\\epsilon}\\right\\rceil+2\\right\\}$ that approximates $f\\in\n\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$ within a given tolerance $\\epsilon >0$\nmeasured in $L^p$ norm $p\\in[1,\\infty)$, where\n$\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$ denotes the H\\\"older continuous function\nclass defined on $[0,1]^d$ with order $\\alpha \\in (0,1]$ and constant $\\mu >\n0$. Therefore, the ReLU-sine-$2^x$ networks overcome the curse of\ndimensionality on $\\mathcal{H}_{\\mu}^{\\alpha}([0,1]^d)$. In addition to its\nsupper expressive power, functions implemented by ReLU-sine-$2^x$ networks are\n(generalized) differentiable, enabling us to apply SGD to train.",
          "link": "http://arxiv.org/abs/2103.00542",
          "publishedOn": "2021-09-07T07:20:14.938Z",
          "wordCount": null,
          "title": "Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on H\\\"older Class. (arXiv:2103.00542v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Peiyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Andrew K.C. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leatherdale_S/0/1/0/all/0/1\">Scott T. Leatherdale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battista_K/0/1/0/all/0/1\">Kate Battista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butt_Z/0/1/0/all/0/1\">Zahid A. Butt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>",
          "description": "COMPASS is a longitudinal, prospective cohort study collecting data annually\nfrom students attending high school in jurisdictions across Canada. We aimed to\ndiscover significant frequent/rare associations of behavioral factors among\nCanadian adolescents related to cannabis use. We use a subset of COMPASS\ndataset which contains 18,761 records of students in grades 9 to 12 with 31\nselected features (attributes) involving various characteristics, from living\nhabits to academic performance. We then used the Pattern Discovery and\nDisentanglement (PDD) algorithm that we have developed to detect strong and\nrare (yet statistically significant) associations from the dataset. PDD used\nthe criteria derived from disentangled statistical spaces (known as\nRe-projected Adjusted-Standardized Residual Vector Spaces, notated as RARV). It\noutperformed methods using other criteria (i.e. support and confidence) popular\nas reported in the literature. Association results showed that PDD can\ndiscover: i) a smaller set of succinct significant associations in clusters;\nii) frequent and rare, yet significant, patterns supported by population health\nrelevant study; iii) patterns from a dataset with extremely imbalanced groups\n(majority class: minority class = 88.3%: 11.7%).",
          "link": "http://arxiv.org/abs/2109.01739",
          "publishedOn": "2021-09-07T07:20:14.937Z",
          "wordCount": null,
          "title": "Cohort Characteristics and Factors Associated with Cannabis Use among Adolescents in Canada Using Pattern Discovery and Disentanglement Method. (arXiv:2109.01739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02032",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhenhui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaohong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guanghua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowei Yang</a>",
          "description": "The recent progress in multi-agent deep reinforcement learning(MADRL) makes\nit more practical in real-world tasks, but its relatively poor scalability and\nthe partially observable constraints raise challenges to its performance and\ndeployment. Based on our intuitive observation that the human society could be\nregarded as a large-scale partially observable environment, where each\nindividual has the function of communicating with neighbors and remembering its\nown experience, we propose a novel network structure called hierarchical graph\nrecurrent network(HGRN) for multi-agent cooperation under partial\nobservability. Specifically, we construct the multi-agent system as a graph,\nuse the hierarchical graph attention network(HGAT) to achieve communication\nbetween neighboring agents, and exploit GRU to enable agents to record\nhistorical information. To encourage exploration and improve robustness, we\ndesign a maximum-entropy learning method to learn stochastic policies of a\nconfigurable target action entropy. Based on the above technologies, we\nproposed a value-based MADRL algorithm called Soft-HGRN and its actor-critic\nvariant named SAC-HRGN. Experimental results based on three homogeneous tasks\nand one heterogeneous environment not only show that our approach achieves\nclear improvements compared with four baselines, but also demonstrates the\ninterpretability, scalability, and transferability of the proposed model.\nAblation studies prove the function and necessity of each component.",
          "link": "http://arxiv.org/abs/2109.02032",
          "publishedOn": "2021-09-07T07:20:14.937Z",
          "wordCount": null,
          "title": "Soft Hierarchical Graph Recurrent Networks for Many-Agent Partially Observable Environments. (arXiv:2109.02032v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.16413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shankai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>",
          "description": "The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.",
          "link": "http://arxiv.org/abs/2010.16413",
          "publishedOn": "2021-09-07T07:20:14.937Z",
          "wordCount": null,
          "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1\">Anson Bastos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaul_M/0/1/0/all/0/1\">Manohar Kaul</a>",
          "description": "Active learning has emerged as a standard paradigm in areas with scarcity of\nlabeled training data, such as in the medical domain. Language models have\nemerged as the prevalent choice of several natural language tasks due to the\nperformance boost offered by these models. However, in several domains, such as\nmedicine, the scarcity of labeled training data is a common issue. Also, these\nmodels may not work well in cases where class imbalance is prevalent. Active\nlearning may prove helpful in these cases to boost the performance with a\nlimited label budget. To this end, we propose a novel method using sampling\ntechniques based on submodular optimization and optimal transport for active\nlearning in language models, dubbed ALLWAS. We construct a sampling strategy\nbased on submodular optimization of the designed objective in the gradient\ndomain. Furthermore, to enable learning from few samples, we propose a novel\nstrategy for sampling from the Wasserstein barycenters. Our empirical\nevaluations on standard benchmark datasets for text classification show that\nour methods perform significantly better (>20% relative increase in some cases)\nthan existing approaches for active learning on language models.",
          "link": "http://arxiv.org/abs/2109.01691",
          "publishedOn": "2021-09-07T07:20:14.936Z",
          "wordCount": null,
          "title": "ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01718",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiuchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Li Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhavani_S/0/1/0/all/0/1\">Sivasubramanium Bhavani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Joyce C. Ho</a>",
          "description": "Tensor factorization has been proved as an efficient unsupervised learning\napproach for health data analysis, especially for computational phenotyping,\nwhere the high-dimensional Electronic Health Records (EHRs) with patients\nhistory of medical procedures, medications, diagnosis, lab tests, etc., are\nconverted to meaningful and interpretable medical concepts. Federated tensor\nfactorization distributes the tensor computation to multiple workers under the\ncoordination of a central server, which enables jointly learning the phenotypes\nacross multiple hospitals while preserving the privacy of the patient\ninformation. However, existing federated tensor factorization algorithms\nencounter the single-point-failure issue with the involvement of the central\nserver, which is not only easily exposed to external attacks, but also limits\nthe number of clients sharing information with the server under restricted\nuplink bandwidth. In this paper, we propose CiderTF, a communication-efficient\ndecentralized generalized tensor factorization, which reduces the uplink\ncommunication cost by leveraging a four-level communication reduction strategy\ndesigned for a generalized tensor factorization, which has the flexibility of\nmodeling different tensor distribution with multiple kinds of loss functions.\nExperiments on two real-world EHR datasets demonstrate that CiderTF achieves\ncomparable convergence with the communication reduction up to 99.99%.",
          "link": "http://arxiv.org/abs/2109.01718",
          "publishedOn": "2021-09-07T07:20:14.934Z",
          "wordCount": null,
          "title": "Communication Efficient Tensor Factorization for Decentralized Healthcare Networks. (arXiv:2109.01718v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01785",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Seddik_M/0/1/0/all/0/1\">Mohamed El Amine Seddik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Changmin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutzeyer_J/0/1/0/all/0/1\">Johannes F. Lutzeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a>",
          "description": "The robustness of the much-used Graph Convolutional Networks (GCNs) to\nperturbations of their input is becoming a topic of increasing importance. In\nthis paper, the random GCN is introduced for which a random matrix theory\nanalysis is possible. This analysis suggests that if the graph is sufficiently\nperturbed, or in the extreme case random, then the GCN fails to benefit from\nthe node features. It is furthermore observed that enhancing the message\npassing step in GCNs by adding the node feature kernel to the adjacency matrix\nof the graph structure solves this problem. An empirical study of a GCN\nutilised for node classification on six real datasets further confirms the\ntheoretical findings and demonstrates that perturbations of the graph structure\ncan result in GCNs performing significantly worse than Multi-Layer Perceptrons\nrun on the node features alone. In practice, adding a node feature kernel to\nthe message passing of perturbed graphs results in a significant improvement of\nthe GCN's performance, thereby rendering it more robust to graph perturbations.\nOur code is publicly available at:https://github.com/ChangminWu/RobustGCN.",
          "link": "http://arxiv.org/abs/2109.01785",
          "publishedOn": "2021-09-07T07:20:14.934Z",
          "wordCount": null,
          "title": "Node Feature Kernels Increase Graph Convolutional Network Robustness. (arXiv:2109.01785v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01949",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yifan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>",
          "description": "Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR",
          "link": "http://arxiv.org/abs/2109.01949",
          "publishedOn": "2021-09-07T07:20:14.934Z",
          "wordCount": null,
          "title": "Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoyama_K/0/1/0/all/0/1\">Kazuo Aoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawada_H/0/1/0/all/0/1\">Hiroshi Sawada</a>",
          "description": "An optical neural network (ONN) is a promising system due to its high-speed\nand low-power operation. Its linear unit performs a multiplication of an input\nvector and a weight matrix in optical analog circuits. Among them, a circuit\nwith a multiple-layered structure of programmable Mach-Zehnder interferometers\n(MZIs) can realize a specific class of unitary matrices with a limited number\nof MZIs as its weight matrix. The circuit is effective for balancing the number\nof programmable MZIs and ONN performance. However, it takes a lot of time to\nlearn MZI parameters of the circuit with a conventional automatic\ndifferentiation (AD), which machine learning platforms are equipped with. To\nsolve the time-consuming problem, we propose an acceleration method for\nlearning MZI parameters. We create customized complex-valued derivatives for an\nMZI, exploiting Wirtinger derivatives and a chain rule. They are incorporated\ninto our newly developed function module implemented in C++ to collectively\ncalculate their values in a multi-layered structure. Our method is simple,\nfast, and versatile as well as compatible with the conventional AD. We\ndemonstrate that our method works 20 times faster than the conventional AD when\na pixel-by-pixel MNIST task is performed in a complex-valued recurrent neural\nnetwork with an MZI-based hidden unit.",
          "link": "http://arxiv.org/abs/2109.01731",
          "publishedOn": "2021-09-07T07:20:14.933Z",
          "wordCount": null,
          "title": "Acceleration Method for Learning Fine-Layered Optical Neural Networks. (arXiv:2109.01731v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02353",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying-Jun Angela Zhang</a>",
          "description": "Federated edge learning (FEEL) has emerged as a revolutionary paradigm to\ndevelop AI services at the edge of 6G wireless networks as it supports\ncollaborative model training at a massive number of mobile devices. However,\nmodel communication over wireless channels, especially in uplink model\nuploading of FEEL, has been widely recognized as a bottleneck that critically\nlimits the efficiency of FEEL. Although over-the-air computation can alleviate\nthe excessive cost of radio resources in FEEL model uploading, practical\nimplementations of over-the-air FEEL still suffer from several challenges,\nincluding strong straggler issues, large communication overheads, and potential\nprivacy leakage. In this article, we study these challenges in over-the-air\nFEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of\nfuture wireless systems, to address these challenges. We study the\nstate-of-the-art solutions on RIS-empowered FEEL and explore the promising\nresearch opportunities for adopting RIS to enhance FEEL performance.",
          "link": "http://arxiv.org/abs/2109.02353",
          "publishedOn": "2021-09-07T07:20:14.933Z",
          "wordCount": null,
          "title": "Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge Learning. (arXiv:2109.02353v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Garttner_S/0/1/0/all/0/1\">Stephan G&#xe4;rttner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alpak_F/0/1/0/all/0/1\">Faruk O. Alpak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_A/0/1/0/all/0/1\">Andreas Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nadja Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_F/0/1/0/all/0/1\">Florian Frank</a>",
          "description": "In recent years, convolutional neural networks (CNNs) have experienced an\nincreasing interest for their ability to perform fast approximation of\neffective hydrodynamic parameters in porous media research and applications.\nThis paper presents a novel methodology for permeability prediction from\nmicro-CT scans of geological rock samples. The training data set for CNNs\ndedicated to permeability prediction consists of permeability labels that are\ntypically generated by classical lattice Boltzmann methods (LBM) that simulate\nthe flow through the pore space of the segmented image data. We instead perform\ndirect numerical simulation (DNS) by solving the stationary Stokes equation in\nan efficient and distributed-parallel manner. As such, we circumvent the\nconvergence issues of LBM that frequently are observed on complex pore\ngeometries, and therefore, improve on the generality and accuracy of our\ntraining data set. Using the DNS-computed permeabilities, a physics-informed\nCNN PhyCNN) is trained by additionally providing a tailored characteristic\nquantity of the pore space. More precisely, by exploiting the connection to\nflow problems on a graph representation of the pore space, additional\ninformation about confined structures is provided to the network in terms of\nthe maximum flow value, which is the key innovative component of our workflow.\nAs a result, unprecedented prediction accuracy and robustness are observed for\na variety of sandstone samples from archetypal rock formations.",
          "link": "http://arxiv.org/abs/2109.01818",
          "publishedOn": "2021-09-07T07:20:14.932Z",
          "wordCount": null,
          "title": "Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS. (arXiv:2109.01818v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1905.10115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Badong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+yuan_Z/0/1/0/all/0/1\">Zejian yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengju Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>",
          "description": "As a novel similarity measure that is defined as the expectation of a kernel\nfunction between two random variables, correntropy has been successfully\napplied in robust machine learning and signal processing to combat large\noutliers. The kernel function in correntropy is usually a zero-mean Gaussian\nkernel. In a recent work, the concept of mixture correntropy (MC) was proposed\nto improve the learning performance, where the kernel function is a mixture\nGaussian kernel, namely a linear combination of several zero-mean Gaussian\nkernels with different widths. In both correntropy and mixture correntropy, the\ncenter of the kernel function is, however, always located at zero. In the\npresent work, to further improve the learning performance, we propose the\nconcept of multi-kernel correntropy (MKC), in which each component of the\nmixture Gaussian kernel can be centered at a different location. The properties\nof the MKC are investigated and an efficient approach is proposed to determine\nthe free parameters in MKC. Experimental results show that the learning\nalgorithms under the maximum multi-kernel correntropy criterion (MMKCC) can\noutperform those under the original maximum correntropy criterion (MCC) and the\nmaximum mixture correntropy criterion (MMCC).",
          "link": "http://arxiv.org/abs/1905.10115",
          "publishedOn": "2021-09-07T07:20:14.932Z",
          "wordCount": null,
          "title": "Multi-Kernel Correntropy for Robust Learning. (arXiv:1905.10115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02549",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1\">Thorben Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1\">Stefan Chmiela</a>",
          "description": "Attention mechanisms are developing into a viable alternative to\nconvolutional layers as elementary building block of NNs. Their main advantage\nis that they are not restricted to capture local dependencies in the input, but\ncan draw arbitrary connections. This unprecedented capability coincides with\nthe long-standing problem of modeling global atomic interactions in molecular\nforce fields and other many-body problems. In its original formulation,\nhowever, attention is not applicable to the continuous domains in which the\natoms live. For this purpose we propose a variant to describe geometric\nrelations for arbitrary atomic configurations in Euclidean space that also\nrespects all relevant physical symmetries. We furthermore demonstrate, how the\nsuccessive application of our learned attention matrices effectively translates\nthe molecular geometry into a set of individual atomic contributions\non-the-fly.",
          "link": "http://arxiv.org/abs/2106.02549",
          "publishedOn": "2021-09-07T07:20:14.932Z",
          "wordCount": null,
          "title": "Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01667",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Salanitri_F/0/1/0/all/0/1\">Federica Proietto Salanitri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bellitto_G/0/1/0/all/0/1\">Giovanni Bellitto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Palazzo_S/0/1/0/all/0/1\">Simone Palazzo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>",
          "description": "We propose a novel 3D fully convolutional deep network for automated pancreas\nsegmentation from both MRI and CT scans. More specifically, the proposed model\nconsists of a 3D encoder that learns to extract volume features at different\nscales; features taken at different points of the encoder hierarchy are then\nsent to multiple 3D decoders that individually predict intermediate\nsegmentation maps. Finally, all segmentation maps are combined to obtain a\nunique detailed segmentation mask. We test our model on both CT and MRI imaging\ndata: the publicly available NIH Pancreas-CT dataset (consisting of 82\ncontrast-enhanced CTs) and a private MRI dataset (consisting of 40 MRI scans).\nExperimental results show that our model outperforms existing methods on CT\npancreas segmentation, obtaining an average Dice score of about 88%, and yields\npromising segmentation performance on a very challenging MRI data set (average\nDice score is about 77%). Additional control experiments demonstrate that the\nachieved performance is due to the combination of our 3D fully-convolutional\ndeep network and the hierarchical representation decoding, thus substantiating\nour architectural design.",
          "link": "http://arxiv.org/abs/2109.01667",
          "publishedOn": "2021-09-07T07:20:14.931Z",
          "wordCount": null,
          "title": "Hierarchical 3D Feature Learning for Pancreas Segmentation. (arXiv:2109.01667v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_G/0/1/0/all/0/1\">Gilles Blanchard</a> (CNRS, LMO, DATASHAPE), <a href=\"http://arxiv.org/find/cs/1/au:+Fermanian_J/0/1/0/all/0/1\">Jean-Baptiste Fermanian</a> (ENS Rennes)",
          "description": "Let $\\mathbf{X} = (X_i)_{1\\leq i \\leq n}$ be an i.i.d. sample of\nsquare-integrable variables in $\\mathbb{R}^d$, with common expectation $\\mu$\nand covariance matrix $\\Sigma$, both unknown. We consider the problem of\ntesting if $\\mu$ is $\\eta$-close to zero, i.e. $\\|\\mu\\| \\leq \\eta $ against\n$\\|\\mu\\| \\geq (\\eta + \\delta)$; we also tackle the more general two-sample mean\ncloseness testing problem. The aim of this paper is to obtain nonasymptotic\nupper and lower bounds on the minimal separation distance $\\delta$ such that we\ncan control both the Type I and Type II errors at a given level. The main\ntechnical tools are concentration inequalities, first for a suitable estimator\nof $\\|\\mu\\|^2$ used a test statistic, and secondly for estimating the operator\nand Frobenius norms of $\\Sigma$ coming into the quantiles of said test\nstatistic. These properties are obtained for Gaussian and bounded\ndistributions. A particular attention is given to the dependence in the\npseudo-dimension $d_*$ of the distribution, defined as $d_* :=\n\\|\\Sigma\\|_2^2/\\|\\Sigma\\|_\\infty^2$. In particular, for $\\eta=0$, the minimum\nseparation distance is ${\\Theta}(d_*^{\\frac{1}{4}}\\sqrt{\\|\\Sigma\\|_\\infty/n})$,\nin contrast with the minimax estimation distance for $\\mu$, which is\n${\\Theta}(d_e^{\\frac{1}{2}}\\sqrt{\\|\\Sigma\\|_\\infty/n})$ (where\n$d_e:=\\|\\Sigma\\|_1/\\|\\Sigma\\|_\\infty$). This generalizes a phenomenon spelled\nout in particular by Baraud (2002).",
          "link": "http://arxiv.org/abs/2109.01730",
          "publishedOn": "2021-09-07T07:20:14.931Z",
          "wordCount": null,
          "title": "Nonasymptotic one-and two-sample tests in high dimension with unknown covariance structure. (arXiv:2109.01730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.13973",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc Hoan Tran</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Nakajima_K/0/1/0/all/0/1\">Kohei Nakajima</a>",
          "description": "Quantifying and verifying the control level in preparing a quantum state are\ncentral challenges in building quantum devices. The quantum state is\ncharacterized from experimental measurements, using a procedure known as\ntomography, which requires a vast number of resources. Furthermore, the\ntomography for a quantum device with temporal processing, which is\nfundamentally different from the standard tomography, has not been formulated.\nWe develop a practical and approximate tomography method using a recurrent\nmachine learning framework for this intriguing situation. The method is based\non repeated quantum interactions between a system called quantum reservoir with\na stream of quantum states. Measurement data from the reservoir are connected\nto a linear readout to train a recurrent relation between quantum channels\napplied to the input stream. We demonstrate our algorithms for quantum learning\ntasks followed by the proposal of a quantum short-term memory capacity to\nevaluate the temporal processing ability of near-term quantum devices.",
          "link": "http://arxiv.org/abs/2103.13973",
          "publishedOn": "2021-09-07T07:20:14.931Z",
          "wordCount": null,
          "title": "Learning Temporal Quantum Tomography. (arXiv:2103.13973v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jhin_S/0/1/0/all/0/1\">Sheo Yon Jhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Heejoo Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Seoyoung Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Solhee Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1\">Noseong Park</a>",
          "description": "Neural networks inspired by differential equations have proliferated for the\npast several years. Neural ordinary differential equations (NODEs) and neural\ncontrolled differential equations (NCDEs) are two representative examples of\nthem. In theory, NCDEs provide better representation learning capability for\ntime-series data than NODEs. In particular, it is known that NCDEs are suitable\nfor processing irregular time-series data. Whereas NODEs have been successfully\nextended after adopting attention, however, it had not been studied yet how to\nintegrate attention into NCDEs. To this end, we present the method of Attentive\nNeural Controlled Differential Equations (ANCDEs) for time-series\nclassification and forecasting, where dual NCDEs are used: one for generating\nattention values, and the other for evolving hidden vectors for a downstream\nmachine learning task. We conduct experiments with three real-world time-series\ndatasets and 10 baselines. After dropping some values, we also conduct\nirregular time-series experiments. Our method consistently shows the best\naccuracy in all cases by non-trivial margins. Our visualizations also show that\nthe presented attention mechanism works as intended by focusing on crucial\ninformation.",
          "link": "http://arxiv.org/abs/2109.01876",
          "publishedOn": "2021-09-07T07:20:14.930Z",
          "wordCount": null,
          "title": "Attentive Neural Controlled Differential Equations for Time-series Classification and Forecasting. (arXiv:2109.01876v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.02644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1\">Juan P. Vigueras-Guill&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1\">Arijit Patra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1\">Ola Engkvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1\">Frank Seeliger</a>",
          "description": "Capsule Networks (CapsNets) is a machine learning architecture proposed to\novercome some of the shortcomings of convolutional neural networks (CNNs).\nHowever, CapsNets have mainly outperformed CNNs in datasets where images are\nsmall and/or the objects to identify have minimal background noise. In this\nwork, we present a new architecture, parallel CapsNets, which exploits the\nconcept of branching the network to isolate certain capsules, allowing each\nbranch to identify different entities. We applied our concept to the two\ncurrent types of CapsNet architectures, studying the performance for networks\nwith different layers of capsules. We tested our design in a public, highly\nunbalanced dataset of acute myeloid leukaemia images (15 classes). Our\nexperiments showed that conventional CapsNets show similar performance than our\nbaseline CNN (ResNeXt-50) but depict instability problems. In contrast,\nparallel CapsNets can outperform ResNeXt-50, is more stable, and shows better\nrotational invariance than both, conventional CapsNets and ResNeXt-50.",
          "link": "http://arxiv.org/abs/2108.02644",
          "publishedOn": "2021-09-07T07:20:14.930Z",
          "wordCount": null,
          "title": "Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>",
          "description": "Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. NLP models built with the conventional paradigm, however, often\nstruggle with generalization across tasks (e.g., a question-answering system\ncannot solve classification tasks). A long-standing challenge in AI is to build\na model that is equipped with the understanding of human-readable instructions\nthat define the tasks, and can generalize to new tasks. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions and 193k task instances. The instructions are\nobtained from crowdsourcing instructions used to collect existing NLP datasets\nand mapped to a unified schema. We adopt generative pre-trained language models\nto encode task-specific instructions along with input and generate task output.\nOur results indicate that models can benefit from instructions to generalize\nacross tasks. These models, however, are far behind supervised task-specific\nmodels, indicating significant room for more progress in this direction.",
          "link": "http://arxiv.org/abs/2104.08773",
          "publishedOn": "2021-09-07T07:20:14.929Z",
          "wordCount": null,
          "title": "Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yuxuan Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1\">Fuzhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zangwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yang You</a>",
          "description": "Mixture of Experts (MoE) with sparse conditional computation has been proved\nan effective architecture for scaling attention-based models to more parameters\nwith comparable computation cost. In this paper, we propose Sparse-MLP, scaling\nthe recent MLP-Mixer model with sparse MoE layers, to achieve a more\ncomputation-efficient architecture. We replace a subset of dense MLP blocks in\nthe MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two\nstages of MoE layers: one with MLP experts mixing information within channels\nalong image patch dimension, one with MLP experts mixing information within\npatches along the channel dimension. Besides, to reduce computational cost in\nrouting and improve experts capacity, we design Re-represent layers in each\nSparse block. These layers are to re-scale image representations by two simple\nbut effective linear transformations. By pre-training on ImageNet-1k with MoCo\nv3 algorithm, our models can outperform dense MLP models with comparable\nparameters and less computational cost on several downstream image\nclassification tasks.",
          "link": "http://arxiv.org/abs/2109.02008",
          "publishedOn": "2021-09-07T07:20:14.928Z",
          "wordCount": null,
          "title": "Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.16894",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Iacus_S/0/1/0/all/0/1\">Stefano Maria Iacus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Santamaria_C/0/1/0/all/0/1\">Carlos Santamaria</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sermi_F/0/1/0/all/0/1\">Francesco Sermi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Spyratos_S/0/1/0/all/0/1\">Spyridon Spyratos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tarchi_D/0/1/0/all/0/1\">Dario Tarchi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vespe_M/0/1/0/all/0/1\">Michele Vespe</a>",
          "description": "This work introduces a new concept of functional areas called Mobility\nFunctional Areas (MFAs), i.e., the geographic zones highly interconnected\naccording to the analysis of mobile positioning data. The MFAs do not coincide\nnecessarily with administrative borders as they are built observing natural\nhuman mobility and, therefore, they can be used to inform, in a bottom-up\napproach, local transportation, spatial planning, health and economic policies.\nAfter presenting the methodology behind the MFAs, this study focuses on the\nlink between the COVID-19 pandemic and the MFAs in Austria. It emerges that the\nMFAs registered an average number of infections statistically larger than the\nareas in the rest of the country, suggesting the usefulness of the MFAs in the\ncontext of targeted re-escalation policy responses to this health crisis. The\nMFAs dataset is openly available to other scholars for further analyses.",
          "link": "http://arxiv.org/abs/2103.16894",
          "publishedOn": "2021-09-07T07:20:14.779Z",
          "wordCount": 654,
          "title": "Mobility Functional Areas and COVID-19 Spread. (arXiv:2103.16894v2 [stat.AP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Pratyay Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1\">Tejas Gokhale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.",
          "link": "http://arxiv.org/abs/2109.01934",
          "publishedOn": "2021-09-07T07:20:14.767Z",
          "wordCount": 643,
          "title": "Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Daniel_B/0/1/0/all/0/1\">Beatriz Cabrero Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marques_R/0/1/0/all/0/1\">Ricardo Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoyet_L/0/1/0/all/0/1\">Ludovic Hoyet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettre_J/0/1/0/all/0/1\">Julien Pettr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blat_J/0/1/0/all/0/1\">Josep Blat</a>",
          "description": "Simulating crowds requires controlling a very large number of trajectories\nand is usually performed using crowd motion algorithms for which appropriate\nparameter values need to be found. The study of the relation between parametric\nvalues for simulation techniques and the quality of the resulting trajectories\nhas been studied either through perceptual experiments or by comparison with\nreal crowd trajectories. In this paper, we integrate both strategies. A quality\nmetric, QF, is proposed to abstract from reference data while capturing the\nmost salient features that affect the perception of trajectory realism. QF\nweights and combines cost functions that are based on several individual, local\nand global properties of trajectories. These trajectory features are selected\nfrom the literature and from interviews with experts. To validate the capacity\nof QF to capture perceived trajectory quality, we conduct an online experiment\nthat demonstrates the high agreement between the automatic quality score and\nnon-expert users. To further demonstrate the usefulness of QF, we use it in a\ndata-free parameter tuning application able to tune any parametric microscopic\ncrowd simulation model that outputs independent trajectories for characters.\nThe learnt parameters for the tuned crowd motion model maintain the influence\nof the reference data which was used to weight the terms of QF.",
          "link": "http://arxiv.org/abs/2108.12346",
          "publishedOn": "2021-09-07T07:20:14.758Z",
          "wordCount": 674,
          "title": "A Perceptually-Validated Metric for Crowd Trajectory Quality Evaluation. (arXiv:2108.12346v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Csordas_R/0/1/0/all/0/1\">R&#xf3;bert Csord&#xe1;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irie_K/0/1/0/all/0/1\">Kazuki Irie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "Recently, many datasets have been proposed to test the systematic\ngeneralization ability of neural networks. The companion baseline Transformers,\ntypically trained with default hyper-parameters from standard tasks, are shown\nto fail dramatically. Here we demonstrate that by revisiting model\nconfigurations as basic as scaling of embeddings, early stopping, relative\npositional embedding, and Universal Transformer variants, we can drastically\nimprove the performance of Transformers on systematic generalization. We report\nimprovements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics\ndataset. Our models improve accuracy from 50% to 85% on the PCFG productivity\nsplit, and from 35% to 81% on COGS. On SCAN, relative positional embedding\nlargely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%\naccuracy on the length split with a cutoff at 26. Importantly, performance\ndifferences between these models are typically invisible on the IID data split.\nThis calls for proper generalization validation sets for developing neural\nnetworks that generalize systematically. We publicly release the code to\nreproduce our results.",
          "link": "http://arxiv.org/abs/2108.12284",
          "publishedOn": "2021-09-07T07:20:14.741Z",
          "wordCount": 643,
          "title": "The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. (arXiv:2108.12284v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10828",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhou_T/0/1/0/all/0/1\">Taotao Zhou</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Droguett_E/0/1/0/all/0/1\">Enrique Lopez Droguett</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mosleh_A/0/1/0/all/0/1\">Ali Mosleh</a>",
          "description": "Considerable research has been devoted to deep learning-based predictive\nmodels for system prognostics and health management in the reliability and\nsafety community. However, there is limited study on the utilization of deep\nlearning for system reliability assessment. This paper aims to bridge this gap\nand explore this new interface between deep learning and system reliability\nassessment by exploiting the recent advances of physics-informed deep learning.\nParticularly, we present an approach to frame system reliability assessment in\nthe context of physics-informed deep learning and discuss the potential value\nof physics-informed generative adversarial networks for the uncertainty\nquantification and measurement data incorporation in system reliability\nassessment. The proposed approach is demonstrated by three numerical examples\ninvolving a dual-processor computing system. The results indicate the potential\nvalue of physics-informed deep learning to alleviate computational challenges\nand combine measurement data and mathematical models for system reliability\nassessment.",
          "link": "http://arxiv.org/abs/2108.10828",
          "publishedOn": "2021-09-07T07:20:14.718Z",
          "wordCount": 610,
          "title": "Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment. (arXiv:2108.10828v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02255",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Cheng_S/0/1/0/all/0/1\">Sheng Cheng</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>",
          "description": "This paper considers the open challenge of identifying complete, concise, and\nexplainable quantitative microstructure representations for disordered\nheterogeneous material systems. Completeness and conciseness have been achieved\nthrough existing data-driven methods, e.g., deep generative models, which,\nhowever, do not provide mathematically explainable latent representations. This\nstudy investigates representations composed of three-point correlation\nfunctions, which are a special type of spatial convolutions. We show that a\nvariety of microstructures can be characterized by a concise subset of\nthree-point correlations, and the identification of such subsets can be\nachieved by Bayesian optimization. Lastly, we show that the proposed\nrepresentation can directly be used to compute material properties based on the\neffective medium theory.",
          "link": "http://arxiv.org/abs/2109.02255",
          "publishedOn": "2021-09-07T07:20:14.703Z",
          "wordCount": 558,
          "title": "Data-Driven Learning of 3-Point Correlation Functions as Microstructure Representations. (arXiv:2109.02255v1 [cond-mat.mtrl-sci])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06332",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rodriguez_Galvez_B/0/1/0/all/0/1\">Borja Rodr&#xed;guez-G&#xe1;lvez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thobaben_R/0/1/0/all/0/1\">Ragnar Thobaben</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Skoglund_M/0/1/0/all/0/1\">Mikael Skoglund</a>",
          "description": "In this article, we propose a new variational approach to learn private\nand/or fair representations. This approach is based on the Lagrangians of a new\nformulation of the privacy and fairness optimization problems that we propose.\nIn this formulation, we aim to generate representations of the data that keep a\nprescribed level of the relevant information that is not shared by the private\nor sensitive data, while minimizing the remaining information they keep. The\nproposed approach (i) exhibits the similarities of the privacy and fairness\nproblems, (ii) allows us to control the trade-off between utility and privacy\nor fairness through the Lagrange multiplier parameter, and (iii) can be\ncomfortably incorporated to common representation learning algorithms such as\nthe VAE, the $\\beta$-VAE, the VIB, or the nonlinear IB.",
          "link": "http://arxiv.org/abs/2006.06332",
          "publishedOn": "2021-09-07T07:20:14.680Z",
          "wordCount": 633,
          "title": "A Variational Approach to Privacy and Fairness. (arXiv:2006.06332v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slavicek_J/0/1/0/all/0/1\">Josef Slav&#xed;&#x10d;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swart_A/0/1/0/all/0/1\">Albert Swart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klco_M/0/1/0/all/0/1\">Michal Kl&#x10d;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brummer_N/0/1/0/all/0/1\">Niko Br&#xfc;mmer</a>",
          "description": "We describe the Phonexia submission for the VoxCeleb Speaker Recognition\nChallenge 2021 (VoxSRC-21) in the unsupervised speaker verification track. Our\nsolution was very similar to IDLab's winning submission for VoxSRC-20. An\nembedding extractor was bootstrapped using momentum contrastive learning, with\ninput augmentations as the only source of supervision. This was followed by\nseveral iterations of clustering to assign pseudo-speaker labels that were then\nused for supervised embedding extractor training. Finally, a score fusion was\ndone, by averaging the zt-normalized cosine scores of five different embedding\nextractors. We briefly also describe unsuccessful solutions involving i-vectors\ninstead of DNN embeddings and PLDA instead of cosine scoring.",
          "link": "http://arxiv.org/abs/2109.02052",
          "publishedOn": "2021-09-07T07:20:14.364Z",
          "wordCount": null,
          "title": "The Phonexia VoxCeleb Speaker Recognition Challenge 2021 System Description. (arXiv:2109.02052v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2010.08843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_J/0/1/0/all/0/1\">Jayakumar Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1\">Amit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seraj_R/0/1/0/all/0/1\">Raihan Seraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Aditya Mahajan</a>",
          "description": "We propose a theoretical framework for approximate planning and learning in\npartially observed systems. Our framework is based on the fundamental notion of\ninformation state. We provide two equivalent definitions of information state\n-- i) a function of history which is sufficient to compute the expected reward\nand predict its next value; ii) equivalently, a function of the history which\ncan be recursively updated and is sufficient to compute the expected reward and\npredict the next observation. An information state always leads to a dynamic\nprogramming decomposition. Our key result is to show that if a function of the\nhistory (called approximate information state (AIS)) approximately satisfies\nthe properties of the information state, then there is a corresponding\napproximate dynamic program. We show that the policy computed using this is\napproximately optimal with bounded loss of optimality. We show that several\napproximations in state, observation and action spaces in literature can be\nviewed as instances of AIS. In some of these cases, we obtain tighter bounds. A\nsalient feature of AIS is that it can be learnt from data. We present AIS based\nmulti-time scale policy gradient algorithms. and detailed numerical experiments\nwith low, moderate and high dimensional environments.",
          "link": "http://arxiv.org/abs/2010.08843",
          "publishedOn": "2021-09-07T07:20:14.293Z",
          "wordCount": null,
          "title": "Approximate information state for approximate planning and reinforcement learning in partially observed systems. (arXiv:2010.08843v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaotie Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mguni_D/0/1/0/all/0/1\">David Henry Mguni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaodong Yang</a>",
          "description": "Similar to the role of Markov decision processes in reinforcement learning,\nStochastic Games (SGs) lay the foundation for the study of multi-agent\nreinforcement learning (MARL) and sequential agent interactions. In this paper,\nwe derive that computing an approximate Markov Perfect Equilibrium (MPE) in a\nfinite-state discounted Stochastic Game within the exponential precision is\n\\textbf{PPAD}-complete. We adopt a function with a polynomially bounded\ndescription in the strategy space to convert the MPE computation to a\nfixed-point problem, even though the stochastic game may demand an exponential\nnumber of pure strategies, in the number of states, for each agent. The\ncompleteness result follows the reduction of the fixed-point problem to {\\sc\nEnd of the Line}. Our results indicate that finding an MPE in SGs is highly\nunlikely to be \\textbf{NP}-hard unless \\textbf{NP}=\\textbf{co-NP}. Our work\noffers confidence for MARL research to study MPE computation on general-sum SGs\nand to develop fruitful algorithms as currently on zero-sum SGs.",
          "link": "http://arxiv.org/abs/2109.01795",
          "publishedOn": "2021-09-07T07:20:14.284Z",
          "wordCount": null,
          "title": "On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games. (arXiv:2109.01795v1 [cs.GT])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02027",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junran Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yicheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>",
          "description": "In deep neural networks, better results can often be obtained by increasing\nthe complexity of previously developed basic models. However, it is unclear\nwhether there is a way to boost performance by decreasing the complexity of\nsuch models. Here, based on an optimization method, we investigate the\nfeasibility of improving graph classification performance while simplifying the\nmodel learning process. Inspired by progress in structural information\nassessment, we optimize the given data sample from graphs to encoding trees. In\nparticular, we minimize the structural entropy of the transformed encoding tree\nto decode the key structure underlying a graph. This transformation is denoted\nas structural optimization. Furthermore, we propose a novel feature combination\nscheme, termed hierarchical reporting, for encoding trees. In this scheme,\nfeatures are transferred from leaf nodes to root nodes by following the\nhierarchical structures of encoding trees. We then present an implementation of\nthe scheme in a tree kernel and a convolutional network to perform graph\nclassification. The tree kernel follows label propagation in the\nWeisfeiler-Lehman (WL) subtree kernel, but it has a lower runtime complexity\n$O(n)$. The convolutional network is a special implementation of our tree\nkernel in the deep learning field and is called Encoding Tree Learning (ETL).\nWe empirically validate our tree kernel and convolutional network with several\ngraph classification benchmarks and demonstrate that our methods achieve better\nperformance and lower computational consumption than competing approaches.",
          "link": "http://arxiv.org/abs/2109.02027",
          "publishedOn": "2021-09-07T07:20:14.265Z",
          "wordCount": null,
          "title": "Structural Optimization Makes Graph Classification Simpler and Better. (arXiv:2109.02027v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01887",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dobshik_A/0/1/0/all/0/1\">Anna Dobshik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulupov_A/0/1/0/all/0/1\">Andrey Tulupov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berikov_V/0/1/0/all/0/1\">Vladimir Berikov</a>",
          "description": "This paper presents an automatic algorithm for the segmentation of areas\naffected by an acute stroke on the non-contrast computed tomography brain\nimages. The proposed algorithm is designed for learning in a weakly supervised\nscenario when some images are labeled accurately, and some images are labeled\ninaccurately. Wrong labels appear as a result of inaccuracy made by a\nradiologist in the process of manual annotation of computed tomography images.\nWe propose methods for solving the segmentation problem in the case of\ninaccurately labeled training data. We use the U-Net neural network\narchitecture with several modifications. Experiments on real computed\ntomography scans show that the proposed methods increase the segmentation\naccuracy.",
          "link": "http://arxiv.org/abs/2109.01887",
          "publishedOn": "2021-09-07T07:20:14.258Z",
          "wordCount": null,
          "title": "Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_P/0/1/0/all/0/1\">Pedro H. T. Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1\">Jos&#xe9; Marcato Junior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Semantic segmentation is a classic computer vision task with multiple\napplications, which includes medical and remote sensing image analysis. Despite\nrecent advances with deep-based approaches, labeling samples (pixels) for\ntraining models is laborious and, in some cases, unfeasible. In this paper, we\npresent two novel meta learning methods, named WeaSeL and ProtoSeg, for the\nfew-shot semantic segmentation task with sparse annotations. We conducted\nextensive evaluation of the proposed methods in different applications (12\ndatasets) in medical imaging and agricultural remote sensing, which are very\ndistinct fields of knowledge and usually subject to data scarcity. The results\ndemonstrated the potential of our method, achieving suitable results for\nsegmenting both coffee/orange crops and anatomical parts of the human body in\ncomparison with full dense annotation.",
          "link": "http://arxiv.org/abs/2109.01693",
          "publishedOn": "2021-09-07T07:20:14.256Z",
          "wordCount": null,
          "title": "Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01980",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1\">Kfir Aberman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1\">Yossi Gandelsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David E. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohlhoff_K/0/1/0/all/0/1\">Kai Kohlhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1\">Yael Pritch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1\">Michael Rubinstein</a>",
          "description": "Using only a model that was trained to predict where people look at images,\nand no additional training data, we can produce a range of powerful editing\neffects for reducing distraction in images. Given an image and a mask\nspecifying the region to edit, we backpropagate through a state-of-the-art\nsaliency model to parameterize a differentiable editing operator, such that the\nsaliency within the masked region is reduced. We demonstrate several operators,\nincluding: a recoloring operator, which learns to apply a color transform that\ncamouflages and blends distractors into their surroundings; a warping operator,\nwhich warps less salient image regions to cover distractors, gradually\ncollapsing objects into themselves and effectively removing them (an effect\nakin to inpainting); a GAN operator, which uses a semantic prior to fully\nreplace image regions with plausible, less salient alternatives. The resulting\neffects are consistent with cognitive research on the human visual system\n(e.g., since color mismatch is salient, the recoloring operator learns to\nharmonize objects' colors with their surrounding to reduce their saliency),\nand, importantly, are all achieved solely through the guidance of the\npretrained saliency model, with no additional supervision. We present results\non a variety of natural images and conduct a perceptual study to evaluate and\nvalidate the changes in viewers' eye-gaze between the original images and our\nedited results.",
          "link": "http://arxiv.org/abs/2109.01980",
          "publishedOn": "2021-09-07T07:20:14.246Z",
          "wordCount": null,
          "title": "Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmucker_R/0/1/0/all/0/1\">Robin Schmucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shijia Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom M. Mitchell</a>",
          "description": "We consider the problem of assessing the changing knowledge state of\nindividual students as they go through online courses. This student performance\n(SP) modeling problem, also known as knowledge tracing, is a critical step for\nbuilding adaptive online teaching systems. Specifically, we conduct a study of\nhow to utilize various types and large amounts of students log data to train\naccurate machine learning models that predict the knowledge state of future\nstudents. This study is the first to use four very large datasets made\navailable recently from four distinct intelligent tutoring systems. Our results\ninclude a new machine learning approach that defines a new state of the art for\nSP modeling, improving over earlier methods in several ways: First, we achieve\nimproved accuracy by introducing new features that can be easily computed from\nconventional question-response logs (e.g., the pattern in the student's most\nrecent answers). Second, we take advantage of features of the student history\nthat go beyond question-response pairs (e.g., which video segments the student\nwatched, or skipped) as well as information about prerequisite structure in the\ncurriculum. Third, we train multiple specialized modeling models for different\naspects of the curriculum (e.g., specializing in early versus later segments of\nthe student history), then combine these specialized models to create a group\nprediction of student knowledge. Taken together, these innovations yield an\naverage AUC score across these four datasets of 0.807 compared to the previous\nbest logistic regression approach score of 0.766, and also outperforming\nstate-of-the-art deep neural net approaches. Importantly, we observe consistent\nimprovements from each of our three methodological innovations, in each\ndataset, suggesting that our methods are of general utility and likely to\nproduce improvements for other online tutoring systems as well.",
          "link": "http://arxiv.org/abs/2109.01753",
          "publishedOn": "2021-09-07T07:20:14.242Z",
          "wordCount": null,
          "title": "Assessing the Knowledge State of Online Students -- New Data, New Approaches, Improved Accuracy. (arXiv:2109.01753v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arab_M/0/1/0/all/0/1\">Mohammad Arab</a>",
          "description": "One of the goals of every business enterprise is to increase customer\nloyalty. The degree of customer loyalty is called customer quality which its\nforecasting will affect strategic marketing practices. The purpose of this\nstudy is to predict the quality of customers of large e-commerce social\nnetworks by big data algorithms and unsupervised learning. For this purpose, a\ngraph-based social network analysis framework was used for community detection\nin the Stanford Network Analysis Platform (SNAP). Then in the found\ncommunities, the quality of customers was predicted. The results showed that\nvarious visits with an impact of 37.13% can have the greatest impact on\ncustomer quality and the order of impact of other parameters were from highest\nto lowest: number of frequent customer visits (28.56%), role in social networks\n(28.37%), Indirect transactions (26.74%), activity days (25.62%) and customer\nsocial network size (25.06%).",
          "link": "http://arxiv.org/abs/2109.02080",
          "publishedOn": "2021-09-07T07:20:14.191Z",
          "wordCount": null,
          "title": "Providing an Approach to Predicting Customer Quality in E-Commerce Social Networks Based on Big Data and Unsupervised Learning Method. (arXiv:2109.02080v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdelkarim_S/0/1/0/all/0/1\">Sherif Abdelkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aniket Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achlioptas_P/0/1/0/all/0/1\">Panos Achlioptas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "Several approaches have been proposed in recent literature to alleviate the\nlong-tail problem, mainly in object classification tasks. In this paper, we\nmake the first large-scale study concerning the task of Long-Tail Visual\nRelationship Recognition (LTVRR). LTVRR aims at improving the learning of\nstructured visual relationships that come from the long-tail (e.g., \"rabbit\ngrazing on grass\"). In this setup, the subject, relation, and object classes\neach follow a long-tail distribution. To begin our study and make a future\nbenchmark for the community, we introduce two LTVRR-related benchmarks, dubbed\nVG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.\nWe use these benchmarks to study the performance of several state-of-the-art\nlong-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic\nhubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR\nsetup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top\nof existing models and despite being simple, our results show that they can\nremarkably improve the performance, especially on tail classes. Benchmarks,\ncode, and models have been made available at:\nhttps://github.com/Vision-CAIR/LTVRR.",
          "link": "http://arxiv.org/abs/2004.00436",
          "publishedOn": "2021-09-07T07:20:14.179Z",
          "wordCount": null,
          "title": "Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huimin Peng</a>",
          "description": "General AI system solves a wide range of tasks with high performance in an\nautomated fashion. The best general AI algorithm designed by one individual is\ndifferent from that devised by another. The best performance records achieved\nby different users are also different. An inevitable component of general AI is\ntacit knowledge that depends upon user-specific comprehension of task\ninformation and individual model design preferences that are related to user\ntechnical experiences. Tacit knowledge affects model performance but cannot be\nautomatically optimized in general AI algorithms. In this paper, we propose\nUser-Oriented Smart General AI System under Causal Inference, abbreviated as\nUOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means\nunder the framework of causal inference. User characteristics that have a\nsignificant influence upon tacit knowledge can be extracted from observed model\ntraining experiences of many users in external memory modules. Under the\nframework of causal inference, we manage to identify the optimal value of user\ncharacteristics that are connected with the best model performance designed by\nusers. We make suggestions to users about how different user characteristics\ncan improve the best model performance achieved by users. By recommending\nupdating user characteristics associated with individualized tacit knowledge\ncomprehension and technical preferences, UOGAS helps users design models with\nbetter performance.",
          "link": "http://arxiv.org/abs/2103.14561",
          "publishedOn": "2021-09-07T07:20:14.172Z",
          "wordCount": null,
          "title": "User-Oriented Smart General AI System under Causal Inference. (arXiv:2103.14561v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12883",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Bhattacharjee_S/0/1/0/all/0/1\">Subhransu Bhattacharjee</a>, <a href=\"http://arxiv.org/find/math/1/au:+Petersen_I/0/1/0/all/0/1\">Ian Petersen</a>",
          "description": "We introduce a novel adaptive damping technique for an inertial gradient\nsystem which finds application as a gradient descent algorithm for\nunconstrained optimisation. In an example using the non-convex Rosenbrock's\nfunction, we show an improvement on existing momentum-based gradient\noptimisation methods. Also using Lyapunov stability analysis, we demonstrate\nthe performance of the continuous-time version of the algorithm. Using\nnumerical simulations, we consider the performance of its discrete-time\ncounterpart obtained by using the symplectic Euler method of discretisation.",
          "link": "http://arxiv.org/abs/2108.12883",
          "publishedOn": "2021-09-07T07:20:14.121Z",
          "wordCount": null,
          "title": "A Closed Loop Gradient Descent Algorithm applied to Rosenbrock's function. (arXiv:2108.12883v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02877",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1\">Rupesh Kumar Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shyam_P/0/1/0/all/0/1\">Pranav Shyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutz_F/0/1/0/all/0/1\">Filipe Mutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaskowski_W/0/1/0/all/0/1\">Wojciech Ja&#x15b;kowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>",
          "description": "We develop Upside-Down Reinforcement Learning (UDRL), a method for learning\nto act using only supervised learning techniques. Unlike traditional\nalgorithms, UDRL does not use reward prediction or search for an optimal\npolicy. Instead, it trains agents to follow commands such as \"obtain so much\ntotal reward in so much time.\" Many of its general principles are outlined in a\ncompanion report; the goal of this paper is to develop a practical learning\nalgorithm and show that this conceptually simple perspective on agent training\ncan produce a range of rewarding behaviors for multiple episodic environments.\nExperiments show that on some tasks UDRL's performance can be surprisingly\ncompetitive with, and even exceed that of some traditional baseline algorithms\ndeveloped over decades of research. Based on these results, we suggest that\nalternative approaches to expected reward maximization have an important role\nto play in training useful autonomous agents.",
          "link": "http://arxiv.org/abs/1912.02877",
          "publishedOn": "2021-09-07T07:20:14.117Z",
          "wordCount": null,
          "title": "Training Agents using Upside-Down Reinforcement Learning. (arXiv:1912.02877v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1\">Ferran Alet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauza_M/0/1/0/all/0/1\">Maria Bauza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1\">Kenji Kawaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuru_N/0/1/0/all/0/1\">Nurullah Giray Kuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1\">Tomas Lozano-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1\">Leslie Pack Kaelbling</a>",
          "description": "From CNNs to attention mechanisms, encoding inductive biases into neural\nnetworks has been a fruitful source of improvement in machine learning. Adding\nauxiliary losses to the main objective function is a general way of encoding\nbiases that can help networks learn better representations. However, since\nauxiliary losses are minimized only on training data, they suffer from the same\ngeneralization gap as regular task losses. Moreover, by adding a term to the\nloss function, the model optimizes a different objective than the one we care\nabout. In this work we address both problems: first, we take inspiration from\n\\textit{transductive learning} and note that after receiving an input but\nbefore making a prediction, we can fine-tune our networks on any unsupervised\nloss. We call this process {\\em tailoring}, because we customize the model to\neach input to ensure our prediction satisfies the inductive bias. Second, we\nformulate {\\em meta-tailoring}, a nested optimization similar to that in\nmeta-learning, and train our models to perform well on the task objective after\nadapting them using an unsupervised loss. The advantages of tailoring and\nmeta-tailoring are discussed theoretically and demonstrated empirically on a\ndiverse set of examples.",
          "link": "http://arxiv.org/abs/2009.10623",
          "publishedOn": "2021-09-07T07:20:14.110Z",
          "wordCount": null,
          "title": "Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00636",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>",
          "description": "We learn an interactive vision-based driving policy from pre-recorded driving\nlogs via a model-based approach. A forward model of the world supervises a\ndriving policy that predicts the outcome of any potential driving trajectory.\nTo support learning from pre-recorded logs, we assume that the world is on\nrails, meaning neither the agent nor its actions influence the environment.\nThis assumption greatly simplifies the learning problem, factorizing the\ndynamics into a nonreactive world model and a low-dimensional and compact\nforward model of the ego-vehicle. Our approach computes action-values for each\ntraining trajectory using a tabular dynamic-programming evaluation of the\nBellman equations; these action-values in turn supervise the final vision-based\ndriving policy. Despite the world-on-rails assumption, the final driving policy\nacts well in a dynamic and reactive world. At the time of writing, our method\nranks first on the CARLA leaderboard, attaining a 25% higher driving score\nwhile using 40 times less data. Our method is also an order of magnitude more\nsample-efficient than state-of-the-art model-free reinforcement learning\ntechniques on navigational tasks in the ProcGen benchmark.",
          "link": "http://arxiv.org/abs/2105.00636",
          "publishedOn": "2021-09-07T07:20:14.099Z",
          "wordCount": null,
          "title": "Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nemati_E/0/1/0/all/0/1\">Ebrahim Nemati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_T/0/1/0/all/0/1\">Tousif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Mahbubur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1\">Jilong Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">Alex Gao</a>",
          "description": "Cough is a major symptom of respiratory-related diseases. There exists a\ntremendous amount of work in detecting coughs from audio but there has been no\neffort to identify coughs from solely inertial measurement unit (IMU). Coughing\ncauses motion across the whole body and especially on the neck and head.\nTherefore, head motion data during coughing captured by a head-worn IMU sensor\ncould be leveraged to detect coughs using a template matching algorithm. In\ntime series template matching problems, K-Nearest Neighbors (KNN) combined with\nelastic distance measurement (esp. Dynamic Time Warping (DTW)) achieves\noutstanding performance. However, it is often regarded as prohibitively\ntime-consuming. Nearest Centroid Classifier is thereafter proposed. But the\naccuracy is comprised of only one centroid obtained for each class.\nCentroid-based Classifier performs clustering and averaging for each cluster,\nbut requires manually setting the number of clusters. We propose a novel\nself-tuning multi-centroid template-matching algorithm, which can automatically\nadjust the number of clusters to balance accuracy and inference time. Through\nexperiments conducted on synthetic datasets and a real-world earbud-based cough\ndataset, we demonstrate the superiority of our proposed algorithm and present\nthe result of cough detection with a single accelerometer sensor on the earbuds\nplatform.",
          "link": "http://arxiv.org/abs/2109.00630",
          "publishedOn": "2021-09-07T07:20:14.098Z",
          "wordCount": null,
          "title": "A Novel Multi-Centroid Template Matching Algorithm and Its Application to Cough Detection. (arXiv:2109.00630v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jung Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1\">Jihun Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Network quantization, which aims to reduce the bit-lengths of the network\nweights and activations, has emerged for their deployments to resource-limited\ndevices. Although recent studies have successfully discretized a full-precision\nnetwork, they still incur large quantization errors after training, thus giving\nrise to a significant performance gap between a full-precision network and its\nquantized counterpart. In this work, we propose a novel quantization method for\nneural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal\nquantization grids while naturally encouraging the underlying full-precision\nweights to gather around those quantization grids cohesively during training.\nThis property of CPQ is thanks to our two main ingredients that enable\ndifferentiable quantization: i) the use of the categorical distribution\ndesigned by a specific probabilistic parametrization in the forward pass and\nii) our proposed multi-class straight-through estimator (STE) in the backward\npass. Since our second component, multi-class STE, is intrinsically biased, we\nadditionally propose a new bit-drop technique, DropBits, that revises the\nstandard dropout regularization to randomly drop bits instead of neurons. As a\nnatural extension of DropBits, we further introduce the way of learning\nheterogeneous quantization levels to find proper bit-length for each layer by\nimposing an additional regularization on DropBits. We experimentally validate\nour method on various benchmark datasets and network architectures, and also\nsupport a new hypothesis for quantization: learning heterogeneous quantization\nlevels outperforms the case using the same but fixed quantization levels from\nscratch.",
          "link": "http://arxiv.org/abs/2109.02100",
          "publishedOn": "2021-09-07T07:20:14.094Z",
          "wordCount": null,
          "title": "Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schlagenhauf_T/0/1/0/all/0/1\">Tobias Schlagenhauf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yildirim_F/0/1/0/all/0/1\">Faruk Yildirim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruckner_B/0/1/0/all/0/1\">Benedikt Br&#xfc;ckner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleischer_J/0/1/0/all/0/1\">J&#xfc;rgen Fleischer</a>",
          "description": "Training deep learning models in technical domains is often accompanied by\nthe challenge that although the task is clear, insufficient data for training\nis available. In this work, we propose a novel approach based on the\ncombination of Siamese networks and radial basis function networks to perform\ndata-efficient classification without pretraining by measuring the distance\nbetween images in semantic space in a data-efficient manner. We develop the\nmodels using three technical datasets, the NEU dataset, the BSD dataset, and\nthe TEX dataset. In addition to the technical domain, we show the general\napplicability to classical datasets (cifar10 and MNIST) as well. The approach\nis tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise\nreduction of the number of samples available for training. The authors show\nthat the proposed approach outperforms the state-of-the-art models in the low\ndata regime.",
          "link": "http://arxiv.org/abs/2012.01338",
          "publishedOn": "2021-09-07T07:20:14.090Z",
          "wordCount": null,
          "title": "Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.13866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cong_W/0/1/0/all/0/1\">Weilin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsati_R/0/1/0/all/0/1\">Rana Forsati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1\">Mahmut Kandemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mehrdad Mahdavi</a>",
          "description": "Sampling methods (e.g., node-wise, layer-wise, or subgraph) has become an\nindispensable strategy to speed up training large-scale Graph Neural Networks\n(GNNs). However, existing sampling methods are mostly based on the graph\nstructural information and ignore the dynamicity of optimization, which leads\nto high variance in estimating the stochastic gradients. The high variance\nissue can be very pronounced in extremely large graphs, where it results in\nslow convergence and poor generalization. In this paper, we theoretically\nanalyze the variance of sampling methods and show that, due to the composite\nstructure of empirical risk, the variance of any sampling method can be\ndecomposed into \\textit{embedding approximation variance} in the forward stage\nand \\textit{stochastic gradient variance} in the backward stage that\nnecessities mitigating both types of variance to obtain faster convergence\nrate. We propose a decoupled variance reduction strategy that employs\n(approximate) gradient information to adaptively sample nodes with minimal\nvariance, and explicitly reduces the variance introduced by embedding\napproximation. We show theoretically and empirically that the proposed method,\neven with smaller mini-batch sizes, enjoys a faster convergence rate and\nentails a better generalization compared to the existing methods.",
          "link": "http://arxiv.org/abs/2006.13866",
          "publishedOn": "2021-09-07T07:20:14.088Z",
          "wordCount": null,
          "title": "Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks. (arXiv:2006.13866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hong-Gyu Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Sin-Han Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hee-Dong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dong-Ok Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>",
          "description": "To understand the black-box characteristics of deep networks, counterfactual\nexplanation that deduces not only the important features of an input space but\nalso how those features should be modified to classify input as a target class\nhas gained an increasing interest. The patterns that deep networks have learned\nfrom a training dataset can be grasped by observing the feature variation among\nvarious classes. However, current approaches perform the feature modification\nto increase the classification probability for the target class irrespective of\nthe internal characteristics of deep networks. This often leads to unclear\nexplanations that deviate from real-world data distributions. To address this\nproblem, we propose a counterfactual explanation method that exploits the\nstatistics learned from a training dataset. Especially, we gradually construct\nan explanation by iterating over masking and composition steps. The masking\nstep aims to select an important feature from the input data to be classified\nas a target class. Meanwhile, the composition step aims to optimize the\npreviously selected feature by ensuring that its output score is close to the\nlogit space of the training data that are classified as the target class.\nExperimental results show that our method produces human-friendly\ninterpretations on various classification datasets and verify that such\ninterpretations can be achieved with fewer feature modification.",
          "link": "http://arxiv.org/abs/2008.01897",
          "publishedOn": "2021-09-07T07:20:14.088Z",
          "wordCount": null,
          "title": "Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01965",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Han_C/0/1/0/all/0/1\">Cuize Han</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rao_N/0/1/0/all/0/1\">Nikhil Rao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sorokina_D/0/1/0/all/0/1\">Daria Sorokina</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Subbian_K/0/1/0/all/0/1\">Karthik Subbian</a>",
          "description": "Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking\nand relevance models in search and recommendation. Considerations such as\nlatency and interpretability dictate the use of as few features as possible to\ntrain these models. Feature selection in GBDT models typically involves\nheuristically ranking the features by importance and selecting the top few, or\nby performing a full backward feature elimination routine. On-the-fly feature\nselection methods proposed previously scale suboptimally with the number of\nfeatures, which can be daunting in high dimensional settings. We develop a\nscalable forward feature selection variant for GBDT, via a novel group testing\nprocedure that works well in high dimensions, and enjoys favorable theoretical\nperformance and computational guarantees. We show via extensive experiments on\nboth public and proprietary datasets that the proposed method offers\nsignificant speedups in training time, while being as competitive as existing\nGBDT methods in terms of model performance metrics. We also extend the method\nto the multitask setting, allowing the practitioner to select common features\nacross tasks, as well as selecting task-specific features.",
          "link": "http://arxiv.org/abs/2109.01965",
          "publishedOn": "2021-09-07T07:20:14.067Z",
          "wordCount": null,
          "title": "Scalable Feature Selection for (Multitask) Gradient Boosted Trees. (arXiv:2109.01965v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2108.07790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_H/0/1/0/all/0/1\">Helen Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raterink_C/0/1/0/all/0/1\">Cooper Raterink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araujo_J/0/1/0/all/0/1\">Jo&#xe3;o G.M. Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_I/0/1/0/all/0/1\">Ivan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Carol Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morisot_A/0/1/0/all/0/1\">Adrien Morisot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frosst_N/0/1/0/all/0/1\">Nicholas Frosst</a>",
          "description": "Language models trained on large-scale unfiltered datasets curated from the\nopen web acquire systemic biases, prejudices, and harmful views from their\ntraining data. We present a methodology for programmatically identifying and\nremoving harmful text from web-scale datasets. A pretrained language model is\nused to calculate the log-likelihood of researcher-written trigger phrases\nconditioned on a specific document, which is used to identify and filter\ndocuments from the dataset. We demonstrate that models trained on this filtered\ndataset exhibit lower propensity to generate harmful text, with a marginal\ndecrease in performance on standard language modeling benchmarks compared to\nunfiltered baselines. We provide a partial explanation for this performance gap\nby surfacing examples of hate speech and other undesirable content from\nstandard language modeling benchmarks. Finally, we discuss the generalization\nof this method and how trigger phrases which reflect specific values can be\nused by researchers to build language models which are more closely aligned\nwith their values.",
          "link": "http://arxiv.org/abs/2108.07790",
          "publishedOn": "2021-09-07T07:20:14.065Z",
          "wordCount": null,
          "title": "Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02235",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Lun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_H/0/1/0/all/0/1\">Hong-Han Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_Z/0/1/0/all/0/1\">Zhi-Rui Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hong-Yu Chiu</a>",
          "description": "In this paper, we propose a novel normalization method called gradient\nnormalization (GN) to tackle the training instability of Generative Adversarial\nNetworks (GANs) caused by the sharp gradient space. Unlike existing work such\nas gradient penalty and spectral normalization, the proposed GN only imposes a\nhard 1-Lipschitz constraint on the discriminator function, which increases the\ncapacity of the discriminator. Moreover, the proposed gradient normalization\ncan be applied to different GAN architectures with little modification.\nExtensive experiments on four datasets show that GANs trained with gradient\nnormalization outperform existing methods in terms of both Frechet Inception\nDistance and Inception Score.",
          "link": "http://arxiv.org/abs/2109.02235",
          "publishedOn": "2021-09-07T07:20:14.061Z",
          "wordCount": null,
          "title": "Gradient Normalization for Generative Adversarial Networks. (arXiv:2109.02235v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.02843",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhati_A/0/1/0/all/0/1\">Agastya P. Bhati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Shunzhou Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfe_D/0/1/0/all/0/1\">Dario Alf&#xe8;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1\">Austin R. Clyde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bode_M/0/1/0/all/0/1\">Mathis Bode</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_M/0/1/0/all/0/1\">Mikhail Titov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merzky_A/0/1/0/all/0/1\">Andre Merzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turilli_M/0/1/0/all/0/1\">Matteo Turilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Shantenu Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Highfield_R/0/1/0/all/0/1\">Roger R. Highfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocchia_W/0/1/0/all/0/1\">Walter Rocchia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scafuri_N/0/1/0/all/0/1\">Nicola Scafuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Succi_S/0/1/0/all/0/1\">Sauro Succi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kranzlmuller_D/0/1/0/all/0/1\">Dieter Kranzlm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_G/0/1/0/all/0/1\">Gerald Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wifling_D/0/1/0/all/0/1\">David Wifling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donon_Y/0/1/0/all/0/1\">Yann Donon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meglio_A/0/1/0/all/0/1\">Alberto Di Meglio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vallecorsa_S/0/1/0/all/0/1\">Sofia Vallecorsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Heng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trifan_A/0/1/0/all/0/1\">Anda Trifan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1\">Tom Brettin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Partin_A/0/1/0/all/0/1\">Alexander Partin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fangfang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1\">Xiaotan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coveney_P/0/1/0/all/0/1\">Peter V. Coveney</a>",
          "description": "The race to meet the challenges of the global pandemic has served as a\nreminder that the existing drug discovery process is expensive, inefficient and\nslow. There is a major bottleneck screening the vast number of potential small\nmolecules to shortlist lead compounds for antiviral drug development. New\nopportunities to accelerate drug discovery lie at the interface between machine\nlearning methods, in this case developed for linear accelerators, and\nphysics-based methods. The two in silico methods, each have their own\nadvantages and limitations which, interestingly, complement each other. Here,\nwe present an innovative infrastructural development that combines both\napproaches to accelerate drug discovery. The scale of the potential resulting\nworkflow is such that it is dependent on supercomputing to achieve extremely\nhigh throughput. We have demonstrated the viability of this workflow for the\nstudy of inhibitors for four COVID-19 target proteins and our ability to\nperform the required large-scale calculations to identify lead antiviral\ncompounds through repurposing on a variety of supercomputers.",
          "link": "http://arxiv.org/abs/2103.02843",
          "publishedOn": "2021-09-07T07:20:14.061Z",
          "wordCount": null,
          "title": "Pandemic Drugs at Pandemic Speed: Infrastructure for Accelerating COVID-19 Drug Discovery with Hybrid Machine Learning- and Physics-based Simulations on High Performance Computers. (arXiv:2103.02843v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00678",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yanbo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "Adversarial training (AT) has been demonstrated as one of the most promising\ndefense methods against various adversarial attacks. To our knowledge, existing\nAT-based methods usually train with the locally most adversarial perturbed\npoints and treat all the perturbed points equally, which may lead to\nconsiderably weaker adversarial robust generalization on test data. In this\nwork, we introduce a new adversarial training framework that considers the\ndiversity as well as characteristics of the perturbed points in the vicinity of\nbenign samples. To realize the framework, we propose a Regional Adversarial\nTraining (RAT) defense method that first utilizes the attack path generated by\nthe typical iterative attack method of projected gradient descent (PGD), and\nconstructs an adversarial region based on the attack path. Then, RAT samples\ndiverse perturbed training points efficiently inside this region, and utilizes\na distance-aware label smoothing mechanism to capture our intuition that\nperturbed points at different locations should have different impact on the\nmodel performance. Extensive experiments on several benchmark datasets show\nthat RAT consistently makes significant improvement on standard adversarial\ntraining (SAT), and exhibits better robust generalization.",
          "link": "http://arxiv.org/abs/2109.00678",
          "publishedOn": "2021-09-07T07:20:14.011Z",
          "wordCount": null,
          "title": "Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08143",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Sarwan Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tamkanat-E-Ali/0/1/0/all/0/1\">Tamkanat-E-Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Asad Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1\">Imdadullah Khan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1\">Murray Patterson</a>",
          "description": "SARS-CoV-2, like any other virus, continues to mutate as it spreads,\naccording to an evolutionary process. Unlike any other virus, the number of\ncurrently available sequences of SARS-CoV-2 in public databases such as GISAID\nis already several million. This amount of data has the potential to uncover\nthe evolutionary dynamics of a virus like never before. However, a million is\nalready several orders of magnitude beyond what can be processed by the\ntraditional methods designed to reconstruct a virus's evolutionary history,\nsuch as those that build a phylogenetic tree. Hence, new and scalable methods\nwill need to be devised in order to make use of the ever increasing number of\nviral sequences being collected.\n\nSince identifying variants is an important part of understanding the\nevolution of a virus, in this paper, we propose an approach based on clustering\nsequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer\nbased feature vector generation and efficient feature selection methods, our\napproach is effective in identifying variants, as well as being efficient and\nscalable to millions of sequences. Such a clustering method allows us to show\nthe relative proportion of each variant over time, giving the rate of spread of\neach variant in different locations -- something which is important for vaccine\ndevelopment and distribution. We also compute the importance of each amino acid\nposition of the spike protein in identifying a given variant in terms of\ninformation gain. Positions of high variant-specific importance tend to agree\nwith those reported by the USA's Centers for Disease Control and Prevention\n(CDC), further demonstrating our approach.",
          "link": "http://arxiv.org/abs/2108.08143",
          "publishedOn": "2021-09-07T07:20:13.975Z",
          "wordCount": null,
          "title": "Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v3 [q-bio.PE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.04733",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhao_Z/0/1/0/all/0/1\">Zheng Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Emzir_M/0/1/0/all/0/1\">Muhammad Emzir</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sarkka_S/0/1/0/all/0/1\">Simo S&#xe4;rkk&#xe4;</a>",
          "description": "This paper is concerned with a state-space approach to deep Gaussian process\n(DGP) regression. We construct the DGP by hierarchically putting transformed\nGaussian process (GP) priors on the length scales and magnitudes of the next\nlevel of Gaussian processes in the hierarchy. The idea of the state-space\napproach is to represent the DGP as a non-linear hierarchical system of linear\nstochastic differential equations (SDEs), where each SDE corresponds to a\nconditional GP. The DGP regression problem then becomes a state estimation\nproblem, and we can estimate the state efficiently with sequential methods by\nusing the Markov property of the state-space DGP. The computational complexity\nscales linearly with respect to the number of measurements. Based on this, we\nformulate state-space MAP as well as Bayesian filtering and smoothing solutions\nto the DGP regression problem. We demonstrate the performance of the proposed\nmodels and methods on synthetic non-stationary signals and apply the\nstate-space DGP to detection of the gravitational waves from LIGO measurements.",
          "link": "http://arxiv.org/abs/2008.04733",
          "publishedOn": "2021-09-07T07:20:13.908Z",
          "wordCount": null,
          "title": "Deep State-Space Gaussian Processes. (arXiv:2008.04733v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arnab Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heger_A/0/1/0/all/0/1\">Andrew Heger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+England_D/0/1/0/all/0/1\">Darin England</a>",
          "description": "In this article, we propose a systematic approach for fire station location\nplanning. We develop a machine learning model, based on Random Forest, for\ndemand prediction and utilize the model further to define a generalized index\nto measure quality of fire service in urban settings. Our model is built upon\nspatial data collected from multiple different sources. Efficacy of proper\nfacility planning depends on choice of candidates where fire stations can be\nlocated along with existing stations, if any. Also, the travel time from these\ncandidates to demand locations need to be taken care of to maintain fire safety\nstandard. Here, we propose a travel time based clustering technique to identify\nsuitable candidates. Finally, we develop an optimization problem to select best\nlocations to install new fire stations. Our optimization problem is built upon\nmaximum coverage problem, based on integer programming. We present a detailed\nexperimental study of our proposed approach in collaboration with city of\nVictoria Fire Department, MN, USA. Our demand prediction model achieves true\npositive rate of 70% and false positive rate of 22% approximately. We aid\nVictoria Fire Department to select a location for a new fire station using our\napproach. We present detailed results on improvement statistics by locating a\nnew facility, as suggested by our methodology, in the city of Victoria.",
          "link": "http://arxiv.org/abs/2109.02160",
          "publishedOn": "2021-09-07T07:20:13.907Z",
          "wordCount": null,
          "title": "Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02248",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gharsallaoui_M/0/1/0/all/0/1\">Mohammed Amine Gharsallaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1\">Islem Rekik</a>",
          "description": "Graph neural networks (GNNs) have witnessed an unprecedented proliferation in\ntackling several problems in computer vision, computer-aided diagnosis, and\nrelated fields. While prior studies have focused on boosting the model\naccuracy, quantifying the reproducibility of the most discriminative features\nidentified by GNNs is still an intact problem that yields concerns about their\nreliability in clinical applications in particular. Specifically, the\nreproducibility of biological markers across clinical datasets and distribution\nshifts across classes (e.g., healthy and disordered brains) is of paramount\nimportance in revealing the underpinning mechanisms of diseases as well as\npropelling the development of personalized treatment. Motivated by these\nissues, we propose, for the first time, reproducibility-based GNN selection\n(RG-Select), a framework for GNN reproducibility assessment via the\nquantification of the most discriminative features (i.e., biomarkers) shared\nbetween different models. To ascertain the soundness of our framework, the\nreproducibility assessment embraces variations of different factors such as\ntraining strategies and data perturbations. Despite these challenges, our\nframework successfully yielded replicable conclusions across different training\nstrategies and various clinical datasets. Our findings could thus pave the way\nfor the development of biomarker trustworthiness and reliability assessment\nmethods for computer-aided diagnosis and prognosis tasks. RG-Select code is\navailable on GitHub at https://github.com/basiralab/RG-Select.",
          "link": "http://arxiv.org/abs/2109.02248",
          "publishedOn": "2021-09-07T07:20:13.902Z",
          "wordCount": null,
          "title": "Quantifying the Reproducibility of Graph Neural Networks using Multigraph Brain Data. (arXiv:2109.02248v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>",
          "description": "How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.",
          "link": "http://arxiv.org/abs/2109.01815",
          "publishedOn": "2021-09-07T07:20:13.899Z",
          "wordCount": null,
          "title": "Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1\">Yonatan Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhadad_M/0/1/0/all/0/1\">Michael Elhadad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>",
          "description": "Masked language modeling (MLM) is one of the key sub-tasks in vision-language\npretraining. In the cross-modal setting, tokens in the sentence are masked at\nrandom, and the model predicts the masked tokens given the image and the text.\nIn this paper, we observe several key disadvantages of MLM in this setting.\nFirst, as captions tend to be short, in a third of the sentences no token is\nsampled. Second, the majority of masked tokens are stop-words and punctuation,\nleading to under-utilization of the image. We investigate a range of\nalternative masking strategies specific to the cross-modal setting that address\nthese shortcomings, aiming for better fusion of text and image in the learned\nrepresentation. When pre-training the LXMERT model, our alternative masking\nstrategies consistently improve over the original masking strategy on three\ndownstream tasks, especially in low resource settings. Further, our\npre-training approach substantially outperforms the baseline model on a\nprompt-based probing task designed to elicit image objects. These results and\nour analysis indicate that our method allows for better utilization of the\ntraining data.",
          "link": "http://arxiv.org/abs/2109.02040",
          "publishedOn": "2021-09-07T07:20:13.850Z",
          "wordCount": null,
          "title": "Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.09790",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong-Xing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>",
          "description": "We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D\nspatial-temporal representation of a dynamic scene from a set of RGB images.\nKey to our approach is the use of a neural implicit representation that learns\nto capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing\nconsistency across different modalities, our representation enables multi-view\nrendering in diverse dynamic scenes, including water pouring, robotic\ninteraction, and real images, outperforming state-of-the-art methods for\nspatial-temporal view synthesis. Our approach works even when inputs images are\ncaptured with only one camera. We further demonstrate that the learned\nrepresentation can serve as an implicit scene prior, enabling video processing\ntasks such as image super-resolution and de-noising without any additional\nsupervision.",
          "link": "http://arxiv.org/abs/2012.09790",
          "publishedOn": "2021-09-07T07:20:13.838Z",
          "wordCount": null,
          "title": "Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02250",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Vinod_V/0/1/0/all/0/1\">Vishal Vinod</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_R/0/1/0/all/0/1\">Rahul Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pingale_R/0/1/0/all/0/1\">Rohit Pingale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jagarlapudi_A/0/1/0/all/0/1\">Adinarayana Jagarlapudi</a>",
          "description": "Plant water stress may occur due to the limited availability of water to the\nroots/soil or due to increased transpiration. These factors adversely affect\nplant physiology and photosynthetic ability to the extent that it has been\nshown to have inhibitory effects in both growth and yield [18]. Early\nidentification of plant water stress status enables suitable corrective\nmeasures to be applied to obtain the expected crop yield. Further, improving\ncrop yield through precision agriculture methods is a key component of climate\npolicy and the UN sustainable development goals [1]. Leaf water content (LWC)\nis a measure that can be used to estimate water content and identify stressed\nplants. LWC during the early crop growth stages is an important indicator of\nplant productivity and yield. The effect of water stress can be instantaneous\n[15], affecting gaseous exchange or long-term, significantly reducing [9, 18,\n22]. It is thus necessary to identify potential plant water stress during the\nearly stages of growth [15] to introduce corrective irrigation and alleviate\nstress. LWC is also useful for identifying plant genotypes that are tolerant to\nwater stress and salinity by measuring the stability of LWC even under\nartificially induced water stress [18, 25]. Such experiments generally employ\ndestructive procedures to obtain the LWC, which is time-consuming and labor\nintensive. Accordingly, this research has developed a non-destructive method to\nestimate LWC from UAV-based hyperspectral data.",
          "link": "http://arxiv.org/abs/2109.02250",
          "publishedOn": "2021-09-07T07:20:13.819Z",
          "wordCount": null,
          "title": "Estimating Leaf Water Content using Remotely Sensed Hyperspectral Data. (arXiv:2109.02250v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2004.07351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Richeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Huaiyu Dai</a>",
          "description": "In federated learning (FL), reducing the communication overhead is one of the\nmost critical challenges since the parameter server and the mobile devices\nshare the training parameters over wireless links. With such consideration, we\nadopt the idea of SignSGD in which only the signs of the gradients are\nexchanged. Moreover, most of the existing works assume Channel State\nInformation (CSI) available at both the mobile devices and the parameter\nserver, and thus the mobile devices can adopt fixed transmission rates dictated\nby the channel capacity. In this work, only the parameter server side CSI is\nassumed, and channel capacity with outage is considered. In this case, an\nessential problem for the mobile devices is to select appropriate local\nprocessing and communication parameters (including the transmission rates) to\nachieve a desired balance between the overall learning performance and their\nenergy consumption. Two optimization problems are formulated and solved, which\noptimize the learning performance given the energy consumption requirement, and\nvice versa. Furthermore, considering that the data may be distributed across\nthe mobile devices in a highly uneven fashion in FL, a stochastic sign-based\nalgorithm is proposed. Extensive simulations are performed to demonstrate the\neffectiveness of the proposed methods.",
          "link": "http://arxiv.org/abs/2004.07351",
          "publishedOn": "2021-09-07T07:20:13.809Z",
          "wordCount": null,
          "title": "Communication Efficient Federated Learning with Energy Awareness over Wireless Networks. (arXiv:2004.07351v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.02478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Q. Vera Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruen_D/0/1/0/all/0/1\">Daniel Gruen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Sarah Miller</a>",
          "description": "A surge of interest in explainable AI (XAI) has led to a vast collection of\nalgorithmic work on the topic. While many recognize the necessity to\nincorporate explainability features in AI systems, how to address real-world\nuser needs for understanding AI remains an open question. By interviewing 20 UX\nand design practitioners working on various AI products, we seek to identify\ngaps between the current XAI algorithmic work and practices to create\nexplainable AI products. To do so, we develop an algorithm-informed XAI\nquestion bank in which user needs for explainability are represented as\nprototypical questions users might ask about the AI, and use it as a study\nprobe. Our work contributes insights into the design space of XAI, informs\nefforts to support design practices in this space, and identifies opportunities\nfor future XAI work. We also provide an extended XAI question bank and discuss\nhow it can be used for creating user-centered XAI.",
          "link": "http://arxiv.org/abs/2001.02478",
          "publishedOn": "2021-09-07T07:20:13.803Z",
          "wordCount": null,
          "title": "Questioning the AI: Informing Design Practices for Explainable AI User Experiences. (arXiv:2001.02478v3 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeren Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kerong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Furui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_H/0/1/0/all/0/1\">Hui-ling Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingxuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Cutting plane methods play a significant role in modern solvers for tackling\nmixed-integer programming (MIP) problems. Proper selection of cuts would remove\ninfeasible solutions in the early stage, thus largely reducing the\ncomputational burden without hurting the solution accuracy. However, the major\ncut selection approaches heavily rely on heuristics, which strongly depend on\nthe specific problem at hand and thus limit their generalization capability. In\nthis paper, we propose a data-driven and generalizable cut selection approach,\nnamed Cut Ranking, in the settings of multiple instance learning. To measure\nthe quality of the candidate cuts, a scoring function, which takes the\ninstance-specific cut features as inputs, is trained and applied in cut ranking\nand selection. In order to evaluate our method, we conduct extensive\nexperiments on both synthetic datasets and real-world datasets. Compared with\ncommonly used heuristics for cut selection, the learning-based policy has shown\nto be more effective, and is capable of generalizing over multiple problems\nwith different properties. Cut Ranking has been deployed in an industrial\nsolver for large-scale MIPs. In the online A/B testing of the product planning\nproblems with more than $10^7$ variables and constraints daily, Cut Ranking has\nachieved the average speedup ratio of 12.42% over the production solver without\nany accuracy loss of solution.",
          "link": "http://arxiv.org/abs/2105.13645",
          "publishedOn": "2021-09-07T07:20:13.644Z",
          "wordCount": null,
          "title": "Learning to Select Cuts for Efficient Mixed-Integer Programming. (arXiv:2105.13645v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.03066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Duvocelle_B/0/1/0/all/0/1\">Benoit Duvocelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1\">Panayotis Mertikopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staudigl_M/0/1/0/all/0/1\">Mathias Staudigl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeulen_D/0/1/0/all/0/1\">Dries Vermeulen</a>",
          "description": "We examine the long-run behavior of multi-agent online learning in games that\nevolve over time. Specifically, we focus on a wide class of policies based on\nmirror descent, and we show that the induced sequence of play (a) converges to\nNash equilibrium in time-varying games that stabilize in the long run to a\nstrictly monotone limit; and (b) it stays asymptotically close to the evolving\nequilibrium of the sequence of stage games (assuming they are strongly\nmonotone). Our results apply to both gradient-based and payoff-based feedback -\ni.e., the \"bandit feedback\" case where players only get to observe the payoffs\nof their chosen actions.",
          "link": "http://arxiv.org/abs/1809.03066",
          "publishedOn": "2021-09-07T07:20:13.624Z",
          "wordCount": null,
          "title": "Multi-agent online learning in time-varying games. (arXiv:1809.03066v3 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02230",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Schulz_J/0/1/0/all/0/1\">J&#xf6;rn Schulz</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Taheri_M/0/1/0/all/0/1\">Mohsen Taheri</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Styner_M/0/1/0/all/0/1\">Martin Styner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Damon_J/0/1/0/all/0/1\">James Damon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pizer_S/0/1/0/all/0/1\">Stephen Pizer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Marron_J/0/1/0/all/0/1\">J. S. Marron</a>",
          "description": "This paper considers joint analysis of multiple functionally related\nstructures in classification tasks. In particular, our method developed is\ndriven by how functionally correlated brain structures vary together between\nautism and control groups. To do so, we devised a method based on a novel\ncombination of (1) non-Euclidean statistics that can faithfully represent\nnon-Euclidean data in Euclidean spaces and (2) a non-parametric integrative\nanalysis method that can decompose multi-block Euclidean data into joint,\nindividual, and residual structures. We find that the resulting joint structure\nis effective, robust, and interpretable in recognizing the underlying patterns\nof the joint variation of multi-block non-Euclidean data. We verified the\nmethod in classifying the structural shape data collected from cases that\ndeveloped and did not develop into Autistic Spectrum Disorder (ASD).",
          "link": "http://arxiv.org/abs/2109.02230",
          "publishedOn": "2021-09-07T07:20:13.616Z",
          "wordCount": null,
          "title": "Non-Euclidean Analysis of Joint Variations in Multi-Object Shapes. (arXiv:2109.02230v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carrell_S/0/1/0/all/0/1\">Steven Carrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>",
          "description": "The use of mobiles phones when driving have been a major factor when it comes\nto road traffic incidents and the process of capturing such violations can be a\nlaborious task. Advancements in both modern object detection frameworks and\nhigh-performance hardware has paved the way for a more automated approach when\nit comes to video surveillance. In this work, we propose a custom-trained\nstate-of-the-art object detector to work with roadside cameras to capture\ndriver phone usage without the need for human intervention. The proposed\napproach also addresses the issues caused by windscreen glare and introduces\nthe steps required to remedy this. Twelve pre-trained models are fine-tuned\nwith our custom dataset using four popular object detection methods: YOLO, SSD,\nFaster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO\nyields the highest accuracy levels of up to 96% (AP10) and frame rates of up to\n~30 FPS. DeepSort object tracking algorithm is also integrated into the\nbest-performing model to collect records of only the unique violations, and\nenable the proposed approach to count the number of vehicles. The proposed\nautomated system will collect the output images of the identified violations,\ntimestamps of each violation, and total vehicle count. Data can be accessed via\na purpose-built user interface.",
          "link": "http://arxiv.org/abs/2109.02119",
          "publishedOn": "2021-09-07T07:20:13.613Z",
          "wordCount": null,
          "title": "Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01696",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yeqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yin Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1\">Rui Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_I/0/1/0/all/0/1\">Irwan Bello</a>",
          "description": "A recent work from Bello shows that training and scaling strategies may be\nmore significant than model architectures for visual recognition. This short\nnote studies effective training and scaling strategies for video recognition\nmodels. We propose a simple scaling strategy for 3D ResNets, in combination\nwith improved training strategies and minor architectural changes. The\nresulting models, termed 3D ResNet-RS, attain competitive performance of 81.0\non Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained\non a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on\nKinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated\nin a self-supervised setup using contrastive learning, demonstrating improved\nperformance. Code is available at:\nhttps://github.com/tensorflow/models/tree/master/official.",
          "link": "http://arxiv.org/abs/2109.01696",
          "publishedOn": "2021-09-07T07:20:13.567Z",
          "wordCount": null,
          "title": "Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fofadiya_D/0/1/0/all/0/1\">Darshan Fofadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_P/0/1/0/all/0/1\">Prathap Ramachandra</a>",
          "description": "Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for <0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.",
          "link": "http://arxiv.org/abs/2109.01754",
          "publishedOn": "2021-09-07T07:20:13.561Z",
          "wordCount": null,
          "title": "Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yihao Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lizhen Lin</a>",
          "description": "In this work, we propose to train a graph neural network via resampling from\na graphon estimate obtained from the underlying network data. More\nspecifically, the graphon or the link probability matrix of the underlying\nnetwork is first obtained from which a new network will be resampled and used\nduring the training process at each layer. Due to the uncertainty induced from\nthe resampling, it helps mitigate the well-known issue of over-smoothing in a\ngraph neural network (GNN) model. Our framework is general, computationally\nefficient, and conceptually simple. Another appealing feature of our method is\nthat it requires minimal additional tuning during the training process.\nExtensive numerical results show that our approach is competitive with and in\nmany cases outperform the other over-smoothing reducing GNN training methods.",
          "link": "http://arxiv.org/abs/2109.01918",
          "publishedOn": "2021-09-07T07:20:13.555Z",
          "wordCount": null,
          "title": "Training Graph Neural Networks by Graphon Estimation. (arXiv:2109.01918v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Itkina_M/0/1/0/all/0/1\">Masha Itkina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_Y/0/1/0/all/0/1\">Ye-Ji Mun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1\">Katherine Driggs-Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1\">Mykel J. Kochenderfer</a>",
          "description": "Autonomous vehicles must reason about spatial occlusions in urban\nenvironments to ensure safety without being overly cautious. Prior work\nexplored occlusion inference from observed social behaviors of road agents.\nInferring occupancy from agent behaviors is an inherently multimodal problem; a\ndriver may behave in the same manner for different occupancy patterns ahead of\nthem (e.g., a driver may move at constant speed in traffic or on an open road).\nPast work, however, does not account for this multimodality, thus neglecting to\nmodel this source of aleatoric uncertainty in the relationship between driver\nbehaviors and their environment. We propose an occlusion inference method that\ncharacterizes observed behaviors of human agents as sensor measurements, and\nfuses them with those from a standard sensor suite. To capture the aleatoric\nuncertainty, we train a conditional variational autoencoder with a discrete\nlatent space to learn a multimodal mapping from observed driver trajectories to\nan occupancy grid representation of the view ahead of the driver. Our method\nhandles multi-agent scenarios, combining measurements from multiple observed\ndrivers using evidential theory to solve the sensor fusion problem. Our\napproach is validated on a real-world dataset, outperforming baselines and\ndemonstrating real-time capable performance. Our code is available at\nhttps://github.com/sisl/MultiAgentVariationalOcclusionInference .",
          "link": "http://arxiv.org/abs/2109.02173",
          "publishedOn": "2021-09-07T07:20:13.554Z",
          "wordCount": null,
          "title": "Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akula_A/0/1/0/all/0/1\">Arjun R. Akula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saba_Sadiya_S/0/1/0/all/0/1\">Sari Saba-Sadiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hongjing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovic_S/0/1/0/all/0/1\">Sinisa Todorovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>",
          "description": "We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.",
          "link": "http://arxiv.org/abs/2109.01401",
          "publishedOn": "2021-09-07T07:20:13.541Z",
          "wordCount": null,
          "title": "CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02145",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1\">Deepak-George Thomas</a>",
          "description": "The function approximators employed by traditional image based Deep\nReinforcement Learning (DRL) algorithms usually lack a temporal learning\ncomponent and instead focus on learning the spatial component. We propose a\ntechnique wherein both temporal as well as spatial components are jointly\nlearned. Our tested was tested with a generic DQN and it outperformed it in\nterms of maximum rewards as well as sample complexity. This algorithm has\nimplications in the robotics as well as sequential decision making domains.",
          "link": "http://arxiv.org/abs/2109.02145",
          "publishedOn": "2021-09-07T07:20:13.514Z",
          "wordCount": null,
          "title": "Temporal Aware Deep Reinforcement Learning. (arXiv:2109.02145v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.12943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noel_M/0/1/0/all/0/1\">Mathew Mithra Noel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_A/0/1/0/all/0/1\">Arunkumar L</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Advait Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Praneet Dutta</a>",
          "description": "Convolution neural networks have been successful in solving many socially\nimportant and economically significant problems. Their ability to learn complex\nhigh-dimensional functions hierarchically can be attributed to the use of\nnonlinear activation functions. A key discovery that made training deep\nnetworks feasible was the adoption of the Rectified Linear Unit (ReLU)\nactivation function to alleviate the vanishing gradient problem caused by using\nsaturating activation functions. Since then many improved variants of the ReLU\nactivation have been proposed. However a majority of activation functions used\ntoday are non-oscillatory and monotonically increasing due to their biological\nplausibility. This paper demonstrates that oscillatory activation functions can\nimprove gradient flow and reduce network size. It is shown that oscillatory\nactivation functions allow neurons to switch classification (sign of output)\nwithin the interior of neuronal hyperplane positive and negative half-spaces\nallowing complex decisions with fewer neurons. A new oscillatory activation\nfunction C(z) = z cos z that outperforms Sigmoids, Swish, Mish and ReLU on a\nvariety of architectures and benchmarks is presented. This new activation\nfunction allows even single neurons to exhibit nonlinear decision boundaries.\nThis paper presents a single neuron solution to the famous XOR problem.\nExperimental results indicate that replacing the activation function in the\nconvolutional layers with C(z) significantly improves performance on CIFAR-10,\nCIFAR-100 and Imagenette.",
          "link": "http://arxiv.org/abs/2108.12943",
          "publishedOn": "2021-09-07T07:20:13.512Z",
          "wordCount": null,
          "title": "Growing Cosine Unit: A Novel Oscillatory Activation Function That Can Speedup Training and Reduce Parameters in Convolutional Neural Networks. (arXiv:2108.12943v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01659",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthy_G/0/1/0/all/0/1\">Gayathri Krishnamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Anamika Dubey</a>",
          "description": "Reinforcement learning has been found useful in solving optimal power flow\n(OPF) problems in electric power distribution systems. However, the use of\nlargely model-free reinforcement learning algorithms that completely ignore the\nphysics-based modeling of the power grid compromises the optimizer performance\nand poses scalability challenges. This paper proposes a novel approach to\nsynergistically combine the physics-based models with learning-based algorithms\nusing imitation learning to solve distribution-level OPF problems.\nSpecifically, we propose imitation learning based improvements in deep\nreinforcement learning (DRL) methods to solve the OPF problem for a specific\ncase of battery storage dispatch in the power distribution systems. The\nproposed imitation learning algorithm uses the approximate optimal solutions\nobtained from a linearized model-based OPF solver to provide a good initial\npolicy for the DRL algorithms while improving the training efficiency. The\neffectiveness of the proposed approach is demonstrated using IEEE 34-bus and\n123-bus distribution feeders with numerous distribution-level battery storage\nsystems.",
          "link": "http://arxiv.org/abs/2109.01659",
          "publishedOn": "2021-09-07T07:20:13.505Z",
          "wordCount": null,
          "title": "Reinforcement Learning for Battery Energy Storage Dispatch augmented with Model-based Optimizer. (arXiv:2109.01659v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1\">Nitin Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hima Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afzal_S/0/1/0/all/0/1\">Shazia Afzal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panwar_N/0/1/0/all/0/1\">Naveen Panwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_R/0/1/0/all/0/1\">Ruhi Sharma Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guttula_S/0/1/0/all/0/1\">Shanmukha Guttula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Abhinav Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagalapatti_L/0/1/0/all/0/1\">Lokesh Nagalapatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Sameep Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hans_S/0/1/0/all/0/1\">Sandeep Hans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lohia_P/0/1/0/all/0/1\">Pranay Lohia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_A/0/1/0/all/0/1\">Aniya Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_D/0/1/0/all/0/1\">Diptikalyan Saha</a>",
          "description": "The quality of training data has a huge impact on the efficiency, accuracy\nand complexity of machine learning tasks. Various tools and techniques are\navailable that assess data quality with respect to general cleaning and\nprofiling checks. However these techniques are not applicable to detect data\nissues in the context of machine learning tasks, like noisy labels, existence\nof overlapping classes etc. We attempt to re-look at the data quality issues in\nthe context of building a machine learning pipeline and build a tool that can\ndetect, explain and remediate issues in the data, and systematically and\nautomatically capture all the changes applied to the data. We introduce the\nData Quality Toolkit for machine learning as a library of some key quality\nmetrics and relevant remediation techniques to analyze and enhance the\nreadiness of structured training datasets for machine learning projects. The\ntoolkit can reduce the turn-around times of data preparation pipelines and\nstreamline the data quality assessment process. Our toolkit is publicly\navailable via IBM API Hub [1] platform, any developer can assess the data\nquality using the IBM's Data Quality for AI apis [2]. Detailed tutorials are\nalso available on IBM Learning Path [3].",
          "link": "http://arxiv.org/abs/2108.05935",
          "publishedOn": "2021-09-07T07:20:13.501Z",
          "wordCount": null,
          "title": "Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets. (arXiv:2108.05935v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.03757",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zirui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shao-Bo Lin</a>",
          "description": "This paper focuses on learning rate analysis of distributed kernel ridge\nregression for strong mixing sequences. Using a recently developed integral\noperator approach and a classical covariance inequality for Banach-valued\nstrong mixing sequences, we succeed in deriving optimal learning rate for\ndistributed kernel ridge regression. As a byproduct, we also deduce a\nsufficient condition for the mixing property to guarantee the optimal learning\nrates for kernel ridge regression. Our results extend the applicable range of\ndistributed learning from i.i.d. samples to non-i.i.d. sequences.",
          "link": "http://arxiv.org/abs/2002.03757",
          "publishedOn": "2021-09-07T07:20:13.498Z",
          "wordCount": null,
          "title": "Distributed Learning with Dependent Samples. (arXiv:2002.03757v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01657",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jinliang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiusi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Renhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor W. Tsang</a>",
          "description": "Multi-variate time series (MTS) data is a ubiquitous class of data\nabstraction in the real world. Any instance of MTS is generated from a hybrid\ndynamical system and their specific dynamics are usually unknown. The hybrid\nnature of such a dynamical system is a result of complex external attributes,\nsuch as geographic location and time of day, each of which can be categorized\ninto either spatial attributes or temporal attributes. Therefore, there are two\nfundamental views which can be used to analyze MTS data, namely the spatial\nview and the temporal view. Moreover, from each of these two views, we can\npartition the set of data samples of MTS into disjoint forecasting tasks in\naccordance with their associated attribute values. Then, samples of the same\ntask will manifest similar forthcoming pattern, which is less sophisticated to\nbe predicted in comparison with the original single-view setting. Considering\nthis insight, we propose a novel multi-view multi-task (MVMT) learning\nframework for MTS forecasting. Instead of being explicitly presented in most\nscenarios, MVMT information is deeply concealed in the MTS data, which severely\nhinders the model from capturing it naturally. To this end, we develop two\nkinds of basic operations, namely task-wise affine transformation and task-wise\nnormalization, respectively. Applying these two operations with prior knowledge\non the spatial and temporal view allows the model to adaptively extract MVMT\ninformation while predicting. Extensive experiments on three datasets are\nconducted to illustrate that canonical architectures can be greatly enhanced by\nthe MVMT learning framework in terms of both effectiveness and efficiency. In\naddition, we design rich case studies to reveal the properties of\nrepresentations produced at different phases in the entire prediction\nprocedure.",
          "link": "http://arxiv.org/abs/2109.01657",
          "publishedOn": "2021-09-07T07:20:13.493Z",
          "wordCount": null,
          "title": "A Multi-view Multi-task Learning Framework for Multi-variate Time Series Forecasting. (arXiv:2109.01657v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.",
          "link": "http://arxiv.org/abs/2109.01924",
          "publishedOn": "2021-09-07T07:20:13.491Z",
          "wordCount": null,
          "title": "A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yangdi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bo_Y/0/1/0/all/0/1\">Yang Bo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wenbo He</a>",
          "description": "Recent studies on the memorization effects of deep neural networks on noisy\nlabels show that the networks first fit the correctly-labeled training samples\nbefore memorizing the mislabeled samples. Motivated by this early-learning\nphenomenon, we propose a novel method to prevent memorization of the mislabeled\nsamples. Unlike the existing approaches which use the model output to identify\nor ignore the mislabeled samples, we introduce an indicator branch to the\noriginal model and enable the model to produce a confidence value for each\nsample. The confidence values are incorporated in our loss function which is\nlearned to assign large confidence values to correctly-labeled samples and\nsmall confidence values to mislabeled samples. We also propose an auxiliary\nregularization term to further improve the robustness of the model. To improve\nthe performance, we gradually correct the noisy labels with a well-designed\ntarget estimation strategy. We provide the theoretical analysis and conduct the\nexperiments on synthetic and real-world datasets, demonstrating that our\napproach achieves comparable results to the state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2108.08212",
          "publishedOn": "2021-09-07T07:20:13.490Z",
          "wordCount": null,
          "title": "Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02972",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodolfa_K/0/1/0/all/0/1\">Kit T. Rodolfa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamba_H/0/1/0/all/0/1\">Hemank Lamba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghani_R/0/1/0/all/0/1\">Rayid Ghani</a>",
          "description": "Growing use of machine learning in policy and social impact settings have\nraised concerns for fairness implications, especially for racial minorities.\nThese concerns have generated considerable interest among machine learning and\nartificial intelligence researchers, who have developed new methods and\nestablished theoretical bounds for improving fairness, focusing on the source\ndata, regularization and model training, or post-hoc adjustments to model\nscores. However, little work has studied the practical trade-offs between\nfairness and accuracy in real-world settings to understand how these bounds and\nmethods translate into policy choices and impact on society. Our empirical\nstudy fills this gap by investigating the impact of mitigating disparities on\naccuracy, focusing on the common context of using machine learning to inform\nbenefit allocation in resource-constrained programs across education, mental\nhealth, criminal justice, and housing safety. Here we describe applied work in\nwhich we find fairness-accuracy trade-offs to be negligible in practice. In\neach setting studied, explicitly focusing on achieving equity and using our\nproposed post-hoc disparity mitigation methods, fairness was substantially\nimproved without sacrificing accuracy. This observation was robust across\npolicy contexts studied, scale of resources available for intervention, time,\nand relative size of the protected groups. These empirical results challenge a\ncommonly held assumption that reducing disparities either requires accepting an\nappreciable drop in accuracy or the development of novel, complex methods,\nmaking reducing disparities in these applications more practical.",
          "link": "http://arxiv.org/abs/2012.02972",
          "publishedOn": "2021-09-07T07:20:13.478Z",
          "wordCount": null,
          "title": "Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy. (arXiv:2012.02972v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.11187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_S/0/1/0/all/0/1\">Sandesh Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyawali_P/0/1/0/all/0/1\">Prashnna K Gyawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linwei Wang</a>",
          "description": "Several scalable sample-based methods to compute the Kullback Leibler (KL)\ndivergence between two distributions have been proposed and applied in\nlarge-scale machine learning models. While they have been found to be unstable,\nthe theoretical root cause of the problem is not clear. In this paper, we study\na generative adversarial network based approach that uses a neural network\ndiscriminator to estimate KL divergence. We argue that, in such case, high\nfluctuations in the estimates are a consequence of not controlling the\ncomplexity of the discriminator function space. We provide a theoretical\nunderpinning and remedy for this problem by first constructing a discriminator\nin the Reproducing Kernel Hilbert Space (RKHS). This enables us to leverage\nsample complexity and mean embedding to theoretically relate the error\nprobability bound of the KL estimates to the complexity of the discriminator in\nRKHS. Based on this theory, we then present a scalable way to control the\ncomplexity of the discriminator for a reliable estimation of KL divergence. We\nsupport both our proposed theory and method to control the complexity of the\nRKHS discriminator through controlled experiments.",
          "link": "http://arxiv.org/abs/2002.11187",
          "publishedOn": "2021-09-07T07:20:13.477Z",
          "wordCount": null,
          "title": "Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation. (arXiv:2002.11187v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.10423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1\">Zheda Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruiwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jihwan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1\">David Quispe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1\">Scott Sanner</a>",
          "description": "Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.",
          "link": "http://arxiv.org/abs/2101.10423",
          "publishedOn": "2021-09-07T07:20:13.466Z",
          "wordCount": null,
          "title": "Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02273",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Sarkar_J/0/1/0/all/0/1\">Jyotirmoy Sarkar</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bhatia_K/0/1/0/all/0/1\">Kartik Bhatia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Saha_S/0/1/0/all/0/1\">Snehanshu Saha</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Safonova_M/0/1/0/all/0/1\">Margarita Safonova</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sarkar_S/0/1/0/all/0/1\">Santonu Sarkar</a>",
          "description": "A profound shift in the study of cosmology came with the discovery of\nthousands of exoplanets and the possibility of the existence of billions of\nthem in our Galaxy. The biggest goal in these searches is whether there are\nother life-harbouring planets. However, the question which of these detected\nplanets are habitable, potentially-habitable, or maybe even inhabited, is still\nnot answered. Some potentially habitable exoplanets have been hypothesized, but\nsince Earth is the only known habitable planet, measures of habitability are\nnecessarily determined with Earth as the reference. Several recent works\nintroduced new habitability metrics based on optimization methods.\nClassification of potentially habitable exoplanets using supervised learning is\nanother emerging area of study. However, both modeling and supervised learning\napproaches suffer from drawbacks. We propose an anomaly detection method, the\nMulti-Stage Memetic Algorithm (MSMA), to detect anomalies and extend it to an\nunsupervised clustering algorithm MSMVMCA to use it to detect potentially\nhabitable exoplanets as anomalies. The algorithm is based on the postulate that\nEarth is an anomaly, with the possibility of existence of few other anomalies\namong thousands of data points. We describe an MSMA-based clustering approach\nwith a novel distance function to detect habitable candidates as anomalies\n(including Earth). The results are cross-matched with the habitable exoplanet\ncatalog (PHL-HEC) of the Planetary Habitability Laboratory (PHL) with both\noptimistic and conservative lists of potentially habitable exoplanets.",
          "link": "http://arxiv.org/abs/2109.02273",
          "publishedOn": "2021-09-07T07:20:13.463Z",
          "wordCount": null,
          "title": "Postulating Exoplanetary Habitability via a Novel Anomaly Detection Method. (arXiv:2109.02273v1 [astro-ph.EP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02165",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1\">Pedro R. A. S. Bassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Attux_R/0/1/0/all/0/1\">Romis Attux</a>",
          "description": "Objective: To propose a novel deep neural network (DNN) architecture -- the\nfilter bank convolutional neural network (FBCNN) -- to improve SSVEP\nclassification in single-channel BCIs with small data lengths.\n\nMethods: We propose two models: the FBCNN-2D and the FBCNN-3D. The FBCNN-2D\nutilizes a filter bank to create sub-band components of the\nelectroencephalography (EEG) signal, which it transforms using the fast Fourier\ntransform (FFT) and analyzes with a 2D CNN. The FBCNN-3D utilizes the same\nfilter bank, but it transforms the sub-band components into spectrograms via\nshort-time Fourier transform (STFT), and analyzes them with a 3D CNN. We made\nuse of transfer learning. To train the FBCNN-3D, we proposed a new technique,\ncalled inter-dimensional transfer learning, to transfer knowledge from a 2D DNN\nto a 3D DNN. Our BCI was conceived so as not to require calibration from the\nfinal user: therefore, the test subject data was separated from training and\nvalidation.\n\nResults: The mean test accuracy was 85.7% for the FBCCA-2D and 85% for the\nFBCCA-3D. Mean F1-Scores were 0.858 and 0.853. Alternative classification\nmethods, SVM, FBCCA and a CNN, had mean accuracy of 79.2%, 80.1% and 81.4%,\nrespectively.\n\nConclusion: The FBCNNs surpassed traditional SSVEP classification methods in\nour simulated BCI, by a considerable margin (about 5% higher accuracy).\nTransfer learning and inter-dimensional transfer learning made training much\nfaster and more predictable.\n\nSignificance: We proposed a new and flexible type of DNN, which had a better\nperformance than standard methods in SSVEP classification for portable and fast\nBCIs.",
          "link": "http://arxiv.org/abs/2109.02165",
          "publishedOn": "2021-09-07T07:20:13.357Z",
          "wordCount": null,
          "title": "FBCNN: A Deep Neural Network Architecture for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mallat_K/0/1/0/all/0/1\">Khawla Mallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Becerra_Riera_F/0/1/0/all/0/1\">Fabiola Becerra-Riera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_Gonzalez_A/0/1/0/all/0/1\">Annette Morales-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_Vazquez_H/0/1/0/all/0/1\">Heydi M&#xe9;ndez-V&#xe1;zquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugelay_J/0/1/0/all/0/1\">Jean-Luc Dugelay</a>",
          "description": "In this paper, we explore whether automatic face recognition can help in\nverifying widespread misinformation on social media, particularly conspiracy\ntheories that are based on the existence of body doubles. The conspiracy theory\naddressed in this paper is the case of the Melania Trump body double. We\nemployed four different state-of-the-art descriptors for face recognition to\nverify the integrity of the claim of the studied conspiracy theory. In\naddition, we assessed the impact of different image quality metrics on the\nvariation of face recognition results. Two sets of image quality metrics were\nconsidered: acquisition-related metrics and subject-related metrics.",
          "link": "http://arxiv.org/abs/2109.02283",
          "publishedOn": "2021-09-07T07:20:13.260Z",
          "wordCount": null,
          "title": "Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01766",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangke Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lingling Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Adversarial attacks have been expanded to speaker recognition (SR). However,\nexisting attacks are often assessed using different SR models, recognition\ntasks and datasets, and only few adversarial defenses borrowed from computer\nvision are considered. Yet,these defenses have not been thoroughly evaluated\nagainst adaptive attacks. Thus, there is still a lack of quantitative\nunderstanding about the strengths and limitations of adversarial attacks and\ndefenses. More effective defenses are also required for securing SR systems. To\nbridge this gap, we present SEC4SR, the first platform enabling researchers to\nsystematically and comprehensively evaluate adversarial attacks and defenses in\nSR. SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses\nincluding our novel feature-level transformations. It also contains techniques\nfor mounting adaptive attacks. Using SEC4SR, we conduct thus far the\nlargest-scale empirical study on adversarial attacks and defenses in SR,\ninvolving 23 defenses, 15 attacks and 4 attack settings. Our study provides\nlots of useful findings that may advance future research: such as (1) all the\ntransformations slightly degrade accuracy on benign examples and their\neffectiveness vary with attacks; (2) most transformations become less effective\nunder adaptive attacks, but some transformations become more effective; (3) few\ntransformations combined with adversarial training yield stronger defenses over\nsome but not all attacks, while our feature-level transformation combined with\nadversarial training yields the strongest defense over all the attacks.\nExtensive experiments demonstrate capabilities and advantages of SEC4SR which\ncan benefit future research in SR.",
          "link": "http://arxiv.org/abs/2109.01766",
          "publishedOn": "2021-09-07T07:20:13.182Z",
          "wordCount": null,
          "title": "SEC4SR: A Security Analysis Platform for Speaker Recognition. (arXiv:2109.01766v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_Gonzalez_N/0/1/0/all/0/1\">Nurudin Alvarez-Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaltenbrunner_A/0/1/0/all/0/1\">Andreas Kaltenbrunner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "Identifying emotions from text is crucial for a variety of real world tasks.\nWe consider the two largest now-available corpora for emotion classification:\nGoEmotions, with 58k messages labelled by readers, and Vent, with 33M\nwriter-labelled messages. We design a benchmark and evaluate several feature\nspaces and learning algorithms, including two simple yet novel models on top of\nBERT that outperform previous strong baselines on GoEmotions. Through an\nexperiment with human participants, we also analyze the differences between how\nwriters express emotions and how readers perceive them. Our results suggest\nthat emotions expressed by writers are harder to identify than emotions that\nreaders perceive. We share a public web interface for researchers to explore\nour models.",
          "link": "http://arxiv.org/abs/2109.01900",
          "publishedOn": "2021-09-07T07:20:13.172Z",
          "wordCount": null,
          "title": "Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maletzky_A/0/1/0/all/0/1\">Alexander Maletzky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thumfart_S/0/1/0/all/0/1\">Stefan Thumfart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wruss_C/0/1/0/all/0/1\">Christoph Wru&#xdf;</a>",
          "description": "We compare the machine readability of pictograms found on Austrian and German\ntraffic signs. To that end, we train classification models on synthetic data\nsets and evaluate their classification accuracy in a controlled setting. In\nparticular, we focus on differences between currently deployed pictograms in\nthe two countries, and a set of new pictograms designed to increase human\nreadability. Besides other results, we find that machine-learning models\ngeneralize poorly to data sets with pictogram designs they have not been\ntrained on. We conclude that manufacturers of advanced driver-assistance\nsystems (ADAS) must take special care to properly address small visual\ndifferences between current and newly designed traffic sign pictograms, as well\nas between pictograms from different countries.",
          "link": "http://arxiv.org/abs/2109.02362",
          "publishedOn": "2021-09-07T07:20:13.167Z",
          "wordCount": null,
          "title": "Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krolicki_A/0/1/0/all/0/1\">Alexander Krolicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavertu_P/0/1/0/all/0/1\">Pierre-Yves Lavertu</a>",
          "description": "Koopman spectral theory has provided a new perspective in the field of\ndynamical systems in recent years. Modern dynamical systems are becoming\nincreasingly non-linear and complex, and there is a need for a framework to\nmodel these systems in a compact and comprehensive representation for\nprediction and control. The central problem in applying Koopman theory to a\nsystem of interest is that the choice of finite-dimensional basis functions is\ntypically done apriori, using expert knowledge of the systems dynamics. Our\napproach learns these basis functions using a supervised learning approach\nwhere a combination of autoencoders and deep neural networks learn the basis\nfunctions for any given system. We demonstrate this approach on a simple\npendulum example in which we obtain a linear representation of the non-linear\nsystem and then predict the future state trajectories given some initial\nconditions. We also explore how changing the input representation of the\ndynamic systems time series data can impact the quality of learned basis\nfunctions. This alternative representation is compared to the traditional raw\ntime series data approach to determine which method results in lower\nreconstruction and prediction error of the true non-linear dynamics of the\nsystem.",
          "link": "http://arxiv.org/abs/2109.02241",
          "publishedOn": "2021-09-07T07:20:13.156Z",
          "wordCount": 649,
          "title": "Supervised DKRC with Images for Offline System Identification. (arXiv:2109.02241v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>",
          "description": "The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (< 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (<= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.",
          "link": "http://arxiv.org/abs/2109.01951",
          "publishedOn": "2021-09-07T07:20:12.986Z",
          "wordCount": 692,
          "title": "FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uddin_A/0/1/0/all/0/1\">Ajim Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xinyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1\">Chia-Ching Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dantong Yu</a>",
          "description": "Firms earning prediction plays a vital role in investment decisions,\ndividends expectation, and share price. It often involves multiple\ntensor-compatible datasets with non-linear multi-way relationships,\nspatiotemporal structures, and different levels of sparsity. Current non-linear\ntensor completion algorithms tend to learn noisy embedding and incur\noverfitting. This paper focuses on the embedding learning aspect of the tensor\ncompletion problem and proposes a new multi-layer neural network architecture\nfor tensor factorization and completion (MLCTR). The network architecture\nentails multiple advantages: a series of low-rank matrix factorizations (MF)\nbuilding blocks to minimize overfitting, interleaved transfer functions in each\nlayer for non-linearity, and by-pass connections to reduce the gradient\ndiminishing problem and increase the depths of neural networks. Furthermore,\nthe model employs Stochastic Gradient Descent(SGD) based optimization for fast\nconvergence in training. Our algorithm is highly efficient for imputing missing\nvalues in the EPS data. Experiments confirm that our strategy of incorporating\nnon-linearity in factor matrices demonstrates impressive performance in\nembedding learning and end-to-end tensor models, and outperforms approaches\nwith non-linearity in the phase of reconstructing tensors from factor matrices.",
          "link": "http://arxiv.org/abs/2109.01773",
          "publishedOn": "2021-09-07T07:20:12.788Z",
          "wordCount": 626,
          "title": "MLCTR: A Fast Scalable Coupled Tensor Completion Based on Multi-Layer Non-Linear Matrix Factorization. (arXiv:2109.01773v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01902",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_B/0/1/0/all/0/1\">Boyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishwar_P/0/1/0/all/0/1\">Prakash Ishwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheutz_M/0/1/0/all/0/1\">Matthias Scheutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1\">Shuchin Aeron</a>",
          "description": "For the Domain Generalization (DG) problem where the hypotheses are composed\nof a common representation function followed by a labeling function, we point\nout a shortcoming in existing approaches that fail to explicitly optimize for a\nterm, appearing in a well-known and widely adopted upper bound to the risk on\nthe unseen domain, that is dependent on the representation to be learned. To\nthis end, we first derive a novel upper bound to the prediction risk. We show\nthat imposing a mild assumption on the representation to be learned, namely\nmanifold restricted invertibility, is sufficient to deal with this issue.\nFurther, unlike existing approaches, our novel upper bound doesn't require the\nassumption of Lipschitzness of the loss function. In addition, the\ndistributional discrepancy in the representation space is handled via the\nWasserstein-2 barycenter cost. In this context, we creatively leverage old and\nrecent transport inequalities, which link various optimal transport metrics, in\nparticular the $L^1$ distance (also known as the total variation distance) and\nthe Wasserstein-2 distances, with the Kullback-Liebler divergence. These\nanalyses and insights motivate a new representation learning cost for DG that\nadditively balances three competing objectives: 1) minimizing classification\nerror across seen domains via cross-entropy, 2) enforcing domain-invariance in\nthe representation space via the Wasserstein-2 barycenter cost, and 3)\npromoting non-degenerate, nearly-invertible representation via one of two\nmechanisms, viz., an autoencoder-based reconstruction loss or a mutual\ninformation loss. It is to be noted that the proposed algorithms completely\nbypass the use of any adversarial training mechanism that is typical of many\ncurrent domain generalization approaches. Simulation results on several\nstandard datasets demonstrate superior performance compared to several\nwell-known DG algorithms.",
          "link": "http://arxiv.org/abs/2109.01902",
          "publishedOn": "2021-09-07T07:20:12.740Z",
          "wordCount": 721,
          "title": "Barycenteric distribution alignment and manifold-restricted invertibility for domain generalization. (arXiv:2109.01902v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shengen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>",
          "description": "Modern GPU datacenters are critical for delivering Deep Learning (DL) models\nand services in both the research community and industry. When operating a\ndatacenter, optimization of resource scheduling and management can bring\nsignificant financial benefits. Achieving this goal requires a deep\nunderstanding of the job features and user behaviors. We present a\ncomprehensive study about the characteristics of DL jobs and resource\nmanagement. First, we perform a large-scale analysis of real-world job traces\nfrom SenseTime. We uncover some interesting conclusions from the perspectives\nof clusters, jobs and users, which can facilitate the cluster system designs.\nSecond, we introduce a general-purpose framework, which manages resources based\non historical data. As case studies, we design: a Quasi-Shortest-Service-First\nscheduling service, which can minimize the cluster-wide average job completion\ntime by up to 6.5x; and a Cluster Energy Saving service, which improves overall\ncluster utilization by up to 13%.",
          "link": "http://arxiv.org/abs/2109.01313",
          "publishedOn": "2021-09-07T07:20:12.389Z",
          "wordCount": null,
          "title": "Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters. (arXiv:2109.01313v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Young-Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>",
          "description": "Temporal set prediction is becoming increasingly important as many companies\nemploy recommender systems in their online businesses, e.g., personalized\npurchase prediction of shopping baskets. While most previous techniques have\nfocused on leveraging a user's history, the study of combining it with others'\nhistories remains untapped potential. This paper proposes Global-Local Item\nEmbedding (GLOIE) that learns to utilize the temporal properties of sets across\nwhole users as well as within a user by coining the names as global and local\ninformation to distinguish the two temporal patterns. GLOIE uses Variational\nAutoencoder (VAE) and dynamic graph-based model to capture global and local\ninformation and then applies attention to integrate resulting item embeddings.\nAdditionally, we propose to use Tweedie output for the decoder of VAE as it can\neasily model zero-inflated and long-tailed distribution, which is more suitable\nfor several real-world data distributions than Gaussian or multinomial\ncounterparts. When evaluated on three public benchmarks, our algorithm\nconsistently outperforms previous state-of-the-art methods in most ranking\nmetrics.",
          "link": "http://arxiv.org/abs/2109.02074",
          "publishedOn": "2021-09-07T07:20:12.381Z",
          "wordCount": null,
          "title": "Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.08481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovachki_N/0/1/0/all/0/1\">Nikola Kovachki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Burigede Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kaushik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuart_A/0/1/0/all/0/1\">Andrew Stuart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "The classical development of neural networks has primarily focused on\nlearning mappings between finite dimensional Euclidean spaces or finite sets.\nWe propose a generalization of neural networks tailored to learn operators\nmapping between infinite dimensional function spaces. We formulate the\napproximation of operators by composition of a class of linear integral\noperators and nonlinear activation functions, so that the composed operator can\napproximate complex nonlinear operators. We prove a universal approximation\ntheorem for our construction. Furthermore, we introduce four classes of\noperator parameterizations: graph-based operators, low-rank operators,\nmultipole graph-based operators, and Fourier operators and describe efficient\nalgorithms for computing with each one. The proposed neural operators are\nresolution-invariant: they share the same network parameters between different\ndiscretizations of the underlying function spaces and can be used for zero-shot\nsuper-resolutions. Numerically, the proposed models show superior performance\ncompared to existing machine learning based methodologies on Burgers' equation,\nDarcy flow, and the Navier-Stokes equation, while being several order of\nmagnitude faster compared to conventional PDE solvers.",
          "link": "http://arxiv.org/abs/2108.08481",
          "publishedOn": "2021-09-07T07:20:12.378Z",
          "wordCount": null,
          "title": "Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02355",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dar_Y/0/1/0/all/0/1\">Yehuda Dar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Muthukumar_V/0/1/0/all/0/1\">Vidya Muthukumar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>",
          "description": "The rapid recent progress in machine learning (ML) has raised a number of\nscientific questions that challenge the longstanding dogma of the field. One of\nthe most important riddles is the good empirical generalization of\noverparameterized models. Overparameterized models are excessively complex with\nrespect to the size of the training dataset, which results in them perfectly\nfitting (i.e., interpolating) the training data, which is usually noisy. Such\ninterpolation of noisy data is traditionally associated with detrimental\noverfitting, and yet a wide range of interpolating models -- from simple linear\nmodels to deep neural networks -- have recently been observed to generalize\nextremely well on fresh test data. Indeed, the recently discovered double\ndescent phenomenon has revealed that highly overparameterized models often\nimprove over the best underparameterized model in test performance.\n\nUnderstanding learning in this overparameterized regime requires new theory\nand foundational empirical studies, even for the simplest case of the linear\nmodel. The underpinnings of this understanding have been laid in very recent\nanalyses of overparameterized linear regression and related statistical\nlearning tasks, which resulted in precise analytic characterizations of double\ndescent. This paper provides a succinct overview of this emerging theory of\noverparameterized ML (henceforth abbreviated as TOPML) that explains these\nrecent findings through a statistical signal processing perspective. We\nemphasize the unique aspects that define the TOPML research area as a subfield\nof modern ML theory and outline interesting open questions that remain.",
          "link": "http://arxiv.org/abs/2109.02355",
          "publishedOn": "2021-09-07T07:20:12.377Z",
          "wordCount": null,
          "title": "A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning. (arXiv:2109.02355v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1910.01723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nottingham_K/0/1/0/all/0/1\">Kolby Nottingham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balakrishnan_A/0/1/0/all/0/1\">Anand Balakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1\">Jyotirmoy Deshmukh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wingate_D/0/1/0/all/0/1\">David Wingate</a>",
          "description": "It is notoriously difficult to control the behavior of reinforcement learning\nagents. Agents often learn to exploit the environment or reward signal and need\nto be retrained multiple times. The multi-objective reinforcement learning\n(MORL) framework separates a reward function into several objectives. An ideal\nMORL agent learns to generalize to novel combinations of objectives allowing\nfor better control of an agent's behavior without requiring retraining. Many\nMORL approaches use a weight vector to parameterize the importance of each\nobjective. However, this approach suffers from lack of expressiveness and\ninterpretability. We propose using propositional logic to specify the\nimportance of multiple objectives. By using a logic where predicates correspond\ndirectly to objectives, specifications are inherently more interpretable.\nAdditionally the set of specifications that can be expressed with formal\nlanguages is a superset of what can be expressed by weight vectors. In this\npaper, we define a formal language based on propositional logic with\nquantitative semantics. We encode logical specifications using a recurrent\nneural network and show that MORL agents parameterized by these encodings are\nable to generalize to novel specifications over objectives and achieve\nperformance comparable to single objective baselines.",
          "link": "http://arxiv.org/abs/1910.01723",
          "publishedOn": "2021-09-07T07:20:12.377Z",
          "wordCount": null,
          "title": "Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning. (arXiv:1910.01723v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Carlsson_E/0/1/0/all/0/1\">Emil Carlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubhashi_D/0/1/0/all/0/1\">Devdatt Dubhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansson_F/0/1/0/all/0/1\">Fredrik D. Johansson</a>",
          "description": "We propose algorithms based on a multi-level Thompson sampling scheme, for\nthe stochastic multi-armed bandit and its contextual variant with linear\nexpected rewards, in the setting where arms are clustered. We show, both\ntheoretically and empirically, how exploiting a given cluster structure can\nsignificantly improve the regret and computational cost compared to using\nstandard Thompson sampling. In the case of the stochastic multi-armed bandit we\ngive upper bounds on the expected cumulative regret showing how it depends on\nthe quality of the clustering. Finally, we perform an empirical evaluation\nshowing that our algorithms perform well compared to previously proposed\nalgorithms for bandits with clustered arms.",
          "link": "http://arxiv.org/abs/2109.01656",
          "publishedOn": "2021-09-07T07:20:12.375Z",
          "wordCount": null,
          "title": "Thompson Sampling for Bandits with Clustered Arms. (arXiv:2109.01656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.13233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1\">Aaqib Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ungureanu_V/0/1/0/all/0/1\">Victor Ungureanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gfeller_B/0/1/0/all/0/1\">Beat Gfeller</a>",
          "description": "Learning general-purpose representations from multisensor data produced by\nthe omnipresent sensing systems (or IoT in general) has numerous applications\nin diverse use cases. Existing purely supervised end-to-end deep learning\ntechniques depend on the availability of a massive amount of well-curated data,\nacquiring which is notoriously difficult but required to achieve a sufficient\nlevel of generalization on a task of interest. In this work, we leverage the\nself-supervised learning paradigm towards realizing the vision of continual\nlearning from unlabeled inputs. We present a generalized framework named Sense\nand Learn for representation or feature learning from raw sensory data. It\nconsists of several auxiliary tasks that can learn high-level and broadly\nuseful features entirely from unannotated data without any human involvement in\nthe tedious labeling process. We demonstrate the efficacy of our approach on\nseveral publicly available datasets from different domains and in various\nsettings, including linear separability, semi-supervised or few shot learning,\nand transfer learning. Our methodology achieves results that are competitive\nwith the supervised approaches and close the gap through fine-tuning a network\nwhile learning the downstream tasks in most cases. In particular, we show that\nthe self-supervised network can be utilized as initialization to significantly\nboost the performance in a low-data regime with as few as 5 labeled instances\nper class, which is of high practical importance to real-world problems.\nLikewise, the learned representations with self-supervision are found to be\nhighly transferable between related datasets, even when few labeled instances\nare available from the target domains. The self-learning nature of our\nmethodology opens up exciting possibilities for on-device continual learning.",
          "link": "http://arxiv.org/abs/2009.13233",
          "publishedOn": "2021-09-07T07:20:12.374Z",
          "wordCount": null,
          "title": "Sense and Learn: Self-Supervision for Omnipresent Sensors. (arXiv:2009.13233v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Haoyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lauriere_M/0/1/0/all/0/1\">Mathieu Lauri&#xe8;re</a>",
          "description": "Generative adversarial networks (GANs) have enjoyed tremendous success in\nimage generation and processing, and have recently attracted growing interests\nin financial modelings. This paper analyzes GANs from the perspectives of\nmean-field games (MFGs) and optimal transport. More specifically, from the game\ntheoretical perspective, GANs are interpreted as MFGs under Pareto Optimality\ncriterion or mean-field controls; from the optimal transport perspective, GANs\nare to minimize the optimal transport cost indexed by the generator from the\nknown latent distribution to the unknown true distribution of data. The MFGs\nperspective of GANs leads to a GAN-based computational method (MFGANs) to solve\nMFGs: one neural network for the backward Hamilton-Jacobi-Bellman equation and\none neural network for the forward Fokker-Planck equation, with the two neural\nnetworks trained in an adversarial way. Numerical experiments demonstrate\nsuperior performance of this proposed algorithm, especially in the higher\ndimensional case, when compared with existing neural network approaches.",
          "link": "http://arxiv.org/abs/2002.04112",
          "publishedOn": "2021-09-07T07:20:12.372Z",
          "wordCount": null,
          "title": "Connecting GANs, MFGs, and OT. (arXiv:2002.04112v4 [cs.GT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jacob_V/0/1/0/all/0/1\">Vincent Jacob</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Fei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiegler_A/0/1/0/all/0/1\">Arnaud Stiegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_B/0/1/0/all/0/1\">Bijan Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yanlei Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1\">Nesime Tatbul</a>",
          "description": "Access to high-quality data repositories and benchmarks have been\ninstrumental in advancing the state of the art in many experimental research\ndomains. While advanced analytics tasks over time series data have been gaining\nlots of attention, lack of such community resources severely limits scientific\nprogress. In this paper, we present Exathlon, the first comprehensive public\nbenchmark for explainable anomaly detection over high-dimensional time series\ndata. Exathlon has been systematically constructed based on real data traces\nfrom repeated executions of large-scale stream processing jobs on an Apache\nSpark cluster. Some of these executions were intentionally disturbed by\nintroducing instances of six different types of anomalous events (e.g.,\nmisbehaving inputs, resource contention, process failures). For each of the\nanomaly instances, ground truth labels for the root cause interval as well as\nthose for the extended effect interval are provided, supporting the development\nand evaluation of a wide range of anomaly detection (AD) and explanation\ndiscovery (ED) tasks. We demonstrate the practical utility of Exathlon's\ndataset, evaluation methodology, and end-to-end data science pipeline design\nthrough an experimental study with three state-of-the-art AD and ED techniques.",
          "link": "http://arxiv.org/abs/2010.05073",
          "publishedOn": "2021-09-07T07:20:12.354Z",
          "wordCount": null,
          "title": "Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. (arXiv:2010.05073v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01690",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Nelson_J/0/1/0/all/0/1\">Jon Nelson</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Vuffray_M/0/1/0/all/0/1\">Marc Vuffray</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Lokhov_A/0/1/0/all/0/1\">Andrey Y. Lokhov</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Albash_T/0/1/0/all/0/1\">Tameem Albash</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Coffrin_C/0/1/0/all/0/1\">Carleton Coffrin</a>",
          "description": "Quantum Annealing (QA) was originally intended for accelerating the solution\nof combinatorial optimization tasks that have natural encodings as Ising\nmodels. However, recent experiments on QA hardware platforms have demonstrated\nthat, in the operating regime corresponding to weak interactions, the QA\nhardware behaves like a noisy Gibbs sampler at a hardware-specific effective\ntemperature. This work builds on those insights and identifies a class of small\nhardware-native Ising models that are robust to noise effects and proposes a\nnovel procedure for executing these models on QA hardware to maximize Gibbs\nsampling performance. Experimental results indicate that the proposed protocol\nresults in high-quality Gibbs samples from a hardware-specific effective\ntemperature and that the QA annealing time can be used to adjust the effective\ntemperature of the output distribution. The procedure proposed in this work\nprovides a new approach to using QA hardware for Ising model sampling\npresenting potential new opportunities for applications in machine learning and\nphysics simulation.",
          "link": "http://arxiv.org/abs/2109.01690",
          "publishedOn": "2021-09-07T07:20:12.349Z",
          "wordCount": null,
          "title": "High-quality Thermal Gibbs Sampling with Quantum Annealing Hardware. (arXiv:2109.01690v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jang_W/0/1/0/all/0/1\">Wonbong Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agapito_L/0/1/0/all/0/1\">Lourdes Agapito</a>",
          "description": "CodeNeRF is an implicit 3D neural representation that learns the variation of\nobject shapes and textures across a category and can be trained, from a set of\nposed images, to synthesize novel views of unseen objects. Unlike the original\nNeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture\nby learning separate embeddings. At test time, given a single unposed image of\nan unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and\nappearance codes via optimization. Unseen objects can be reconstructed from a\nsingle image, and then rendered from new viewpoints or their shape and texture\nedited by varying the latent codes. We conduct experiments on the SRN\nbenchmark, which show that CodeNeRF generalises well to unseen objects and\nachieves on-par performance with methods that require known camera pose at test\ntime. Our results on real-world images demonstrate that CodeNeRF can bridge the\nsim-to-real gap. Project page: \\url{https://github.com/wayne1123/code-nerf}",
          "link": "http://arxiv.org/abs/2109.01750",
          "publishedOn": "2021-09-07T07:20:12.348Z",
          "wordCount": null,
          "title": "CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13837",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jockel_L/0/1/0/all/0/1\">Lisa J&#xf6;ckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_T/0/1/0/all/0/1\">Thomas Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klas_M/0/1/0/all/0/1\">Michael Kl&#xe4;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauer_M/0/1/0/all/0/1\">Marc P. Hauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1\">Janek Gro&#xdf;</a>",
          "description": "Analytical quality assurance, especially testing, is an integral part of\nsoftware-intensive system development. With the increased usage of Artificial\nIntelligence (AI) and Machine Learning (ML) as part of such systems, this\nbecomes more difficult as well-understood software testing approaches cannot be\napplied directly to the AI-enabled parts of the system. The required adaptation\nof classical testing approaches and development of new concepts for AI would\nbenefit from a deeper understanding and exchange between AI and software\nengineering experts. A major obstacle on this way, we see in the different\nterminologies used in the two communities. As we consider a mutual\nunderstanding of the testing terminology as a key, this paper contributes a\nmapping between the most important concepts from classical software testing and\nAI testing. In the mapping, we highlight differences in relevance and naming of\nthe mapped concepts.",
          "link": "http://arxiv.org/abs/2108.13837",
          "publishedOn": "2021-09-07T07:20:12.271Z",
          "wordCount": null,
          "title": "Towards a Common Testing Terminology for Software Engineering and Artificial Intelligence Experts. (arXiv:2108.13837v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bonnici_R/0/1/0/all/0/1\">Russell Sammut Bonnici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saitis_C/0/1/0/all/0/1\">Charalampos Saitis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benning_M/0/1/0/all/0/1\">Martin Benning</a>",
          "description": "This research project investigates the application of deep learning to timbre\ntransfer, where the timbre of a source audio can be converted to the timbre of\na target audio with minimal loss in quality. The adopted approach combines\nVariational Autoencoders with Generative Adversarial Networks to construct\nmeaningful representations of the source audio and produce realistic\ngenerations of the target audio and is applied to the Flickr 8k Audio dataset\nfor transferring the vocal timbre between speakers and the URMP dataset for\ntransferring the musical timbre between instruments. Furthermore, variations of\nthe adopted approach are trained, and generalised performance is compared using\nthe metrics SSIM (Structural Similarity Index) and FAD (Frech\\'et Audio\nDistance). It was found that a many-to-many approach supersedes a one-to-one\napproach in terms of reconstructive capabilities, and that the adoption of a\nbasic over a bottleneck residual block design is more suitable for enriching\ncontent information about a latent space. It was also found that the decision\non whether cyclic loss takes on a variational autoencoder or vanilla\nautoencoder approach does not have a significant impact on reconstructive and\nadversarial translation aspects of the model.",
          "link": "http://arxiv.org/abs/2109.02096",
          "publishedOn": "2021-09-07T07:20:12.258Z",
          "wordCount": null,
          "title": "Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinhai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yuyuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guoxu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qibin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weijun Sun</a>",
          "description": "For the high dimensional data representation, nonnegative tensor ring (NTR)\ndecomposition equipped with manifold learning has become a promising model to\nexploit the multi-dimensional structure and extract the feature from tensor\ndata. However, the existing methods such as graph regularized tensor ring\ndecomposition (GNTR) only models the pair-wise similarities of objects. For\ntensor data with complex manifold structure, the graph can not exactly\nconstruct similarity relationships. In this paper, in order to effectively\nutilize the higher-dimensional and complicated similarities among objects, we\nintroduce hypergraph to the framework of NTR to further enhance the feature\nextraction, upon which a hypergraph regularized nonnegative tensor ring\ndecomposition (HGNTR) method is developed. To reduce the computational\ncomplexity and suppress the noise, we apply the low-rank approximation trick to\naccelerate HGNTR (called LraHGNTR). Our experimental results show that compared\nwith other state-of-the-art algorithms, the proposed HGNTR and LraHGNTR can\nachieve higher performance in clustering tasks, in addition, LraHGNTR can\ngreatly reduce running time without decreasing accuracy.",
          "link": "http://arxiv.org/abs/2109.02314",
          "publishedOn": "2021-09-07T07:20:12.258Z",
          "wordCount": null,
          "title": "Fast Hypergraph Regularized Nonnegative Tensor Ring Factorization Based on Low-Rank Approximation. (arXiv:2109.02314v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01806",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1\">Xiuxian Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Lin_K/0/1/0/all/0/1\">Kuo-Yi Lin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hong_Y/0/1/0/all/0/1\">Yiguang Hong</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>",
          "description": "Communication has been seen as a significant bottleneck in industrial\napplications over large-scale networks. To alleviate the communication burden,\nsign-based optimization algorithms have gained popularity recently in both\nindustrial and academic communities, which is shown to be closely related to\nadaptive gradient methods, such as Adam. Along this line, this paper\ninvestigates faster convergence for a variant of sign-based gradient descent,\ncalled scaled signGD, in three cases: 1) the objective function is strongly\nconvex; 2) the objective function is nonconvex but satisfies the\nPolyak-Lojasiewicz (PL) inequality; 3) the gradient is stochastic, called\nscaled signGD in this case. For the first two cases, it can be shown that the\nscaled signGD converges at a linear rate. For case 3), the algorithm is shown\nto converge linearly to a neighborhood of the optimal value when a constant\nlearning rate is employed, and the algorithm converges at a rate of $O(1/k)$\nwhen using a diminishing learning rate, where $k$ is the iteration number. The\nresults are also extended to the distributed setting by majority vote in a\nparameter-server framework. Finally, numerical experiments on logistic\nregression are performed to corroborate the theoretical findings.",
          "link": "http://arxiv.org/abs/2109.01806",
          "publishedOn": "2021-09-07T07:20:12.247Z",
          "wordCount": null,
          "title": "On Faster Convergence of Scaled Sign Gradient Descent. (arXiv:2109.01806v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01768",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruizhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yansong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kaizhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">TianYun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">JiYuan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shaohui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zidong Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunji Chen</a>",
          "description": "With AlphaGo defeats top human players, reinforcement learning(RL) algorithms\nhave gradually become the code-base of building stronger artificial\nintelligence(AI). The RL algorithm design firstly needs to adapt to the\nspecific environment, so the designed environment guides the rapid and profound\ndevelopment of RL algorithms. However, the existing environments, which can be\ndivided into real world games and customized toy environments, have obvious\nshortcomings. For real world games, it is designed for human entertainment, and\ntoo much difficult for most of RL researchers. For customized toy environments,\nthere is no widely accepted unified evaluation standard for all RL algorithms.\nTherefore, we introduce the first virtual user-friendly environment framework\nfor RL. In this framework, the environment can be easily configured to realize\nall kinds of RL tasks in the mainstream research. Then all the mainstream\nstate-of-the-art(SOTA) RL algorithms can be conveniently evaluated and\ncompared. Therefore, our contributions mainly includes the following aspects:\n1.single configured environment for all classification of SOTA RL algorithms;\n2.combined environment of more than one classification RL algorithms; 3.the\nevaluation standard for all kinds of RL algorithms. With all these efforts, a\npossibility for breeding an AI with capability of general competency in a\nvariety of tasks is provided, and maybe it will open up a new chapter for AI.",
          "link": "http://arxiv.org/abs/2109.01768",
          "publishedOn": "2021-09-07T07:20:12.244Z",
          "wordCount": null,
          "title": "Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms. (arXiv:2109.01768v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masarczyk_W/0/1/0/all/0/1\">Wojciech Masarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deja_K/0/1/0/all/0/1\">Kamil Deja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>",
          "description": "Catastrophic forgetting of previously learned knowledge while learning new\ntasks is a widely observed limitation of contemporary neural networks. Although\nmany continual learning methods are proposed to mitigate this drawback, the\nmain question remains unanswered: what is the root cause of catastrophic\nforgetting? In this work, we aim at answering this question by posing and\nvalidating a set of research hypotheses related to the specificity of\nrepresentations built internally by neural models. More specifically, we design\na set of empirical evaluations that compare the robustness of representations\nin discriminative and generative models against catastrophic forgetting. We\nobserve that representations learned by discriminative models are more prone to\ncatastrophic forgetting than their generative counterparts, which sheds new\nlight on the advantages of developing generative models for continual learning.\nFinally, our work opens new research pathways and possibilities to adopt\ngenerative models in continual learning beyond mere replay mechanisms.",
          "link": "http://arxiv.org/abs/2109.01844",
          "publishedOn": "2021-09-07T07:20:12.239Z",
          "wordCount": null,
          "title": "On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01404",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_K/0/1/0/all/0/1\">Kei Nakagawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_K/0/1/0/all/0/1\">Kenshi Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1\">Tetsuro Morimura</a>",
          "description": "Risk management is critical in decision making, and mean-variance (MV)\ntrade-off is one of the most common criteria. However, in reinforcement\nlearning (RL) for sequential decision making under uncertainty, most of the\nexisting methods for MV control suffer from computational difficulties caused\nby the double sampling problem. In this paper, in contrast to strict MV\ncontrol, we consider learning MV efficient policies that achieve Pareto\nefficiency regarding MV trade-off. To achieve this purpose, we train an agent\nto maximize the expected quadratic utility function, a common objective of risk\nmanagement in finance and economics. We call our approach direct expected\nquadratic utility maximization (EQUM). The EQUM does not suffer from the double\nsampling issue because it does not include gradient estimation of variance. We\nconfirm that the maximizer of the objective in the EQUM directly corresponds to\nan MV efficient policy under a certain condition. We conduct experiments with\nbenchmark settings to demonstrate the effectiveness of the EQUM.",
          "link": "http://arxiv.org/abs/2010.01404",
          "publishedOn": "2021-09-07T07:20:12.102Z",
          "wordCount": null,
          "title": "Mean-Variance Efficient Reinforcement Learning by Expected Quadratic Utility Maximization. (arXiv:2010.01404v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>",
          "description": "In this work, we introduce pixel wise tensor normalization, which is inserted\nafter rectifier linear units and, together with batch normalization, provides a\nsignificant improvement in the accuracy of modern deep neural networks. In\naddition, this work deals with the robustness of networks. We show that the\nfactorized superposition of images from the training set and the reformulation\nof the multi class problem into a multi-label problem yields significantly more\nrobust networks. The reformulation and the adjustment of the multi class log\nloss also improves the results compared to the overlay with only one class as\nlabel.\nhttps://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FTNandFDT&mode=list",
          "link": "http://arxiv.org/abs/2109.02345",
          "publishedOn": "2021-09-07T07:20:11.650Z",
          "wordCount": null,
          "title": "Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.11665",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ray_K/0/1/0/all/0/1\">Kolyan Ray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Szabo_B/0/1/0/all/0/1\">Botond Szabo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Clara_G/0/1/0/all/0/1\">Gabriel Clara</a>",
          "description": "Variational Bayes (VB) is a popular scalable alternative to Markov chain\nMonte Carlo for Bayesian inference. We study a mean-field spike and slab VB\napproximation of widely used Bayesian model selection priors in sparse\nhigh-dimensional logistic regression. We provide non-asymptotic theoretical\nguarantees for the VB posterior in both $\\ell_2$ and prediction loss for a\nsparse truth, giving optimal (minimax) convergence rates. Since the VB\nalgorithm does not depend on the unknown truth to achieve optimality, our\nresults shed light on effective prior choices. We confirm the improved\nperformance of our VB algorithm over common sparse VB approaches in a numerical\nstudy.",
          "link": "http://arxiv.org/abs/2010.11665",
          "publishedOn": "2021-09-07T07:20:11.616Z",
          "wordCount": null,
          "title": "Spike and slab variational Bayes for high dimensional logistic regression. (arXiv:2010.11665v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xiaowu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shoukry_Y/0/1/0/all/0/1\">Yasser Shoukry</a>",
          "description": "In this paper, we consider the problem of training neural network (NN)\ncontrollers for nonlinear dynamical systems that are guaranteed to satisfy\nsafety and liveness (e.g., reach-avoid) properties. Our approach is to combine\nmodel-based design methodologies for dynamical systems with data-driven\napproaches to achieve this target. We confine our attention to NNs with\nRectifier Linear Unit (ReLU) nonlinearity which are known to represent\nContinuous Piece-Wise Affine (CPWA) functions. Given a mathematical model of\nthe dynamical system, we compute a finite-state abstract model that captures\nthe closed-loop behavior under all possible CPWA controllers. Using this\nfinite-state abstract model, our framework identifies a family of CPWA\nfunctions guaranteed to satisfy the safety requirements. We augment the\nlearning algorithm with a NN weight projection operator during training that\nenforces the resulting NN to represent a CPWA function from the provably safe\nfamily of CPWA functions. Moreover, the proposed framework uses the\nfinite-state abstract model to identify candidate CPWA functions that may\nsatisfy the liveness properties. Using such candidate CPWA functions, the\nproposed framework biases the NN training to achieve the liveness\nspecification. We show the efficacy of the proposed framework both in\nsimulation and on an actual robotic vehicle.",
          "link": "http://arxiv.org/abs/2102.10806",
          "publishedOn": "2021-09-07T07:20:11.612Z",
          "wordCount": null,
          "title": "Provably Correct Training of Neural Network Controllers Using Reachability Analysis. (arXiv:2102.10806v2 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.06806",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1\">Fabio Bonassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farina_M/0/1/0/all/0/1\">Marcello Farina</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1\">Riccardo Scattolini</a>",
          "description": "The goal of this paper is to provide sufficient conditions for guaranteeing\nthe Input-to-State Stability (ISS) and the Incremental Input-to-State Stability\n({\\delta}ISS) of Gated Recurrent Units (GRUs) neural networks. These\nconditions, devised for both single-layer and multi-layer architectures,\nconsist of nonlinear inequalities on network's weights. They can be employed to\ncheck the stability of trained networks, or can be enforced as constraints\nduring the training procedure of a GRU. The resulting training procedure is\ntested on a Quadruple Tank nonlinear benchmark system, showing satisfactory\nmodeling performances.",
          "link": "http://arxiv.org/abs/2011.06806",
          "publishedOn": "2021-09-07T07:20:11.608Z",
          "wordCount": null,
          "title": "On the stability properties of Gated Recurrent Units neural networks. (arXiv:2011.06806v5 [eess.SY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1\">Kaan Gokcesu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1\">Hakan Gokcesu</a>",
          "description": "In this paper, we propose a nonparametric approach that can be used in\nenvelope extraction, peak-burst detection and clustering in time series. Our\nproblem formalization results in a naturally defined splitting/forking of the\ntime series. With a possibly hierarchical implementation, it can be used for\nvarious applications in machine learning, signal processing and mathematical\nfinance. From an incoming input signal, our iterative procedure sequentially\ncreates two signals (one upper bounding and one lower bounding signal) by\nminimizing the cumulative $L_1$ drift. We show that a solution can be\nefficiently calculated by use of a Viterbi-like path tracking algorithm\ntogether with an optimal elimination rule. We consider many interesting\nsettings, where our algorithm has near-linear time complexities.",
          "link": "http://arxiv.org/abs/2109.02082",
          "publishedOn": "2021-09-07T07:20:11.603Z",
          "wordCount": null,
          "title": "Nonparametric Extrema Analysis in Time Series for Envelope Extraction, Peak Detection and Clustering. (arXiv:2109.02082v1 [cs.LG])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2005.00033",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>",
          "description": "With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.",
          "link": "http://arxiv.org/abs/2005.00033",
          "publishedOn": "2021-09-14T07:20:09.053Z",
          "wordCount": 773,
          "title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.12289",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Corbara_S/0/1/0/all/0/1\">Silvia Corbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavoni_M/0/1/0/all/0/1\">Mirko Tavoni</a>",
          "description": "We present and make available MedLatinEpi and MedLatinLit, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,\nrespectively, labelled by author; MedLatinEpi texts are of epistolary nature,\nwhile MedLatinLit texts consist of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification. Along with the datasets\nwe provide experimental results, obtained on these datasets, for the authorship\nverification task, i.e., the task of predicting whether a text of unknown\nauthorship was written by a candidate author or not. We also make available the\nsource code of the authorship verification system we have used, thus allowing\nour experiments to be reproduced, and to be used as baselines, by other\nresearchers. We also describe the application of the above authorship\nverification system, using these datasets as training data, for investigating\nthe authorship of two medieval epistles whose authorship has been disputed by\nscholars.",
          "link": "http://arxiv.org/abs/2006.12289",
          "publishedOn": "2021-09-14T07:20:08.920Z",
          "wordCount": 661,
          "title": "MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.12139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>",
          "description": "Passage retrieval and ranking is a key task in open-domain question answering\nand information retrieval. Current effective approaches mostly rely on\npre-trained deep language model-based retrievers and rankers. These methods\nhave been shown to effectively model the semantic matching between queries and\npassages, also in presence of keyword mismatch, i.e. passages that are relevant\nto a query but do not contain important query keywords. In this paper we\nconsider the Dense Retriever (DR), a passage retrieval method, and the BERT\nre-ranker, a popular passage re-ranking method. In this context, we formally\ninvestigate how these models respond and adapt to a specific type of keyword\nmismatch -- that caused by keyword typos occurring in queries. Through\nempirical investigation, we find that typos can lead to a significant drop in\nretrieval and ranking effectiveness. We then propose a simple typos-aware\ntraining framework for DR and BERT re-ranker to address this issue. Our\nexperimental results on the MS MARCO passage ranking dataset show that, with\nour proposed typos-aware training, DR and BERT re-ranker can become robust to\ntypos in queries, resulting in significantly improved effectiveness compared to\nmodels trained without appropriately accounting for typos.",
          "link": "http://arxiv.org/abs/2108.12139",
          "publishedOn": "2021-09-14T07:20:08.912Z",
          "wordCount": 652,
          "title": "Dealing with Typos for BERT-based Passage Retrieval and Ranking. (arXiv:2108.12139v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.11668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Strzelecki_A/0/1/0/all/0/1\">Artur Strzelecki</a>",
          "description": "In this paper a framework for app store optimization is proposed. The\nframework is based on two main areas: developer dependent elements and user\ndependent elements. Developer dependent elements are similar to factors in\nsearch engine optimization. User dependent elements are similar to activities\nin social media. The proposed framework is modelled after downloading sample\ndata from two leading app stores: Google Play and Apple iTunes. Results show\nthat developer dependent elements can be better optimized. Names and\ndescriptions of mobile apps are not fully utilized.",
          "link": "http://arxiv.org/abs/1905.11668",
          "publishedOn": "2021-09-14T07:20:08.896Z",
          "wordCount": 569,
          "title": "A Framework for App Store Optimization. (arXiv:1905.11668v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varasteh_M/0/1/0/all/0/1\">Meysam Varasteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nejad_M/0/1/0/all/0/1\">Mehdi Soleiman Nejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mohammad Amin Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a>",
          "description": "One of the main challenges in recommender systems is data sparsity which\nleads to high variance. Several attempts have been made to improve the\nbias-variance trade-off using auxiliary information. In particular, document\nmodeling-based methods have improved the model's accuracy by using textual data\nsuch as reviews, abstracts, and storylines when the user-to-item rating matrix\nis sparse. However, such models are insufficient to learn optimal\nrepresentation for users and items. User-based and item-based collaborative\nfiltering, owing to their efficiency and interpretability, have been long used\nfor building recommender systems. They create a profile for each user and item\nrespectively as their historically interacted items and the users who\ninteracted with the target item.\n\nThis work combines these two approaches with document context-aware\nrecommender systems by considering users' opinions on these items. Another\nadvantage of our model is that it supports online personalization. If a user\nhas new interactions, it needs to refresh the user and item history\nrepresentation vectors instead of updating model parameters. The proposed\nalgorithm is implemented and tested on three real-world datasets that\ndemonstrate our model's effectiveness over the baseline methods.",
          "link": "http://arxiv.org/abs/2109.05516",
          "publishedOn": "2021-09-14T07:20:08.885Z",
          "wordCount": 631,
          "title": "An Improved Hybrid Recommender System: Integrating Document Context-Based and Behavior-Based Methods. (arXiv:2109.05516v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2012.14541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>",
          "description": "Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.",
          "link": "http://arxiv.org/abs/2012.14541",
          "publishedOn": "2021-09-14T07:20:08.856Z",
          "wordCount": 621,
          "title": "YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Ruihong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>",
          "description": "The sequential recommendation aims to recommend items, such as products,\nsongs and places, to users based on the sequential patterns of their historical\nrecords. Most existing sequential recommender models consider the next item\nprediction task as the training signal. Unfortunately, there are two essential\nchallenges for these methods: (1) the long-term preference is difficult to\ncapture, and (2) the supervision signal is too sparse to effectively train a\nmodel. In this paper, we propose a novel sequential recommendation framework to\novercome these challenges based on a memory augmented multi-instance\ncontrastive predictive coding scheme, denoted as MMInfoRec. The basic\ncontrastive predictive coding (CPC) serves as encoders of sequences and items.\nThe memory module is designed to augment the auto-regressive prediction in CPC\nto enable a flexible and general representation of the encoded preference,\nwhich can improve the ability to capture the long-term preference. For\neffective training of the MMInfoRec model, a novel multi-instance noise\ncontrastive estimation (MINCE) loss is proposed, using multiple positive\nsamples, which offers effective exploitation of samples inside a mini-batch.\nThe proposed MMInfoRec framework falls into the contrastive learning style,\nwithin which, however, a further finetuning step is not required given that its\ncontrastive training task is well aligned with the target recommendation task.\nWith extensive experiments on four benchmark datasets, MMInfoRec can outperform\nthe state-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2109.00368",
          "publishedOn": "2021-09-14T07:20:08.848Z",
          "wordCount": 684,
          "title": "Memory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation. (arXiv:2109.00368v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qinyong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Alexander Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>",
          "description": "In the mobile Internet era, the recommender system has become an\nirreplaceable tool to help users discover useful items, and thus alleviating\nthe information overload problem. Recent deep neural network (DNN)-based\nrecommender system research have made significant progress in improving\nprediction accuracy, which is largely attributed to the access to a large\namount of users' personal data collected from users' devices and then centrally\nstored in the cloud server. However, as there are rising concerns around the\nglobe on user privacy leakage in the online platform, the public is becoming\nanxious by such abuse of user privacy. Therefore, it is urgent and beneficial\nto develop a recommender system that can achieve both high prediction accuracy\nand high degree of user privacy protection.\n\nTo this end, we propose a DNN-based recommendation model called PrivRec\nrunning on the decentralized federated learning (FL) environment, which ensures\nthat a user's data never leaves his/her during the course of model training. On\nthe other hand, to better embrace the data heterogeneity commonly existing in\nFL, we innovatively introduce a first-order meta-learning method that enables\nfast in-device personalization with only few data points. Furthermore, to\ndefense from potential malicious participant that poses serious security threat\nto other users, we develop a user-level differentially private DP-PrivRec model\nso that it is unable to determine whether a particular user is present or not\nsolely based on the trained model. Finally, we conduct extensive experiments on\ntwo large-scale datasets in a simulated FL environment, and the results\nvalidate the superiority of our proposed PrivRec and DP-PrivRec.",
          "link": "http://arxiv.org/abs/2104.00919",
          "publishedOn": "2021-09-14T07:20:08.539Z",
          "wordCount": 738,
          "title": "Fast-adapting and Privacy-preserving Federated Recommender System. (arXiv:2104.00919v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.01470",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lequn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Ranking items by their probability of relevance has long been the goal of\nconventional ranking systems. While this maximizes traditional criteria of\nranking performance, there is a growing understanding that it is an\noversimplification in online platforms that serve not only a diverse user\npopulation, but also the producers of the items. In particular, ranking\nalgorithms are expected to be fair in how they serve all groups of users -- not\njust the majority group -- and they also need to be fair in how they divide\nexposure among the items. These fairness considerations can partially be met by\nadding diversity to the rankings, as done in several recent works. However, we\nshow in this paper that user fairness, item fairness and diversity are\nfundamentally different concepts. In particular, we find that algorithms that\nconsider only one of the three desiderata can fail to satisfy and even harm the\nother two. To overcome this shortcoming, we present the first ranking algorithm\nthat explicitly enforces all three desiderata. The algorithm optimizes user and\nitem fairness as a convex optimization problem which can be solved optimally.\nFrom its solution, a ranking policy can be derived via a novel Birkhoff-von\nNeumann decomposition algorithm that optimizes diversity. Beyond the\ntheoretical analysis, we investigate empirically on a new benchmark dataset how\neffectively the proposed ranking algorithm can control user fairness, item\nfairness and diversity, as well as the trade-offs between them.",
          "link": "http://arxiv.org/abs/2010.01470",
          "publishedOn": "2021-09-14T07:20:08.530Z",
          "wordCount": 730,
          "title": "User Fairness, Item Fairness, and Diversity for Rankings in Two-Sided Markets. (arXiv:2010.01470v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengyue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Quanyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhenhua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiuqiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Top-N recommendation, which aims to learn user ranking-based preference, has\nlong been a fundamental problem in a wide range of applications. Traditional\nmodels usually motivate themselves by designing complex or tailored\narchitectures based on different assumptions. However, the training data of\nrecommender system can be extremely sparse and imbalanced, which poses great\nchallenges for boosting the recommendation performance. To alleviate this\nproblem, in this paper, we propose to reformulate the recommendation task\nwithin the causal inference framework, which enables us to counterfactually\nsimulate user ranking-based preferences to handle the data scarce problem. The\ncore of our model lies in the counterfactual question: \"what would be the\nuser's decision if the recommended items had been different?\". To answer this\nquestion, we firstly formulate the recommendation process with a series of\nstructural equation models (SEMs), whose parameters are optimized based on the\nobserved data. Then, we actively indicate many recommendation lists (called\nintervention in the causal inference terminology) which are not recorded in the\ndataset, and simulate user feedback according to the learned SEMs for\ngenerating new training samples. Instead of randomly intervening on the\nrecommendation list, we design a learning-based method to discover more\ninformative training samples. Considering that the learned SEMs can be not\nperfect, we, at last, theoretically analyze the relation between the number of\ngenerated samples and the model prediction error, based on which a heuristic\nmethod is designed to control the negative effect brought by the prediction\nerror. Extensive experiments are conducted based on both synthetic and\nreal-world datasets to demonstrate the effectiveness of our framework.",
          "link": "http://arxiv.org/abs/2109.02444",
          "publishedOn": "2021-09-14T07:20:08.519Z",
          "wordCount": 721,
          "title": "Top-N Recommendation with Counterfactual User Preference Simulation. (arXiv:2109.02444v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Dongjie Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yundong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haiwen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhaoshuo Tian</a>",
          "description": "Recently, using different channels to model social semantic information, and\nusing self-supervised learning tasks to maintain the characteristics of each\nchannel when fusing the information, which has been proven to be a very\npromising work. However, how to deeply dig out the relationship between\ndifferent channels and make full use of it while maintaining the uniqueness of\neach channel is a problem that has not been well studied and resolved in this\nfield. Under such circumstances, this paper explores and verifies the\ndeficiency of directly constructing contrastive learning tasks on different\nchannels with practical experiments and proposes the scheme of interactive\nmodeling and matching representation across different channels. This is the\nfirst attempt in the field of recommender systems, we believe the insight of\nthis paper is inspirational to future self-supervised learning research based\non multi-channel information. To solve this problem, we propose a cross-channel\nmatching representation model based on attentive interaction, which realizes\nefficient modeling of the relationship between cross-channel information. Based\non this, we also proposed a hierarchical self-supervised learning model, which\nrealized two levels of self-supervised learning within and between channels and\nimproved the ability of self-supervised tasks to autonomously mine different\nlevels of potential information. We have conducted abundant experiments, and\nmany experimental metrics on multiple public data sets show that the method\nproposed in this paper has a significant improvement compared with the\nstate-of-the-art methods, no matter in the general or cold-start scenario. And\nin the experiment of model variant analysis, the benefits of the cross-channel\nmatching representation model and the hierarchical self-supervised model\nproposed in this paper are also fully verified.",
          "link": "http://arxiv.org/abs/2109.00676",
          "publishedOn": "2021-09-14T07:20:08.507Z",
          "wordCount": 745,
          "title": "Self-supervised Recommendation with Cross-channel Matching Representation and Hierarchical Contrastive Learning. (arXiv:2109.00676v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>",
          "description": "BERT-based information retrieval models are expensive, in both time (query\nlatency) and computational resources (energy, hardware cost), making many of\nthese models impractical especially under resource constraints. The reliance on\na query encoder that only performs tokenization and on the pre-processing of\npassage representations at indexing, has allowed the recently proposed TILDE\nmethod to overcome the high query latency issue typical of BERT-based models.\nThis however is at the expense of a lower effectiveness compared to other\nBERT-based re-rankers and dense retrievers. In addition, the original TILDE\nmethod is characterised by indexes with a very high memory footprint, as it\nexpands each passage into the size of the BERT vocabulary. In this paper, we\npropose TILDEv2, a new model that stems from the original TILDE but that\naddresses its limitations. TILDEv2 relies on contextualized exact term matching\nwith expanded passages. This requires to only store in the index the score of\ntokens that appear in the expanded passages (rather than all the vocabulary),\nthus producing indexes that are 99% smaller than those of TILDE. This matching\nmechanism also improves ranking effectiveness by 24%, without adding to the\nquery latency. This makes TILDEv2 the state-of-the-art passage re-ranking\nmethod for CPU-only environments, capable of maintaining query latency below\n100ms on commodity hardware.",
          "link": "http://arxiv.org/abs/2108.08513",
          "publishedOn": "2021-09-14T07:20:08.484Z",
          "wordCount": 681,
          "title": "Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion. (arXiv:2108.08513v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02735",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lequn Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yiwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1\">Thorsten Joachims</a>",
          "description": "Contextual bandit algorithms have become widely used for recommendation in\nonline systems (e.g. marketplaces, music streaming, news), where they now wield\nsubstantial influence on which items get exposed to the users. This raises\nquestions of fairness to the items -- and to the sellers, artists, and writers\nthat benefit from this exposure. We argue that the conventional bandit\nformulation can lead to an undesirable and unfair winner-takes-all allocation\nof exposure. To remedy this problem, we propose a new bandit objective that\nguarantees merit-based fairness of exposure to the items while optimizing\nutility to the users. We formulate fairness regret and reward regret in this\nsetting, and present algorithms for both stochastic multi-armed bandits and\nstochastic linear bandits. We prove that the algorithms achieve sub-linear\nfairness regret and reward regret. Beyond the theoretical analysis, we also\nprovide empirical evidence that these algorithms can fairly allocate exposure\nto different arms effectively.",
          "link": "http://arxiv.org/abs/2103.02735",
          "publishedOn": "2021-09-14T07:20:08.443Z",
          "wordCount": 625,
          "title": "Fairness of Exposure in Stochastic Bandits. (arXiv:2103.02735v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Antonio Camara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>",
          "description": "Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.",
          "link": "http://arxiv.org/abs/2104.07814",
          "publishedOn": "2021-09-14T07:20:08.403Z",
          "wordCount": 668,
          "title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.08927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pourkamali_F/0/1/0/all/0/1\">Farzad Pourkamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macris_N/0/1/0/all/0/1\">Nicolas Macris</a>",
          "description": "We consider the estimation of an n-dimensional vector s from the noisy\nelement-wise measurements of $\\mathbf{s}\\mathbf{s}^T$, a generic problem that\narises in statistics and machine learning. We study a mismatched Bayesian\ninference setting, where some of the parameters are not known to the\nstatistician. We derive the full exact analytic expression of the asymptotic\nmean squared error (MSE) in the large system size limit for the particular case\nof Gaussian priors and additive noise. From our formulas, we see that\nestimation is still possible in the mismatched case; and also that the minimum\nMSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our\ntechnique relies on the asymptotics of the spherical integrals and can be\napplied as long as the statistician chooses a rotationally invariant prior.",
          "link": "http://arxiv.org/abs/2107.08927",
          "publishedOn": "2021-09-14T07:20:08.383Z",
          "wordCount": 597,
          "title": "Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>",
          "description": "Deep hashing approaches, including deep quantization and deep binary hashing,\nhave become a common solution to large-scale image retrieval due to high\ncomputation and storage efficiency. Most existing hashing methods can not\nproduce satisfactory results for fine-grained retrieval, because they usually\nadopt the outputs of the last CNN layer to generate binary codes, which is less\neffective to capture subtle but discriminative visual details. To improve\nfine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization\n(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to\ncapture and preserve fine-grained semantic information from multi-level\nfeatures. Besides, we propose a learnable quantization module with a partial\nattention mechanism, which helps to optimize the most relevant codewords and\nimproves the quantization. Comprehensive experiments demonstrate that PHPQ\noutperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05206",
          "publishedOn": "2021-09-14T07:20:08.114Z",
          "wordCount": 583,
          "title": "Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>",
          "description": "Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.",
          "link": "http://arxiv.org/abs/2109.05125",
          "publishedOn": "2021-09-14T07:20:08.091Z",
          "wordCount": 621,
          "title": "MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khritankov_A/0/1/0/all/0/1\">Anton S. Khritankov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilkevich_A/0/1/0/all/0/1\">Anton A. Pilkevich</a>",
          "description": "We explore a hidden feedback loops effect in online recommender systems.\nFeedback loops result in degradation of online multi-armed bandit (MAB)\nrecommendations to a small subset and loss of coverage and novelty. We study\nhow uncertainty and noise in user interests influence the existence of feedback\nloops. First, we show that an unbiased additive random noise in user interests\ndoes not prevent a feedback loop. Second, we demonstrate that a non-zero\nprobability of resetting user interests is sufficient to limit the feedback\nloop and estimate the size of the effect. Our experiments confirm the\ntheoretical findings in a simulated environment for four bandit algorithms.",
          "link": "http://arxiv.org/abs/2109.05278",
          "publishedOn": "2021-09-14T07:20:08.080Z",
          "wordCount": 554,
          "title": "Existence conditions for hidden feedback loops in online recommender systems. (arXiv:2109.05278v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05446",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingwei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "News recommendation is critical for personalized news access. Most existing\nnews recommendation methods rely on centralized storage of users' historical\nnews click behavior data, which may lead to privacy concerns and hazards.\nFederated Learning is a privacy-preserving framework for multiple clients to\ncollaboratively train models without sharing their private data. However, the\ncomputation and communication cost of directly learning many existing news\nrecommendation models in a federated way are unacceptable for user clients. In\nthis paper, we propose an efficient federated learning framework for\nprivacy-preserving news recommendation. Instead of training and communicating\nthe whole model, we decompose the news recommendation model into a large news\nmodel maintained in the server and a light-weight user model shared on both\nserver and clients, where news representations and user model are communicated\nbetween server and clients. More specifically, the clients request the user\nmodel and news representations from the server, and send their locally computed\ngradients to the server for aggregation. The server updates its global user\nmodel with the aggregated gradients, and further updates its news model to\ninfer updated news representations. Since the local gradients may contain\nprivate information, we propose a secure aggregation method to aggregate\ngradients in a privacy-preserving way. Experiments on two real-world datasets\nshow that our method can reduce the computation and communication cost on\nclients while keep promising model performance.",
          "link": "http://arxiv.org/abs/2109.05446",
          "publishedOn": "2021-09-14T07:20:08.067Z",
          "wordCount": 668,
          "title": "Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation. (arXiv:2109.05446v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "News recommendation is important for personalized online news services. Most\nexisting news recommendation methods rely on centrally stored user behavior\ndata to both train models offline and provide online recommendation services.\nHowever, user data is usually highly privacy-sensitive, and centrally storing\nthem may raise privacy concerns and risks. In this paper, we propose a unified\nnews recommendation framework, which can utilize user data locally stored in\nuser clients to train models and serve users in a privacy-preserving way.\nFollowing a widely used paradigm in real-world recommender systems, our\nframework contains two stages. The first one is for candidate news generation\n(i.e., recall) and the second one is for candidate news ranking (i.e.,\nranking). At the recall stage, each client locally learns multiple interest\nrepresentations from clicked news to comprehensively model user interests.\nThese representations are uploaded to the server to recall candidate news from\na large news pool, which are further distributed to the user client at the\nranking stage for personalized news display. In addition, we propose an\ninterest decomposer-aggregator method with perturbation noise to better protect\nprivate user information encoded in user interest representations. Besides, we\ncollaboratively train both recall and ranking models on the data decentralized\nin a large number of user clients in a privacy-preserving way. Experiments on\ntwo real-world news datasets show that our method can outperform baseline\nmethods and effectively protect user privacy.",
          "link": "http://arxiv.org/abs/2109.05236",
          "publishedOn": "2021-09-14T07:20:08.045Z",
          "wordCount": 684,
          "title": "Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving. (arXiv:2109.05236v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05261",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shengyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1\">Dong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>",
          "description": "Learning user representations based on historical behaviors lies at the core\nof modern recommender systems. Recent advances in sequential recommenders have\nconvincingly demonstrated high capability in extracting effective user\nrepresentations from the given behavior sequences. Despite significant\nprogress, we argue that solely modeling the observational behaviors sequences\nmay end up with a brittle and unstable system due to the noisy and sparse\nnature of user interactions logged. In this paper, we propose to learn accurate\nand robust user representations, which are required to be less sensitive to\n(attack on) noisy behaviors and trust more on the indispensable ones, by\nmodeling counterfactual data distribution. Specifically, given an observed\nbehavior sequence, the proposed CauseRec framework identifies dispensable and\nindispensable concepts at both the fine-grained item level and the abstract\ninterest level. CauseRec conditionally samples user concept sequences from the\ncounterfactual data distributions by replacing dispensable and indispensable\nconcepts within the original concept sequence. With user representations\nobtained from the synthesized user sequences, CauseRec performs contrastive\nuser representation learning by contrasting the counterfactual with the\nobservational. We conduct extensive experiments on real-world public\nrecommendation benchmarks and justify the effectiveness of CauseRec with\nmulti-aspects model analysis. The results demonstrate that the proposed\nCauseRec outperforms state-of-the-art sequential recommenders by learning\naccurate and robust user representations.",
          "link": "http://arxiv.org/abs/2109.05261",
          "publishedOn": "2021-09-14T07:20:07.997Z",
          "wordCount": 671,
          "title": "CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation. (arXiv:2109.05261v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Ziyun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1\">Tao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>",
          "description": "The high efficiency in computation and storage makes hashing (including\nbinary hashing and quantization) a common strategy in large-scale retrieval\nsystems. To alleviate the reliance on expensive annotations, unsupervised deep\nhashing becomes an important research problem. This paper provides a novel\nsolution to unsupervised deep quantization, namely Contrastive Quantization\nwith Code Memory (MeCoQ). Different from existing reconstruction-based\nstrategies, we learn unsupervised binary descriptors by contrastive learning,\nwhich can better capture discriminative visual semantics. Besides, we uncover\nthat codeword diversity regularization is critical to prevent contrastive\nlearning-based quantization from model degeneration. Moreover, we introduce a\nnovel quantization code memory module that boosts contrastive learning with\nlower feature drift than conventional feature memories. Extensive experiments\non benchmark datasets show that MeCoQ outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2109.05205",
          "publishedOn": "2021-09-14T07:20:07.978Z",
          "wordCount": 578,
          "title": "Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.05142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aurpon Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Subhasis Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Snehasis Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Amarnath Gupta</a>",
          "description": "Knowledge analysis is an important application of knowledge graphs. In this\npaper, we present a complex knowledge analysis problem that discovers the gaps\nin the technology areas of interest to an organization. Our knowledge graph is\ndeveloped on a heterogeneous data management platform. The analysis combines\nsemantic search, graph analytics, and polystore query optimization.",
          "link": "http://arxiv.org/abs/2109.05142",
          "publishedOn": "2021-09-14T07:20:07.942Z",
          "wordCount": 495,
          "title": "Discovering Technology Gaps using the IntSight Knowledge Navigator. (arXiv:2109.05142v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13751",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Keeping track of scientific challenges, advances and emerging directions is a\nfundamental part of research. However, researchers face a flood of papers that\nhinders discovery of important knowledge. In biomedicine, this directly impacts\nhuman lives. To address this problem, we present a novel task of extraction and\nsearch of scientific challenges and directions, to facilitate rapid knowledge\ndiscovery. We construct and release an expert-annotated corpus of texts sampled\nfrom full-length papers, labeled with novel semantic categories that generalize\nacross many types of challenges and directions. We focus on a large corpus of\ninterdisciplinary work relating to the COVID-19 pandemic, ranging from\nbiomedicine to areas such as AI and economics. We apply a model trained on our\ndata to identify challenges and directions across the corpus and build a\ndedicated search engine. In experiments with 19 researchers and clinicians\nusing our system, we outperform a popular scientific search engine in assisting\nknowledge discovery. Finally, we show that models trained on our resource\ngeneralize to the wider biomedical domain and to AI papers, highlighting its\nbroad utility. We make our data, model and search engine publicly available.\nhttps://challenges.apps.allenai.org/",
          "link": "http://arxiv.org/abs/2108.13751",
          "publishedOn": "2021-09-13T07:20:24.864Z",
          "wordCount": 720,
          "title": "A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.13301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parola_M/0/1/0/all/0/1\">Marco Parola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nannini_A/0/1/0/all/0/1\">Alice Nannini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poleggi_S/0/1/0/all/0/1\">Stefano Poleggi</a>",
          "description": "To implement a good Content Based Image Retrieval (CBIR) system, it is\nessential to adopt efficient search methods. One way to achieve this results is\nby exploiting approximate search techniques. In fact, when we deal with very\nlarge collections of data, using an exact search method makes the system very\nslow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to\nimplement a CBIR system that allows us to perform fast similarity search on\ndeep features. Specifically, we exploit transfer learning techniques to extract\ndeep features from images; this phase is done using two famous Convolutional\nNeural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both\npre-trained on ImageNet. Then we try out several fully connected deep neural\nnetworks, built on top of both of the previously mentioned CNNs in order to\nfine-tuned them on our dataset. In both of previous cases, we index the\nfeatures within our LSH index implementation and within a sequential scan, to\nbetter understand how much the introduction of the index affects the results.\nFinally, we carry out a performance analysis: we evaluate the relevance of the\nresult set, computing the mAP (mean Average Precision) value obtained during\nthe different experiments with respect to the number of done comparison and\nvarying the hyper-parameter values of the LSH index.",
          "link": "http://arxiv.org/abs/2108.13301",
          "publishedOn": "2021-09-13T07:20:22.805Z",
          "wordCount": 668,
          "title": "Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torbati_G/0/1/0/all/0/1\">Ghazaleh Haratinezhad Torbati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Prior work on personalized recommendations has focused on exploiting explicit\nsignals from user-specific queries, clicks, likes, and ratings. This paper\ninvestigates tapping into a different source of implicit signals of interests\nand tastes: online chats between users. The paper develops an expressive model\nand effective methods for personalizing search-based entity recommendations.\nUser models derived from chats augment different methods for re-ranking entity\nanswers for medium-grained queries. The paper presents specific techniques to\nenhance the user models by capturing domain-specific vocabularies and by\nentity-based expansion. Experiments are based on a collection of online chats\nfrom a controlled user study covering three domains: books, travel, food. We\nevaluate different configurations and compare chat-based user models against\nconcise user profiles from questionnaires. Overall, these two variants perform\non par in terms of NCDG@20, but each has advantages in certain domains.",
          "link": "http://arxiv.org/abs/2109.04716",
          "publishedOn": "2021-09-13T07:20:22.198Z",
          "wordCount": 582,
          "title": "You Get What You Chat: Using Conversations to Personalize Search-based Recommendations. (arXiv:2109.04716v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.12800",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>",
          "description": "We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSimilar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,\nQuery Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from\n55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query\nBank and Relevance Set, where the former contains 1,236 human-paraphrased\nqueries while the latter contains ~32 human-annotated FAQ items for each query.\nWe analyze COUGH by testing different FAQ retrieval models built on top of BM25\nand BERT, among which the best model achieves 48.8 under P@5, indicating a\ngreat challenge presented by COUGH and encouraging future research for further\nimprovement. Our COUGH dataset is available at\nhttps://github.com/sunlab-osu/covid-faq.",
          "link": "http://arxiv.org/abs/2010.12800",
          "publishedOn": "2021-09-13T07:20:22.183Z",
          "wordCount": 652,
          "title": "COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04726",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>",
          "description": "Deep neural models for low-resource named entity recognition (NER) have shown\nimpressive results by leveraging distant super-vision or other meta-level\ninformation (e.g. explanation). However, the costs of acquiring such additional\ninformation are generally prohibitive, especially in domains where existing\nresources (e.g. databases to be used for distant supervision) may not exist. In\nthis paper, we present a novel two-stage framework (AutoTriggER) to improve NER\nperformance by automatically generating and leveraging \"entity triggers\" which\nare essentially human-readable clues in the text that can help guide the model\nto make better decisions. Thus, the framework is able to both create and\nleverage auxiliary supervision by itself. Through experiments on three\nwell-studied NER datasets, we show that our automatically extracted triggers\nare well-matched to human triggers, and AutoTriggER improves performance over a\nRoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a\nlow resource setting.",
          "link": "http://arxiv.org/abs/2109.04726",
          "publishedOn": "2021-09-13T07:20:22.150Z",
          "wordCount": 620,
          "title": "AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04713",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Torbati_G/0/1/0/all/0/1\">Ghazaleh Haratinezhad Torbati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Prior work on personalizing web search results has focused on considering\nquery-and-click logs to capture users individual interests. For product search,\nextensive user histories about purchases and ratings have been exploited.\nHowever, for general entity search, such as for books on specific topics or\ntravel destinations with certain features, personalization is largely\nunderexplored. In this paper, we address personalization of book search, as an\nexemplary case of entity search, by exploiting sparse user profiles obtained\nthrough online questionnaires. We devise and compare a variety of re-ranking\nmethods based on language models or neural learning. Our experiments show that\neven very sparse information about individuals can enhance the effectiveness of\nthe search results.",
          "link": "http://arxiv.org/abs/2109.04713",
          "publishedOn": "2021-09-13T07:20:22.138Z",
          "wordCount": 553,
          "title": "Personalized Entity Search by Sparse and Scrutable User Profiles. (arXiv:2109.04713v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04584",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikolakopoulos_A/0/1/0/all/0/1\">Athanasios N. Nikolakopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xia Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Collaborative recommendation approaches based on nearest-neighbors are still\nhighly popular today due to their simplicity, their efficiency, and their\nability to produce accurate and personalized recommendations. This chapter\noffers a comprehensive survey of neighborhood-based methods for the item\nrecommendation problem. It presents the main characteristics and benefits of\nsuch methods, describes key design choices for implementing a\nneighborhood-based recommender system, and gives practical information on how\nto make these choices. A broad range of methods is covered in the chapter,\nincluding traditional algorithms like k-nearest neighbors as well as advanced\napproaches based on matrix factorization, sparse coding and random walks.",
          "link": "http://arxiv.org/abs/2109.04584",
          "publishedOn": "2021-09-13T07:20:22.125Z",
          "wordCount": 566,
          "title": "Trust your neighbors: A comprehensive survey of neighborhood-based methods for recommender systems. (arXiv:2109.04584v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05669",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>",
          "description": "Isolated silos of scientific research and the growing challenge of\ninformation overload limit awareness across the literature and hinder\ninnovation. Algorithmic curation and recommendation, which often prioritize\nrelevance, can further reinforce these informational \"filter bubbles.\" In\nresponse, we describe Bridger, a system for facilitating discovery of scholars\nand their work, to explore design tradeoffs between relevant and novel\nrecommendations. We construct a faceted representation of authors with\ninformation gleaned from their papers and inferred author personas, and use it\nto develop an approach that locates commonalities (\"bridges\") and contrasts\nbetween scientists -- retrieving partially similar authors rather than aiming\nfor strict similarity. In studies with computer science researchers, this\napproach helps users discover authors considered useful for generating novel\nresearch directions, outperforming a state-of-art neural model. In addition to\nrecommending new content, we also demonstrate an approach for displaying it in\na manner that boosts researchers' ability to understand the work of authors\nwith whom they are unfamiliar. Finally, our analysis reveals that Bridger\nconnects authors who have different citation profiles, publish in different\nvenues, and are more distant in social co-authorship networks, raising the\nprospect of bridging diverse communities and facilitating discovery.",
          "link": "http://arxiv.org/abs/2108.05669",
          "publishedOn": "2021-09-13T07:20:22.104Z",
          "wordCount": 684,
          "title": "Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.05000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pulastya_V/0/1/0/all/0/1\">Vaibhav Pulastya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuti_G/0/1/0/all/0/1\">Gaurav Nuti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atri_Y/0/1/0/all/0/1\">Yash Kumar Atri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Due to the over-emphasize of the quantity of data, the data quality has often\nbeen overlooked. However, not all training data points contribute equally to\nlearning. In particular, if mislabeled, it might actively damage the\nperformance of the model and the ability to generalize out of distribution, as\nthe model might end up learning spurious artifacts present in the dataset. This\nproblem gets compounded by the prevalence of heavily parameterized and complex\ndeep neural networks, which can, with their high capacity, end up memorizing\nthe noise present in the dataset. This paper proposes a novel statistic --\nnoise score, as a measure for the quality of each data point to identify such\nmislabeled samples based on the variations in the latent space representation.\nIn our work, we use the representations derived by the inference network of\ndata quality supervised variational autoencoder (AQUAVS). Our method leverages\nthe fact that samples belonging to the same class will have similar latent\nrepresentations. Therefore, by identifying the outliers in the latent space, we\ncan find the mislabeled samples. We validate our proposed statistic through\nexperimentation by corrupting MNIST, FashionMNIST, and CIFAR10/100 datasets in\ndifferent noise settings for the task of identifying mislabelled samples. We\nfurther show significant improvements in accuracy for the classification task\nfor each dataset.",
          "link": "http://arxiv.org/abs/2109.05000",
          "publishedOn": "2021-09-13T07:20:22.072Z",
          "wordCount": 668,
          "title": "Assessing the Quality of the Datasets by Identifying Mislabeled Samples. (arXiv:2109.05000v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergmanis_T/0/1/0/all/0/1\">Toms Bergmanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>",
          "description": "The majority of language domains require prudent use of terminology to ensure\nclarity and adequacy of information conveyed. While the correct use of\nterminology for some languages and domains can be achieved by adapting\ngeneral-purpose MT systems on large volumes of in-domain parallel data, such\nquantities of domain-specific data are seldom available for less-resourced\nlanguages and niche domains. Furthermore, as exemplified by COVID-19 recently,\nno domain-specific parallel data is readily available for emerging domains.\nHowever, the gravity of this recent calamity created a high demand for reliable\ntranslation of critical information regarding pandemic and infection\nprevention. This work is part of WMT2021 Shared Task: Machine Translation using\nTerminologies, where we describe Tilde MT systems that are capable of dynamic\nterminology integration at the time of translation. Our systems achieve up to\n94% COVID-19 term use accuracy on the test set of the EN-FR language pair\nwithout having access to any form of in-domain information during system\ntraining. We conclude our work with a broader discussion considering the Shared\nTask itself and terminology translation in MT.",
          "link": "http://arxiv.org/abs/2109.04708",
          "publishedOn": "2021-09-13T07:20:22.052Z",
          "wordCount": 668,
          "title": "Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04611",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>",
          "description": "Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.",
          "link": "http://arxiv.org/abs/2109.04611",
          "publishedOn": "2021-09-13T07:20:22.002Z",
          "wordCount": 626,
          "title": "Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Congcong Miao</a>",
          "description": "Trip recommendation is a significant and engaging location-based service that\ncan help new tourists make more customized travel plans. It often attempts to\nsuggest a sequence of point of interests (POIs) for a user who requests a\npersonalized travel demand. Conventional methods either leverage the heuristic\nalgorithms (e.g., dynamic programming) or statistical analysis (e.g., Markov\nmodels) to search or rank a POI sequence. These procedures may fail to capture\nthe diversity of human needs and transitional regularities. They even provide\nrecommendations that deviate from tourists' real travel intention when the trip\ndata is sparse. Although recent deep recursive models (e.g., RNN) are capable\nof alleviating these concerns, existing solutions hardly recognize the\npractical reality, such as the diversity of tourist demands, uncertainties in\nthe trip generation, and the complex visiting preference. Inspired by the\nadvance in deep learning, we introduce a novel self-supervised representation\nlearning framework for trip recommendation -- SelfTrip, aiming at tackling the\naforementioned challenges. Specifically, we propose a two-step contrastive\nlearning mechanism concerning the POI representation, as well as trip\nrepresentation. Furthermore, we present four trip augmentation methods to\ncapture the visiting uncertainties in trip planning. We evaluate our SelfTrip\non four real-world datasets, and extensive results demonstrate the promising\ngain compared with several cutting-edge benchmarks, e.g., up to 4% and 12% on\nF1 and pair-F1, respectively.",
          "link": "http://arxiv.org/abs/2109.00968",
          "publishedOn": "2021-09-10T07:20:10.359Z",
          "wordCount": 678,
          "title": "Self-supervised Representation Learning for Trip Recommendation. (arXiv:2109.00968v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.10511",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xidong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Practical recommender systems experience a cold-start problem when observed\nuser-item interactions in the history are insufficient. Meta learning,\nespecially gradient based one, can be adopted to tackle this problem by\nlearning initial parameters of the model and thus allowing fast adaptation to a\nspecific task from limited data examples. Though with significant performance\nimprovement, it commonly suffers from two critical issues: the\nnon-compatibility with mainstream industrial deployment and the heavy\ncomputational burdens, both due to the inner-loop gradient operation. These two\nissues make them hard to be applied in practical recommender systems. To enjoy\nthe benefits of meta learning framework and mitigate these problems, we propose\na recommendation framework called Contextual Modulation Meta Learning (CMML).\nCMML is composed of fully feed-forward operations so it is computationally\nefficient and completely compatible with the mainstream industrial deployment.\nCMML consists of three components, including a context encoder that can\ngenerate context embedding to represent a specific task, a hybrid context\ngenerator that aggregates specific user-item features with task-level context,\nand a contextual modulation network, which can modulate the recommendation\nmodel to adapt effectively. We validate our approach on both scenario-specific\nand user-specific cold-start setting on various real-world datasets, showing\nCMML can achieve comparable or even better performance with gradient based\nmethods yet with much higher computational efficiency and better\ninterpretability.",
          "link": "http://arxiv.org/abs/2108.10511",
          "publishedOn": "2021-09-10T07:20:10.329Z",
          "wordCount": 713,
          "title": "CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation. (arXiv:2108.10511v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.14415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haokui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>",
          "description": "We propose a generic feature compression method for Approximate Nearest\nNeighbor Search (ANNS) problems, which speeds up existing ANNS methods in a\nplug-and-play manner. Specifically, we propose a new network structure called\nCompression Network with Transformer (CNT) to compress the feature into a low\ndimensional space, and an inhomogeneous neighborhood relationship preserving\n(INRP) loss that aims to maintain high search accuracy. In CNT, we use multiple\ncompression projections to cast the feature into many low dimensional spaces,\nand then use transformer to globally optimize these projections such that the\nfeatures are well compressed following the guidance from our loss function. The\nloss function is designed to assign high weights on point pairs that are close\nin original feature space, and keep their distances in projected space. Keeping\nthese distances helps maintain the eventual top-k retrieval accuracy, and down\nweighting others creates room for feature compression. In experiments, we run\nour compression method on public datasets, and use the compressed features in\ngraph based, product quantization and scalar quantization based ANNS solutions.\nExperimental results show that our compression method can significantly improve\nthe efficiency of these methods while preserves or even improves search\naccuracy, suggesting its broad potential impact on real world applications.",
          "link": "http://arxiv.org/abs/2107.14415",
          "publishedOn": "2021-09-10T07:20:10.272Z",
          "wordCount": 676,
          "title": "Compression Network with Transformer for Approximate Nearest Neighbor Search. (arXiv:2107.14415v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.05009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuexin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongtao Liu</a>",
          "description": "Rating prediction is a core problem in recommender systems to quantify users\npreferences towards different items. Due to the imbalanced rating distributions\nin training data, existing recommendation methods suffer from the biased\nprediction problem that generates biased prediction results. Thus, their\nperformance on predicting ratings which rarely appear in training data is\nunsatisfactory. In this paper, inspired by the superior capability of Extreme\nValue Distribution (EVD)-based methods in modeling the distribution of rare\ndata, we propose a novel \\underline{\\emph{G}}umbel Distribution-based\n\\underline{\\emph{R}}ating \\underline{\\emph{P}}rediction framework (GRP) which\ncan accurately predict both frequent and rare ratings between users and items.\nIn our approach, we first define different Gumbel distributions for each rating\nlevel, which can be learned by historical rating statistics of users and items.\nSecond, we incorporate the Gumbel-based representations of users and items with\ntheir original representations learned from the rating matrix and/or reviews to\nenrich the representations of users and items via a proposed multi-scale\nconvolutional fusion layer. Third, we propose a data-driven rating prediction\nmodule to predict the ratings of user-item pairs. It's worthy to note that our\napproach can be readily applied to existing recommendation methods for\naddressing their biased prediction problem. To verify the effectiveness of GRP,\nwe conduct extensive experiments on eight benchmark datasets. Compared with\nseveral baseline models, the results show that: 1) GRP achieves\nstate-of-the-art overall performance on all eight datasets; 2) GRP makes a\nsubstantial improvement in predicting rare ratings, which shows the\neffectiveness of our model in addressing the bias prediction problem.",
          "link": "http://arxiv.org/abs/2012.05009",
          "publishedOn": "2021-09-10T07:20:10.166Z",
          "wordCount": 714,
          "title": "A Gumbel-based activation function for imbalanced datasets. (arXiv:2012.05009v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.04467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Abhinav Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anubhav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Jose Mathew</a>",
          "description": "Digital maps are commonly used across the globe for exploring places that\nusers are interested in, commonly referred to as points of interest (PoI). In\nonline food delivery platforms, PoIs could represent any major private\ncompounds where customers could order from such as hospitals, residential\ncomplexes, office complexes, educational institutes and hostels. In this work,\nwe propose an end-to-end unsupervised system design for obtaining polygon\nrepresentations of PoIs (PoI polygons) from address locations and address\ntexts. We preprocess the address texts using locality names and generate\nembeddings for the address texts using a deep learning-based architecture, viz.\nRoBERTa, trained on our internal address dataset. The PoI candidates are\nidentified by jointly clustering the anonymised customer phone GPS locations\n(obtained during address onboarding) and the embeddings of the address texts.\nThe final list of PoI polygons is obtained from these PoI candidates using\nnovel post-processing steps. This algorithm identified 74.8 % more PoIs than\nthose obtained using the Mummidi-Krumm baseline algorithm run on our internal\ndataset. The proposed algorithm achieves a median area precision of 98 %, a\nmedian area recall of 8 %, and a median F-score of 0.15. In order to improve\nthe recall of the algorithmic polygons, we post-process them using building\nfootprint polygons from the OpenStreetMap (OSM) database. The post-processing\nalgorithm involves reshaping the algorithmic polygon using intersecting\npolygons and closed private roads from the OSM database, and accounting for\nintersection with public roads on the OSM database. We achieve a median area\nrecall of 70 %, a median area precision of 69 %, and a median F-score of 0.69\non these post-processed polygons.",
          "link": "http://arxiv.org/abs/2109.04467",
          "publishedOn": "2021-09-10T07:20:10.099Z",
          "wordCount": 715,
          "title": "Mining Points of Interest via Address Embeddings: An Unsupervised Approach. (arXiv:2109.04467v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bera_D/0/1/0/all/0/1\">Debajyoti Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_R/0/1/0/all/0/1\">Rameshwar Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_B/0/1/0/all/0/1\">Bhisham Dev Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_B/0/1/0/all/0/1\">Biswadeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Representation learning using network embedding has received tremendous\nattention due to its efficacy to solve downstream tasks. Popular embedding\nmethods (such as deepwalk, node2vec, LINE) are based on a neural architecture,\nthus unable to scale on large networks both in terms of time and space usage.\nRecently, we proposed BinSketch, a sketching technique for compressing binary\nvectors to binary vectors. In this paper, we show how to extend BinSketch and\nuse it for network hashing. Our proposal named QUINT is built upon BinSketch,\nand it embeds nodes of a sparse network onto a low-dimensional space using\nsimple bi-wise operations. QUINT is the first of its kind that provides\ntremendous gain in terms of speed and space usage without compromising much on\nthe accuracy of the downstream tasks. Extensive experiments are conducted to\ncompare QUINT with seven state-of-the-art network embedding methods for two end\ntasks - link prediction and node classification. We observe huge performance\ngain for QUINT in terms of speedup (up to 7000x) and space saving (up to 800x)\ndue to its bit-wise nature to obtain node embedding. Moreover, QUINT is a\nconsistent top-performer for both the tasks among the baselines across all the\ndatasets. Our empirical observations are backed by rigorous theoretical\nanalysis to justify the effectiveness of QUINT. In particular, we prove that\nQUINT retains enough structural information which can be used further to\napproximate many topological properties of networks with high confidence.",
          "link": "http://arxiv.org/abs/2109.04206",
          "publishedOn": "2021-09-10T07:20:09.789Z",
          "wordCount": 690,
          "title": "QUINT: Node embedding using network hashing. (arXiv:2109.04206v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.",
          "link": "http://arxiv.org/abs/2109.04312",
          "publishedOn": "2021-09-10T07:20:09.775Z",
          "wordCount": 623,
          "title": "MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Diaz_Aviles_E/0/1/0/all/0/1\">Ernesto Diaz-Aviles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orellana_Rodriguez_C/0/1/0/all/0/1\">Claudia Orellana-Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brigadir_I/0/1/0/all/0/1\">Igor Brigadir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutty_R/0/1/0/all/0/1\">Reshma Narayanan Kutty</a>",
          "description": "Newsletters have (re-) emerged as a powerful tool for publishers to engage\nwith their readers directly and more effectively. Despite the diversity in\ntheir audiences, publishers' newsletters remain largely a one-size-fits-all\noffering, which is suboptimal. In this paper, we present NU:BRIEF, a web\napplication for publishers that enables them to personalize their newsletters\nwithout harvesting personal data. Personalized newsletters build a habit and\nbecome a great conversion tool for publishers, providing an alternative\nreaders-generated revenue model to a declining ad/clickbait-centered business\nmodel.",
          "link": "http://arxiv.org/abs/2109.03955",
          "publishedOn": "2021-09-10T07:20:09.763Z",
          "wordCount": 556,
          "title": "NU:BRIEF -- A Privacy-aware Newsletter Personalization Engine for Publishers. (arXiv:2109.03955v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lahoti_P/0/1/0/all/0/1\">Preethi Lahoti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1\">Krishna P. Gummadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>",
          "description": "Reliably predicting potential failure risks of machine learning (ML) systems\nwhen deployed with production data is a crucial aspect of trustworthy AI. This\npaper introduces Risk Advisor, a novel post-hoc meta-learner for estimating\nfailure risks and predictive uncertainties of any already-trained black-box\nclassification model. In addition to providing a risk score, the Risk Advisor\ndecomposes the uncertainty estimates into aleatoric and epistemic uncertainty\ncomponents, thus giving informative insights into the sources of uncertainty\ninducing the failures. Consequently, Risk Advisor can distinguish between\nfailures caused by data variability, data shifts and model limitations and\nadvise on mitigation actions (e.g., collecting more data to counter data\nshift). Extensive experiments on various families of black-box classification\nmodels and on real-world and synthetic datasets covering common ML failure\nscenarios show that the Risk Advisor reliably predicts deployment-time failure\nrisks in all the scenarios, and outperforms strong baselines.",
          "link": "http://arxiv.org/abs/2109.04432",
          "publishedOn": "2021-09-10T07:20:09.733Z",
          "wordCount": 611,
          "title": "Detecting and Mitigating Test-time Failure Risks via Model-agnostic Uncertainty Learning. (arXiv:2109.04432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2109.04200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Min Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junliang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jundong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>",
          "description": "With the prevalence of social media, there has recently been a proliferation\nof recommenders that shift their focus from individual modeling to group\nrecommendation. Since the group preference is a mixture of various\npredilections from group members, the fundamental challenge of group\nrecommendation is to model the correlations among members. Existing methods\nmostly adopt heuristic or attention-based preference aggregation strategies to\nsynthesize group preferences. However, these models mainly focus on the\npairwise connections of users and ignore the complex high-order interactions\nwithin and beyond groups. Besides, group recommendation suffers seriously from\nthe problem of data sparsity due to severely sparse group-item interactions. In\nthis paper, we propose a self-supervised hypergraph learning framework for\ngroup recommendation to achieve two goals: (1) capturing the intra- and\ninter-group interactions among users; (2) alleviating the data sparsity issue\nwith the raw data itself. Technically, for (1), a hierarchical hypergraph\nconvolutional network based on the user- and group-level hypergraphs is\ndeveloped to model the complex tuplewise correlations among users within and\nbeyond groups. For (2), we design a double-scale node dropout strategy to\ncreate self-supervision signals that can regularize user representations with\ndifferent granularities against the sparsity issue. The experimental analysis\non multiple benchmark datasets demonstrates the superiority of the proposed\nmodel and also elucidates the rationality of the hypergraph modeling and the\ndouble-scale self-supervision.",
          "link": "http://arxiv.org/abs/2109.04200",
          "publishedOn": "2021-09-10T07:20:09.711Z",
          "wordCount": 673,
          "title": "Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. (arXiv:2109.04200v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>",
          "description": "Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.",
          "link": "http://arxiv.org/abs/2109.03848",
          "publishedOn": "2021-09-10T07:20:09.675Z",
          "wordCount": 656,
          "title": "Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03915",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Villermet_Q/0/1/0/all/0/1\">Quentin Villermet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poiroux_J/0/1/0/all/0/1\">J&#xe9;r&#xe9;mie Poiroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallam_M/0/1/0/all/0/1\">Manuel Moussallam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louail_T/0/1/0/all/0/1\">Thomas Louail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_C/0/1/0/all/0/1\">Camille Roth</a>",
          "description": "The role of recommendation systems in the diversity of content consumption on\nplatforms is a much-debated issue. The quantitative state of the art often\noverlooks the existence of individual attitudes toward guidance, and eventually\nof different categories of users in this regard. Focusing on the case of music\nstreaming, we analyze the complete listening history of about 9k users over one\nyear and demonstrate that there is no blanket answer to the intertwinement of\nrecommendation use and consumption diversity: it depends on users. First we\ncompute for each user the relative importance of different access modes within\ntheir listening history, introducing a trichotomy distinguishing so-called\n`organic' use from algorithmic and editorial guidance. We thereby identify four\ncategories of users. We then focus on two scales related to content diversity,\nboth in terms of dispersion -- how much users consume the same content\nrepeatedly -- and popularity -- how popular is the content they consume. We\nshow that the two types of recommendation offered by music platforms --\nalgorithmic and editorial -- may drive the consumption of more or less diverse\ncontent in opposite directions, depending also strongly on the type of users.\nFinally, we compare users' streaming histories with the music programming of a\nselection of popular French radio stations during the same period. While radio\nprograms are usually more tilted toward repetition than users' listening\nhistories, they often program more songs from less popular artists. On the\nwhole, our results highlight the nontrivial effects of platform-mediated\nrecommendation on consumption, and lead us to speak of `filter niches' rather\nthan `filter bubbles'. They hint at further ramifications for the study and\ndesign of recommendation systems.",
          "link": "http://arxiv.org/abs/2109.03915",
          "publishedOn": "2021-09-10T07:20:09.460Z",
          "wordCount": 787,
          "title": "Follow the guides: disentangling human and algorithmic curation in online music consumption. (arXiv:2109.03915v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kshetramade_R/0/1/0/all/0/1\">Reema Kshetramade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houser_J/0/1/0/all/0/1\">John Houser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "Compliments and concerns in reviews are valuable for understanding users'\nshopping interests and their opinions with respect to specific aspects of\ncertain items. Existing review-based recommenders favor large and complex\nlanguage encoders that can only learn latent and uninterpretable text\nrepresentations. They lack explicit user attention and item property modeling,\nwhich however could provide valuable information beyond the ability to\nrecommend items. Therefore, we propose a tightly coupled two-stage approach,\nincluding an Aspect-Sentiment Pair Extractor (ASPE) and an\nAttention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines\nAspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as\nconcrete aspect-level evidence. Extensive experiments on seven real-world\nAmazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs\nwhich enable APRE to deliver superior accuracy over the leading baselines.",
          "link": "http://arxiv.org/abs/2109.03821",
          "publishedOn": "2021-09-10T07:20:09.416Z",
          "wordCount": 584,
          "title": "Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction. (arXiv:2109.03821v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chiragkumar_S/0/1/0/all/0/1\">Shah Riya Chiragkumar</a>",
          "description": "Music Information Retrieval (MIR) is a collaborative scientific study that\nhelp to build innovative information research themes, novel frameworks, and\ndeveloping connected delivery mechanisms in addition to making the world's\nmassive collection of music open for everyone. Modern rock music proved to be\ndifficult to estimate tempo and chord recognition did not work. All of the\nfindings indicate that modern rock and metal music can be analysed, despite its\ncomplexity, but that further research is needed in this area to make it useful.\nUsing a neural network has been one of the simplest ways of dealing with it.\nThe pitch class profile vector is used in the neural network method. Because\nthe vector only contains 12 elements of semi-tone values, it is enough for\nchord recognition. Of course, there are other ways of achieving this work, most\nof them depend on pitch class profiling to transform the chord into a type that\ncan be recognised, but the recognition process is time-consuming centred on\nextremely complicated and memory-intensive methods.",
          "link": "http://arxiv.org/abs/2105.07019",
          "publishedOn": "2021-09-09T07:20:40.667Z",
          "wordCount": 645,
          "title": "Chord Recognition- Music and Audio Information Retrieval. (arXiv:2105.07019v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08663",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>",
          "description": "Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.",
          "link": "http://arxiv.org/abs/2104.08663",
          "publishedOn": "2021-09-09T07:20:40.318Z",
          "wordCount": 676,
          "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04170",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hande Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xin Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guli Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Keping Yang</a>",
          "description": "Recommender systems rely on user behavior data like ratings and clicks to\nbuild personalization model. However, the collected data is observational\nrather than experimental, causing various biases in the data which\nsignificantly affect the learned model. Most existing work for recommendation\ndebiasing, such as the inverse propensity scoring and imputation approaches,\nfocuses on one or two specific biases, lacking the universal capacity that can\naccount for mixed or even unknown biases in the data. Towards this research\ngap, we first analyze the origin of biases from the perspective of \\textit{risk\ndiscrepancy} that represents the difference between the expectation empirical\nrisk and the true risk. Remarkably, we derive a general learning framework that\nwell summarizes most existing debiasing strategies by specifying some\nparameters of the general framework. This provides a valuable opportunity to\ndevelop a universal solution for debiasing, e.g., by learning the debiasing\nparameters from data. However, the training data lacks important signal of how\nthe data is biased and what the unbiased data looks like. To move this idea\nforward, we propose \\textit{AotoDebias} that leverages another (small) set of\nuniform data to optimize the debiasing parameters by solving the bi-level\noptimization problem with meta-learning. Through theoretical analyses, we\nderive the generalization bound for AutoDebias and prove its ability to acquire\nthe appropriate debiasing strategy. Extensive experiments on two real datasets\nand a simulated dataset demonstrated effectiveness of AutoDebias. The code is\navailable at \\url{https://github.com/DongHande/AutoDebias}.",
          "link": "http://arxiv.org/abs/2105.04170",
          "publishedOn": "2021-09-09T07:20:40.301Z",
          "wordCount": 745,
          "title": "AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03777",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>",
          "description": "Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms most graph-based\nmodels. We further fine-tune DistilBERT for comparison and find that it\noutperforms all state-of-the-art models. We suggest that future studies use at\nleast an MLP baseline to contextualize the results. We provide recommendations\nfor the design and training of such a baseline.",
          "link": "http://arxiv.org/abs/2109.03777",
          "publishedOn": "2021-09-09T07:20:40.153Z",
          "wordCount": 567,
          "title": "Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03540",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaocong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lina Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xianzhi Wang</a>",
          "description": "In light of the emergence of deep reinforcement learning (DRL) in recommender\nsystems research and several fruitful results in recent years, this survey aims\nto provide a timely and comprehensive overview of the recent trends of deep\nreinforcement learning in recommender systems. We start with the motivation of\napplying DRL in recommender systems. Then, we provide a taxonomy of current\nDRL-based recommender systems and a summary of existing methods. We discuss\nemerging topics and open issues, and provide our perspective on advancing the\ndomain. This survey serves as introductory material for readers from academia\nand industry into the topic and identifies notable opportunities for further\nresearch.",
          "link": "http://arxiv.org/abs/2109.03540",
          "publishedOn": "2021-09-09T07:20:40.105Z",
          "wordCount": 570,
          "title": "A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions. (arXiv:2109.03540v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>",
          "description": "This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank\ntwice, reaD twice). The pipeline is composed of a retriever, passage reranker,\nextractive reader, generative reader and a mechanism that aggregates the final\nprediction from all system's components. We demonstrate its strength across\nthree open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,\nsurpassing state-of-the-art on the first two. Our analysis demonstrates that:\n(i) combining extractive and generative reader yields absolute improvements up\nto 5 exact match and it is at least twice as effective as the posterior\naveraging ensemble of the same models with different parameters, (ii) the\nextractive reader with fewer parameters can match the performance of the\ngenerative reader on extractive QA datasets.",
          "link": "http://arxiv.org/abs/2109.03502",
          "publishedOn": "2021-09-09T07:20:40.091Z",
          "wordCount": 582,
          "title": "R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03538",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shakespeare_D/0/1/0/all/0/1\">Dougal Shakespeare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_C/0/1/0/all/0/1\">Camille Roth</a>",
          "description": "Popular music streaming platforms offer users a diverse network of content\nexploration through a triad of affordances: organic, algorithmic and editorial\naccess modes. Whilst offering great potential for discovery, such platform\ndevelopments also pose the modern user with daily adoption decisions on two\nfronts: platform affordance adoption and the adoption of recommendations\ntherein. Following a carefully constrained set of Deezer users over a 2-year\nobservation period, our work explores factors driving user behaviour in the\nbroad sense, by differentiating users on the basis of their temporal daily\nusage, adoption of the main platform affordances, and the ways in which they\nreact to them, especially in terms of recommendation adoption. Diverging from a\nperspective common in studies on the effects of recommendation, we assume and\nconfirm that users exhibit very diverse behaviours in using and adopting the\nplatform affordances. The resulting complex and quite heterogeneous picture\ndemonstrates that there is no blanket answer for adoption practices of both\nrecommendation features and recommendations.",
          "link": "http://arxiv.org/abs/2109.03538",
          "publishedOn": "2021-09-09T07:20:40.061Z",
          "wordCount": 606,
          "title": "Tracing Affordance and Item Adoption on Music Streaming Platforms. (arXiv:2109.03538v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sencun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>",
          "description": "Current app ranking and recommendation systems are mainly based on\nuser-generated information, e.g., number of downloads and ratings. However, new\napps often have few (or even no) user feedback, suffering from the classic\ncold-start problem. How to quickly identify and then recommend new apps of high\nquality is a challenging issue. Here, a fundamental requirement is the\ncapability to accurately measure an app's quality based on its inborn features,\nrather than user-generated features. Since users obtain first-hand experience\nof an app by interacting with its views, we speculate that the inborn features\nare largely related to the visual quality of individual views in an app and the\nways the views switch to one another. In this work, we propose AppQ, a novel\napp quality grading and recommendation system that extracts inborn features of\napps based on app source code. In particular, AppQ works in parallel to perform\ncode analysis to extract app-level features as well as dynamic analysis to\ncapture view-level layout hierarchy and the switching among views. Each app is\nthen expressed as an attributed view graph, which is converted into a vector\nand fed to classifiers for recognizing its quality classes. Our evaluation with\nan app dataset from Google Play reports that AppQ achieves the best performance\nwith accuracy of 85.0\\%. This shows a lot of promise to warm-start app grading\nand recommendation systems with AppQ.",
          "link": "http://arxiv.org/abs/2109.03798",
          "publishedOn": "2021-09-09T07:20:40.045Z",
          "wordCount": 687,
          "title": "AppQ: Warm-starting App Recommendation Based on View Graphs. (arXiv:2109.03798v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngjune Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kee-Eung Kim</a>",
          "description": "Knowledge Distillation (KD), which transfers the knowledge of a well-trained\nlarge model (teacher) to a small model (student), has become an important area\nof research for practical deployment of recommender systems. Recently, Relaxed\nRanking Distillation (RRD) has shown that distilling the ranking information in\nthe recommendation list significantly improves the performance. However, the\nmethod still has limitations in that 1) it does not fully utilize the\nprediction errors of the student model, which makes the training not fully\nefficient, and 2) it only distills the user-side ranking information, which\nprovides an insufficient view under the sparse implicit feedback. This paper\npresents Dual Correction strategy for Distillation (DCD), which transfers the\nranking information from the teacher model to the student model in a more\nefficient manner. Most importantly, DCD uses the discrepancy between the\nteacher model and the student model predictions to decide which knowledge to be\ndistilled. By doing so, DCD essentially provides the learning guidance tailored\nto \"correcting\" what the student model has failed to accurately predict. This\nprocess is applied for transferring the ranking information from the user-side\nas well as the item-side to address sparse implicit user feedback. Our\nexperiments show that the proposed method outperforms the state-of-the-art\nbaselines, and ablation studies validate the effectiveness of each component.",
          "link": "http://arxiv.org/abs/2109.03459",
          "publishedOn": "2021-09-09T07:20:40.014Z",
          "wordCount": 659,
          "title": "Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2004.12835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Samenko_I/0/1/0/all/0/1\">Igor Samenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>",
          "description": "This paper shows that, modern word embeddings contain information that\ndistinguishes synonyms and antonyms despite small cosine similarities between\ncorresponding vectors. This information is encoded in the geometry of the\nembeddings and could be extracted with a straight-forward and intuitive\nmanifold learning procedure or a contrasting map. Such a map is trained on a\nsmall labeled subset of the data and can produce new embeddings that explicitly\nhighlight specific semantic attributes of the word. The new embeddings produced\nby the map are shown to improve the performance on downstream tasks.",
          "link": "http://arxiv.org/abs/2004.12835",
          "publishedOn": "2021-09-08T07:20:09.386Z",
          "wordCount": 571,
          "title": "Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.03150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>",
          "description": "Driven by the need to capture users' evolving interests and optimize their\nlong-term experiences, more and more recommender systems have started to model\nrecommendation as a Markov decision process and employ reinforcement learning\nto address the problem. Shouldn't research on the fairness of recommender\nsystems follow the same trend from static evaluation and one-shot intervention\nto dynamic monitoring and non-stop control? In this paper, we portray the\nrecent developments in recommender systems first and then discuss how fairness\ncould be baked into the reinforcement learning techniques for recommendation.\nMoreover, we argue that in order to make further progress in recommendation\nfairness, we may want to consider multi-agent (game-theoretic) optimization,\nmulti-objective (Pareto) optimization, and simulation-based optimization, in\nthe general framework of stochastic games.",
          "link": "http://arxiv.org/abs/2109.03150",
          "publishedOn": "2021-09-08T07:20:09.354Z",
          "wordCount": 570,
          "title": "Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bulathwela_S/0/1/0/all/0/1\">Sahan Bulathwela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_M/0/1/0/all/0/1\">Maria Perez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novak_E/0/1/0/all/0/1\">Erik Novak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>",
          "description": "Educational recommenders have received much less attention in comparison to\ne-commerce and entertainment-related recommenders, even though efficient\nintelligent tutors have great potential to improve learning gains. One of the\nmain challenges in advancing this research direction is the scarcity of large,\npublicly available datasets. In this work, we release a large, novel dataset of\nlearners engaging with educational videos in-the-wild. The dataset, named\nPersonalised Educational Engagement with Knowledge Topics PEEK, is the first\npublicly available dataset of this nature. The video lectures have been\nassociated with Wikipedia concepts related to the material of the lecture, thus\nproviding a humanly intuitive taxonomy. We believe that granular learner\nengagement signals in unison with rich content representations will pave the\nway to building powerful personalization algorithms that will revolutionise\neducational and informational recommendation systems. Towards this goal, we 1)\nconstruct a novel dataset from a popular video lecture repository, 2) identify\na set of benchmark algorithms to model engagement, and 3) run extensive\nexperimentation on the PEEK dataset to demonstrate its value. Our experiments\nwith the dataset show promise in building powerful informational recommender\nsystems. The dataset and the support code is available publicly.",
          "link": "http://arxiv.org/abs/2109.03154",
          "publishedOn": "2021-09-08T07:20:09.160Z",
          "wordCount": 674,
          "title": "PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.01314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1\">Manish Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aditya Jain</a>",
          "description": "Recommendation engines are integral to the modern e-commerce experience, both\nfor the seller and the end user. Accurate recommendations lead to higher\nrevenue and better user experience. In this paper, we are presenting our\nsolution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of\nthis challenge is to maximize the chances of a click when the users are\npresented with set of fashion items. We have approached this problem as a\nbinary classification problem. Our winning solution utilizes Catboost as the\nclassifier and Bayesian Optimization for hyper parameter tuning. Our baseline\nmodel achieved MRR of 0.5153 on the validation set. Bayesian optimization of\nhyper parameters improved the MRR to 0.5240 on the validation set. Our final\nsubmission on the test set achieved a MRR of 0.5257.",
          "link": "http://arxiv.org/abs/2108.01314",
          "publishedOn": "2021-09-08T07:20:09.127Z",
          "wordCount": 599,
          "title": "Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>",
          "description": "Pretrained contextualized representations offer great success for many\ndownstream tasks, including document ranking. The multilingual versions of such\npretrained representations provide a possibility of jointly learning many\nlanguages with the same model. Although it is expected to gain big with such\njoint training, in the case of cross lingual information retrieval (CLIR), the\nmodels under a multilingual setting are not achieving the same level of\nperformance as those under a monolingual setting. We hypothesize that the\nperformance drop is due to the translation gap between query and documents. In\nthe monolingual retrieval task, because of the same lexical inputs, it is\neasier for model to identify the query terms that occurred in documents.\nHowever, in the multilingual pretrained models that the words in different\nlanguages are projected into the same hyperspace, the model tends to translate\nquery terms into related terms, i.e., terms that appear in a similar context,\nin addition to or sometimes rather than synonyms in the target language. This\nproperty is creating difficulties for the model to connect terms that cooccur\nin both query and document. To address this issue, we propose a novel Mixed\nAttention Transformer (MAT) that incorporates external word level knowledge,\nsuch as a dictionary or translation table. We design a sandwich like\narchitecture to embed MAT into the recent transformer based deep neural models.\nBy encoding the translation knowledge into an attention matrix, the model with\nMAT is able to focus on the mutually translated words in the input sequence.\nExperimental results demonstrate the effectiveness of the external knowledge\nand the significant improvement of MAT embedded neural reranking model on CLIR\ntask.",
          "link": "http://arxiv.org/abs/2109.02789",
          "publishedOn": "2021-09-08T07:20:09.104Z",
          "wordCount": 723,
          "title": "Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1\">Martin Milenkoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "In this paper, we describe a method to tackle data sparsity and create\nrecommendations in domains with limited knowledge about user preferences. We\nexpand the variational autoencoder collaborative filtering from a single-domain\nto a multi-domain setting. The intuition is that user-item interactions in a\nsource domain can augment the recommendation quality in a target domain. The\nintuition can be taken to its extreme, where, in a cross-domain setup, the user\nhistory in a source domain is enough to generate high-quality recommendations\nin a target one. We thus create a Product-of-Experts (POE) architecture for\nrecommendations that jointly models user-item interactions across multiple\ndomains. The method is resilient to missing data for one or more of the\ndomains, which is a situation often found in real life. We present results on\ntwo widely-used datasets - Amazon and Yelp, which support the claim that\nholistic user preference knowledge leads to better recommendations.\nSurprisingly, we find that in some cases, a POE recommender that does not\naccess the target domain user representation can surpass a strong VAE\nrecommender baseline trained on the target domain.",
          "link": "http://arxiv.org/abs/2104.12822",
          "publishedOn": "2021-09-08T07:20:09.092Z",
          "wordCount": 686,
          "title": "Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02867",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qinliang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>",
          "description": "Existing unsupervised document hashing methods are mostly established on\ngenerative models. Due to the difficulties of capturing long dependency\nstructures, these methods rarely model the raw documents directly, but instead\nto model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In\nthis paper, we propose to learn hash codes from BERT embeddings after observing\ntheir tremendous successes on downstream tasks. As a first try, we modify\nexisting generative hashing models to accommodate the BERT embeddings. However,\nlittle improvement is observed over the codes learned from the old BOW or TFIDF\nfeatures. We attribute this to the reconstruction requirement in the generative\nhashing, which will enforce irrelevant information that is abundant in the BERT\nembeddings also compressed into the codes. To remedy this issue, a new\nunsupervised hashing paradigm is further proposed based on the mutual\ninformation (MI) maximization principle. Specifically, the method first\nconstructs appropriate global and local codes from the documents and then seeks\nto maximize their mutual information. Experimental results on three benchmark\ndatasets demonstrate that the proposed method is able to generate hash codes\nthat outperform existing ones learned from BOW features by a substantial\nmargin.",
          "link": "http://arxiv.org/abs/2109.02867",
          "publishedOn": "2021-09-08T07:20:09.079Z",
          "wordCount": 637,
          "title": "Refining BERT Embeddings for Document Hashing via Mutual Information Maximization. (arXiv:2109.02867v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zobaed_S/0/1/0/all/0/1\">Sm Zobaed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabby_M/0/1/0/all/0/1\">Md Fazle Rabby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Md Istiaq Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1\">Ekram Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1\">Sazib Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karim_A/0/1/0/all/0/1\">Asif Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasib_K/0/1/0/all/0/1\">Khan Md. Hasib</a>",
          "description": "The rapid advancement in deep learning makes the differentiation of authentic\nand manipulated facial images and video clips unprecedentedly harder. The\nunderlying technology of manipulating facial appearances through deep\ngenerative approaches, enunciated as DeepFake that have emerged recently by\npromoting a vast number of malicious face manipulation applications.\nSubsequently, the need of other sort of techniques that can assess the\nintegrity of digital visual content is indisputable to reduce the impact of the\ncreations of DeepFake. A large body of research that are performed on DeepFake\ncreation and detection create a scope of pushing each other beyond the current\nstatus. This study presents challenges, research trends, and directions related\nto DeepFake creation and detection techniques by reviewing the notable research\nin the DeepFake domain to facilitate the development of more robust approaches\nthat could deal with the more advance DeepFake in the future.",
          "link": "http://arxiv.org/abs/2109.02874",
          "publishedOn": "2021-09-08T07:20:09.037Z",
          "wordCount": 609,
          "title": "DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03111",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ajwani_R/0/1/0/all/0/1\">Rohan Deepak Ajwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalan_A/0/1/0/all/0/1\">Arshika Lalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_B/0/1/0/all/0/1\">Basabdatta Sen Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_J/0/1/0/all/0/1\">Joy Bose</a>",
          "description": "We present a Spiking Neural Network (SNN) based Sparse Distributed Memory\n(SDM) implemented on the Nengo framework. We have based our work on previous\nwork by Furber et al, 2004, implementing SDM using N-of-M codes. As an integral\npart of the SDM design, we have implemented Correlation Matrix Memory (CMM)\nusing SNN on Nengo. Our SNN implementation uses Leaky Integrate and Fire (LIF)\nspiking neuron models on Nengo. Our objective is to understand how well\nSNN-based SDMs perform in comparison to conventional SDMs. Towards this, we\nhave simulated both conventional and SNN-based SDM and CMM on Nengo. We observe\nthat SNN-based models perform similarly as the conventional ones. In order to\nevaluate the performance of different SNNs, we repeated the experiment using\nAdaptive-LIF, Spiking Rectified Linear Unit, and Izhikevich models and obtained\nsimilar results. We conclude that it is indeed feasible to develop some types\nof associative memories using spiking neurons whose memory capacity and other\nfeatures are similar to the performance without SNNs. Finally we have\nimplemented an application where MNIST images, encoded with N-of-M codes, are\nassociated with their labels and stored in the SNN-based SDM.",
          "link": "http://arxiv.org/abs/2109.03111",
          "publishedOn": "2021-09-08T07:20:08.698Z",
          "wordCount": 654,
          "title": "Sparse Distributed Memory using Spiking Neural Networks on Nengo. (arXiv:2109.03111v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2109.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zeyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Ke Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiaxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_M/0/1/0/all/0/1\">Max L. Wilson</a>",
          "description": "Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.",
          "link": "http://arxiv.org/abs/2109.03039",
          "publishedOn": "2021-09-08T07:20:08.682Z",
          "wordCount": 609,
          "title": "POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02859",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongxu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guandong Xu</a>",
          "description": "User purchasing prediction with multi-behavior information remains a\nchallenging problem for current recommendation systems. Various methods have\nbeen proposed to address it via leveraging the advantages of graph neural\nnetworks (GNNs) or multi-task learning. However, most existing works do not\ntake the complex dependencies among different behaviors of users into\nconsideration. They utilize simple and fixed schemes, like neighborhood\ninformation aggregation or mathematical calculation of vectors, to fuse the\nembeddings of different user behaviors to obtain a unified embedding to\nrepresent a user's behavioral patterns which will be used in downstream\nrecommendation tasks. To tackle the challenge, in this paper, we first propose\nthe concept of hyper meta-path to construct hyper meta-paths or hyper\nmeta-graphs to explicitly illustrate the dependencies among different behaviors\nof a user. How to obtain a unified embedding for a user from hyper meta-paths\nand avoid the previously mentioned limitations simultaneously is critical.\nThanks to the recent success of graph contrastive learning, we leverage it to\nlearn embeddings of user behavior patterns adaptively instead of assigning a\nfixed scheme to understand the dependencies among different behaviors. A new\ngraph contrastive learning based framework is proposed by coupling with hyper\nmeta-paths, namely HMG-CR, which consistently and significantly outperforms all\nbaselines in extensive comparison experiments.",
          "link": "http://arxiv.org/abs/2109.02859",
          "publishedOn": "2021-09-08T07:20:08.641Z",
          "wordCount": 660,
          "title": "Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2108.10750",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gaurav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>",
          "description": "Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.",
          "link": "http://arxiv.org/abs/2108.10750",
          "publishedOn": "2021-09-07T07:20:12.704Z",
          "wordCount": 647,
          "title": "Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02311",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manzoor_A/0/1/0/all/0/1\">Ahtsham Manzoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1\">Dietmar Jannach</a>",
          "description": "Conversational recommender systems have attracted immense attention recently.\nThe most recent approaches rely on neural models trained on recorded dialogs\nbetween humans, implementing an end-to-end learning process. These systems are\ncommonly designed to generate responses given the user's utterances in natural\nlanguage. One main challenge is that these generated responses both have to be\nappropriate for the given dialog context and must be grammatically and\nsemantically correct. An alternative to such generation-based approaches is to\nretrieve responses from pre-recorded dialog data and to adapt them if needed.\nSuch retrieval-based approaches were successfully explored in the context of\ngeneral conversational systems, but have received limited attention in recent\nyears for CRS. In this work, we re-assess the potential of such approaches and\ndesign and evaluate a novel technique for response retrieval and ranking. A\nuser study (N=90) revealed that the responses by our system were on average of\nhigher quality than those of two recent generation-based systems. We\nfurthermore found that the quality ranking of the two generation-based\napproaches is not aligned with the results from the literature, which points to\nopen methodological questions. Overall, our research underlines that\nretrieval-based approaches should be considered an alternative or complement to\nlanguage generation approaches.",
          "link": "http://arxiv.org/abs/2109.02311",
          "publishedOn": "2021-09-07T07:20:10.131Z",
          "wordCount": 635,
          "title": "Towards Retrieval-based Conversational Recommendation. (arXiv:2109.02311v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2010.16413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leaman_R/0/1/0/all/0/1\">Robert Leaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Ling Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chih-Hsuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shankai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>",
          "description": "The COVID-19 pandemic has had a significant impact on society, both because\nof the serious health effects of COVID-19 and because of public health measures\nimplemented to slow its spread. Many of these difficulties are fundamentally\ninformation needs; attempts to address these needs have caused an information\noverload for both researchers and the public. Natural language processing\n(NLP), the branch of artificial intelligence that interprets human language,\ncan be applied to address many of the information needs made urgent by the\nCOVID-19 pandemic. This review surveys approximately 150 NLP studies and more\nthan 50 systems and datasets addressing the COVID-19 pandemic. We detail work\non four core NLP tasks: information retrieval, named entity recognition,\nliterature-based discovery, and question answering. We also describe work that\ndirectly addresses aspects of the pandemic through four additional tasks: topic\nmodeling, sentiment and emotion analysis, caseload forecasting, and\nmisinformation detection. We conclude by discussing observable trends and\nremaining challenges.",
          "link": "http://arxiv.org/abs/2010.16413",
          "publishedOn": "2021-09-07T07:20:10.107Z",
          "wordCount": 731,
          "title": "Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.11899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_H/0/1/0/all/0/1\">Haoyu Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Childs_P/0/1/0/all/0/1\">Peter Childs</a>",
          "description": "To facilitate the knowledge reuse in engineering design, several dataset\napproaches have been proposed and applied by designers. This paper builds a\npatent-based knowledge graph, patent-KG, to represent the knowledge facts in\npatents for engineering design. The arising patent-KG approach proposes a new\nunsupervised mechanism to extract the knowledge facts in patent, by searching\nthe attention graph in language models. This method avoids using expensive\nlabelled data in supervised learning or listing complex syntactic rules in\nrule-based extraction. The extracted entities are compared with other\nbenchmarks and shows a higher coverage of engineering words. The extracted\nrelationships are also compared with other benchmarks, and the result shows\nmeaningful advantages.",
          "link": "http://arxiv.org/abs/2108.11899",
          "publishedOn": "2021-09-07T07:20:09.817Z",
          "wordCount": 585,
          "title": "Patent-KG: A Patent Knowledge Graph for Engineering Design. (arXiv:2108.11899v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.08978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>",
          "description": "Recommender systems research tends to evaluate model performance offline and\non randomly sampled targets, yet the same systems are later used to predict\nuser behavior sequentially from a fixed point in time. Simulating online\nrecommender system performance is notoriously difficult and the discrepancy\nbetween online and offline behaviors is typically not accounted for in offline\nevaluations. This disparity permits weaknesses to go unnoticed until the model\nis deployed in a production setting. In this paper, we first demonstrate how\nomitting temporal context when evaluating recommender system performance leads\nto false confidence. To overcome this, we postulate that offline evaluation\nprotocols can only model real-life use-cases if they account for temporal\ncontext. Next, we propose a training procedure to further embed the temporal\ncontext in existing models. We use a multi-objective approach to introduce\ntemporal context into traditionally time-unaware recommender systems and\nconfirm its advantage via the proposed evaluation protocol. Finally, we\nvalidate that the Pareto Fronts obtained with the added objective dominate\nthose produced by state-of-the-art models that are only optimized for accuracy\non three real-world publicly available datasets. The results show that\nincluding our temporal objective can improve recall@20 by up to 20%.",
          "link": "http://arxiv.org/abs/2009.08978",
          "publishedOn": "2021-09-07T07:20:09.802Z",
          "wordCount": 716,
          "title": "Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seungjae Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Young-Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Hanock Kwak</a>",
          "description": "Temporal set prediction is becoming increasingly important as many companies\nemploy recommender systems in their online businesses, e.g., personalized\npurchase prediction of shopping baskets. While most previous techniques have\nfocused on leveraging a user's history, the study of combining it with others'\nhistories remains untapped potential. This paper proposes Global-Local Item\nEmbedding (GLOIE) that learns to utilize the temporal properties of sets across\nwhole users as well as within a user by coining the names as global and local\ninformation to distinguish the two temporal patterns. GLOIE uses Variational\nAutoencoder (VAE) and dynamic graph-based model to capture global and local\ninformation and then applies attention to integrate resulting item embeddings.\nAdditionally, we propose to use Tweedie output for the decoder of VAE as it can\neasily model zero-inflated and long-tailed distribution, which is more suitable\nfor several real-world data distributions than Gaussian or multinomial\ncounterparts. When evaluated on three public benchmarks, our algorithm\nconsistently outperforms previous state-of-the-art methods in most ranking\nmetrics.",
          "link": "http://arxiv.org/abs/2109.02074",
          "publishedOn": "2021-09-07T07:20:09.753Z",
          "wordCount": 621,
          "title": "Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2108.13875",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>",
          "description": "Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.",
          "link": "http://arxiv.org/abs/2108.13875",
          "publishedOn": "2021-09-07T07:20:09.740Z",
          "wordCount": 577,
          "title": "When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2108.08468",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Danqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Chen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tony Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yiwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiang Yang</a>",
          "description": "We study the problem of query attribute value extraction, which aims to\nidentify named entities from user queries as diverse surface form attribute\nvalues and afterward transform them into formally canonical forms. Such a\nproblem consists of two phases: {named entity recognition (NER)} and {attribute\nvalue normalization (AVN)}. However, existing works only focus on the NER phase\nbut neglect equally important AVN. To bridge this gap, this paper proposes a\nunified query attribute value extraction system in e-commerce search named\nQUEACO, which involves both two phases. Moreover, by leveraging large-scale\nweakly-labeled behavior data, we further improve the extraction performance\nwith less supervision cost. Specifically, for the NER phase, QUEACO adopts a\nnovel teacher-student network, where a teacher network that is trained on the\nstrongly-labeled data generates pseudo-labels to refine the weakly-labeled data\nfor training a student network. Meanwhile, the teacher network can be\ndynamically adapted by the feedback of the student's performance on\nstrongly-labeled data to maximally denoise the noisy supervisions from the weak\nlabels. For the AVN phase, we also leverage the weakly-labeled\nquery-to-attribute behavior data to normalize surface form attribute values\nfrom queries into canonical forms from products. Extensive experiments on a\nreal-world large-scale E-commerce dataset demonstrate the effectiveness of\nQUEACO.",
          "link": "http://arxiv.org/abs/2108.08468",
          "publishedOn": "2021-09-07T07:20:09.729Z",
          "wordCount": 750,
          "title": "QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.00818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1\">Vito Walter Anelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellogin_A/0/1/0/all/0/1\">Alejandro Bellog&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1\">Tommaso Di Noia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_F/0/1/0/all/0/1\">Francesco Maria Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paparella_V/0/1/0/all/0/1\">Vincenzo Paparella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomo_C/0/1/0/all/0/1\">Claudio Pomo</a>",
          "description": "Explainable Recommendation has attracted a lot of attention due to a renewed\ninterest in explainable artificial intelligence. In particular, post-hoc\napproaches have proved to be the most easily applicable ones to increasingly\ncomplex recommendation models, which are then treated as black-boxes. The most\nrecent literature has shown that for post-hoc explanations based on local\nsurrogate models, there are problems related to the robustness of the approach\nitself. This consideration becomes even more relevant in human-related tasks\nlike recommendation. The explanation also has the arduous task of enhancing\nincreasingly relevant aspects of user experience such as transparency or\ntrustworthiness. This paper aims to show how the characteristics of a classical\npost-hoc model based on surrogates is strongly model-dependent and does not\nprove to be accountable for the explanations generated.",
          "link": "http://arxiv.org/abs/2109.00818",
          "publishedOn": "2021-09-07T07:20:09.704Z",
          "wordCount": 597,
          "title": "Adherence and Constancy in LIME-RS Explanations for Recommendation. (arXiv:2109.00818v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>",
          "description": "Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ.",
          "link": "http://arxiv.org/abs/2104.07858",
          "publishedOn": "2021-09-07T07:20:09.688Z",
          "wordCount": 625,
          "title": "Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.02202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dori_Hacohen_S/0/1/0/all/0/1\">Shiri Dori-Hacohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montenegro_R/0/1/0/all/0/1\">Roberto Montenegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_K/0/1/0/all/0/1\">Keen Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_M/0/1/0/all/0/1\">Michela Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edwards_Johnson_J/0/1/0/all/0/1\">Jennifer Edwards-Johnson</a>",
          "description": "Most Fairness in AI research focuses on exposing biases in AI systems. A\nbroader lens on fairness reveals that AI can serve a greater aspiration:\nrooting out societal inequities from their source. Specifically, we focus on\ninequities in health information, and aim to reduce bias in that domain using\nAI. The AI algorithms under the hood of search engines and social media, many\nof which are based on recommender systems, have an outsized impact on the\nquality of medical and health information online. Therefore, embedding bias\ndetection and reduction into these recommender systems serving up medical and\nhealth content online could have an outsized positive impact on patient\noutcomes and wellbeing.\n\nIn this position paper, we offer the following contributions: (1) we propose\na novel framework of Fairness via AI, inspired by insights from medical\neducation, sociology and antiracism; (2) we define a new term, bisinformation,\nwhich is related to, but distinct from, misinformation, and encourage\nresearchers to study it; (3) we propose using AI to study, detect and mitigate\nbiased, harmful, and/or false health information that disproportionately hurts\nminority groups in society; and (4) we suggest several pillars and pose several\nopen problems in order to seed inquiry in this new space. While part (3) of\nthis work specifically focuses on the health domain, the fundamental computer\nscience advances and contributions stemming from research efforts in bias\nreduction and Fairness via AI have broad implications in all areas of society.",
          "link": "http://arxiv.org/abs/2109.02202",
          "publishedOn": "2021-09-07T07:20:09.676Z",
          "wordCount": 714,
          "title": "Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2108.05069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>",
          "description": "Question Answering (QA), a popular and promising technique for intelligent\ninformation access, faces a dilemma about data as most other AI techniques. On\none hand, modern QA methods rely on deep learning models which are typically\ndata-hungry. Therefore, it is expected to collect and fuse all the available QA\ndatasets together in a common site for developing a powerful QA model. On the\nother hand, real-world QA datasets are typically distributed in the form of\nisolated islands belonging to different parties. Due to the increasing\nawareness of privacy security, it is almost impossible to integrate the data\nscattered around, or the cost is prohibited. A possible solution to this\ndilemma is a new approach known as federated learning, which is a\nprivacy-preserving machine learning technique over distributed datasets. In\nthis work, we propose to adopt federated learning for QA with the special\nconcern on the statistical heterogeneity of the QA data. Here the heterogeneity\nrefers to the fact that annotated QA data are typically with non-identical and\nindependent distribution (non-IID) and unbalanced sizes in practice.\nTraditional federated learning methods may sacrifice the accuracy of individual\nmodels under the heterogeneous situation. To tackle this problem, we propose a\nnovel Federated Matching framework for QA, named FedMatch, with a\nbackbone-patch architecture. The shared backbone is to distill the common\nknowledge of all the participants while the private patch is a compact and\nefficient module to retain the domain information for each participant. To\nfacilitate the evaluation, we build a benchmark collection based on several QA\ndatasets from different domains to simulate the heterogeneous situation in\npractice. Empirical studies demonstrate that our model can achieve significant\nimprovements against the baselines over all the datasets.",
          "link": "http://arxiv.org/abs/2108.05069",
          "publishedOn": "2021-09-07T07:20:09.659Z",
          "wordCount": 757,
          "title": "FedMatch: Federated Learning Over Heterogeneous Question Answering Data. (arXiv:2108.05069v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2109.01815",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Casper Hansen</a>",
          "description": "How data is represented and operationalized is critical for building\ncomputational solutions that are both effective and efficient. A common\napproach is to represent data objects as binary vectors, denoted \\textit{hash\ncodes}, which require little storage and enable efficient similarity search\nthrough direct indexing into a hash table or through similarity computations in\nan appropriate space. Due to the limited expressibility of hash codes, compared\nto real-valued representations, a core open challenge is how to generate hash\ncodes that well capture semantic content or latent properties using a small\nnumber of bits, while ensuring that the hash codes are distributed in a way\nthat does not reduce their search efficiency. State of the art methods use\nrepresentation learning for generating such hash codes, focusing on neural\nautoencoder architectures where semantics are encoded into the hash codes by\nlearning to reconstruct the original inputs of the hash codes. This thesis\naddresses the above challenge and makes a number of contributions to\nrepresentation learning that (i) improve effectiveness of hash codes through\nmore expressive representations and a more effective similarity measure than\nthe current state of the art, namely the Hamming distance, and (ii) improve\nefficiency of hash codes by learning representations that are especially suited\nto the choice of search method. The contributions are empirically validated on\nseveral tasks related to similarity search and recommendation.",
          "link": "http://arxiv.org/abs/2109.01815",
          "publishedOn": "2021-09-07T07:20:09.645Z",
          "wordCount": 686,
          "title": "Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yankai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiangchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "To alleviate data sparsity and cold-start problems of traditional recommender\nsystems (RSs), incorporating knowledge graphs (KGs) to supplement auxiliary\ninformation has attracted considerable attention recently. However, simply\nintegrating KGs in current KG-based RS models is not necessarily a guarantee to\nimprove the recommendation performance, which may even weaken the holistic\nmodel capability. This is because the construction of these KGs is independent\nof the collection of historical user-item interactions; hence, information in\nthese KGs may not always be helpful for recommendation to all users.\n\nIn this paper, we propose attentive Knowledge-aware Graph convolutional\nnetworks with Collaborative Guidance for personalized Recommendation (CG-KGR).\nCG-KGR is a novel knowledge-aware recommendation model that enables ample and\ncoherent learning of KGs and user-item interactions, via our proposed\nCollaborative Guidance Mechanism. Specifically, CG-KGR first encapsulates\nhistorical interactions to interactive information summarization. Then CG-KGR\nutilizes it as guidance to extract information out of KGs, which eventually\nprovides more precise personalized recommendation. We conduct extensive\nexperiments on four real-world datasets over two recommendation tasks, i.e.,\nTop-K recommendation and Click-Through rate (CTR) prediction. The experimental\nresults show that the CG-KGR model significantly outperforms recent\nstate-of-the-art models by 4.0-53.2% and 0.4-3.2%, in terms of Recall metric on\nTop-K recommendation and AUC on CTR prediction, respectively.",
          "link": "http://arxiv.org/abs/2109.02046",
          "publishedOn": "2021-09-07T07:20:09.622Z",
          "wordCount": 657,
          "title": "Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Recommendation. (arXiv:2109.02046v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02022",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Deepak Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1\">Bijendra Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chand_S/0/1/0/all/0/1\">Satish Chand</a>",
          "description": "The aim of this paper is to uncover the researchers in machine learning using\nthe author-topic model (ATM). We collect 16,855 scientific papers from six top\njournals in the field of machine learning published from 1997 to 2016 and\nanalyze them using ATM. The dataset is broken down into 4 intervals to identify\nthe top researchers and find similar researchers using their similarity score.\nThe similarity score is calculated using Hellinger distance. The researchers\nare plotted using t-SNE, which reduces the dimensionality of the data while\nkeeping the same distance between the points. The analysis of our study helps\nthe upcoming researchers to find the top researchers in their area of interest.",
          "link": "http://arxiv.org/abs/2109.02022",
          "publishedOn": "2021-09-07T07:20:09.573Z",
          "wordCount": 561,
          "title": "Recommending Researchers in Machine Learning based on Author-Topic Model. (arXiv:2109.02022v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2109.01732",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Benjamin Charles Germain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baco_J/0/1/0/all/0/1\">Joshua Ortiz Baco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salter_S/0/1/0/all/0/1\">Sarah H. Salter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casey_J/0/1/0/all/0/1\">Jim Casey</a>",
          "description": "This paper presents a computational method of analysis that draws from\nmachine learning, library science, and literary studies to map the visual\nlayouts of multi-ethnic newspapers from the late 19th and early 20th century\nUnited States. This work departs from prior approaches to newspapers that focus\non individual pieces of textual and visual content. Our method combines\nChronicling America's MARC data and the Newspaper Navigator machine learning\ndataset to identify the visual patterns of newspaper page layouts. By analyzing\nhigh-dimensional visual similarity, we aim to better understand how editors\nspoke and protested through the layout of their papers.",
          "link": "http://arxiv.org/abs/2109.01732",
          "publishedOn": "2021-09-07T07:20:09.554Z",
          "wordCount": 571,
          "title": "Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2109.02160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arnab Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heger_A/0/1/0/all/0/1\">Andrew Heger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+England_D/0/1/0/all/0/1\">Darin England</a>",
          "description": "In this article, we propose a systematic approach for fire station location\nplanning. We develop a machine learning model, based on Random Forest, for\ndemand prediction and utilize the model further to define a generalized index\nto measure quality of fire service in urban settings. Our model is built upon\nspatial data collected from multiple different sources. Efficacy of proper\nfacility planning depends on choice of candidates where fire stations can be\nlocated along with existing stations, if any. Also, the travel time from these\ncandidates to demand locations need to be taken care of to maintain fire safety\nstandard. Here, we propose a travel time based clustering technique to identify\nsuitable candidates. Finally, we develop an optimization problem to select best\nlocations to install new fire stations. Our optimization problem is built upon\nmaximum coverage problem, based on integer programming. We present a detailed\nexperimental study of our proposed approach in collaboration with city of\nVictoria Fire Department, MN, USA. Our demand prediction model achieves true\npositive rate of 70% and false positive rate of 22% approximately. We aid\nVictoria Fire Department to select a location for a new fire station using our\napproach. We present detailed results on improvement statistics by locating a\nnew facility, as suggested by our methodology, in the city of Victoria.",
          "link": "http://arxiv.org/abs/2109.02160",
          "publishedOn": "2021-09-07T07:20:09.510Z",
          "wordCount": 677,
          "title": "Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}