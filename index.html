<!DOCTYPE html>
<html lang="en">

<head>
  <title>osmos::feed</title>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="noindex, nofollow" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
  <link rel="alternate" type="application/rss+xml" title="osmos::feed" href="feed.atom" />
  <link href="index.css" rel="stylesheet" />
  <!-- %before-head-end.html% -->
</head>

<body>
<!-- %after-body-begin.html% -->
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-08">2021-09-08</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An individual&#x27;s variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03171">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent work on opinion summarization produces general summaries based on a
set of input reviews and the popularity of opinions expressed in them. In this
paper, we propose an approach that allows the generation of customized
summaries based on aspect queries (e.g., describing the location and room of a
hotel). Using a review corpus, we create a synthetic training dataset of
(review, summary) pairs enriched with aspect controllers which are induced by a
multi-instance learning model that predicts the aspects of a document at
different levels of granularity. We fine-tune a pretrained model using our
synthetic dataset and generate aspect-specific summaries by modifying the
aspect controllers. Experiments on two benchmarks show that our model
outperforms the previous state of the art and generates personalized summaries
by controlling the number of aspects discussed in them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03094">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The availability of language representations learned by large pretrained
neural network models (such as BERT and ELECTRA) has led to improvements in
many downstream Natural Language Processing tasks in recent years. Pretrained
models usually differ in pretraining objectives, architectures, and datasets
they are trained on which can affect downstream performance. In this
contribution, we fine-tuned German BERT and German ELECTRA models to identify
toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)
in Facebook data provided by the GermEval 2021 competition. We created
ensembles of these models and investigated whether and how classification
performance depends on the number of ensemble members and their composition. On
out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for
all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,
respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02995">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the most popular methods for context-aware machine translation (MT) is
to use separate encoders for the source sentence and context as multiple
sources for one target sentence. Recent work has cast doubt on whether these
models actually learn useful signals from the context or are improvements in
automatic evaluation metrics just a side-effect. We show that multi-source
transformer models improve MT over standard transformer-base models even with
empty lines provided as context, but the translation quality improves
significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is
provided. We also show that even though randomly shuffling in-domain context
can also improve over baselines, the correct context further improves
translation quality and random out-of-domain context further degrades it.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Over the years customers&#x27; expectation of getting information instantaneously
has given rise to the increased usage of channels like virtual assistants.
Typically, customers try to get their questions answered by low-touch channels
like search and virtual assistant first, before getting in touch with a live
chat agent or the phone representative. Higher usage of these low-touch systems
is a win-win for both customers and the organization since it enables
organizations to attain a low cost of service while customers get served
without delay. In this paper, we propose a two-part framework where the first
part describes methods to combine the information from different interaction
channels like call, search, and chat. We do this by summarizing (using a
stacked Bi-LSTM network) the high-touch interaction channel data such as call
and chat into short searchquery like customer intents and then creating an
organically grown intent taxonomy from interaction data (using Hierarchical
Agglomerative Clustering). The second part of the framework focuses on
extracting customer questions by analyzing interaction data sources. It
calculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It
also maps these identified questions to the output of the first part of the
framework using syntactic and semantic similarity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business&#x27;s financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03881">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities&#x27; stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities&#x27; stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13140">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep learning models have greatly improved the performance of most
artificial intelligence tasks, they are often criticized to be untrustworthy
due to the black-box problem. Consequently, many works have been proposed to
study the trustworthiness of deep learning. However, as most open datasets are
designed for evaluating the accuracy of model outputs, there is still a lack of
appropriate datasets for evaluating the inner workings of neural networks. The
lack of datasets obviously hinders the development of trustworthiness research.
Therefore, in order to systematically evaluate the factors for building
trustworthy systems, we propose a novel and well-annotated sentiment analysis
dataset to evaluate robustness and interpretability. To evaluate these factors,
our dataset contains diverse annotations about the challenging distribution of
instances, manual adversarial instances and sentiment explanations. Several
evaluation metrics are further proposed for interpretability and robustness.
Based on the dataset and metrics, we conduct comprehensive comparisons for the
trustworthiness of three typical models, and also study the relations between
accuracy, robustness and interpretability. We release this trustworthiness
evaluation dataset at \url{https://github/xyz} and hope our work can facilitate
the progress on building more trustworthy systems for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00904">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce MULTI-EURLEX, a new multilingual dataset for topic
classification of legal documents. The dataset comprises 65k European Union
(EU) laws, officially translated in 23 languages, annotated with multiple
labels from the EUROVOC taxonomy. We highlight the effect of temporal concept
drift and the importance of chronological, instead of random splits. We use the
dataset as a testbed for zero-shot cross-lingual transfer, where we exploit
annotated training documents in one language (source) to classify documents in
another language (target). We find that fine-tuning a multilingually pretrained
model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic
forgetting of multilingual knowledge and, consequently, poor zero-shot transfer
to other languages. Adaptation strategies, namely partial fine-tuning,
adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new
end-tasks, help retain multilingual knowledge from pretraining, substantially
improving zero-shot cross-lingual transfer, but their impact also depends on
the pretrained model used and the size of the label set.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03121">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Simultaneous neural machine translation(SNMT) models start emitting the
target sequence before they have processed the source sequence. The recent
adaptive policies for SNMT use monotonic attention to perform read/write
decisions based on the partial source and target sequences. The lack of
sufficient information might cause the monotonic attention to take poor
read/write decisions, which in turn negatively affects the performance of the
SNMT model. On the other hand, human translators make better read/write
decisions since they can anticipate the immediate future words using linguistic
information and domain knowledge.Motivated by human translators, in this work,
we propose a framework to aid monotonic attention with an external language
model to improve its decisions.We conduct experiments on the MuST-C
English-German and English-French speech-to-text translation tasks to show the
effectiveness of the proposed framework.The proposed SNMT method improves the
quality-latency trade-off over the state-of-the-art monotonic multihead
attention.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02403">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Aspect-level sentiment classification (ALSC) aims at identifying the
sentiment polarity of a specified aspect in a sentence. ALSC is a practical
setting in aspect-based sentiment analysis due to no opinion term labeling
needed, but it fails to interpret why a sentiment polarity is derived for the
aspect. To address this problem, recent works fine-tune pre-trained Transformer
encoders for ALSC to extract an aspect-centric dependency tree that can locate
the opinion words. However, the induced opinion words only provide an intuitive
cue far below human-level interpretability. Besides, the pre-trained encoder
tends to internalize an aspect&#x27;s intrinsic sentiment, causing sentiment bias
and thus affecting model performance. In this paper, we propose a span-based
anti-bias aspect representation learning framework. It first eliminates the
sentiment bias in the aspect embedding by adversarial learning against aspects&#x27;
prior sentiment. Then, it aligns the distilled opinion candidates with the
aspect by span-based dependency modeling to highlight the interpretable opinion
terms. Our method achieves new state-of-the-art performance on five benchmarks,
with the capability of unsupervised opinion extraction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people&#x27;s perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers-based pretrained language models achieve outstanding results in
many well-known NLU benchmarks. However, while pretraining methods are very
convenient, they are expensive in terms of time and resources. This calls for a
study of the impact of pretraining data size on the knowledge of the models. We
explore this impact on the syntactic capabilities of RoBERTa, using models
trained on incremental sizes of raw text data. First, we use syntactic
structural probes to determine whether models pretrained on more data encode a
higher amount of syntactic information. Second, we perform a targeted syntactic
evaluation to analyze the impact of pretraining data size on the syntactic
generalization performance of the models. Third, we compare the performance of
the different models on three downstream applications: part-of-speech tagging,
dependency parsing and paraphrase identification. We complement our study with
an analysis of the cost-benefit trade-off of training such models. Our
experiments show that while models pretrained on more data encode more
syntactic knowledge and perform better on downstream applications, they do not
always offer a better performance across the different syntactic phenomena and
come at a higher financial and environmental cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous work in slogan generation focused on utilising slogan skeletons
mined from existing slogans. While some generated slogans can be catchy, they
are often not coherent with the company&#x27;s focus or style across their marketing
communications because the skeletons are mined from other companies&#x27; slogans.
We propose a sequence-to-sequence (seq2seq) transformer model to generate
slogans from a brief company description. A naive seq2seq model fine-tuned for
slogan generation is prone to introducing false information. We use company
name delexicalisation and entity masking to alleviate this problem and improve
the generated slogans&#x27; quality and truthfulness. Furthermore, we apply
conditional training based on the first words&#x27; POS tag to generate
syntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score
of 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that
our method generates significantly more factual, diverse and catchy slogans
than strong LSTM and transformer seq2seq baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03598">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Intersectional bias is a bias caused by an overlap of multiple social factors
like gender, sexuality, race, disability, religion, etc. A recent study has
shown that word embedding models can be laden with biases against
intersectional groups like African American females, etc. The first step
towards tackling such intersectional biases is to identify them. However,
discovering biases against different intersectional groups remains a
challenging task. In this work, we present WordBias, an interactive visual tool
designed to explore biases against intersectional groups encoded in static word
embeddings. Given a pretrained static word embedding, WordBias computes the
association of each word along different groups based on race, age, etc. and
then visualizes them using a novel interactive interface. Using a case study,
we demonstrate how WordBias can help uncover biases against intersectional
groups like Black Muslim Males, Poor Females, etc. encoded in word embedding.
In addition, we also evaluate our tool using qualitative feedback from expert
interviews. The source code for this tool can be publicly accessed for
reproducibility at github.com/bhavyaghai/WordBias.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generating paragraphs of diverse contents is important in many applications.
Existing generation models produce similar contents from homogenized contexts
due to the fixed left-to-right sentence order. Our idea is permuting the
sentence orders to improve the content diversity of multi-sentence paragraph.
We propose a novel framework PermGen whose objective is to maximize the
expected log-likelihood of output paragraph distributions with respect to all
possible sentence orders. PermGen uses hierarchical positional embedding and
designs new procedures for training, decoding, and candidate ranking in the
sentence-permuted generation. Experiments on three paragraph generation
benchmarks demonstrate PermGen generates more diverse outputs with a higher
quality than existing models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07555">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>QuestEval is a reference-less metric used in text-to-text tasks, that
compares the generated summaries directly to the source text, by automatically
asking and answering questions. Its adaptation to Data-to-Text tasks is not
straightforward, as it requires multimodal Question Generation and Answering
systems on the considered tasks, which are seldom available. To this purpose,
we propose a method to build synthetic multimodal corpora enabling to train
multimodal components for a data-QuestEval metric. The resulting metric is
reference-less and multimodal; it obtains state-of-the-art correlations with
human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval&#x27;s
code and models available for reproducibility purpose, as part of the QuestEval
project.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The semantic understanding of natural dialogues composes of several parts.
Some of them, like intent classification and entity detection, have a crucial
role in deciding the next steps in handling user input. Handling each task as
an individual problem can be wasting of training resources, and also each
problem can benefit from each other. This paper tackles these problems as one.
Our new model, which combine intent and entity recognition into one system, is
achieving better metrics in both tasks with lower training requirements than
solving each task separately. We also optimize the model based on the inputs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03199">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversation disentanglement aims to separate intermingled messages into
detached sessions, which is a fundamental task in understanding multi-party
conversations. Existing work on conversation disentanglement relies heavily
upon human-annotated datasets, which are expensive to obtain in practice. In
this work, we explore to train a conversation disentanglement model without
referencing any human annotations. Our method is built upon a deep co-training
algorithm, which consists of two neural networks: a message-pair classifier and
a session classifier. The former is responsible for retrieving local relations
between two messages while the latter categorizes a message to a session by
capturing context-aware information. Both networks are initialized respectively
with pseudo data built from an unannotated corpus. During the deep co-training
process, we use the session classifier as a reinforcement learning component to
learn a session assigning policy by maximizing the local rewards given by the
message-pair classifier. For the message-pair classifier, we enrich its
training data by retrieving message pairs with high confidence from the
disentangled sessions predicted by the session classifier. Experimental results
on the large Movie Dialogue Dataset demonstrate that our proposed approach
achieves competitive performance compared to the previous supervised methods.
Further experiments show that the predicted disentangled conversations can
promote the performance on the downstream task of multi-party response
selection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Practical dialogue systems require robust methods of detecting out-of-scope
(OOS) utterances to avoid conversational breakdowns and related failure modes.
Directly training a model with labeled OOS examples yields reasonable
performance, but obtaining such data is a resource-intensive process. To tackle
this limited-data problem, previous methods focus on better modeling the
distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal
technique that augments existing data to train better OOS detectors operating
in low-data regimes. GOLD generates pseudo-labeled candidates using samples
from an auxiliary dataset and keeps only the most beneficial candidates for
training through a novel filtering mechanism. In experiments across three
target benchmarks, the top GOLD model outperforms all existing methods on all
key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median
baseline performance. We also analyze the unique properties of OOS data to
identify key factors for optimally applying our proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03009">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, large pre-trained neural language models have attained remarkable
performance on many downstream natural language processing (NLP) applications
via fine-tuning. In this paper, we target at how to further improve the token
representations on the language models. We, therefore, propose a simple yet
effective plug-and-play module, Sequential Attention Module (SAM), on the token
embeddings learned from a pre-trained language model. Our proposed SAM consists
of two main attention modules deployed sequentially: Feature-wise Attention
Module (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can
effectively identify the importance of features at each dimension and promote
the effect via dot-product on the original token embeddings for downstream NLP
applications. Meanwhile, TAM can further re-weight the features at the
token-wise level. Moreover, we propose an adaptive filter on FAM to prevent
noise impact and increase information absorption. Finally, we conduct extensive
experiments to demonstrate the advantages and properties of our proposed SAM.
We first show how SAM plays a primary role in the champion solution of two
subtasks of SemEval&#x27;21 Task 7. After that, we apply SAM on sentiment analysis
and three popular NLP tasks and demonstrate that SAM consistently outperforms
the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03175">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Differential privacy provides a formal approach to privacy of individuals.
Applications of differential privacy in various scenarios, such as protecting
users&#x27; original utterances, must satisfy certain mathematical properties. Our
contribution is a formal analysis of ADePT, a differentially private
auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising
results on downstream tasks while providing tight privacy guarantees. Our proof
reveals that ADePT is not differentially private, thus rendering the
experimental results unsubstantiated. We also quantify the impact of the error
in its private mechanism, showing that the true sensitivity is higher by at
least factor 6 in an optimistic case of a very small encoder&#x27;s dimension and
that the amount of utterances that are not privatized could easily reach 100%
of the entire dataset. Our intention is neither to criticize the authors, nor
the peer-reviewing process, but rather point out that if differential privacy
applications in NLP rely on formal guarantees, these should be outlined in full
and put under detailed scrutiny.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, &amp; Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02966">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we describe the methods we used for our submissions to the
GermEval 2021 shared task on the identification of toxic, engaging, and
fact-claiming comments. For all three subtasks we fine-tuned freely available
transformer-based models from the Huggingface model hub. We evaluated the
performance of various pre-trained models after fine-tuning on 80% of the
training data with different hyperparameters and submitted predictions of the
two best performing resulting models. We found that this approach worked best
for subtask 3, for which we achieved an F1-score of 0.736.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03127">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite advances in neural network language model, the representation
degeneration problem of embeddings is still challenging. Recent studies have
found that the learned output embeddings are degenerated into a narrow-cone
distribution which makes the similarity between each embeddings positive. They
analyzed the cause of the degeneration problem has been demonstrated as common
to most embeddings. However, we found that the degeneration problem is
especially originated from the training of embeddings of rare words. In this
study, we analyze the intrinsic mechanism of the degeneration of rare word
embeddings with respect of their gradient about the negative log-likelihood
loss function. Furthermore, we theoretically and empirically demonstrate that
the degeneration of rare word embeddings causes the degeneration of non-rare
word embeddings, and that the overall degeneration problem can be alleviated by
preventing the degeneration of rare word embeddings. Based on our analyses, we
propose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the
degeneration problem. Experimental results demonstrate the effectiveness of the
proposed method qualitatively and quantitatively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generate &amp; Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03034">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Math word problem (MWP) is a challenging and critical task in natural
language processing. Many recent studies formalize MWP as a generation task and
have adopted sequence-to-sequence models to transform problem descriptions to
mathematical expressions. However, mathematical expressions are prone to minor
mistakes while the generation objective does not explicitly handle such
mistakes. To address this limitation, we devise a new ranking task for MWP and
propose Generate &amp; Rank, a multi-task framework based on a generative
pre-trained language model. By joint training with generation and ranking, the
model learns from its own mistakes and is able to distinguish between correct
and incorrect expressions. Meanwhile, we perform tree-based disturbance
specially designed for MWP and an online update to boost the ranker. We
demonstrate the effectiveness of our proposed method on the benchmark and the
results show that our method consistently outperforms baselines in all
datasets. Particularly, in the classical Math23k, our method is 7% (78.4%
$\rightarrow$ 85.4%) higher than the state-of-the-art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Advertising is an important revenue source for many companies. However, it is
expensive to manually create advertisements that meet the needs of various
queries for massive items. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisements for different queries with various needs given the item
keywords. In this task, for many different queries there is only one general
purposed advertisement with no predefined query-advertisement pair, which would
discourage traditional End-to-End models from generating query-variant
advertisements for different queries with different needs. To deal with the
problem, we propose a query-variant advertisement text generation model that
takes keywords and associated external knowledge as input during training and
adds different queries during inference. Adding external knowledge helps the
model adapted to the information besides the item keywords during training,
which makes the transition between training and inference more smoothing when
the query is added during inference. Both automatic and human evaluation show
that our model can generate more attractive and query-focused advertisements
than the strong baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03062">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multitask deep learning has been applied to patient outcome prediction from
text, taking clinical notes as input and training deep neural networks with a
joint loss function of multiple tasks. However, the joint training scheme of
multitask learning suffers from inter-task interference, and diagnosis
prediction among the multiple tasks has the generalizability issue due to rare
diseases or unseen diagnoses. To solve these challenges, we propose a
hypernetwork-based approach that generates task-conditioned parameters and
coefficients of multitask prediction heads to learn task-specific prediction
and balance the multitask learning. We also incorporate semantic task
information to improves the generalizability of our task-conditioned multitask
model. Experiments on early and discharge notes extracted from the real-world
MIMIC database show our method can achieve better performance on multitask
patient outcome prediction than strong baselines in most cases. Besides, our
method can effectively handle the scenario with limited information and improve
zero-shot prediction on unseen diagnosis categories.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02938">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15455">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper explores augmenting monolingual data for knowledge distillation in
neural machine translation. Source language monolingual text can be
incorporated as a forward translation. Interestingly, we find the best way to
incorporate target language monolingual text is to translate it to the source
language and round-trip translate it back to the target language, resulting in
a fully synthetic corpus. We find that combining monolingual data from both
source and target languages yields better performance than a corpus twice as
large only in one language. Moreover, experiments reveal that the improvement
depends upon the provenance of the test set. If the test set was originally in
the source language (with the target side written by translators), then forward
translating source monolingual data matters. If the test set was originally in
the target language (with the source written by translators), then
incorporating target monolingual data matters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02905">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel Chain Guided Retriever-reader ({\tt CGR}) framework to
model the reasoning chain for multi-hop Science Question Answering. Our
framework is capable of performing explainable reasoning without the need of
any corpus-specific annotations, such as the ground-truth reasoning chain, or
human-annotated entity mentions. Specifically, we first generate reasoning
chains from a semantic graph constructed by Abstract Meaning Representation of
retrieved evidence facts. A \textit{Chain-aware loss}, concerning both local
and global chain information, is also designed to enable the generated chains
to serve as distant supervision signals for training the retriever, where
reinforcement learning is also adopted to maximize the utility of the reasoning
chains. Our framework allows the retriever to capture step-by-step clues of the
entire reasoning process, which is not only shown to be effective on two
challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,
but also favors explainability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present IndicBART, a multilingual, sequence-to-sequence
pre-trained model focusing on 11 Indic languages and English. Different from
existing pre-trained models, IndicBART utilizes the orthographic similarity
between Indic scripts to improve transfer learning between similar Indic
languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation
(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs
and extreme summarization for 7 languages using multilingual fine-tuning show
that IndicBART is competitive with or better than mBART50 despite containing
significantly fewer parameters. Our analyses focus on identifying the impact of
script unification (to Devanagari), corpora size as well as multilingualism on
the final performance. The IndicBART model is available under the MIT license
at https://indicnlp.ai4bharat.org/indic-bart .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Online hate speech has caught everyone&#x27;s attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity&#x27;s identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03004">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>One challenge for dialogue agents is to recognize feelings of the
conversation partner and respond accordingly. In this work, RoBERTa-GPT2 is
proposed for empathetic dialogue generation, where the pre-trained
auto-encoding RoBERTa is utilised as encoder and the pre-trained
auto-regressive GPT-2 as decoder. With the combination of the pre-trained
RoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.
To enable the empathetic ability of RoBERTa-GPT2 model, we propose a
commonsense knowledge and emotional concepts extractor, in which the
commonsensible and emotional concepts of dialogue context are extracted for the
GPT-2 decoder. The experiment results demonstrate that the empathetic dialogue
generation benefits from both pre-trained encoder-decoder architecture and
external knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Don&#x27;t Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper introduces a novel approach to learn visually grounded meaning
representations of words as low-dimensional node embeddings on an underlying
graph hierarchy. The lower level of the hierarchy models modality-specific word
representations through dedicated but communicating graphs, while the higher
level puts these representations together on a single graph to learn a
representation jointly from both modalities. The topology of each graph models
similarity relations among words, and is estimated jointly with the graph
embedding. The assumption underlying this model is that words sharing similar
meaning correspond to communities in an underlying similarity graph in a
low-dimensional space. We named this model Hierarchical Multi-Modal Similarity
Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE
to simulate human similarity judgements and concept categorization,
outperforming the state of the art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02882">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human-designed rules are widely used to build industry applications. However,
it is infeasible to maintain thousands of such hand-crafted rules. So it is
very important to integrate the rule knowledge into neural networks to build a
hybrid model that achieves better performance. Specifically, the human-designed
rules are formulated as Regular Expressions (REs), from which the equivalent
Minimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to
use the MDFA as an intermediate model to capture the matched RE patterns as
rule-based features for each input sentence and introduce these additional
features into neural networks. We evaluate the proposed method on the ATIS
intent classification task. The experiment results show that the proposed
method achieves the best performance compared to neural networks and four other
methods that combine REs and neural networks when the training dataset is
relatively small.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02950">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a new paradigm for paraphrase generation by
treating the task as unsupervised machine translation (UMT) based on the
assumption that there must be pairs of sentences expressing the same meaning in
a large-scale unlabeled monolingual corpus. The proposed paradigm first splits
a large unlabeled corpus into multiple clusters, and trains multiple UMT models
using pairs of these clusters. Then based on the paraphrase pairs produced by
these UMT models, a unified surrogate model can be trained to serve as the
final Seq2Seq model to generate paraphrases, which can be directly used for
test in the unsupervised setup, or be finetuned on labeled datasets in the
supervised setup. The proposed method offers merits over
machine-translation-based paraphrase generation methods, as it avoids reliance
on bilingual sentence pairs. It also allows human intervene with the model so
that more diverse paraphrases can be generated using different filtering
criteria. Extensive experiments on existing paraphrase dataset for both the
supervised and unsupervised setups demonstrate the effectiveness the proposed
paradigm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02738">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>People convey their intention and attitude through linguistic styles of the
text that they write. In this study, we investigate lexicon usages across
styles throughout two lenses: human perception and machine word importance,
since words differ in the strength of the stylistic cues that they provide. To
collect labels of human perception, we curate a new dataset, Hummingbird, on
top of benchmarking style datasets. We have crowd workers highlight the
representative words in the text that makes them think the text has the
following styles: politeness, sentiment, offensiveness, and five emotion types.
We then compare these human word labels with word importance derived from a
popular fine-tuned style classifier like BERT. Our results show that the BERT
often finds content words not relevant to the target style as important words
used in style prediction, but humans do not perceive the same way even though
for some styles (e.g., positive sentiment and joy) human- and
machine-identified words share significant overlap for some styles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The scale, variety, and quantity of publicly-available NLP datasets has grown
rapidly as researchers propose new tasks, larger models, and novel benchmarks.
Datasets is a community library for contemporary NLP designed to support this
ecosystem. Datasets aims to standardize end-user interfaces, versioning, and
documentation, while providing a lightweight front-end that behaves similarly
for small datasets as for internet-scale corpora. The design of the library
incorporates a distributed, community-driven approach to adding datasets and
documenting usage. After a year of development, the library now includes more
than 650 unique datasets, has more than 250 contributors, and has helped
support a variety of novel cross-dataset research projects and shared tasks.
The library is available at https://github.com/huggingface/datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Commonsense reasoning benchmarks have been largely solved by fine-tuning
language models. The downside is that fine-tuning may cause models to overfit
to task-specific data and thereby forget their knowledge gained during
pre-training. Recent works only propose lightweight model updates as models may
already possess useful knowledge from past experience, but a challenge remains
in understanding what parts and to what extent models should be refined for a
given task. In this paper, we investigate what models learn from commonsense
reasoning datasets. We measure the impact of three different adaptation methods
on the generalization and accuracy of models. Our experiments with two models
show that fine-tuning performs best, by learning both the content and the
structure of the task, but suffers from overfitting and limited generalization
to novel answers. We observe that alternative adaptation methods like
prefix-tuning have comparable accuracy, but generalize better to unseen answers
and are more robust to adversarial splits.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik&#x27;s Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02734">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Inspiration moves a person to see new possibilities and transforms the way
they perceive their own potential. Inspiration has received little attention in
psychology, and has not been researched before in the NLP community. To the
best of our knowledge, this work is the first to study inspiration through
machine learning methods. We aim to automatically detect inspiring content from
social media data. To this end, we analyze social media posts to tease out what
makes a post inspiring and what topics are inspiring. We release a dataset of
5,800 inspiring and 5,800 non-inspiring English-language public post unique ids
collected from a dump of Reddit public posts made available by a third party
and use linguistic heuristics to automatically detect which social media
English-language posts are inspiring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most previous studies on information status (IS) classification and bridging
anaphora recognition assume that the gold mention or syntactic tree information
is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,
2020). In this paper, we propose an end-to-end neural approach for information
status classification. Our approach consists of a mention extraction component
and an information status assignment component. During the inference time, our
system takes a raw text as the input and generates mentions together with their
information status. On the ISNotes corpus (Markert et al., 2012), we show that
our information status assignment component achieves new state-of-the-art
results on fine-grained IS classification based on gold mentions. Furthermore,
our system performs significantly better than other baselines for both mention
extraction and fine-grained IS classification in the end-to-end setting.
Finally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,
2016) to recognize referential bridging anaphora. We find that our end-to-end
system trained on ISNotes achieves competitive results on bridging anaphora
recognition compared to the previous state-of-the-art system that relies on
syntactic information and is trained on the in-domain datasets (Yu and Poesio,
2020).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of &quot;Subjectivity&quot; and &quot;Identity Terms&quot;. (arXiv:2109.02691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
&quot;Muslim&quot; and &quot;black&quot;. Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02707">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study a new problem setting of information extraction (IE), referred to as
text-to-table, which can be viewed as an inverse problem of the well-studied
table-to-text. In text-to-table, given a text, one creates a table or several
tables expressing the main content of the text, while the model is learned from
text-table pair data. The problem setting differs from those of the existing
methods for IE. First, the extraction can be carried out from long texts to
large tables with complex structures. Second, the extraction is entirely
data-driven, and there is no need to explicitly define the schemas. As far as
we know, there has been no previous work that studies the problem. In this
work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.
We first employ a seq2seq model fine-tuned from a pre-trained language model to
perform the task. We also develop a new method within the seq2seq approach,
exploiting two additional techniques in table generation: table constraint and
table relation embeddings. We make use of four existing table-to-text datasets
in our experiments on text-to-table. Experimental results show that the vanilla
seq2seq model can outperform the baseline methods of using relation extraction
and named entity extraction. The results also show that our method can further
boost the performances of the vanilla seq2seq model. We further discuss the
main challenges of the proposed task. The code and data will be made publicly
available.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance. (arXiv:2109.02700v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02700">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>There are few industries which use manually controlled robots for carrying
material and this cannot be used all the time in all the places. So, it is very
tranquil to have robots which can follow a specific human by following the
unique coloured object held by that person. So, we propose a robotic system
which uses robot vision and deep learning to get the required linear and
angular velocities which are {\nu} and {\omega}, respectively. Which in turn
makes the robot to avoid obstacles when following the unique coloured object
held by the human. The novel methodology that we are proposing is accurate in
detecting the position of the unique coloured object in any kind of lighting
and tells us the horizontal pixel value where the robot is present and also
tells if the object is close to or far from the robot. Moreover, the artificial
neural networks that we have used in this problem gave us a meagre error in
linear and angular velocity prediction and the PI controller which was used to
control the linear and angular velocities, which in turn controls the position
of the robot gave us impressive results and this methodology outperforms all
other methodologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training such inference models is challenging
due to the lack of paired (shape, program) data in most domains. A popular
approach is to pre-train a model on synthetic data and then fine-tune on real
shapes using slow, unstable reinforcement learning. In this paper, we argue
that self-training is a viable alternative for fine-tuning such models.
Self-training is a semi-supervised learning paradigm where a model assigns
pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)
pairs as the new ground truth. We show that for constructive solid geometry and
assembly-based modeling, self-training outperforms state-of-the-art
reinforcement learning approaches. Additionally, shape program inference has a
unique property that circumvents a potential downside of self-training
(incorrect pseudo-label assignment): inferred programs are executable. For a
given shape from our distribution of interest $\mathbf{x}^*$ and its predicted
program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape
$\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than
$(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution
self training (LEST). We demonstrate that self training infers shape programs
with higher shape reconstruction accuracy and converges significantly faster
than reinforcement learning approaches, and in some domains, LEST can further
improve this performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Grassmannian Graph-attentional Landmark Selection for Domain Adaptation. (arXiv:2109.02990v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Domain adaptation aims to leverage information from the source domain to
improve the classification performance in the target domain. It mainly utilizes
two schemes: sample reweighting and feature matching. While the first scheme
allocates different weights to individual samples, the second scheme matches
the feature of two domains using global structural statistics. The two schemes
are complementary with each other, which are expected to jointly work for
robust domain adaptation. Several methods combine the two schemes, but the
underlying relationship of samples is insufficiently analyzed due to the
neglect of the hierarchy of samples and the geometric properties between
samples. To better combine the advantages of the two schemes, we propose a
Grassmannian graph-attentional landmark selection (GGLS) framework for domain
adaptation. GGLS presents a landmark selection scheme using attention-induced
neighbors of the graphical structure of samples and performs distribution
adaptation and knowledge adaptation over Grassmann manifold. the former treats
the landmarks of each sample differently, and the latter avoids feature
distortion and achieves better geometric properties. Experimental results on
different real-world cross-domain visual recognition tasks demonstrate that
GGLS provides better classification accuracies compared with state-of-the-art
domain adaptation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Aerial scene recognition is a fundamental research problem in interpreting
high-resolution aerial imagery. Over the past few years, most studies focus on
classifying an image into one scene category, while in real-world scenarios, it
is more often that a single image contains multiple scenes. Therefore, in this
paper, we investigate a more practical yet underexplored task -- multi-scene
recognition in single images. To this end, we create a large-scale dataset,
called MultiScene, composed of 100,000 unconstrained high-resolution aerial
images. Considering that manually labeling such images is extremely arduous, we
resort to low-cost annotations from crowdsourcing platforms, e.g.,
OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and
incorrectness, which introduce noise into image labels. To address this issue,
we visually inspect 14,000 images and correct their scene labels, yielding a
subset of cleanly-annotated images, named MultiScene-Clean. With it, we can
develop and evaluate deep networks for multi-scene recognition using clean
data. Moreover, we provide crowdsourced annotations of all images for the
purpose of studying network learning with noisy labels. We conduct experiments
with extensive baseline models on both MultiScene-Clean and MultiScene to offer
benchmarks for multi-scene recognition in single images and learning from noisy
labels for this task, respectively. To facilitate progress, we make our dataset
and trained models available on
https://gitlab.lrz.de/ai4eo/reasoning/multiscene.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing. (arXiv:2104.14735v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14735">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Parsing an image into a hierarchy of objects, parts, and relations is
important and also challenging in many computer vision tasks. This paper
proposes a simple and effective capsule autoencoder to address this issue,
called DPR-CAE. In our approach, the encoder parses the input into a set of
part capsules, including pose, intensity, and dynamic vector. The decoder
introduces a novel dynamic part representation (DPR) by combining the dynamic
vector and a shared template bank. These part representations are then
regulated by corresponding capsules to composite the final output in an
interpretable way. Besides, an extra translation-invariant module is proposed
to avoid directly learning the uncertain scene-part relationship in our
DPR-CAE, which makes the resulting method achieves a promising performance gain
on $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship
DPR-CAE can be easily combined with the existing stacked capsule autoencoder
and experimental results show it significantly improves performance in terms of
unsupervised object classification. Our code is available in the Appendix.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training sample re-weighting is an effective approach for tackling data
biases such as imbalanced and corrupted labels. Recent methods develop
learning-based algorithms to learn sample re-weighting strategies jointly with
model training based on the frameworks of reinforcement learning and meta
learning. However, depending on additional unbiased reward data is limiting
their general applicability. Furthermore, existing learning-based sample
re-weighting methods require nested optimizations of models and weighting
parameters, which requires expensive second-order computation. This paper
addresses these two problems and presents a novel learning-based fast sample
re-weighting (FSR) method that does not require additional reward data. The
method is based on two key ideas: learning from history to build proxy reward
data and feature sharing to reduce the optimization cost. Our experiments show
the proposed method achieves competitive results compared to state of the arts
on label noise robustness and long-tailed recognition, and does so while
achieving significantly improved training efficiency. The source code is
publicly available at
https://github.com/google-research/google-research/tree/master/ieg.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Modal RGB-D Scene Recognition Across Domains. (arXiv:2103.14672v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14672">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scene recognition is one of the basic problems in computer vision research
with extensive applications in robotics. When available, depth images provide
helpful geometric cues that complement the RGB texture information and help to
identify discriminative scene image features. Depth sensing technology
developed fast in the last years and a great variety of 3D cameras have been
introduced, each with different acquisition properties. However, those
properties are often neglected when targeting big data collections, so
multi-modal images are gathered disregarding their original nature. In this
work, we put under the spotlight the existence of a possibly severe domain
shift issue within multi-modality scene recognition datasets. As a consequence,
a scene classification model trained on one camera may not generalize on data
from a different camera, only providing a low recognition performance. Starting
from the well-known SUN RGB-D dataset, we designed an experimental testbed to
study this problem and we use it to benchmark the performance of existing
methods. Finally, we introduce a novel adaptive scene recognition approach that
leverages self-supervised translation between modalities. Indeed, learning to
go from RGB to depth and vice-versa is an unsupervised procedure that can be
trained jointly on data of multiple cameras and may help to bridge the gap
among the extracted feature distributions. Our experimental results confirm the
effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08323">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal action proposal generation is an essential and challenging task that
aims at localizing temporal intervals containing human actions in untrimmed
videos. Most of existing approaches are unable to follow the human cognitive
process of understanding the video context due to lack of attention mechanism
to express the concept of an action or an agent who performs the action or the
interaction between the agent and the environment. Based on the action
definition that a human, known as an agent, interacts with the environment and
performs an action that affects the environment, we propose a contextual
Agent-Environment Network. Our proposed contextual AEN involves (i) agent
pathway, operating at a local level to tell about which humans/agents are
acting and (ii) environment pathway operating at a global level to tell about
how the agents interact with the environment. Comprehensive evaluations on
20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different
backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits
outperformance against state-of-the-art methods regardless of the employed
backbone network.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.

In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04886">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning (DL) has been the primary approach used in various computer
vision tasks due to its relevant results achieved on many tasks. However, on
real-world scenarios with partially or no labeled data, DL methods are also
prone to the well-known domain shift problem. Multi-source unsupervised domain
adaptation (MSDA) aims at learning a predictor for an unlabeled domain by
assigning weak knowledge from a bag of source models. However, most works
conduct domain adaptation leveraging only the extracted features and reducing
their domain shift from the perspective of loss function designs. In this
paper, we argue that it is not sufficient to handle domain shift only based on
domain-level features, but it is also essential to align such information on
the feature space. Unlike previous works, we focus on the network design and
propose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at
different levels of the predictor. These layers are designed to match the
feature distributions between different domains and can be easily applied to
various MSDA methods. To show the robustness of our approach, we conducted an
extensive experimental evaluation considering two challenging scenarios: digit
recognition and object classification. The experimental results indicated that
our approach can improve state-of-the-art MSDA methods, yielding relative gains
of up to +30.64% on their classification accuracies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification. (arXiv:2109.02688v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02688">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Annotated images are required for both supervised model training and
evaluation in image classification. Manually annotating images is arduous and
expensive, especially for multi-labeled images. A recent trend for conducting
such laboursome annotation tasks is through crowdsourcing, where images are
annotated by volunteers or paid workers online (e.g., workers of Amazon
Mechanical Turk) from scratch. However, the quality of crowdsourcing image
annotations cannot be guaranteed, and incompleteness and incorrectness are two
major concerns for crowdsourcing annotations. To address such concerns, we have
a rethinking of crowdsourcing annotations: Our simple hypothesis is that if the
annotators only partially annotate multi-label images with salient labels they
are confident in, there will be fewer annotation errors and annotators will
spend less time on uncertain labels. As a pleasant surprise, with the same
annotation budget, we show a multi-label image classifier supervised by images
with salient annotations can outperform models supervised by fully annotated
images. Our method contributions are 2-fold: An active learning way is proposed
to acquire salient labels for multi-label images; and a novel Adaptive
Temperature Associated Model (ATAM) specifically using partial annotations is
proposed for multi-label image classification. We conduct experiments on
practical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark
dataset COCO 2014. When compared with state-of-the-art classification methods
trained on fully annotated images, the proposed ATAM can achieve higher
accuracy. The proposed idea is promising for crowdsourcing data annotation. Our
code will be publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pseudo-mask Matters in Weakly-supervised Semantic Segmentation. (arXiv:2108.12995v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12995">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most weakly supervised semantic segmentation (WSSS) methods follow the
pipeline that generates pseudo-masks initially and trains the segmentation
model with the pseudo-masks in fully supervised manner after. However, we find
some matters related to the pseudo-masks, including high quality pseudo-masks
generation from class activation maps (CAMs), and training with noisy
pseudo-mask supervision. For these matters, we propose the following designs to
push the performance to new state-of-art: (i) Coefficient of Variation
Smoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask
Generation to project the expanded CAMs to pseudo-mask based on a new metric
indicating the importance of each class on each location, instead of the scores
trained from binary classifiers. (iii) Pretended Under-Fitting strategy to
suppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to
boost the pseudo-masks during training of fully supervised semantic
segmentation (FSSS). Experiments based on our methods achieve new state-of-art
results on two changeling weakly supervised semantic segmentation datasets,
pushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014
respectively. Codes including segmentation framework are released at
https://github.com/Eli-YiLi/PMM</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00953">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding the behaviors and intentions of pedestrians is still one of the
main challenges for vehicle autonomy, as accurate predictions of their
intentions can guarantee their safety and driving comfort of vehicles. In this
paper, we address pedestrian crossing prediction in urban traffic environments
by linking the dynamics of a pedestrian&#x27;s skeleton to a binary crossing
intention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch
predictor. TrouSPI-Net extracts spatio-temporal features for different time
resolutions by encoding pseudo-images sequences of skeletal joints&#x27; positions
and processes them with parallel attention modules and atrous convolutions. The
proposed approach is then enhanced by processing features such as relative
distances of skeletal joints, bounding box positions, or ego-vehicle speed with
U-GRUs. Using the newly proposed evaluation procedures for two large public
naturalistic data sets for studying pedestrian behavior in traffic: JAAD and
PIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results
show that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,
therefore outperforming current state-of-the-art while being lightweight and
context-free.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Animatable Neural Radiance Fields from Monocular RGB Videos. (arXiv:2106.13629v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13629">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03156">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>For the task of image classification, researchers work arduously to develop
the next state-of-the-art (SOTA) model, each bench-marking their own
performance against that of their predecessors and of their peers.
Unfortunately, the metric used most frequently to describe a model&#x27;s
performance, average categorization accuracy, is often used in isolation. As
the number of classes increases, such as in fine-grained visual categorization
(FGVC), the amount of information conveyed by average accuracy alone dwindles.
While its most glaring weakness is its failure to describe the model&#x27;s
performance on a class-by-class basis, average accuracy also fails to describe
how performance may vary from one trained model of the same architecture, on
the same dataset, to another (both averaged across all categories and at the
per-class level). We first demonstrate the magnitude of these variations across
models and across class distributions based on attributes of the data,
comparing results on different visual domains and different per-class image
distributions, including long-tailed distributions and few-shot subsets. We
then analyze the impact various FGVC methods have on overall and per-class
variance. From this analysis, we both highlight the importance of reporting and
comparing methods based on information beyond overall accuracy, as well as
point out techniques that mitigate variance in FGVC results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Material Measurement Units for a Circular Economy: Foundations through a Survey. (arXiv:2103.01997v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01997">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Long-term availability of minerals and industrial materials is a necessary
condition for sustainable development as they are the constituents of any
manufacturing product. To enhance the efficiency of material management, we
define a computer-vision-enabled material measurement system and provide a
survey of works relevant to its development with particular emphasis on the
foundations. A network of such systems for wide-area material stock monitoring
is also covered. Finally, challenges and future research directions are
discussed. As the first article bridging industrial ecology and advanced
computer vision, this survey is intended to support both research communities
towards more sustainable manufacturing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generative models for 3D shapes represented by hierarchies of parts can
generate realistic and diverse sets of outputs. However, existing models suffer
from the key practical limitation of modelling shapes holistically and thus
cannot perform conditional sampling, i.e. they are not able to generate
variants on individual parts of generated shapes without modifying the rest of
the shape. This is limiting for applications such as 3D CAD design that involve
adjusting created shapes at multiple levels of detail. To address this, we
introduce LSD-StructureNet, an augmentation to the StructureNet architecture
that enables re-generation of parts situated at arbitrary positions in the
hierarchies of its outputs. We achieve this by learning individual,
probabilistic conditional decoders for each hierarchy depth. We evaluate
LSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes
represented by hierarchies of parts. Our results show that contrarily to
existing methods, LSD-StructureNet can perform conditional sampling without
impacting inference speed or the realism and diversity of its outputs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09262">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Posit Number System was introduced in 2017 as a replacement for
floating-point numbers. Since then, the community has explored its application
in Neural Network related tasks and produced some unit designs which are still
far from being competitive with their floating-point counterparts. This paper
proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to
significantly reduce the complexity of posit multipliers, the most power-hungry
units within Deep Neural Network architectures. When comparing with
state-of-the-art posit multipliers, experiments show that the proposed
technique reduces the area, power, and delay of hardware multipliers up to
72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\&#x27;echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robust and accurate nuclei centroid detection is important for the
understanding of biological structures in fluorescence microscopy images.
Existing automated nuclei localization methods face three main challenges: (1)
Most of object detection methods work only on 2D images and are difficult to
extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes
but it is computational expensive for large microscopy volumes and they have
difficulty distinguishing different instances of objects; (3) Hand annotated
ground truth is limited for 3D microscopy volumes. To address these issues, we
present a scalable approach for nuclei centroid detection of 3D microscopy
volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each
slice of the volume from different directions and 3D agglomerative hierarchical
clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.
The model was trained with the synthetic microscopy data generated using
Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and
tested on different types of real 3D microscopy data. Extensive experimental
results demonstrate that our proposed method can accurately count and detect
the nuclei centroids in a 3D microscopy volume.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02752">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks may be challenging in real world data. Using
models as black-boxes, even with transfer learning, can result in poor
generalization or inconclusive results when it comes to small datasets or
specific applications. This tutorial covers the basic steps as well as more
recent options to improve models, in particular, but not restricted to,
supervised learning. It can be particularly useful in datasets that are not as
well-prepared as those in challenges, and also under scarce annotation and/or
small data. We describe basic procedures: as data preparation, optimization and
transfer learning, but also recent architectural choices such as use of
transformer modules, alternative convolutional layers, activation functions,
wide and deep networks, as well as training procedures including as curriculum,
contrastive and self-supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper studies the model compression problem of vision transformers.
Benefit from the self-attention module, transformer architectures have shown
extraordinary performance on many computer vision tasks. Although the network
performance is boosted, transformers are often required more computational
resources including memory usage and the inference complexity. Compared with
the existing knowledge distillation approaches, we propose to excavate useful
information from the teacher transformer through the relationship between
images and the divided patches. We then explore an efficient fine-grained
manifold distillation approach that simultaneously calculates cross-images,
cross-patch, and random-selected manifolds in teacher and student models.
Experimental results conducted on several benchmarks demonstrate the
superiority of the proposed algorithm for distilling portable transformer
models with higher performance. For example, our approach achieves 75.06% Top-1
accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which
outperforms other ViT distillation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03230">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a self-supervised approach for tumor segmentation.
Specifically, we advocate a zero-shot setting, where models from
self-supervised learning should be directly applicable for the downstream task,
without using any manual annotations whatsoever. We make the following
contributions. First, with careful examination on existing self-supervised
learning approaches, we reveal the surprising result that, given suitable data
augmentation, models trained from scratch in fact achieve comparable
performance to those pre-trained with self-supervised learning. Second,
inspired by the fact that tumors tend to be characterized independently to the
contexts, we propose a scalable pipeline for generating synthetic tumor data,
and train a self-supervised model that minimises the generalisation gap with
the downstream task. Third, we conduct extensive ablation studies on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, including an extreme case of
zero-shot segmentation, the proposed approach demonstrates state-of-the-art
performance, substantially outperforming all existing self-supervised
approaches, and opening up the usage of self-supervised learning in practical
scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Smart Traffic Monitoring System using Computer Vision and Edge Computing. (arXiv:2109.03141v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03141">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Traffic management systems capture tremendous video data and leverage
advances in video processing to detect and monitor traffic incidents. The
collected data are traditionally forwarded to the traffic management center
(TMC) for in-depth analysis and may thus exacerbate the network paths to the
TMC. To alleviate such bottlenecks, we propose to utilize edge computing by
equipping edge nodes that are close to cameras with computing resources (e.g.
cloudlets). A cloudlet, with limited computing resources as compared to TMC,
provides limited video processing capabilities. In this paper, we focus on two
common traffic monitoring tasks, congestion detection, and speed detection, and
propose a two-tier edge computing based model that takes into account of both
the limited computing capability in cloudlets and the unstable network
condition to the TMC. Our solution utilizes two algorithms for each task, one
implemented at the edge and the other one at the TMC, which are designed with
the consideration of different computing resources. While the TMC provides
strong computation power, the video quality it receives depends on the
underlying network conditions. On the other hand, the edge processes very
high-quality video but with limited computing resources. Our model captures
this trade-off. We evaluate the performance of the proposed two-tier model as
well as the traffic monitoring algorithms via test-bed experiments under
different weather as well as network conditions and show that our proposed
hybrid edge-cloud solution outperforms both the cloud-only and edge-only
solutions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction. (arXiv:2109.02965v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02965">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The correct characterization of uncertainty when predicting human motion is
equally important as the accuracy of this prediction. We present a new method
to correctly predict the uncertainty associated with the predicted distribution
of future trajectories. Our approach, CovariaceNet, is based on a Conditional
Generative Model with Gaussian latent variables in order to predict the
parameters of a bi-variate Gaussian distribution. The combination of
CovarianceNet with a motion prediction model results in a hybrid approach that
outputs a uni-modal distribution. We will show how some state of the art
methods in motion prediction become overconfident when predicting uncertainty,
according to our proposed metric and validated in the ETH data-set
\cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which
makes our method suitable for applications that use predicted distributions,
e.g., planning or decision making.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00632">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We train an object detector built from convolutional neural networks to count
interference fringes in elliptical antinode regions in frames of high-speed
video recordings of transient oscillations in Caribbean steelpan drums
illuminated by electronic speckle pattern interferometry (ESPI). The
annotations provided by our model aim to contribute to the understanding of
time-dependent behavior in such drums by tracking the development of
sympathetic vibration modes. The system is trained on a dataset of crowdsourced
human-annotated images obtained from the Zooniverse Steelpan Vibrations
Project. Due to the small number of human-annotated images and the ambiguity of
the annotation task, we also evaluate the model on a large corpus of synthetic
images whose properties have been matched to the real images by style transfer
using a Generative Adversarial Network. Applying the model to thousands of
unlabeled video frames, we measure oscillations consistent with audio
recordings of these drum strikes. One unanticipated result is that sympathetic
oscillations of higher-octave notes significantly precede the rise in sound
intensity of the corresponding second harmonic tones; the mechanism responsible
for this remains unidentified. This paper primarily concerns the development of
the predictive model; further exploration of the steelpan images and deeper
physical insights await its further application.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation. (arXiv:2109.02920v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02920">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>3D Convolutional Neural Networks (CNNs) have been widely adopted for airway
segmentation. The performance of 3D CNNs is greatly influenced by the dataset
while the public airway datasets are mainly clean CT scans with coarse
annotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19
CT scans). In this work, we proposed a new dual-stream network to address the
variability between the clean domain and noisy domain, which utilizes the clean
CT scans and a small amount of labeled noisy CT scans for airway segmentation.
We designed two different encoders to extract the transferable clean features
and the unique noisy features separately, followed by two independent decoders.
Further on, the transferable features are refined by the channel-wise feature
recalibration and Signed Distance Map (SDM) regression. The feature
recalibration module emphasizes critical features and the SDM pays more
attention to the bronchi, which is beneficial to extracting the transferable
topological features robust to the coarse labels. Extensive experimental
results demonstrated the obvious improvement brought by our proposed method.
Compared to other state-of-the-art transfer learning methods, our method
accurately segmented more bronchi in the noisy CT scans.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal
MRI is a challenging field of research. Most of the existing literature on
survival prediction is based on Radiomic features, which does not consider
either non-biological factors or the functional neurological status of the
patient(s). Besides, the selection of an appropriate cut-off for survival and
the presence of censored data create further problems. Application of deep
learning models for OS prediction is also limited due to the lack of large
annotated publicly available datasets. In this scenario we analyse the
potential of two novel neuroimaging feature families, extracted from brain
parcellation atlases and spatial habitats, along with classical radiomic and
geometric features; to study their combined predictive power for analysing
overall survival. A cross validation strategy with grid search is proposed to
simultaneously select and evaluate the most predictive feature subset based on
its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for
univariate feature selection, followed by the prediction of patient-specific
survival functions by three multivariate parsimonious models viz. Coxnet,
Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI
data used for this research was taken from two open-access collections TCGA-GBM
and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding
survival data for each patient was downloaded from The Cancer Genome Atlas
(TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved
using RSF with the best $24$ selected features. Age was found to be the most
important biological predictor. There were $9$, $6$, $6$ and $2$ features
selected from the parcellation, habitat, radiomic and region-based feature
groups respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation. (arXiv:2109.02804v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02804">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Kinship verification is a long-standing research challenge in computer
vision. The visual differences presented to the face have a significant effect
on the recognition capabilities of the kinship systems. We argue that
aggregating multiple visual knowledge can better describe the characteristics
of the subject for precise kinship identification. Typically, the age-invariant
features can represent more natural facial details. Such age-related
transformations are essential for face recognition due to the biological
effects of aging. However, the existing methods mainly focus on employing the
single-view image features for kinship identification, while more meaningful
visual properties such as race and age are directly ignored in the feature
learning step. To this end, we propose a novel deep collaborative multi-modal
learning (DCML) to integrate the underlying information presented in facial
properties in an adaptive manner to strengthen the facial details for effective
unsupervised kinship verification. Specifically, we construct a well-designed
adaptive feature fusion mechanism, which can jointly leverage the complementary
properties from different visual perspectives to produce composite features and
draw greater attention to the most informative components of spatial feature
maps. Particularly, an adaptive weighting strategy is developed based on a
novel attention mechanism, which can enhance the dependencies between different
properties by decreasing the information redundancy in channels in a
self-adaptive manner. To validate the effectiveness of the proposed method,
extensive experimental evaluations conducted on four widely-used datasets show
that our DCML method is always superior to some state-of-the-art kinship
verification methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional sparse coding improves on the standard sparse approximation by
incorporating a global shift-invariant model. The most efficient convolutional
sparse coding methods are based on the alternating direction method of
multipliers and the convolution theorem. The only major difference between
these methods is how they approach a convolutional least-squares fitting
subproblem. This letter presents a solution to this subproblem, which improves
the efficiency of the state-of-the-art algorithms. We also use the same
approach for developing an efficient convolutional dictionary learning method.
Furthermore, we propose a novel algorithm for convolutional sparse coding with
a constraint on the approximation error.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03229">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many existing works have made great strides towards reducing racial bias in
face recognition. However, most of these methods attempt to rectify bias that
manifests in models during training instead of directly addressing a major
source of the bias, the dataset itself. Exceptions to this are
BUPT-Balancedface/RFW and Fairface, but these works assume that primarily
training on a single race or not racially balancing the dataset are inherently
disadvantageous. We demonstrate that these assumptions are not necessarily
valid. In our experiments, training on only African faces induced less bias
than training on a balanced distribution of faces and distributions skewed to
include more African faces produced more equitable models. We additionally
notice that adding more images of existing identities to a dataset in place of
adding new identities can lead to accuracy boosts across racial categories. Our
code is available at
https://github.com/j-alex-hanson/rethinking-race-face-datasets</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model&#x27;s dependence on key facial features. Second,
four of today&#x27;s state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative-Adversarial-Networks-based Ghost Recognition. (arXiv:2103.13858v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Nowadays, target recognition technique plays an important role in many
fields. However, the current target image information based methods suffer from
the influence of image quality and the time cost of image reconstruction. In
this paper, we propose a novel imaging-free target recognition method combining
ghost imaging (GI) and generative adversarial networks (GAN). Based on the
mechanism of GI, a set of random speckles sequence is employed to illuminate
target, and a bucket detector without resolution is utilized to receive echo
signal. The bucket signal sequence formed after continuous detections is
constructed into a bucket signal array, which is regarded as the sample of GAN.
Then, conditional GAN is used to map bucket signal array and target category.
In practical application, the speckles sequence in training step is employed to
illuminate target, and the bucket signal array is input GAN for recognition.
The proposed method can improve the problems caused by conventional recognition
methods that based on target image information, and provide a certain
turbulence-free ability. Extensive experiments show that the proposed method
achieves promising performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03144">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Optical Character Recognition (OCR) systems have been widely used in various
of application scenarios. Designing an OCR system is still a challenging task.
In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)
to balance the accuracy against the efficiency. In order to improve the
accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more
robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better
text detector and a better text recognizer, which include Collaborative Mutual
Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual
Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the
precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.
It is also comparable to the server models of the PP-OCR which uses ResNet
series as backbones. All of the above mentioned models are open-sourced and the
code is available in the GitHub repository PaddleOCR which is powered by
PaddlePaddle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention. (arXiv:2109.02955v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02955">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatically describing video, or video captioning, has been widely studied
in the multimedia field. This paper proposes a new task of sensor-augmented
egocentric-video captioning, a newly constructed dataset for it called MMAC
Captions, and a method for the newly proposed task that effectively utilizes
multi-modal data of video and motion sensors, or inertial measurement units
(IMUs). While conventional video captioning tasks have difficulty in dealing
with detailed descriptions of human activities due to the limited view of a
fixed camera, egocentric vision has greater potential to be used for generating
the finer-grained descriptions of human activities on the basis of a much
closer view. In addition, we utilize wearable-sensor data as auxiliary
information to mitigate the inherent problems in egocentric vision: motion
blur, self-occlusion, and out-of-camera-range activities. We propose a method
for effectively utilizing the sensor data in combination with the video data on
the basis of an attention mechanism that dynamically determines the modality
that requires more attention, taking the contextual information into account.
We compared the proposed sensor-fusion method with strong baselines on the MMAC
Captions dataset and found that using sensor data as supplementary information
to the egocentric-video data was beneficial, and that our proposed method
outperformed the strong baselines, demonstrating the effectiveness of the
proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors. (arXiv:2109.02993v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02993">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Significant advancements made in the generation of deepfakes have caused
security and privacy issues. Attackers can easily impersonate a person&#x27;s
identity in an image by replacing his face with the target person&#x27;s face.
Moreover, a new domain of cloning human voices using deep-learning technologies
is also emerging. Now, an attacker can generate realistic cloned voices of
humans using only a few seconds of audio of the target person. With the
emerging threat of potential harm deepfakes can cause, researchers have
proposed deepfake detection methods. However, they only focus on detecting a
single modality, i.e., either video or audio. On the other hand, to develop a
good deepfake detector that can cope with the recent advancements in deepfake
generation, we need to have a detector that can detect deepfakes of multiple
modalities, i.e., videos and audios. To build such a detector, we need a
dataset that contains video and respective audio deepfakes. We were able to
find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection
Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized
fake audios as well. We used this multimodal deepfake dataset and performed
detailed baseline experiments using state-of-the-art unimodal, ensemble-based,
and multimodal detection methods to evaluate it. We conclude through detailed
experimentation that unimodals, addressing only a single modality, video or
audio, do not perform well compared to ensemble-based methods. Whereas purely
multimodal-based baselines provide the worst performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged as one of the key ingredients to reduce
the size of neural networks for their deployments to resource-limited devices.
In order to overcome the nature of transforming continuous activations and
weights to discrete ones, recent study called Relaxed Quantization (RQ)
[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that
allows this transformation with efficient gradient-based optimization. However,
RQ with this Gumbel-Softmax relaxation still suffers from bias-variance
trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve
the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses
multi-class straight-through estimator to effectively reduce the bias and
variance, along with a new regularization technique, DropBits that replaces
dropout regularization to randomly drop the bits instead of neurons to further
reduce the bias of the multi-class straight-through estimator in SRQ. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer
using DropBits. We experimentally validate our method on various benchmark
datasets and network architectures, and also support the quantized lottery
ticket hypothesis: learning heterogeneous quantization levels outperforms the
case using the same but fixed quantization levels from scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene. (arXiv:2109.02917v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02917">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper contributes a new high-quality dataset for hand gesture
recognition in hand hygiene systems, named &quot;MFH&quot;. Generally, current datasets
are not focused on: (i) fine-grained actions; and (ii) data mismatch between
different viewpoints, which are available under realistic settings. To address
the aforementioned issues, the MFH dataset is proposed to contain a total of
731147 samples obtained by different camera views in 6 non-overlapping
locations. Additionally, each sample belongs to one of seven steps introduced
by the World Health Organization (WHO). As a minor contribution, inspired by
advances in fine-grained image recognition and distribution adaptation, this
paper recommends using the self-supervised learning method to handle these
preceding problems. The extensive experiments on the benchmarking MFH dataset
show that the introduced method yields competitive performance in both the
Accuracy and the Macro F1-score. The code and the MFH dataset are available at
https://github.com/willogy-team/hand-gesture-recognition-smc2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03075">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge distillation (KD) is an effective framework that aims to transfer
meaningful information from a large teacher to a smaller student. Generally, KD
often involves how to define and transfer knowledge. Previous KD methods often
focus on mining various forms of knowledge, for example, feature maps and
refined information. However, the knowledge is derived from the primary
supervised task and thus is highly task-specific. Motivated by the recent
success of self-supervised representation learning, we propose an auxiliary
self-supervision augmented task to guide networks to learn more meaningful
features. Therefore, we can derive soft self-supervision augmented
distributions as richer dark knowledge from this task for KD. Unlike previous
knowledge, this distribution encodes joint knowledge from supervised and
self-supervised feature learning. Beyond knowledge exploration, another crucial
aspect is how to learn and distill our proposed knowledge effectively. To fully
take advantage of hierarchical feature maps, we propose to append several
auxiliary branches at various hidden layers. Each auxiliary branch is guided to
learn self-supervision augmented task and distill this distribution from
teacher to student. Thus we call our KD method as Hierarchical Self-Supervision
Augmented Knowledge Distillation (HSSAKD). Experiments on standard image
classification show that both offline and online HSSAKD achieves
state-of-the-art performance in the field of KD. Further transfer experiments
on object detection further verify that HSSAKD can guide the network to learn
better features, which can be attributed to learn and distill an auxiliary
self-supervision augmented task effectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02412">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a simple yet effective progressive self-guided loss function to
facilitate deep learning-based salient object detection (SOD) in images. The
saliency maps produced by the most relevant works still suffer from incomplete
predictions due to the internal complexity of salient objects. Our proposed
progressive self-guided loss simulates a morphological closing operation on the
model predictions for progressively creating auxiliary training supervisions to
step-wisely guide the training process. We demonstrate that this new loss
function can guide the SOD model to highlight more complete salient objects
step-by-step and meanwhile help to uncover the spatial dependencies of the
salient object pixels in a region growing manner. Moreover, a new feature
aggregation module is proposed to capture multi-scale features and aggregate
them adaptively by a branch-wise attention mechanism. Benefiting from this
module, our SOD framework takes advantage of adaptively aggregated multi-scale
features to locate and detect salient objects effectively. Experimental results
on several benchmark datasets show that our loss function not only advances the
performance of existing SOD models without architecture modification but also
helps our proposed framework to achieve state-of-the-art performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Attentive 3D Human Pose and Shape Estimation from Videos. (arXiv:2103.14182v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14182">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the task of estimating 3D human pose and shape from videos. While
existing frame-based approaches have made significant progress, these methods
are independently applied to each image, thereby often leading to inconsistent
predictions. In this work, we present a video-based learning algorithm for 3D
human pose and shape estimation. The key insights of our method are two-fold.
First, to address the inconsistent temporal prediction issue, we exploit
temporal information in videos and propose a self-attention module that jointly
considers short-range and long-range dependencies across frames, resulting in
temporally coherent estimations. Second, we model human motion with a
forecasting module that allows the transition between adjacent frames to be
smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M
datasets. Extensive experimental results show that our algorithm performs
favorably against the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End to end hyperspectral imaging system with coded compression imaging process. (arXiv:2109.02643v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02643">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hyperspectral images (HSIs) can provide rich spatial and spectral information
with extensive application prospects. Recently, several methods using
convolutional neural networks (CNNs) to reconstruct HSIs have been developed.
However, most deep learning methods fit a brute-force mapping relationship
between the compressive and standard HSIs. Thus, the learned mapping would be
invalid when the observation data deviate from the training data. To recover
the three-dimensional HSIs from two-dimensional compressive images, we present
dual-camera equipment with a physics-informed self-supervising CNN method based
on a coded aperture snapshot spectral imaging system. Our method effectively
exploits the spatial-spectral relativization from the coded spectral
information and forms a self-supervising system based on the camera quantum
effect model. The experimental results show that our method can be adapted to a
wide imaging environment with good performance. In addition, compared with most
of the network-based methods, our system does not require a dedicated dataset
for pre-training. Therefore, it has greater scenario adaptability and better
generalization ability. Meanwhile, our system can be constantly fine-tuned and
self-improved in real-life scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under fair
evaluation protocols. In this paper, we propose a new learning scheme to
enforce domain invariance in the space of the gradients of the loss function:
specifically, we introduce a regularization term that matches the domain-level
variances of gradients across training domains. Critically, our strategy, named
Fishr, exhibits close relations with the Fisher Information and the Hessian of
the loss. We show that forcing domain-level gradient covariances to be similar
during the learning procedure eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. In
particular, Fishr improves the state of the art on the DomainBed benchmark and
performs significantly better than Empirical Risk Minimization. The code is
released at https://github.com/alexrame/fishr.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Support Vector Machine for Handwritten Character Recognition. (arXiv:2109.03081v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Handwriting recognition has been one of the most fascinating and challenging
research areas in field of image processing and pattern recognition. It
contributes enormously to the improvement of automation process. In this paper,
a system for recognition of unconstrained handwritten Malayalam characters is
proposed. A database of 10,000 character samples of 44 basic Malayalam
characters is used in this work. A discriminate feature set of 64 local and 4
global features are used to train and test SVM classifier and achieved 92.24%
accuracy</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning-based single image deraining (SID) with unpaired information is
of immense importance, as relying on paired synthetic data often limits their
generality and scalability in real-world applications. However, we noticed that
direct employ of unpaired adversarial learning and cycle-consistency
constraints in the SID task is insufficient to learn the underlying
relationship from rainy input to clean outputs, since the domain knowledge
between rainy and rain-free images is asymmetrical. To address such limitation,
we develop an effective unpaired SID method which explores mutual properties of
the unpaired exemplars by a contrastive learning manner in a GAN framework,
named as CDR-GAN. The proposed method mainly consists of two cooperative
branches: Bidirectional Translation Branch (BTB) and Contrastive Guidance
Branch (CGB). Specifically, BTB takes full advantage of the circulatory
architecture of adversarial consistency to exploit latent feature distributions
and guide transfer ability between two domains by equipping it with
bidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings
of different exemplars in rain space by encouraging the similar feature
distributions closer while pushing the dissimilar further away, in order to
better help rain removal and image restoration. During training, we explore
several loss functions to further constrain the proposed CDR-GAN. Extensive
experiments show that our method performs favorably against existing unpaired
deraining approaches on both synthetic and real-world datasets, even
outperforms several fully-supervised or semi-supervised models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers, the default model of choices in natural language processing,
have drawn scant attention from the medical imaging community. Given the
ability to exploit long-term dependencies, transformers are promising to help
atypical convolutional neural networks (convnets) to overcome its inherent
shortcomings of spatial inductive bias. However, most of recently proposed
transformer-based segmentation approaches simply treated transformers as
assisted modules to help encode global context into convolutional
representations without investigating how to optimally combine self-attention
(i.e., the core of transformers) with convolution. To address this issue, in
this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful
segmentation model with an interleaved architecture based on empirical
combination of self-attention and convolution. In practice, nnFormer learns
volumetric representations from 3D local volumes. Compared to the naive
voxel-level self-attention implementation, such volume-based operations help to
reduce the computational complexity by approximate 98% and 99.5% on Synapse and
ACDC datasets, respectively. In comparison to prior-art network configurations,
nnFormer achieves tremendous improvements over previous transformer-based
methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer
outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to
nnUNet, currently the best performing fully-convolutional medical segmentation
network, nnFormer still provides slightly better performance on Synapse and
ACDC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Combine the Modalities of Language and Video for Temporal Moment Localization. (arXiv:2109.02925v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02925">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal moment localization aims to retrieve the best video segment matching
a moment specified by a query. The existing methods generate the visual and
semantic embeddings independently and fuse them without full consideration of
the long-term temporal relationship between them. To address these
shortcomings, we introduce a novel recurrent unit, cross-modal long short-term
memory (CM-LSTM), by mimicking the human cognitive process of localizing
temporal moments that focuses on the part of a video segment related to the
part of a query, and accumulates the contextual information across the entire
video recurrently. In addition, we devise a two-stream attention mechanism for
both attended and unattended video features by the input query to prevent
necessary visual information from being neglected. To obtain more precise
boundaries, we propose a two-stream attentive cross-modal interaction network
(TACI) that generates two 2D proposal maps obtained globally from the
integrated contextual features, which are generated by using CM-LSTM, and
locally from boundary score sequences and then combines them into a final 2D
map in an end-to-end manner. On the TML benchmark dataset,
ActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1
of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we
show that the revised state-of-the-arts methods by replacing the original LSTM
with our CM-LSTM achieve performance gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02929">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present our solution to extract albedo of branded labels for
e-commerce products. To this end, we generate a large-scale photo-realistic
synthetic data set for albedo extraction followed by training a generative
model to translate images with diverse lighting conditions to albedo. We
performed an extensive evaluation to test the generalisation of our method to
in-the-wild images. From the experimental results, we observe that our solution
generalises well compared to the existing method both in the unseen rendered
images as well as in the wild image.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Perceptual Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional generative adversarial network. In our approach, the
recurrent auto-encoder-based generator learns to fully explore the temporal
correlation for compressing video. More importantly, we propose a recurrent
conditional discriminator, which judges raw and compressed video conditioned on
both spatial and temporal information, including the latent representation,
temporal motion and hidden states in recurrent cells. This way, in the
adversarial training, it pushes the generated video to be not only spatially
photo-realistic but also temporally consistent with groundtruth and coherent
among video frames. Therefore, the proposed PLVC model learns to compress video
towards good perceptual quality at low bit-rate. The experimental results show
that our PLVC approach outperforms the previous traditional and learned
approaches on several perceptual quality metrics. The user study further
validates the outstanding perceptual performance of PLVC in comparison with the
latest learned video compression approaches and the official HEVC test model
(HM 16.20). The codes will be released at https://github.com/RenYang-home/PLVC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Generation of Dense Non-rigid Optical Flow. (arXiv:1812.01946v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.01946">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There hardly exists any large-scale datasets with dense optical flow of
non-rigid motion from real-world imagery as of today. The reason lies mainly in
the required setup to derive ground truth optical flows: a series of images
with known camera poses along its trajectory, and an accurate 3D model from a
textured scene. Human annotation is not only too tedious for large databases,
it can simply hardly contribute to accurate optical flow. To circumvent the
need for manual annotation, we propose a framework to automatically generate
optical flow from real-world videos. The method extracts and matches objects
from video frames to compute initial constraints, and applies a deformation
over the objects of interest to obtain dense optical flow fields. We propose
several ways to augment the optical flow variations. Extensive experimental
results show that training on our automatically generated optical flow
outperforms methods that are trained on rigid synthetic data using FlowNet-S,
LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow
generation framework are released at https://github.com/lhoangan/arap_flow</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Kinship Verification Based on Cross-Generation Feature Interaction Learning. (arXiv:2109.02809v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Kinship verification from facial images has been recognized as an emerging
yet challenging technique in many potential computer vision applications. In
this paper, we propose a novel cross-generation feature interaction learning
(CFIL) framework for robust kinship verification. Particularly, an effective
collaborative weighting strategy is constructed to explore the characteristics
of cross-generation relations by corporately extracting features of both
parents and children image pairs. Specifically, we take parents and children as
a whole to extract the expressive local and non-local features. Different from
the traditional works measuring similarity by distance, we interpolate the
similarity calculations as the interior auxiliary weights into the deep CNN
architecture to learn the whole and natural features. These similarity weights
not only involve corresponding single points but also excavate the multiple
relationships cross points, where local and non-local features are calculated
by using these two kinds of distance measurements. Importantly, instead of
separately conducting similarity computation and feature extraction, we
integrate similarity learning and feature extraction into one unified learning
process. The integrated representations deduced from local and non-local
features can comprehensively express the informative semantics embedded in
images and preserve abundant correlation knowledge from image pairs. Extensive
experiments demonstrate the efficiency and superiority of the proposed model
compared to some state-of-the-art kinship verification methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03223">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Out of all existing frameworks for surgical workflow analysis in endoscopic
videos, action triplet recognition stands out as the only one aiming to provide
truly fine-grained and comprehensive information on surgical activities. This
information, presented as  combinations, is highly
challenging to be accurately identified. Triplet components can be difficult to
recognize individually; in this task, it requires not only performing
recognition simultaneously for all three triplet components, but also correctly
establishing the data association between them. To achieve this task, we
introduce our new model, the Rendezvous (RDV), which recognizes triplets
directly from surgical videos by leveraging attention at two different levels.
We first introduce a new form of spatial attention to capture individual action
triplet components in a scene; called the Class Activation Guided Attention
Mechanism (CAGAM). This technique focuses on the recognition of verbs and
targets using activations resulting from instruments. To solve the association
problem, our RDV model adds a new form of semantic attention inspired by
Transformer networks. Using multiple heads of cross and self attentions, RDV is
able to effectively capture relationships between instruments, verbs, and
targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in
which every frame has been annotated with labels from 100 triplet classes. Our
proposed RDV model significantly improves the triplet prediction mAP by over 9%
compared to the state-of-the-art methods on this dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the few-shot learning (FSL) problem, where a model learns to
recognize new objects with extremely few labeled training data per category.
Most of previous FSL approaches resort to the meta-learning paradigm, where the
model accumulates inductive bias through learning many training tasks so as to
solve a new unseen few-shot task. In contrast, we propose a simple approach to
exploit unlabeled data accompanying the few-shot task for improving few-shot
performance. Firstly, we propose a Dependency Maximization method based on the
Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the
statistical dependency between the embedded feature of those unlabeled data and
their label predictions, together with the supervised loss over the support
set. We then use the obtained model to infer the pseudo-labels for those
unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to
evaluate the credibility of each pseudo-labeled example and select the most
faithful ones into an augmented support set to retrain the model as in the
first step. We iterate the above process until the pseudo-labels for the
unlabeled data becomes stable. Following the standard transductive and
semi-supervised FSL setting, our experiments show that the proposed method
out-performs previous state-of-the-art methods on four widely used benchmarks,
including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Statistical analysis of locally parameterized shapes. (arXiv:2109.03027v1 [stat.ME])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The alignment of shapes has been a crucial step in statistical shape
analysis, for example, in calculating mean shape, detecting locational
differences between two shape populations, and classification. Procrustes
alignment is the most commonly used method and state of the art. In this work,
we uncover that alignment might seriously affect the statistical analysis. For
example, alignment can induce false shape differences and lead to misleading
results and interpretations. We propose a novel hierarchical shape
parameterization based on local coordinate systems. The local parameterized
shapes are translation and rotation invariant. Thus, the inherent alignment
problems from the commonly used global coordinate system for shape
representation can be avoided using this parameterization. The new
parameterization is also superior for shape deformation and simulation. The
method&#x27;s power is demonstrated on the hypothesis testing of simulated data as
well as the left hippocampi of patients with Parkinson&#x27;s disease and controls.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02865">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The task of news article image captioning aims to generate descriptive and
informative captions for news article images. Unlike conventional image
captions that simply describe the content of the image in general terms, news
image captions follow journalistic guidelines and rely heavily on named
entities to describe the image content, often drawing context from the whole
article they are associated with. In this work, we propose a new approach to
this task, motivated by caption guidelines that journalists follow. Our
approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),
leverages the structure of captions to improve the generation quality and guide
our representation design. Experimental results, including detailed ablation
studies, on two large-scale publicly available datasets show that JoGANIC
substantially outperforms state-of-the-art methods both on caption generation
and named entity related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting. (arXiv:2109.02974v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02974">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer, as a strong and flexible architecture for modelling long-range
relations, has been widely explored in vision tasks. However, when used in
video inpainting that requires fine-grained representation, existed method
still suffers from yielding blurry edges in detail due to the hard patch
splitting. Here we aim to tackle this problem by proposing FuseFormer, a
Transformer model designed for video inpainting via fine-grained feature fusion
based on novel Soft Split and Soft Composition operations. The soft split
divides feature map into many patches with given overlapping interval. On the
contrary, the soft composition operates by stitching different patches into a
whole feature map where pixels in overlapping regions are summed up. These two
modules are first used in tokenization before Transformer layers and
de-tokenization after Transformer layers, for effective mapping between tokens
and features. Therefore, sub-patch level information interaction is enabled for
more effective feature propagation between neighboring patches, resulting in
synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer,
we elaborately insert the soft composition and soft split into the feed-forward
network, enabling the 1D linear layers to have the capability of modelling 2D
structure. And, the sub-patch level feature fusion ability is further enhanced.
In both quantitative and qualitative evaluations, our proposed FuseFormer
surpasses state-of-the-art methods. We also conduct detailed analysis to
examine its superiority.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Dietary Assessment Via Integrated Hierarchy Food Classification. (arXiv:2109.02736v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02736">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image-based dietary assessment refers to the process of determining what
someone eats and how much energy and nutrients are consumed from visual data.
Food classification is the first and most crucial step. Existing methods focus
on improving accuracy measured by the rate of correct classification based on
visual information alone, which is very challenging due to the high complexity
and inter-class similarity of foods. Further, accuracy in food classification
is conceptual as description of a food can always be improved. In this work, we
introduce a new food classification framework to improve the quality of
predictions by integrating the information from multiple domains while
maintaining the classification accuracy. We apply a multi-task network based on
a hierarchical structure that uses both visual and nutrition domain specific
information to cluster similar foods. Our method is validated on the modified
VIPER-FoodNet (VFN) food image dataset by including associated energy and
nutrient information. We achieve comparable classification accuracy with
existing methods that use visual information only, but with less error in terms
of energy and nutrient values for the wrong predictions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The study of functional brain connectivity (FC) is important for
understanding the underlying mechanisms of many psychiatric disorders. Many
recent analyses adopt graph convolutional networks, to study non-linear
interactions between functionally-correlated states. However, although patterns
of brain activation are known to be hierarchically organised in both space and
time, many methods have failed to extract powerful spatio-temporal features. To
overcome those challenges, and improve understanding of long-range functional
dynamics, we translate an approach, from the domain of skeleton-based action
recognition, designed to model interactions across space and time. We evaluate
this approach using the Human Connectome Project (HCP) dataset on sex
classification and fluid intelligence prediction. To account for subject
topographic variability of functional organisation, we modelled functional
connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.
Results show a prediction accuracy of 94.4% for sex classification (an increase
of 6.2% compared to other methods), and an improvement of correlation with
fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes
space and time separately. Results suggest that explicit encoding of
spatio-temporal dynamics of brain functional activity may improve the precision
with which behavioural and cognitive phenotypes may be predicted in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor. (arXiv:2109.02786v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02786">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Landmark-based robot self-localization has recently garnered interest as a
highly-compressive domain-invariant approach for performing visual place
recognition (VPR) across domains (e.g., time of day, weather, and season).
However, landmark-based self-localization can be an ill-posed problem for a
passive observer (e.g., manual robot control), as many viewpoints may not
provide an effective landmark view. In this study, we consider an active
self-localization task by an active observer and present a novel reinforcement
learning (RL)-based next-best-view (NBV) planner. Our contributions are as
follows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based
compact scene description as SIMBAD (similarity-based pattern recognition) and
further present its deep learning extension. (2) VPR-to-NBV knowledge transfer:
We address the challenge of RL under uncertainty (i.e., active
self-localization) by transferring the state recognition ability of VPR to the
NBV. (3) NNQL-based NBV: We regard the available VPR as the experience database
by adapting nearest-neighbor approximation of Q-learning (NNQL). The result
shows an extremely compact data structure that compresses both the VPR and NBV
into a single incremental inverted index. Experiments using the public NCLT
dataset validated the effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid advancement in deep learning makes the differentiation of authentic
and manipulated facial images and video clips unprecedentedly harder. The
underlying technology of manipulating facial appearances through deep
generative approaches, enunciated as DeepFake that have emerged recently by
promoting a vast number of malicious face manipulation applications.
Subsequently, the need of other sort of techniques that can assess the
integrity of digital visual content is indisputable to reduce the impact of the
creations of DeepFake. A large body of research that are performed on DeepFake
creation and detection create a scope of pushing each other beyond the current
status. This study presents challenges, research trends, and directions related
to DeepFake creation and detection techniques by reviewing the notable research
in the DeepFake domain to facilitate the development of more robust approaches
that could deal with the more advance DeepFake in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks. (arXiv:2109.02710v1 [stat.AP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02710">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Reducing traffic fatalities and serious injuries is a top priority of the US
Department of Transportation. The computer vision (CV)-based crash anticipation
in the near-crash phase is receiving growing attention. The ability to perceive
fatal crash risks earlier is also critical because it will improve the
reliability of crash anticipation. Yet, annotated image data for training a
reliable AI model for the early visual perception of crash risks are not
abundant. The Fatality Analysis Reporting System contains big data of fatal
crashes. It is a reliable data source for learning the relationship between
driving scene characteristics and fatal crashes to compensate for the
limitation of CV. Therefore, this paper develops a data analytics model, named
scenario-wise, Spatio-temporal attention guidance, from fatal crash report
data, which can estimate the relevance of detected objects to fatal crashes
from their environment and context information. First, the paper identifies
five sparse variables that allow for decomposing the 5-year fatal crash dataset
to develop scenario-wise attention guidance. Then, exploratory analysis of
location- and time-related variables of the crash report data suggests reducing
fatal crashes to spatially defined groups. The group&#x27;s temporal pattern is an
indicator of the similarity of fatal crashes in the group. Hierarchical
clustering and K-means clustering merge the spatially defined groups into six
clusters according to the similarity of their temporal patterns. After that,
association rule mining discovers the statistical relationship between the
temporal information of driving scenes with crash features, for each cluster.
The paper shows how the developed attention guidance supports the design and
implementation of a preliminary CV model that can identify objects of a
possibility to involve in fatal crashes from their environment and context
information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing road pothole detection approaches can be classified as computer
vision-based or machine learning-based. The former approaches typically employ
2-D image analysis/understanding or 3-D point cloud modeling and segmentation
algorithms to detect road potholes from vision sensor data. The latter
approaches generally address road pothole detection using convolutional neural
networks (CNNs) in an end-to-end manner. However, road potholes are not
necessarily ubiquitous and it is challenging to prepare a large well-annotated
dataset for CNN training. In this regard, while computer vision-based methods
were the mainstream research trend in the past decade, machine learning-based
methods were merely discussed. Recently, we published the first stereo
vision-based road pothole detection dataset and a novel disparity
transformation algorithm, whereby the damaged and undamaged road areas can be
highly distinguished. However, there are no benchmarks currently available for
state-of-the-art (SoTA) CNNs trained using either disparity images or
transformed disparity images. Therefore, in this paper, we first discuss the
SoTA CNNs designed for semantic segmentation and evaluate their performance for
road pothole detection with extensive experiments. Additionally, inspired by
graph neural network (GNN), we propose a novel CNN layer, referred to as graph
attention layer (GAL), which can be easily deployed in any existing CNN to
optimize image feature representations for semantic segmentation. Our
experiments compare GAL-DeepLabv3+, our best-performing implementation, with
nine SoTA CNNs on three modalities of training data: RGB images, disparity
images, and transformed disparity images. The experimental results suggest that
our proposed GAL-DeepLabv3+ achieves the best overall pothole detection
accuracy on all training data modalities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration. (arXiv:2109.02722v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02722">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deformable Image Registration (DIR) can benefit from additional guidance
using corresponding landmarks in the images. However, the benefits thereof are
largely understudied, especially due to the lack of automatic detection methods
for corresponding landmarks in three-dimensional (3D) medical images. In this
work, we present a Deep Convolutional Neural Network (DCNN), called DCNN-Match,
that learns to predict landmark correspondences in 3D images in a
self-supervised manner. We explored five variants of DCNN-Match that use
different loss functions and tested DCNN-Match separately as well as in
combination with the open-source registration software Elastix to assess its
impact on a common DIR approach. We employed lower-abdominal Computed
Tomography (CT) scans from cervical cancer patients: 121 pelvic CT scan pairs
containing simulated elastic transformations and 11 pairs demonstrating
clinical deformations. Our results show significant improvement in DIR
performance when landmark correspondences predicted by DCNN-Match were used in
case of simulated as well as clinical deformations. We also observed that the
spatial distribution of the automatically identified landmarks and the
associated matching errors affect the extent of improvement in DIR. Finally,
DCNN-Match was found to generalize well to Magnetic Resonance Imaging (MRI)
scans without requiring retraining, indicating easy applicability to other
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02765">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep neural networks have achieved remarkable success in various
computer vision tasks, they often fail to generalize to new domains and subtle
variations of input images. Several defenses have been proposed to improve the
robustness against these variations. However, current defenses can only
withstand the specific attack used in training, and the models often remain
vulnerable to other input variations. Moreover, these methods often degrade
performance of the model on clean images and do not generalize to out-of-domain
samples. In this paper we present Generative Adversarial Training, an approach
to simultaneously improve the model&#x27;s generalization to the test set and
out-of-domain samples as well as its robustness to unseen adversarial attacks.
Instead of altering a low-level pre-defined aspect of images, we generate a
spectrum of low-level, mid-level and high-level changes using generative models
with a disentangled latent space. Adversarial training with these examples
enable the model to withstand a wide range of attacks by observing a variety of
input alterations during training. We show that our approach not only improves
performance of the model on clean images and out-of-domain samples but also
makes it robust against unforeseen attacks and outperforms prior work. We
validate effectiveness of our method by demonstrating results on various tasks
such as classification, segmentation and object detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CIM: Class-Irrelevant Mapping for Few-Shot Classification. (arXiv:2109.02840v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02840">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot classification (FSC) is one of the most concerned hot issues in
recent years. The general setting consists of two phases: (1) Pre-train a
feature extraction model (FEM) with base data (has large amounts of labeled
samples). (2) Use the FEM to extract the features of novel data (with few
labeled samples and totally different categories from base data), then classify
them with the to-be-designed classifier. The adaptability of pre-trained FEM to
novel data determines the accuracy of novel features, thereby affecting the
final classification performances. To this end, how to appraise the pre-trained
FEM is the most crucial focus in the FSC community. It sounds like traditional
Class Activate Mapping (CAM) based methods can achieve this by overlaying
weighted feature maps. However, due to the particularity of FSC (e.g., there is
no backpropagation when using the pre-trained FEM to extract novel features),
we cannot activate the feature map with the novel classes. To address this
challenge, we propose a simple, flexible method, dubbed as Class-Irrelevant
Mapping (CIM). Specifically, first, we introduce dictionary learning theory and
view the channels of the feature map as the bases in a dictionary. Then we
utilize the feature map to fit the feature vector of an image to achieve the
corresponding channel weights. Finally, we overlap the weighted feature map for
visualization to appraise the ability of pre-trained FEM on novel data. For
fair use of CIM in evaluating different models, we propose a new measurement
index, called Feature Localization Accuracy (FLA). In experiments, we first
compare our CIM with CAM in regular tasks and achieve outstanding performances.
Next, we use our CIM to appraise several classical FSC frameworks without
considering the classification results and discuss them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">STRIVE: Scene Text Replacement In Videos. (arXiv:2109.02762v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose replacing scene text in videos using deep style transfer and
learned photometric transformations.Building on recent progress on still image
text replacement,we present extensions that alter text while preserving the
appearance and motion characteristics of the original video.Compared to the
problem of still image text replacement,our method addresses additional
challenges introduced by video, namely effects induced by changing lighting,
motion blur, diverse variations in camera-object pose over time,and
preservation of temporal consistency. We parse the problem into three steps.
First, the text in all frames is normalized to a frontal pose using a
spatio-temporal trans-former network. Second, the text is replaced in a single
reference frame using a state-of-art still-image text replacement method.
Finally, the new text is transferred from the reference to remaining frames
using a novel learned image transformation network that captures lighting and
blur effects in a temporally consistent manner. Results on synthetic and
challenging real videos show realistic text trans-fer, competitive quantitative
and qualitative performance,and superior inference speed relative to
alternatives. We introduce new synthetic and real-world datasets with paired
text objects. To the best of our knowledge this is the first attempt at deep
video text replacement.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02740">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We address the problem of estimating the shape of a person&#x27;s head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person&#x27;s pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person&#x27;s head, using both traditional facial landmarks and scalp features
extracted from the head&#x27;s segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02749">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pano3D is a new benchmark for depth estimation from spherical panoramas. It
aims to assess performance across all depth estimation traits, the primary
direct depth estimation performance targeting precision and accuracy, and also
the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D
moves beyond typical intra-dataset evaluation to inter-dataset performance
assessment. By disentangling the capacity to generalize to unseen data into
different test splits, Pano3D represents a holistic benchmark for $360^o$ depth
estimation. We use it as a basis for an extended analysis seeking to offer
insights into classical choices for depth estimation. This results in a solid
baseline for panoramic depth that follow-up works can build upon to steer
future progress.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02716">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Crop and weed monitoring is an important challenge for agriculture and food
production nowadays. Thanks to recent advances in data acquisition and
computation technologies, agriculture is evolving to a more smart and precision
farming to meet with the high yield and high quality crop production.
Classification and recognition in Unmanned Aerial Vehicles (UAV) images are
important phases for crop monitoring. Advances in deep learning models relying
on Convolutional Neural Network (CNN) have achieved high performances in image
classification in the agricultural domain. Despite the success of this
architecture, CNN still faces many challenges such as high computation cost,
the need of large labelled datasets, ... Natural language processing&#x27;s
transformer architecture can be an alternative approach to deal with CNN&#x27;s
limitations. Making use of the self-attention paradigm, Vision Transformer
(ViT) models can achieve competitive or better results without applying any
convolution operations. In this paper, we adopt the self-attention mechanism
via the ViT models for plant classification of weeds and crops: red beet,
off-type beet (green leaves), parsley and spinach. Our experiments show that
with small set of labelled training data, ViT models perform better compared to
state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy
of 99.8\% achieved by the ViT model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02860">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph convolutional networks (GCNs) achieve promising performance for
skeleton-based action recognition. However, in most GCN-based methods, the
spatial-temporal graph convolution is strictly restricted by the graph topology
while only captures the short-term temporal context, thus lacking the
flexibility of feature extraction. In this work, we present a novel
architecture, named Graph Convolutional skeleton Transformer (GCsT), which
addresses limitations in GCNs by introducing Transformer. Our GCsT employs all
the benefits of Transformer (i.e. dynamical attention and global context) while
keeps the advantages of GCNs (i.e. hierarchy and local topology structure). In
GCsT, the spatial-temporal GCN forces the capture of local dependencies while
Transformer dynamically extracts global spatial-temporal relationships.
Furthermore, the proposed GCsT shows stronger expressive capability by adding
additional information present in skeleton sequences. Incorporating the
Transformer allows that information to be introduced into the model almost
effortlessly. We validate the proposed GCsT by conducting extensive
experiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU
RGB+D 120 and Northwestern-UCLA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02763">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Humans can robustly recognize and localize objects by using visual and/or
auditory cues. While machines are able to do the same with visual data already,
less work has been done with sounds. This work develops an approach for scene
understanding purely based on binaural sounds. The considered tasks include
predicting the semantic masks of sound-making objects, the motion of
sound-making objects, and the depth map of the scene. To this aim, we propose a
novel sensor setup and record a new audio-visual dataset of street scenes with
eight professional binaural microphones and a 360-degree camera. The
co-existence of visual and audio cues is leveraged for supervision transfer. In
particular, we employ a cross-modal distillation framework that consists of
multiple vision teacher methods and a sound student method -- the student
method is trained to generate the same results as the teacher methods do. This
way, the auditory system can be trained without using human annotations. To
further boost the performance, we propose another novel auxiliary task, coined
Spatial Sound Super-Resolution, to increase the directional resolution of
sounds. We then formulate the four tasks into one end-to-end trainable
multi-tasking network aiming to boost the overall performance. Experimental
results show that 1) our method achieves good results for all four tasks, 2)
the four tasks are mutually beneficial -- training them together achieves the
best performance, 3) the number and orientation of microphones are both
important, and 4) features learned from the standard spectrogram and features
obtained by the classic signal processing pipeline are complementary for
auditory perception tasks. The data and code are released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Driven by the need to capture users&#x27; evolving interests and optimize their
long-term experiences, more and more recommender systems have started to model
recommendation as a Markov decision process and employ reinforcement learning
to address the problem. Shouldn&#x27;t research on the fairness of recommender
systems follow the same trend from static evaluation and one-shot intervention
to dynamic monitoring and non-stop control? In this paper, we portray the
recent developments in recommender systems first and then discuss how fairness
could be baked into the reinforcement learning techniques for recommendation.
Moreover, we argue that in order to make further progress in recommendation
fairness, we may want to consider multi-agent (game-theoretic) optimization,
multi-objective (Pareto) optimization, and simulation-based optimization, in
the general framework of stochastic games.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03154">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Educational recommenders have received much less attention in comparison to
e-commerce and entertainment-related recommenders, even though efficient
intelligent tutors have great potential to improve learning gains. One of the
main challenges in advancing this research direction is the scarcity of large,
publicly available datasets. In this work, we release a large, novel dataset of
learners engaging with educational videos in-the-wild. The dataset, named
Personalised Educational Engagement with Knowledge Topics PEEK, is the first
publicly available dataset of this nature. The video lectures have been
associated with Wikipedia concepts related to the material of the lecture, thus
providing a humanly intuitive taxonomy. We believe that granular learner
engagement signals in unison with rich content representations will pave the
way to building powerful personalization algorithms that will revolutionise
educational and informational recommendation systems. Towards this goal, we 1)
construct a novel dataset from a popular video lecture repository, 2) identify
a set of benchmark algorithms to model engagement, and 3) run extensive
experimentation on the PEEK dataset to demonstrate its value. Our experiments
with the dataset show promise in building powerful informational recommender
systems. The dataset and the support code is available publicly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refining BERT Embeddings for Document Hashing via Mutual Information Maximization. (arXiv:2109.02867v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02867">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing unsupervised document hashing methods are mostly established on
generative models. Due to the difficulties of capturing long dependency
structures, these methods rarely model the raw documents directly, but instead
to model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In
this paper, we propose to learn hash codes from BERT embeddings after observing
their tremendous successes on downstream tasks. As a first try, we modify
existing generative hashing models to accommodate the BERT embeddings. However,
little improvement is observed over the codes learned from the old BOW or TFIDF
features. We attribute this to the reconstruction requirement in the generative
hashing, which will enforce irrelevant information that is abundant in the BERT
embeddings also compressed into the codes. To remedy this issue, a new
unsupervised hashing paradigm is further proposed based on the mutual
information (MI) maximization principle. Specifically, the method first
constructs appropriate global and local codes from the documents and then seeks
to maximize their mutual information. Experimental results on three benchmark
datasets demonstrate that the proposed method is able to generate hash codes
that outperform existing ones learned from BOW features by a substantial
margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid advancement in deep learning makes the differentiation of authentic
and manipulated facial images and video clips unprecedentedly harder. The
underlying technology of manipulating facial appearances through deep
generative approaches, enunciated as DeepFake that have emerged recently by
promoting a vast number of malicious face manipulation applications.
Subsequently, the need of other sort of techniques that can assess the
integrity of digital visual content is indisputable to reduce the impact of the
creations of DeepFake. A large body of research that are performed on DeepFake
creation and detection create a scope of pushing each other beyond the current
status. This study presents challenges, research trends, and directions related
to DeepFake creation and detection techniques by reviewing the notable research
in the DeepFake domain to facilitate the development of more robust approaches
that could deal with the more advance DeepFake in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse Distributed Memory using Spiking Neural Networks on Nengo. (arXiv:2109.03111v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03111">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a Spiking Neural Network (SNN) based Sparse Distributed Memory
(SDM) implemented on the Nengo framework. We have based our work on previous
work by Furber et al, 2004, implementing SDM using N-of-M codes. As an integral
part of the SDM design, we have implemented Correlation Matrix Memory (CMM)
using SNN on Nengo. Our SNN implementation uses Leaky Integrate and Fire (LIF)
spiking neuron models on Nengo. Our objective is to understand how well
SNN-based SDMs perform in comparison to conventional SDMs. Towards this, we
have simulated both conventional and SNN-based SDM and CMM on Nengo. We observe
that SNN-based models perform similarly as the conventional ones. In order to
evaluate the performance of different SNNs, we repeated the experiment using
Adaptive-LIF, Spiking Rectified Linear Unit, and Izhikevich models and obtained
similar results. We conclude that it is indeed feasible to develop some types
of associative memories using spiking neurons whose memory capacity and other
features are similar to the performance without SNNs. Finally we have
implemented an application where MNIST images, encoded with N-of-M codes, are
associated with their labels and stored in the SNN-based SDM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02859">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>User purchasing prediction with multi-behavior information remains a
challenging problem for current recommendation systems. Various methods have
been proposed to address it via leveraging the advantages of graph neural
networks (GNNs) or multi-task learning. However, most existing works do not
take the complex dependencies among different behaviors of users into
consideration. They utilize simple and fixed schemes, like neighborhood
information aggregation or mathematical calculation of vectors, to fuse the
embeddings of different user behaviors to obtain a unified embedding to
represent a user&#x27;s behavioral patterns which will be used in downstream
recommendation tasks. To tackle the challenge, in this paper, we first propose
the concept of hyper meta-path to construct hyper meta-paths or hyper
meta-graphs to explicitly illustrate the dependencies among different behaviors
of a user. How to obtain a unified embedding for a user from hyper meta-paths
and avoid the previously mentioned limitations simultaneously is critical.
Thanks to the recent success of graph contrastive learning, we leverage it to
learn embeddings of user behavior patterns adaptively instead of assigning a
fixed scheme to understand the dependencies among different behaviors. A new
graph contrastive learning based framework is proposed by coupling with hyper
meta-paths, namely HMG-CR, which consistently and significantly outperforms all
baselines in extensive comparison experiments.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Normalizing field flows: Solving forward and inverse stochastic differential equations using physics-informed flow models. (arXiv:2108.12956v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12956">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce in this work the normalizing field flows (NFF) for learning
random fields from scattered measurements. More precisely, we construct a
bijective transformation (a normalizing flow characterizing by neural networks)
between a Gaussian random field with the Karhunen-Lo\&#x60;eve (KL) expansion
structure and the target stochastic field, where the KL expansion coefficients
and the invertible networks are trained by maximizing the sum of the
log-likelihood on scattered measurements. This NFF model can be used to solve
data-driven forward, inverse, and mixed forward/inverse stochastic partial
differential equations in a unified framework. We demonstrate the capability of
the proposed NFF model for learning Non Gaussian processes and different types
of stochastic partial differential equations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14038">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel deep learning framework for predicting permeability of
porous media from their digital images. Unlike convolutional neural networks,
instead of feeding the whole image volume as inputs to the network, we model
the boundary between solid matrix and pore spaces as point clouds and feed them
as inputs to a neural network based on the PointNet architecture. This approach
overcomes the challenge of memory restriction of graphics processing units and
its consequences on the choice of batch size, and convergence. Compared to
convolutional neural networks, the proposed deep learning methodology provides
freedom to select larger batch sizes, due to reducing significantly the size of
network inputs. Specifically, we use the classification branch of PointNet and
adjust it for a regression task. As a test case, two and three dimensional
synthetic digital rock images are considered. We investigate the effect of
different components of our neural network on its performance. We compare our
deep learning strategy with a convolutional neural network from various
perspectives, specifically for maximum possible batch size. We inspect the
generalizability of our network by predicting the permeability of real-world
rock samples as well as synthetic digital rocks that are statistically
different from the samples used during training. The network predicts the
permeability of digital rocks a few thousand times faster than a Lattice
Boltzmann solver with a high level of prediction accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Convergence Analysis of Nonconvex Distributed Stochastic Zeroth-order Coordinate Method. (arXiv:2103.12954v2 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12954">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper investigates the stochastic distributed nonconvex optimization
problem of minimizing a global cost function formed by the summation of $n$
local cost functions. We solve such a problem by involving zeroth-order (ZO)
information exchange. In this paper, we propose a ZO distributed primal-dual
coordinate method (ZODIAC) to solve the stochastic optimization problem. Agents
approximate their own local stochastic ZO oracle along with coordinates with an
adaptive smoothing parameter. We show that the proposed algorithm achieves the
convergence rate of $\mathcal{O}(\sqrt{p}/\sqrt{T})$ for general nonconvex cost
functions. We demonstrate the efficiency of proposed algorithms through a
numerical example in comparison with the existing state-of-the-art centralized
and distributed ZO algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09262">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Posit Number System was introduced in 2017 as a replacement for
floating-point numbers. Since then, the community has explored its application
in Neural Network related tasks and produced some unit designs which are still
far from being competitive with their floating-point counterparts. This paper
proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to
significantly reduce the complexity of posit multipliers, the most power-hungry
units within Deep Neural Network architectures. When comparing with
state-of-the-art posit multipliers, experiments show that the proposed
technique reduces the area, power, and delay of hardware multipliers up to
72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semiparametric Bayesian Networks. (arXiv:2109.03008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce semiparametric Bayesian networks that combine parametric and
nonparametric conditional probability distributions. Their aim is to
incorporate the advantages of both components: the bounded complexity of
parametric models and the flexibility of nonparametric ones. We demonstrate
that semiparametric Bayesian networks generalize two well-known types of
Bayesian networks: Gaussian Bayesian networks and kernel density estimation
Bayesian networks. For this purpose, we consider two different conditional
probability distributions required in a semiparametric Bayesian network. In
addition, we present modifications of two well-known algorithms (greedy
hill-climbing and PC) to learn the structure of a semiparametric Bayesian
network from data. To realize this, we employ a score function based on
cross-validation. In addition, using a validation dataset, we apply an
early-stopping criterion to avoid overfitting. To evaluate the applicability of
the proposed algorithm, we conduct an exhaustive experiment on synthetic data
sampled by mixing linear and nonlinear functions, multivariate normal data
sampled from Gaussian Bayesian networks, real data from the UCI repository, and
bearings degradation data. As a result of this experiment, we conclude that the
proposed algorithm accurately learns the combination of parametric and
nonparametric components, while achieving a performance comparable with those
provided by state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Interactive slice visualization for exploring machine learning models. (arXiv:2101.06986v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine learning models fit complex algorithms to arbitrarily large datasets.
These algorithms are well-known to be high on performance and low on
interpretability. We use interactive visualization of slices of predictor space
to address the interpretability deficit; in effect opening up the black-box of
machine learning algorithms, for the purpose of interrogating, explaining,
validating and comparing model fits. Slices are specified directly through
interaction, or using various touring algorithms designed to visit
high-occupancy sections or regions where the model fits have interesting
properties. The methods presented here are implemented in the R package
\pkg{condvis2}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What you need to know to train recurrent neural networks to make Flip Flops memories and more. (arXiv:2010.07858v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training neural networks to perform different tasks is relevant across
various disciplines that go beyond Machine Learning. In particular, Recurrent
Neural Networks (RNN) are of great interest to different scientific
communities, for example, Computational Neuroscience research and Dynamical
Systems among others. Open-source frameworks dedicated to Machine Learning such
as Tensorflow and Keras has produced significant changes in the development of
technologies that we currently use. One relevant problem that can be approached
is how to build the models for the study of dynamical systems, and how to
extract the relevant information to be able to answer the scientific questions
of interest. The purpose of the present work is to contribute to this aim by
using a temporal processing task, in this case, a 3-bit Flip Flop memory, to
show the modeling procedure in every step: from equations to the software code
using Tensorflow and Keras. The obtained networks are analyzed to describe the
dynamics and to show different visualization and analysis tools. The code
developed in this work is provided to be used as a base for model other
systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Practically Feasible Policies for Online 3D Bin Packing. (arXiv:2108.13680v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13680">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under fair
evaluation protocols. In this paper, we propose a new learning scheme to
enforce domain invariance in the space of the gradients of the loss function:
specifically, we introduce a regularization term that matches the domain-level
variances of gradients across training domains. Critically, our strategy, named
Fishr, exhibits close relations with the Fisher Information and the Hessian of
the loss. We show that forcing domain-level gradient covariances to be similar
during the learning procedure eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. In
particular, Fishr improves the state of the art on the DomainBed benchmark and
performs significantly better than Empirical Risk Minimization. The code is
released at https://github.com/alexrame/fishr.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quick Learning Mechanism with Cross-Domain Adaptation for Intelligent Fault Diagnosis. (arXiv:2103.08889v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The fault diagnostic model trained for a laboratory case machine fails to
perform well on the industrial machines running under variable operating
conditions. For every new operating condition of such machines, a new
diagnostic model has to be trained which is a time-consuming and uneconomical
process. Therefore, we propose a quick learning mechanism that can transform
the existing diagnostic model into a new model suitable for industrial machines
operating in different conditions. The proposed method uses the Net2Net
transformation followed by a fine-tuning to cancel/minimize the maximum mean
discrepancy between the new data and the previous one. The fine-tuning of the
model requires a very less amount of labelled target samples and very few
iterations of training. Therefore, the proposed method is capable of learning
the new target data pattern quickly. The effectiveness of the proposed fault
diagnosis method has been demonstrated on the Case Western Reserve University
dataset, Intelligent Maintenance Systems bearing dataset, and Paderborn
university dataset under the wide variations of the operating conditions. It
has been validated that the diagnostic model trained on artificially damaged
fault datasets can be used to quickly train another model for a real damage
dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Learning for Exotic Option Valuation. (arXiv:2103.12551v2 [q-fin.CP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12551">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A common approach to valuing exotic options involves choosing a model and
then determining its parameters to fit the volatility surface as closely as
possible. We refer to this as the model calibration approach (MCA). A
disadvantage of MCA is that some information in the volatility surface is lost
during the calibration process and the prices of exotic options will not in
general be consistent with those of plain vanilla options. We consider an
alternative approach where the structure of the user&#x27;s preferred model is
preserved but points on the volatility are features input to a neural network.
We refer to this as the volatility feature approach (VFA) model. We conduct
experiments showing that VFA can be expected to outperform MCA for the
volatility surfaces encountered in practice. Once the upfront computational
time has been invested in developing the neural network, the valuation of
exotic options using VFA is very fast.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refined approachability algorithms and application to regret minimization with global costs. (arXiv:2009.03831v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03831">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Blackwell&#x27;s approachability is a framework where two players, the Decision
Maker and the Environment, play a repeated game with vector-valued payoffs. The
goal of the Decision Maker is to make the average payoff converge to a given
set called the target. When this is indeed possible, simple algorithms which
guarantee the convergence are known. This abstract tool was successfully used
for the construction of optimal strategies in various repeated games, but also
found several applications in online learning. By extending an approach
proposed by (Abernethy et al., 2011), we construct and analyze a class of
Follow the Regularized Leader algorithms (FTRL) for Blackwell&#x27;s approachability
which are able to minimize not only the Euclidean distance to the target set
(as it is often the case in the context of Blackwell&#x27;s approachability) but a
wide range of distance-like quantities. This flexibility enables us to apply
these algorithms to closely minimize the quantity of interest in various online
learning problems. In particular, for regret minimization with $\ell_p$ global
costs, we obtain the first bounds with explicit dependence in $p$ and the
dimension $d$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Personalized Glucose Level Forecasting Using Attention-based Recurrent Neural Networks. (arXiv:2106.00884v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00884">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we study the problem of blood glucose forecasting and provide
a deep personalized solution. Predicting blood glucose level in people with
diabetes has significant value because health complications of abnormal glucose
level are serious, sometimes even leading to death. Therefore, having a model
that can accurately and quickly warn patients of potential problems is
essential. To develop a better deep model for blood glucose forecasting, we
analyze the data and detect important patterns. These observations helped us to
propose a method that has several key advantages over existing methods: 1- it
learns a personalized model for each patient as well as a global model; 2- it
uses an attention mechanism and extracted time features to better learn
long-term dependencies in the data; 3- it introduces a new, robust training
procedure for time series data. We empirically show the efficacy of our model
on a real dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training sample re-weighting is an effective approach for tackling data
biases such as imbalanced and corrupted labels. Recent methods develop
learning-based algorithms to learn sample re-weighting strategies jointly with
model training based on the frameworks of reinforcement learning and meta
learning. However, depending on additional unbiased reward data is limiting
their general applicability. Furthermore, existing learning-based sample
re-weighting methods require nested optimizations of models and weighting
parameters, which requires expensive second-order computation. This paper
addresses these two problems and presents a novel learning-based fast sample
re-weighting (FSR) method that does not require additional reward data. The
method is based on two key ideas: learning from history to build proxy reward
data and feature sharing to reduce the optimization cost. Our experiments show
the proposed method achieves competitive results compared to state of the arts
on label noise robustness and long-tailed recognition, and does so while
achieving significantly improved training efficiency. The source code is
publicly available at
https://github.com/google-research/google-research/tree/master/ieg.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v3 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Jeffreys divergence is a renown symmetrization of the oriented
Kullback-Leibler divergence broadly used in information sciences. Since the
Jeffreys divergence between Gaussian mixture models is not available in
closed-form, various techniques with pros and cons have been proposed in the
literature to either estimate, approximate, or lower and upper bound this
divergence. In this paper, we propose a simple yet fast heuristic to
approximate the Jeffreys divergence between two univariate Gaussian mixtures
with arbitrary number of components. Our heuristic relies on converting the
mixtures into pairs of dually parameterized probability densities belonging to
an exponential family. In particular, we consider the versatile polynomial
exponential family densities, and design a divergence to measure in closed-form
the goodness of fit between a Gaussian mixture and its polynomial exponential
density approximation. This goodness-of-fit divergence is a generalization of
the Hyv\&quot;arinen divergence used to estimate models with computationally
intractable normalizers. It allows us to perform model selection by choosing
the orders of the polynomial exponential densities used to approximate the
mixtures. We demonstrate experimentally that our heuristic to approximate the
Jeffreys divergence improves by several orders of magnitude the computational
time of stochastic Monte Carlo estimations while approximating reasonably well
the Jeffreys divergence, specially when the mixtures have a very small number
of modes. Besides, our mixture-to-exponential family conversion techniques may
prove useful in other settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Novelty detection using ensembles with regularized disagreement. (arXiv:2012.05825v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05825">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite their excellent performance on in-distribution (ID) data, machine
learning-based prediction systems often predict out-of-distribution (OOD)
samples incorrectly while indicating high confidence. Instead, they should flag
samples that are not similar to the training data, for example, when new
classes emerge over time. Even though current OOD detection algorithms can
successfully distinguish completely different data sets, they fail to reliably
identify samples from novel classes. We develop a new ensemble-based procedure
that promotes model diversity and exploits regularization to limit disagreement
to only OOD samples, using a batch containing an unknown mixture of ID and OOD
data. We show that our procedure significantly outperforms state-of-the-art
methods, including those that have access, during training, to data that is
known to be OOD. We run extensive comparisons of our approach on a variety of
novel-class detection scenarios, on standard image data sets such as
SVHN/CIFAR-10/CIFAR-100, as well as on new disease detection on medical image
data sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Controlled Actor-Critic. (arXiv:2109.02517v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02517">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>On error of value function inevitably causes an overestimation phenomenon and
has a negative impact on the convergence of the algorithms. To mitigate the
negative effects of the approximation error, we propose Error Controlled
Actor-critic which ensures confining the approximation error in value function.
We present an analysis of how the approximation error can hinder the
optimization process of actor-critic methods.Then, we derive an upper boundary
of the approximation error of Q function approximator and find that the error
can be lowered by restricting on the KL-divergence between every two
consecutive policies when training the policy. The results of experiments on a
range of continuous control tasks demonstrate that the proposed actor-critic
algorithm apparently reduces the approximation error and significantly
outperforms other model-free RL algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Diff-ResNets for Few-shot Learning -- an ODE Perspective. (arXiv:2105.03155v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Interpreting deep neural networks from the ordinary differential equations
(ODEs) perspective has inspired many efficient and robust network
architectures. However, existing ODE based approaches ignore the relationship
among data points, which is a critical component in many problems including
few-shot learning and semi-supervised learning. In this paper, inspired by the
diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to
strengthen the interactions among data points. Under the structured data
assumption, it is proved that the diffusion mechanism can decrease the
distance-diameter ratio that improves the separability of inter-class points
and reduces the distance among local intra-class points. This property can be
easily adopted by the residual networks for constructing the separable
hyperplanes. The synthetic binary classification experiments demonstrate the
effectiveness of the proposed diffusion mechanism. Moreover, extensive
experiments of few-shot image classification and semi-supervised graph node
classification in various datasets validate the advantages of the proposed
Diff-ResNet over existing few-shot learning methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation. (arXiv:2106.10796v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10796">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Communication overhead is the key challenge for distributed training.
Gradient compression is a widely used approach to reduce communication traffic.
When combining with parallel communication mechanism method like pipeline,
gradient compression technique can greatly alleviate the impact of
communication overhead. However, there exists two problems of gradient
compression technique to be solved. Firstly, gradient compression brings in
extra computation cost, which will delay the next training iteration. Secondly,
gradient compression usually leads to the decrease of convergence accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GANSER: A Self-supervised Data Augmentation Framework for EEG-based Emotion Recognition. (arXiv:2109.03124v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03124">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The data scarcity problem in Electroencephalography (EEG) based affective
computing results into difficulty in building an effective model with high
accuracy and stability using machine learning algorithms especially deep
learning models. Data augmentation has recently achieved considerable
performance improvement for deep learning models: increased accuracy,
stability, and reduced over-fitting. In this paper, we propose a novel data
augmentation framework, namely Generative Adversarial Network-based
Self-supervised Data Augmentation (GANSER). As the first to combine adversarial
training with self-supervised learning for EEG-based emotion recognition, the
proposed framework can generate high-quality and high-diversity simulated EEG
samples. In particular, we utilize adversarial training to learn an EEG
generator and force the generated EEG signals to approximate the distribution
of real samples, ensuring the quality of augmented samples. A transformation
function is employed to mask parts of EEG signals and force the generator to
synthesize potential EEG signals based on the remaining parts, to produce a
wide variety of samples. The masking possibility during transformation is
introduced as prior knowledge to guide to extract distinguishable features for
simulated EEG signals and generalize the classifier to the augmented sample
space. Finally, extensive experiments demonstrate our proposed method can help
emotion recognition for performance gain and achieve state-of-the-art results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\&#x27;echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OdoNet: Untethered Speed Aiding for Vehicle Navigation Without Hardware Wheeled Odometer. (arXiv:2109.03091v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03091">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Odometer has been proven to significantly improve the accuracy of the Global
Navigation Satellite System / Inertial Navigation System (GNSS/INS) integrated
vehicle navigation in GNSS-challenged environments. However, the odometer is
inaccessible in many applications, especially for aftermarket devices. To apply
forward speed aiding without hardware wheeled odometer, we propose OdoNet, an
untethered one-dimensional Convolution Neural Network (CNN)-based
pseudo-odometer model learning from a single Inertial Measurement Unit (IMU),
which can act as an alternative to the wheeled odometer. Dedicated experiments
have been conducted to verify the feasibility and robustness of the OdoNet. The
results indicate that the IMU individuality, the vehicle loads, and the road
conditions have little impact on the robustness and precision of the OdoNet,
while the IMU biases and the mounting angles may notably ruin the OdoNet. Thus,
a data-cleaning procedure is added to effectively mitigate the impacts of the
IMU biases and the mounting angles. Compared to the process using only
non-holonomic constraint (NHC), after employing the pseudo-odometer, the
positioning error is reduced by around 68%, while the percentage is around 74%
for the hardware wheeled odometer. In conclusion, the proposed OdoNet can be
employed as an untethered pseudo-odometer for vehicle navigation, which can
efficiently improve the accuracy and reliability of the positioning in
GNSS-denied environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SimJEB: Simulated Jet Engine Bracket Dataset. (arXiv:2105.03534v2 [cs.CE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03534">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper introduces the Simulated Jet Engine Bracket Dataset (SimJEB): a
new, public collection of crowdsourced mechanical brackets and accompanying
structural simulations. SimJEB is applicable to a wide range of geometry
processing tasks; the complexity of the shapes in SimJEB offer a challenge to
automated geometry cleaning and meshing, while categorical labels and
structural simulations facilitate classification and regression (i.e.
engineering surrogate modeling). In contrast to existing shape collections,
SimJEB&#x27;s models are all designed for the same engineering function and thus
have consistent structural loads and support conditions. On the other hand,
SimJEB models are more complex, diverse, and realistic than the synthetically
generated datasets commonly used in parametric surrogate model evaluation. The
designs in SimJEB were derived from submissions to the GrabCAD Jet Engine
Bracket Challenge: an open engineering design competition with over 700
hand-designed CAD entries from 320 designers representing 56 countries. Each
model has been cleaned, categorized, meshed, and simulated with finite element
analysis according to the original competition specifications. The result is a
collection of 381 diverse, high-quality and application-focused designs for
advancing geometric deep learning, engineering surrogate modeling, automated
cleaning and related geometry processing tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Auxiliary Diagnosing Coronary Stenosis Using Machine Learning. (arXiv:2007.10316v4 [q-bio.TO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10316">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How to accurately classify and diagnose whether an individual has Coronary
Stenosis (CS) without invasive physical examination? This problem has not been
solved satisfactorily. To this end, the four machine learning (ML) algorithms,
i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and
Random Forest (RF) are employed in this paper. First, eleven features including
basic information of an individual, symptoms and results of routine physical
examination are selected, as well as one label is specified, indicating whether
an individual suffers from different severity of coronary artery stenosis or
not. On the basis of it, a sample set is constructed. Second, each of these
four ML algorithms learns from the sample set to obtain the corresponding
optimal classified results, respectively. The experimental results show that:
RF performs better than other three algorithms, and the former algorithm
classifies whether an individual has CS with an accuracy of 95.7% (&#x3D;90/94).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00632">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We train an object detector built from convolutional neural networks to count
interference fringes in elliptical antinode regions in frames of high-speed
video recordings of transient oscillations in Caribbean steelpan drums
illuminated by electronic speckle pattern interferometry (ESPI). The
annotations provided by our model aim to contribute to the understanding of
time-dependent behavior in such drums by tracking the development of
sympathetic vibration modes. The system is trained on a dataset of crowdsourced
human-annotated images obtained from the Zooniverse Steelpan Vibrations
Project. Due to the small number of human-annotated images and the ambiguity of
the annotation task, we also evaluate the model on a large corpus of synthetic
images whose properties have been matched to the real images by style transfer
using a Generative Adversarial Network. Applying the model to thousands of
unlabeled video frames, we measure oscillations consistent with audio
recordings of these drum strikes. One unanticipated result is that sympathetic
oscillations of higher-octave notes significantly precede the rise in sound
intensity of the corresponding second harmonic tones; the mechanism responsible
for this remains unidentified. This paper primarily concerns the development of
the predictive model; further exploration of the steelpan images and deeper
physical insights await its further application.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The study of functional brain connectivity (FC) is important for
understanding the underlying mechanisms of many psychiatric disorders. Many
recent analyses adopt graph convolutional networks, to study non-linear
interactions between functionally-correlated states. However, although patterns
of brain activation are known to be hierarchically organised in both space and
time, many methods have failed to extract powerful spatio-temporal features. To
overcome those challenges, and improve understanding of long-range functional
dynamics, we translate an approach, from the domain of skeleton-based action
recognition, designed to model interactions across space and time. We evaluate
this approach using the Human Connectome Project (HCP) dataset on sex
classification and fluid intelligence prediction. To account for subject
topographic variability of functional organisation, we modelled functional
connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.
Results show a prediction accuracy of 94.4% for sex classification (an increase
of 6.2% compared to other methods), and an improvement of correlation with
fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes
space and time separately. Results suggest that explicit encoding of
spatio-temporal dynamics of brain functional activity may improve the precision
with which behavioural and cognitive phenotypes may be predicted in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged as one of the key ingredients to reduce
the size of neural networks for their deployments to resource-limited devices.
In order to overcome the nature of transforming continuous activations and
weights to discrete ones, recent study called Relaxed Quantization (RQ)
[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that
allows this transformation with efficient gradient-based optimization. However,
RQ with this Gumbel-Softmax relaxation still suffers from bias-variance
trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve
the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses
multi-class straight-through estimator to effectively reduce the bias and
variance, along with a new regularization technique, DropBits that replaces
dropout regularization to randomly drop the bits instead of neurons to further
reduce the bias of the multi-class straight-through estimator in SRQ. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer
using DropBits. We experimentally validate our method on various benchmark
datasets and network architectures, and also support the quantized lottery
ticket hypothesis: learning heterogeneous quantization levels outperforms the
case using the same but fixed quantization levels from scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Explanations for Occluded Images. (arXiv:2103.03622v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03622">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing algorithms for explaining the output of image classifiers perform
poorly on inputs where the object of interest is partially occluded. We present
a novel, black-box algorithm for computing explanations that uses a principled
approach based on causal theory. We have implemented the method in the
DEEPCOVER tool. We obtain explanations that are much more accurate than those
generated by the existing explanation tools on images with occlusions and
observe a level of performance comparable to the state of the art when
explaining images without occlusions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model&#x27;s dependence on key facial features. Second,
four of today&#x27;s state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online hate speech has caught everyone&#x27;s attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity&#x27;s identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distance preserving model order reduction of graph-Laplacians and cluster analysis. (arXiv:1809.03048v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.03048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph-Laplacians and their spectral embeddings play an important role in
multiple areas of machine learning. This paper is focused on graph-Laplacian
dimension reduction for the spectral clustering of data as a primary
application. Spectral embedding provides a low-dimensional parametrization of
the data manifold which makes the subsequent task (e.g., clustering) much
easier. However, despite reducing the dimensionality of data, the overall
computational cost may still be prohibitive for large data sets due to two
factors. First, computing the partial eigendecomposition of the graph-Laplacian
typically requires a large Krylov subspace. Second, after the spectral
embedding is complete, one still has to operate with the same number of data
points. For example, clustering of the embedded data is typically performed
with various relaxations of k-means which computational cost scales poorly with
respect to the size of data set. In this work, we switch the focus from the
entire data set to a subset of graph vertices (target subset). We develop two
novel algorithms for such low-dimensional representation of the original graph
that preserves important global distances between the nodes of the target
subset. In particular, it allows to ensure that target subset clustering is
consistent with the spectral clustering of the full data set if one would
perform such. That is achieved by a properly parametrized reduced-order model
(ROM) of the graph-Laplacian that approximates accurately the diffusion
transfer function of the original graph for inputs and outputs restricted to
the target subset. Working with a small target subset reduces greatly the
required dimension of Krylov subspace and allows to exploit the conventional
algorithms (like approximations of k-means) in the regimes when they are most
robust and efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Utilizing a digital swarm intelligence platform to improve consensus among radiologists and exploring its applications. (arXiv:2107.07341v2 [cs.HC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07341">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Radiologists today play a key role in making diagnostic decisions and
labeling images for training A.I. algorithms. Low inter-reader reliability
(IRR) can be seen between experts when interpreting challenging cases. While
teams-based decisions are known to outperform individual decisions,
inter-personal biases often creep up in group interactions which limit
non-dominant participants from expressing true opinions. To overcome the dual
problems of low consensus and inter-personal bias, we explored a solution
modeled on biological swarms of bees. Two separate cohorts; three radiologists
and five radiology residents collaborated on a digital swarm platform in real
time and in a blinded fashion, grading meniscal lesions on knee MR exams. These
consensus votes were benchmarked against clinical (arthroscopy) and
radiological (senior-most radiologist) observations. The IRR of the consensus
votes was compared to the IRR of the majority and most confident votes of the
two cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm
votes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm
votes over majority vote, was observed. The 5-resident swarm had an even higher
improvement of 32% in IRR over majority vote. Swarm consensus votes also
improved specificity by up to 50%. The swarm consensus votes outperformed
individual and majority vote decisions in both the radiologists and resident
cohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating
positive effect of increased swarm size. The attending and resident swarms also
outperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a
digital swarm platform improved agreement and allows participants to express
judgement free intent, resulting in superior clinical performance and robust
A.I. training labels.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal
MRI is a challenging field of research. Most of the existing literature on
survival prediction is based on Radiomic features, which does not consider
either non-biological factors or the functional neurological status of the
patient(s). Besides, the selection of an appropriate cut-off for survival and
the presence of censored data create further problems. Application of deep
learning models for OS prediction is also limited due to the lack of large
annotated publicly available datasets. In this scenario we analyse the
potential of two novel neuroimaging feature families, extracted from brain
parcellation atlases and spatial habitats, along with classical radiomic and
geometric features; to study their combined predictive power for analysing
overall survival. A cross validation strategy with grid search is proposed to
simultaneously select and evaluate the most predictive feature subset based on
its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for
univariate feature selection, followed by the prediction of patient-specific
survival functions by three multivariate parsimonious models viz. Coxnet,
Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI
data used for this research was taken from two open-access collections TCGA-GBM
and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding
survival data for each patient was downloaded from The Cancer Genome Atlas
(TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved
using RSF with the best $24$ selected features. Age was found to be the most
important biological predictor. There were $9$, $6$, $6$ and $2$ features
selected from the parcellation, habitat, radiomic and region-based feature
groups respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Prescriptive Process Monitoring Under Resource Constraints: A Causal Inference Approach. (arXiv:2109.02894v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02894">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Prescriptive process monitoring is a family of techniques to optimize the
performance of a business process by triggering interventions at runtime.
Existing prescriptive process monitoring techniques assume that the number of
interventions that may be triggered is unbounded. In practice, though, specific
interventions consume resources with finite capacity. For example, in a loan
origination process, an intervention may consist of preparing an alternative
loan offer to increase the applicant&#x27;s chances of taking a loan. This
intervention requires a certain amount of time from a credit officer, and thus,
it is not possible to trigger this intervention in all cases. This paper
proposes a prescriptive process monitoring technique that triggers
interventions to optimize a cost function under fixed resource constraints. The
proposed technique relies on predictive modeling to identify cases that are
likely to lead to a negative outcome, in combination with causal inference to
estimate the effect of an intervention on the outcome of the case. These
outputs are then used to allocate resources to interventions to maximize a cost
function. A preliminary empirical evaluation suggests that the proposed
approach produces a higher net gain than a purely predictive (non-causal)
baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fruit-CoV: An Efficient Vision-based Framework for Speedy Detection and Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds. (arXiv:2109.03219v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03219">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>SARS-CoV-2 is colloquially known as COVID-19 that had an initial outbreak in
December 2019. The deadly virus has spread across the world, taking part in the
global pandemic disease since March 2020. In addition, a recent variant of
SARS-CoV-2 named Delta is intractably contagious and responsible for more than
four million deaths over the world. Therefore, it is vital to possess a
self-testing service of SARS-CoV-2 at home. In this study, we introduce
Fruit-CoV, a two-stage vision framework, which is capable of detecting
SARS-CoV-2 infections through recorded cough sounds. Specifically, we convert
sounds into Log-Mel Spectrograms and use the EfficientNet-V2 network to extract
its visual features in the first stage. In the second stage, we use 14
convolutional layers extracted from the large-scale Pretrained Audio Neural
Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN to
aggregate feature representations of the Log-Mel Spectrograms. Finally, we use
the combined features to train a binary classifier. In this study, we use a
dataset provided by the AICovidVN 115M Challenge, which includes a total of
7371 recorded cough sounds collected throughout Vietnam, India, and
Switzerland. Experimental results show that our proposed model achieves an AUC
score of 92.8% and ranks the 1st place on the leaderboard of the AICovidVN
Challenge. More importantly, our proposed framework can be integrated into a
call center or a VoIP system to speed up detecting SARS-CoV-2 infections
through online/recorded cough sounds.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regularized Learning in Banach Spaces. (arXiv:2109.03159v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03159">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This article presents a different way to study the theory of regularized
learning for generalized data including representer theorems and convergence
theorems. The generalized data are composed of linear functionals and real
scalars to represent the discrete information of the local models. By the
extension of the classical machine learning, the empirical risks are computed
by the generalized data and the loss functions. According to the techniques of
regularization, the global solutions are approximated by minimizing the
regularized empirical risks over the Banach spaces. The Banach spaces are
adaptively chosen to endow the generalized input data with compactness such
that the existence and convergence of the approximate solutions are guaranteed
by the weak* topology.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Density Estimation under Besov IPM Losses. (arXiv:2004.08597v2 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08597">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study minimax convergence rates of nonparametric density estimation in the
Huber contamination model, in which a proportion of the data comes from an
unknown outlier distribution. We provide the first results for this problem
under a large family of losses, called Besov integral probability metrics
(IPMs), that includes $\mathcal{L}^p$, Wasserstein, Kolmogorov-Smirnov, and
other common distances between probability distributions. Specifically, under a
range of smoothness assumptions on the population and outlier distributions, we
show that a re-scaled thresholding wavelet series estimator achieves minimax
optimal convergence rates under a wide variety of losses. Finally, based on
connections that have recently been shown between nonparametric density
estimation under IPM losses and generative adversarial networks (GANs), we show
that certain GAN architectures also achieve these minimax rates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Center Federated Learning. (arXiv:2005.01026v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.01026">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning has received great attention for its capability to train a
large-scale model in a decentralized manner without needing to access user data
directly. It helps protect the users&#x27; private data from centralized collecting.
Unlike distributed machine learning, federated learning aims to tackle non-IID
data from heterogeneous sources in various real-world applications, such as
those on smartphones. Existing federated learning approaches usually adopt a
single global model to capture the shared knowledge of all users by aggregating
their gradients, regardless of the discrepancy between their data
distributions. However, due to the diverse nature of user behaviors, assigning
users&#x27; gradients to different global models (i.e., centers) can better capture
the heterogeneity of data distributions across users. Our paper proposes a
novel multi-center aggregation mechanism for federated learning, which learns
multiple global models from the non-IID user data and simultaneously derives
the optimal matching between users and centers. We formulate the problem as a
joint optimization that can be efficiently solved by a stochastic expectation
maximization (EM) algorithm. Our experimental results on benchmark datasets
show that our method outperforms several popular federated learning methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Convolutional Neural Networks Predict Elasticity Tensors and their Bounds in Homogenization. (arXiv:2109.03020v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the present work, 3D convolutional neural networks (CNNs) are trained to
link random heterogeneous, two-phase materials of arbitrary phase fractions to
their elastic macroscale stiffness thus replacing explicit homogenization
simulations. In order to reduce the uncertainty of the true stiffness of the
synthetic composites due to unknown boundary conditions (BCs), the CNNs predict
beyond the stiffness for periodic BC the upper bound through kinematically
uniform BC, and the lower bound through stress uniform BC. This work describes
the workflow of the homogenization-CNN, from microstructure generation over the
CNN design, the operations of convolution, nonlinear activation and pooling as
well as training and validation along with backpropagation up to performance
measurements in tests. Therein the CNNs demonstrate the predictive accuracy not
only for the standard test set but also for samples of the real, two-phase
microstructure of a diamond-based coating. The CNN that covers all three
boundary types is virtually as accurate as the separate treatment in three
different nets. The CNNs of this contribution provide through stiffness bounds
an indicator of the proper RVE size for individual snapshot samples. Moreover,
they enable statistical analyses for the effective elastic stiffness on
ensembles of synthetical microstructures without costly simulations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BioNetExplorer: Architecture-Space Exploration of Bio-Signal Processing Deep Neural Networks for Wearables. (arXiv:2109.02909v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02909">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we propose the BioNetExplorer framework to systematically
generate and explore multiple DNN architectures for bio-signal processing in
wearables. Our framework adapts key neural architecture parameters to search
for an embedded DNN with a low hardware overhead, which can be deployed in
wearable edge devices to analyse the bio-signal data and to extract the
relevant information, such as arrhythmia and seizure. Our framework also
enables hardware-aware DNN architecture search using genetic algorithms by
imposing user requirements and hardware constraints (storage, FLOPs, etc.)
during the exploration stage, thereby limiting the number of networks explored.
Moreover, BioNetExplorer can also be used to search for DNNs based on the
user-required output classes; for instance, a user might require a specific
output class due to genetic predisposition or a pre-existing heart condition.
The use of genetic algorithms reduces the exploration time, on average, by 9x,
compared to exhaustive exploration. We are successful in identifying
Pareto-optimal designs, which can reduce the storage overhead of the DNN by
~30MB for a quality loss of less than 0.5%. To enable low-cost embedded DNNs,
BioNetExplorer also employs different model compression techniques to further
reduce the storage overhead of the network by up to 53x for a quality loss of
&lt;0.2%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Model Drift in a Large Cellular Network. (arXiv:2109.03011v1 [cs.NI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03011">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Operational networks are increasingly using machine learning models for a
variety of tasks, including detecting anomalies, inferring application
performance, and forecasting demand. Accurate models are important, yet
accuracy can degrade over time due to concept drift, whereby either the
characteristics of the data change over time (data drift) or the relationship
between the features and the target predictor change over time (model drift).
Drift is important to detect because changes in properties of the underlying
data or relationships to the target prediction can require model retraining,
which can be time-consuming and expensive. Concept drift occurs in operational
networks for a variety of reasons, ranging from software upgrades to
seasonality to changes in user behavior. Yet, despite the prevalence of drift
in networks, its extent and effects on prediction accuracy have not been
extensively studied. This paper presents an initial exploration into concept
drift in a large cellular network in the United States for a major metropolitan
area in the context of demand forecasting. We find that concept drift arises
largely due to data drift, and it appears across different key performance
indicators (KPIs), models, training set sizes, and time intervals. We identify
the sources of concept drift for the particular problem of forecasting downlink
volume. Weekly and seasonal patterns introduce both high and low-frequency
model drift, while disasters and upgrades result in sudden drift due to
exogenous shocks. Regions with high population density, lower traffic volumes,
and higher speeds also tend to correlate with more concept drift. The features
that contribute most significantly to concept drift are User Equipment (UE)
downlink packets, UE uplink packets, and Real-time Transport Protocol (RTP)
total received packets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Certified Robustness to Programmable Transformations in LSTMs. (arXiv:2102.07818v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks for natural language processing are fragile in the face
of adversarial examples -- small input perturbations, like synonym substitution
or word duplication, which cause a neural network to change its prediction. We
present an approach to certifying the robustness of LSTMs (and extensions of
LSTMs) and training models that can be efficiently certified. Our approach can
certify robustness to intractably large perturbation spaces defined
programmatically in a language of string transformations. Our evaluation shows
that (1) our approach can train models that are more robust to combinations of
string transformations than those produced using existing techniques; (2) our
approach can show high certification accuracy of the resulting models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adversarial Parameter Defense by Multi-Step Risk Minimization. (arXiv:2109.02889v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Previous studies demonstrate DNNs&#x27; vulnerability to adversarial examples and
adversarial training can establish a defense to adversarial examples. In
addition, recent studies show that deep neural networks also exhibit
vulnerability to parameter corruptions. The vulnerability of model parameters
is of crucial value to the study of model robustness and generalization. In
this work, we introduce the concept of parameter corruption and propose to
leverage the loss change indicators for measuring the flatness of the loss
basin and the parameter robustness of neural network parameters. On such basis,
we analyze parameter corruptions and propose the multi-step adversarial
corruption algorithm. To enhance neural networks, we propose the adversarial
parameter defense algorithm that minimizes the average risk of multiple
adversarial parameter corruptions. Experimental results show that the proposed
algorithm can improve both the parameter robustness and accuracy of neural
networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scale-invariant representation of machine learning. (arXiv:2109.02914v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02914">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The success of machine learning stems from its structured data
representation. Similar data have close representation as compressed codes for
classification or emerged labels for clustering. We observe that the frequency
of the internal representation follows power laws in both supervised and
unsupervised learning. The scale-invariant distribution implies that machine
learning largely compresses frequent typical data, and at the same time,
differentiates many atypical data as outliers. In this study, we derive how the
power laws can naturally arise in machine learning. In terms of information
theory, the scale-invariant representation corresponds to a maximally uncertain
data grouping among possible representations that guarantee pre-specified
learning accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting Recursive Least Squares for Training Deep Neural Networks. (arXiv:2109.03220v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03220">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recursive least squares (RLS) algorithms were once widely used for training
small-scale neural networks, due to their fast convergence. However, previous
RLS algorithms are unsuitable for training deep neural networks (DNNs), since
they have high computational complexity and too many preconditions. In this
paper, to overcome these drawbacks, we propose three novel RLS optimization
algorithms for training feedforward neural networks, convolutional neural
networks and recurrent neural networks (including long short-term memory
networks), by using the error backpropagation and our average-approximation RLS
method, together with the equivalent gradients of the linear least squares loss
function with respect to the linear outputs of hidden layers. Compared with
previous RLS optimization algorithms, our algorithms are simple and elegant.
They can be viewed as an improved stochastic gradient descent (SGD) algorithm,
which uses the inverse autocorrelation matrix of each layer as the adaptive
learning rate. Their time and space complexities are only several times those
of SGD. They only require the loss function to be the mean squared error and
the activation function of the output layer to be invertible. In fact, our
algorithms can be also used in combination with other first-order optimization
algorithms without requiring these two preconditions. In addition, we present
two improved methods for our algorithms. Finally, we demonstrate their
effectiveness compared to the Adam algorithm on MNIST, CIFAR-10 and IMDB
datasets, and investigate the influences of their hyperparameters
experimentally.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people&#x27;s perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions. (arXiv:2109.02791v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reinforcement learning (RL) is a promising approach and has limited success
towards real-world applications, because ensuring safe exploration or
facilitating adequate exploitation is a challenges for controlling robotic
systems with unknown models and measurement uncertainties. Such a learning
problem becomes even more intractable for complex tasks over continuous space
(state-space and action-space). In this paper, we propose a learning-based
control framework consisting of several aspects: (1) linear temporal logic
(LTL) is leveraged to facilitate complex tasks over an infinite horizons which
can be translated to a novel automaton structure; (2) we propose an innovative
reward scheme for RL-agent with the formal guarantee such that global optimal
policies maximize the probability of satisfying the LTL specifications; (3)
based on a reward shaping technique, we develop a modular policy-gradient
architecture utilizing the benefits of automaton structures to decompose
overall tasks and facilitate the performance of learned controllers; (4) by
incorporating Gaussian Processes (GPs) to estimate the uncertain dynamic
systems, we synthesize a model-based safeguard using Exponential Control
Barrier Functions (ECBFs) to address problems with high-order relative degrees.
In addition, we utilize the properties of LTL automatons and ECBFs to construct
a guiding process to further improve the efficiency of exploration. Finally, we
demonstrate the effectiveness of the framework via several robotic
environments. And we show such an ECBF-based modular deep RL algorithm achieves
near-perfect success rates and guard safety with a high probability confidence
during training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconfigurable co-processor architecture with limited numerical precision to accelerate deep convolutional neural networks. (arXiv:2109.03040v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional Neural Networks (CNNs) are widely used in deep learning
applications, e.g. visual systems, robotics etc. However, existing software
solutions are not efficient. Therefore, many hardware accelerators have been
proposed optimizing performance, power and resource utilization of the
implementation. Amongst existing solutions, Field Programmable Gate Array
(FPGA) based architecture provides better cost-energy-performance trade-offs as
well as scalability and minimizing development time. In this paper, we present
a model-independent reconfigurable co-processing architecture to accelerate
CNNs. Our architecture consists of parallel Multiply and Accumulate (MAC) units
with caching techniques and interconnection networks to exploit maximum data
parallelism. In contrast to existing solutions, we introduce limited precision
32 bit Q-format fixed point quantization for arithmetic representations and
operations. As a result, our architecture achieved significant reduction in
resource utilization with competitive accuracy. Furthermore, we developed an
assembly-type microinstructions to access the co-processing fabric to manage
layer-wise parallelism, thereby making re-use of limited resources. Finally, we
have tested our architecture up to 9x9 kernel size on Xilinx Virtex 7 FPGA,
achieving a throughput of up to 226.2 GOp/S for 3x3 kernel size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Iterative Pseudo-Labeling with Deep Feature Annotation and Confidence-Based Sampling. (arXiv:2109.02717v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02717">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks is challenging when large and annotated
datasets are unavailable. Extensive manual annotation of data samples is
time-consuming, expensive, and error-prone, notably when it needs to be done by
experts. To address this issue, increased attention has been devoted to
techniques that propagate uncertain labels (also called pseudo labels) to large
amounts of unsupervised samples and use them for training the model. However,
these techniques still need hundreds of supervised samples per class in the
training set and a validation set with extra supervised samples to tune the
model. We improve a recent iterative pseudo-labeling technique, Deep Feature
Annotation (DeepFA), by selecting the most confident unsupervised samples to
iteratively train a deep neural network. Our confidence-based sampling strategy
relies on only dozens of annotated training samples per class with no
validation set, considerably reducing user effort in data annotation. We first
ascertain the best configuration for the baseline -- a self-trained deep neural
network -- and then evaluate our confidence DeepFA for different confidence
thresholds. Experiments on six datasets show that DeepFA already outperforms
the self-trained baseline, but confidence DeepFA can considerably outperform
the original DeepFA and the baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Large-Scale System Identification Using a Randomized SVD. (arXiv:2109.02703v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02703">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning a dynamical system from input/output data is a fundamental task in
the control design pipeline. In the partially observed setting there are two
components to identification: parameter estimation to learn the Markov
parameters, and system realization to obtain a state space model. In both
sub-problems it is implicitly assumed that standard numerical algorithms such
as the singular value decomposition (SVD) can be easily and reliably computed.
When trying to fit a high-dimensional model to data, for example in the
cyber-physical system setting, even computing an SVD is intractable. In this
work we show that an approximate matrix factorization obtained using randomized
methods can replace the standard SVD in the realization algorithm while
maintaining the non-asymptotic (in data-set size) performance and robustness
guarantees of classical methods. Numerical examples illustrate that for large
system models, this is the only method capable of producing a model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02749">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pano3D is a new benchmark for depth estimation from spherical panoramas. It
aims to assess performance across all depth estimation traits, the primary
direct depth estimation performance targeting precision and accuracy, and also
the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D
moves beyond typical intra-dataset evaluation to inter-dataset performance
assessment. By disentangling the capacity to generalize to unseen data into
different test splits, Pano3D represents a holistic benchmark for $360^o$ depth
estimation. We use it as a basis for an extended analysis seeking to offer
insights into classical choices for depth estimation. This results in a solid
baseline for panoramic depth that follow-up works can build upon to steer
future progress.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing road pothole detection approaches can be classified as computer
vision-based or machine learning-based. The former approaches typically employ
2-D image analysis/understanding or 3-D point cloud modeling and segmentation
algorithms to detect road potholes from vision sensor data. The latter
approaches generally address road pothole detection using convolutional neural
networks (CNNs) in an end-to-end manner. However, road potholes are not
necessarily ubiquitous and it is challenging to prepare a large well-annotated
dataset for CNN training. In this regard, while computer vision-based methods
were the mainstream research trend in the past decade, machine learning-based
methods were merely discussed. Recently, we published the first stereo
vision-based road pothole detection dataset and a novel disparity
transformation algorithm, whereby the damaged and undamaged road areas can be
highly distinguished. However, there are no benchmarks currently available for
state-of-the-art (SoTA) CNNs trained using either disparity images or
transformed disparity images. Therefore, in this paper, we first discuss the
SoTA CNNs designed for semantic segmentation and evaluate their performance for
road pothole detection with extensive experiments. Additionally, inspired by
graph neural network (GNN), we propose a novel CNN layer, referred to as graph
attention layer (GAL), which can be easily deployed in any existing CNN to
optimize image feature representations for semantic segmentation. Our
experiments compare GAL-DeepLabv3+, our best-performing implementation, with
nine SoTA CNNs on three modalities of training data: RGB images, disparity
images, and transformed disparity images. The experimental results suggest that
our proposed GAL-DeepLabv3+ achieves the best overall pothole detection
accuracy on all training data modalities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Satellite Imagery and Machine Learning to Estimate the Livelihood Impact of Electricity Access. (arXiv:2109.02890v1 [econ.GN])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02890">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In many regions of the world, sparse data on key economic outcomes inhibits
the development, targeting, and evaluation of public policy. We demonstrate how
advancements in satellite imagery and machine learning can help ameliorate
these data and inference challenges. In the context of an expansion of the
electrical grid across Uganda, we show how a combination of satellite imagery
and computer vision can be used to develop local-level livelihood measurements
appropriate for inferring the causal impact of electricity access on
livelihoods. We then show how ML-based inference techniques deliver more
reliable estimates of the causal impact of electrification than traditional
alternatives when applied to these data. We estimate that grid access improves
village-level asset wealth in rural Uganda by 0.17 standard deviations, more
than doubling the growth rate over our study period relative to untreated
areas. Our results provide country-scale evidence on the impact of a key
infrastructure investment, and provide a low-cost, generalizable approach to
future policy evaluation in data sparse environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v2 [math.NA] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11627">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduced the least-squares ReLU neural network (LSNN) method for solving
the linear advection-reaction problem with discontinuous solution and showed
that the method outperforms mesh-based numerical methods in terms of the number
of degrees of freedom. This paper studies the LSNN method for scalar nonlinear
hyperbolic conservation law. The method is a discretization of an equivalent
least-squares (LS) formulation in the set of neural network functions with the
ReLU activation function. Evaluation of the LS functional is done by using
numerical integration and conservative finite volume scheme. Numerical results
of some test problems show that the method is capable of approximating the
discontinuous interface of the underlying problem automatically through the
free breaking lines of the ReLU neural network. Moreover, the method does not
exhibit the common Gibbs phenomena along the discontinuous interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backpropagation and fuzzy algorithm Modelling to Resolve Blood Supply Chain Issues in the Covid-19 Pandemic. (arXiv:2109.02645v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Bloodstock shortages and its uncertain demand has become a major problem for
all countries worldwide. Therefore, this study aims to provide solution to the
issues of blood distribution during the Covid-19 Pandemic at Bengkulu,
Indonesia. The Backpropagation algorithm was used to improve the possibility of
discovering available and potential donors. Furthermore, the distances, age,
and length of donation were measured to obtain the right person to donate blood
when it needed. The Backpropagation uses three input layers to classify
eligible donors, namely age, body, weight, and bias. In addition, the system
through its query automatically counts the variables via the Fuzzy Tahani and
simultaneously access the vast database.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Driven by the need to capture users&#x27; evolving interests and optimize their
long-term experiences, more and more recommender systems have started to model
recommendation as a Markov decision process and employ reinforcement learning
to address the problem. Shouldn&#x27;t research on the fairness of recommender
systems follow the same trend from static evaluation and one-shot intervention
to dynamic monitoring and non-stop control? In this paper, we portray the
recent developments in recommender systems first and then discuss how fairness
could be baked into the reinforcement learning techniques for recommendation.
Moreover, we argue that in order to make further progress in recommendation
fairness, we may want to consider multi-agent (game-theoretic) optimization,
multi-objective (Pareto) optimization, and simulation-based optimization, in
the general framework of stochastic games.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-adaptive deep neural network: Numerical approximation to functions and PDEs. (arXiv:2109.02839v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Designing an optimal deep neural network for a given task is important and
challenging in many machine learning applications. To address this issue, we
introduce a self-adaptive algorithm: the adaptive network enhancement (ANE)
method, written as loops of the form train, estimate and enhance. Starting with
a small two-layer neural network (NN), the step train is to solve the
optimization problem at the current NN; the step estimate is to compute a
posteriori estimator/indicators using the solution at the current NN; the step
enhance is to add new neurons to the current NN.

Novel network enhancement strategies based on the computed
estimator/indicators are developed in this paper to determine how many new
neurons and when a new layer should be added to the current NN. The ANE method
provides a natural process for obtaining a good initialization in training the
current NN; in addition, we introduce an advanced procedure on how to
initialize newly added neurons for a better approximation. We demonstrate that
the ANE method can automatically design a nearly minimal NN for learning
functions exhibiting sharp transitional layers as well as discontinuous
solutions of hyperbolic partial differential equations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimizing Quantum Variational Circuits with Deep Reinforcement Learning. (arXiv:2109.03188v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03188">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum Machine Learning (QML) is considered to be one of the most promising
applications of near term quantum devices. However, the optimization of quantum
machine learning models presents numerous challenges arising from the
imperfections of hardware and the fundamental obstacles in navigating an
exponentially scaling Hilbert space. In this work, we evaluate the potential of
contemporary methods in deep reinforcement learning to augment gradient based
optimization routines in quantum variational circuits. We find that
reinforcement learning augmented optimizers consistently outperform gradient
descent in noisy environments. All code and pretrained weights are available to
replicate the results or deploy the models at
https://github.com/lockwo/rl_qvc_opt.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Predictable Control. (arXiv:2109.03214v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03214">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many of the challenges facing today&#x27;s reinforcement learning (RL) algorithms,
such as robustness, generalization, transfer, and computational efficiency are
closely related to compression. Prior work has convincingly argued why
minimizing information is useful in the supervised learning setting, but
standard RL algorithms lack an explicit mechanism for compression. The RL
setting is unique because (1) its sequential nature allows an agent to use past
information to avoid looking at future observations and (2) the agent can
optimize its behavior to prefer states where decision making requires few bits.
We take advantage of these properties to propose a method (RPC) for learning
simple policies. This method brings together ideas from information
bottlenecks, model-based RL, and bits-back coding into a simple and
theoretically-justified algorithm. Our method jointly optimizes a latent-space
model and policy to be self-consistent, such that the policy avoids states
where the model is inaccurate. We demonstrate that our method achieves much
tighter compression than prior methods, achieving up to 5x higher reward than a
standard information bottleneck. We also demonstrate that our method learns
policies that are more robust and generalize better to new tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-constrained Deep Semi-Supervised Coupled Factorization Network with Enriched Prior. (arXiv:2009.03714v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03714">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Nonnegative matrix factorization is usually powerful for learning the
&quot;shallow&quot; parts-based representation, but it clearly fails to discover deep
hierarchical information within both the basis and representation spaces. In
this paper, we technically propose a new enriched prior based Dual-constrained
Deep Semi-Supervised Coupled Factorization Network, called DS2CF-Net, for
learning the hierarchical coupled representations. To ex-tract hidden deep
features, DS2CF-Net is modeled as a deep-structure and geometrical
structure-constrained neural network. Specifically, DS2CF-Net designs a deep
coupled factorization architecture using multi-layers of linear
transformations, which coupled updates the bases and new representations in
each layer. To improve the discriminating ability of learned deep
representations and deep coefficients, our network clearly considers enriching
the supervised prior by the joint deep coefficients-regularized label
prediction, and incorporates enriched prior information as additional label and
structure constraints. The label constraint can enable the samples of the same
label to have the same coordinate in the new feature space, while the structure
constraint forces the coefficient matrices in each layer to be block-diagonal
so that the enhanced prior using the self-expressive label propagation are more
accurate. Our network also integrates the adaptive dual-graph learning to
retain the local manifold structures of both the data manifold and feature
manifold by minimizing the reconstruction errors in each layer. Extensive
experiments on several real databases demonstrate that our DS2CF-Net can obtain
state-of-the-art performance for representation learning and clustering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-dependent Label-noise Learning under a Structural Causal Model. (arXiv:2109.02986v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Label noise will degenerate the performance of deep learning algorithms
because deep neural networks easily overfit label errors. Let X and Y denote
the instance and clean label, respectively. When Y is a cause of X, according
to which many datasets have been constructed, e.g., SVHN and CIFAR, the
distributions of P(X) and P(Y|X) are entangled. This means that the
unsupervised instances are helpful to learn the classifier and thus reduce the
side effect of label noise. However, it remains elusive on how to exploit the
causal information to handle the label noise problem. In this paper, by
leveraging a structural causal model, we propose a novel generative approach
for instance-dependent label-noise learning. In particular, we show that
properly modeling the instances will contribute to the identifiability of the
label noise transition matrix and thus lead to a better classifier.
Empirically, our method outperforms all state-of-the-art methods on both
synthetic and real-world label-noise datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03154">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Educational recommenders have received much less attention in comparison to
e-commerce and entertainment-related recommenders, even though efficient
intelligent tutors have great potential to improve learning gains. One of the
main challenges in advancing this research direction is the scarcity of large,
publicly available datasets. In this work, we release a large, novel dataset of
learners engaging with educational videos in-the-wild. The dataset, named
Personalised Educational Engagement with Knowledge Topics PEEK, is the first
publicly available dataset of this nature. The video lectures have been
associated with Wikipedia concepts related to the material of the lecture, thus
providing a humanly intuitive taxonomy. We believe that granular learner
engagement signals in unison with rich content representations will pave the
way to building powerful personalization algorithms that will revolutionise
educational and informational recommendation systems. Towards this goal, we 1)
construct a novel dataset from a popular video lecture repository, 2) identify
a set of benchmark algorithms to model engagement, and 3) run extensive
experimentation on the PEEK dataset to demonstrate its value. Our experiments
with the dataset show promise in building powerful informational recommender
systems. The dataset and the support code is available publicly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional sparse coding improves on the standard sparse approximation by
incorporating a global shift-invariant model. The most efficient convolutional
sparse coding methods are based on the alternating direction method of
multipliers and the convolution theorem. The only major difference between
these methods is how they approach a convolutional least-squares fitting
subproblem. This letter presents a solution to this subproblem, which improves
the efficiency of the state-of-the-art algorithms. We also use the same
approach for developing an efficient convolutional dictionary learning method.
Furthermore, we propose a novel algorithm for convolutional sparse coding with
a constraint on the approximation error.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COCO Denoiser: Using Co-Coercivity for Variance Reduction in Stochastic Convex Optimization. (arXiv:2109.03207v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03207">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>First-order methods for stochastic optimization have undeniable relevance, in
part due to their pivotal role in machine learning. Variance reduction for
these algorithms has become an important research topic. In contrast to common
approaches, which rarely leverage global models of the objective function, we
exploit convexity and L-smoothness to improve the noisy estimates outputted by
the stochastic gradient oracle. Our method, named COCO denoiser, is the joint
maximum likelihood estimator of multiple function gradients from their noisy
observations, subject to co-coercivity constraints between them. The resulting
estimate is the solution of a convex Quadratically Constrained Quadratic
Problem. Although this problem is expensive to solve by interior point methods,
we exploit its structure to apply an accelerated first-order algorithm, the
Fast Dual Proximal Gradient method. Besides analytically characterizing the
proposed estimator, we show empirically that increasing the number and
proximity of the queried points leads to better gradient estimates. We also
apply COCO in stochastic settings by plugging it in existing algorithms, such
as SGD, Adam or STRSAGA, outperforming their vanilla versions, even in
scenarios where our modelling assumptions are mismatched.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Bid in Contextual First Price Auctions. (arXiv:2109.03173v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we investigate the problem about how to bid in repeated
contextual first price auctions. We consider a single bidder (learner) who
repeatedly bids in the first price auctions: at each time $t$, the learner
observes a context $x_t\in \mathbb{R}^d$ and decides the bid based on
historical information and $x_t$. We assume a structured linear model of the
maximum bid of all the others $m_t &#x3D; \alpha_0\cdot x_t + z_t$, where
$\alpha_0\in \mathbb{R}^d$ is unknown to the learner and $z_t$ is randomly
sampled from a noise distribution $\mathcal{F}$ with log-concave density
function $f$. We consider both \emph{binary feedback} (the learner can only
observe whether she wins or not) and \emph{full information feedback} (the
learner can observe $m_t$) at the end of each time $t$. For binary feedback,
when the noise distribution $\mathcal{F}$ is known, we propose a bidding
algorithm, by using maximum likelihood estimation (MLE) method to achieve at
most $\widetilde{O}(\sqrt{\log(d) T})$ regret. Moreover, we generalize this
algorithm to the setting with binary feedback and the noise distribution is
unknown but belongs to a parametrized family of distributions. For the full
information feedback with \emph{unknown} noise distribution, we provide an
algorithm that achieves regret at most $\widetilde{O}(\sqrt{dT})$. Our approach
combines an estimator for log-concave density functions and then MLE method to
learn the noise distribution $\mathcal{F}$ and linear weight $\alpha_0$
simultaneously. We also provide a lower bound result such that any bidding
policy in a broad class must achieve regret at least $\Omega(\sqrt{T})$, even
when the learner receives the full information feedback and $\mathcal{F}$ is
known.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Restricted maximum-likelihood method for learning latent variance components in gene expression data with known and unknown confounders. (arXiv:2005.02921v2 [stat.ME] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02921">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Random effect models are popular statistical models for detecting and
correcting spurious sample correlations due to hidden confounders in
genome-wide gene expression data. In applications where some confounding
factors are known, estimating simultaneously the contribution of known and
latent variance components in random effect models is a challenge that has so
far relied on numerical gradient-based optimizers to maximize the likelihood
function. This is unsatisfactory because the resulting solution is poorly
characterized and the efficiency of the method may be suboptimal. Here we prove
analytically that maximum-likelihood latent variables can always be chosen
orthogonal to the known confounding factors, in other words, that
maximum-likelihood latent variables explain sample covariances not already
explained by known factors. Based on this result we propose a restricted
maximum-likelihood method which estimates the latent variables by maximizing
the likelihood on the restricted subspace orthogonal to the known confounding
factors, and show that this reduces to probabilistic PCA on that subspace. The
method then estimates the variance-covariance parameters by maximizing the
remaining terms in the likelihood function given the latent variables, using a
newly derived analytic solution for this problem. Compared to gradient-based
optimizers, our method attains greater or equal likelihood values, can be
computed using standard matrix operations, results in latent factors that don&#x27;t
overlap with any known factors, and has a runtime reduced by several orders of
magnitude. Hence the restricted maximum-likelihood method facilitates the
application of random effect modelling strategies for learning latent variance
components to much larger gene expression datasets than possible with current
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Convergence of Decentralized Adaptive Gradient Methods. (arXiv:2109.03194v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03194">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Adaptive gradient methods including Adam, AdaGrad, and their variants have
been very successful for training deep learning models, such as neural
networks. Meanwhile, given the need for distributed computing, distributed
optimization algorithms are rapidly becoming a focal point. With the growth of
computing power and the need for using machine learning models on mobile
devices, the communication cost of distributed training algorithms needs
careful consideration. In this paper, we introduce novel convergent
decentralized adaptive gradient methods and rigorously incorporate adaptive
gradient methods into decentralized training procedures. Specifically, we
propose a general algorithmic framework that can convert existing adaptive
gradient methods to their decentralized counterparts. In addition, we
thoroughly analyze the convergence behavior of the proposed algorithmic
framework and show that if a given adaptive gradient method converges, under
some specific conditions, then its decentralized counterpart is also
convergent. We illustrate the benefit of our generic decentralized framework on
a prototype method, i.e., AMSGrad, both theoretically and numerically.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07054">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>End-to-end training of neural network solvers for combinatorial optimization
problems such as the Travelling Salesman Problem is intractable and inefficient
beyond a few hundreds of nodes. While state-of-the-art Machine Learning
approaches perform closely to classical solvers when trained on trivially small
sizes, they are unable to generalize the learnt policy to larger instances of
practical scales. Towards leveraging transfer learning to solve large-scale
TSPs, this paper identifies inductive biases, model architectures and learning
algorithms that promote generalization to instances larger than those seen in
training. Our controlled experiments provide the first principled investigation
into such zero-shot generalization, revealing that extrapolating beyond
training data requires rethinking the neural combinatorial optimization
pipeline, from network layers and learning paradigms to evaluation protocols.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERT based classification system for detecting rumours on Twitter. (arXiv:2109.02975v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02975">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The role of social media in opinion formation has far-reaching implications
in all spheres of society. Though social media provide platforms for expressing
news and views, it is hard to control the quality of posts due to the sheer
volumes of posts on platforms like Twitter and Facebook. Misinformation and
rumours have lasting effects on society, as they tend to influence people&#x27;s
opinions and also may motivate people to act irrationally. It is therefore very
important to detect and remove rumours from these platforms. The only way to
prevent the spread of rumours is through automatic detection and classification
of social media posts. Our focus in this paper is the Twitter social medium, as
it is relatively easy to collect data from Twitter. The majority of previous
studies used supervised learning approaches to classify rumours on Twitter.
These approaches rely on feature extraction to obtain both content and context
features from the text of tweets to distinguish rumours and non-rumours.
Manually extracting features however is time-consuming considering the volume
of tweets. We propose a novel approach to deal with this problem by utilising
sentence embedding using BERT to identify rumours on Twitter, rather than the
usual feature extraction techniques. We use sentence embedding using BERT to
represent each tweet&#x27;s sentences into a vector according to the contextual
meaning of the tweet. We classify those vectors into rumours or non-rumours by
using various supervised learning techniques. Our BERT based models improved
the accuracy by approximately 10% as compared to previous methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Complementing Handcrafted Features with Raw Waveform Using a Light-weight Auxiliary Model. (arXiv:2109.02773v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An emerging trend in audio processing is capturing low-level speech
representations from raw waveforms. These representations have shown promising
results on a variety of tasks, such as speech recognition and speech
separation. Compared to handcrafted features, learning speech features via
backpropagation provides the model greater flexibility in how it represents
data for different tasks theoretically. However, results from empirical study
shows that, in some tasks, such as voice spoof detection, handcrafted features
are more competitive than learned features. Instead of evaluating handcrafted
features and raw waveforms independently, this paper proposes an Auxiliary
Rawnet model to complement handcrafted features with features learned from raw
waveforms. A key benefit of the approach is that it can improve accuracy at a
relatively low computational cost. The proposed Auxiliary Rawnet model is
tested using the ASVspoof 2019 dataset and the results from this dataset
indicate that a light-weight waveform encoder can potentially boost the
performance of handcrafted-features-based encoders in exchange for a small
amount of additional computational work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02929">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present our solution to extract albedo of branded labels for
e-commerce products. To this end, we generate a large-scale photo-realistic
synthetic data set for albedo extraction followed by training a generative
model to translate images with diverse lighting conditions to albedo. We
performed an extensive evaluation to test the generalisation of our method to
in-the-wild images. From the experimental results, we observe that our solution
generalises well compared to the existing method both in the unseen rendered
images as well as in the wild image.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of &quot;Subjectivity&quot; and &quot;Identity Terms&quot;. (arXiv:2109.02691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
&quot;Muslim&quot; and &quot;black&quot;. Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">gen2Out: Detecting and Ranking Generalized Anomalies. (arXiv:2109.02704v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02704">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a cloud of m-dimensional data points, how would we spot, as well as rank,
both single-point- as well as group- anomalies? We are the first to generalize
anomaly detection in two dimensions: The first dimension is that we handle both
point-anomalies, as well as group-anomalies, under a unified view -- we shall
refer to them as generalized anomalies. The second dimension is that gen2Out
not only detects, but also ranks, anomalies in suspiciousness order. Detection,
and ranking, of anomalies has numerous applications: For example, in EEG
recordings of an epileptic patient, an anomaly may indicate a seizure; in
computer network traffic data, it may signify a power failure, or a DoS/DDoS
attack. We start by setting some reasonable axioms; surprisingly, none of the
earlier methods pass all the axioms. Our main contribution is the gen2Out
algorithm, that has the following desirable properties: (a) Principled and
Sound anomaly scoring that obeys the axioms for detectors, (b) Doubly-general
in that it detects, as well as ranks generalized anomaly -- both point- and
group-anomalies, (c) Scalable, it is fast and scalable, linear on input size.
(d) Effective, experiments on real-world epileptic recordings (200GB)
demonstrate effectiveness of gen2Out as confirmed by clinicians. Experiments on
27 real-world benchmark datasets show that gen2Out detects ground truth groups,
matches or outperforms point-anomaly baseline algorithms on accuracy, with no
competition for group-anomalies and requires about 2 minutes for 1 million data
points on a stock machine.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OKSP: A Novel Deep Learning Automatic Event Detection Pipeline for Seismic Monitoringin Costa Rica. (arXiv:2109.02723v1 [physics.geo-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Small magnitude earthquakes are the most abundant but the most difficult to
locate robustly and well due to their low amplitudes and high frequencies
usually obscured by heterogeneous noise sources. They highlight crucial
information about the stress state and the spatio-temporal behavior of fault
systems during the earthquake cycle, therefore, its full characterization is
then crucial for improving earthquake hazard assessment. Modern DL algorithms
along with the increasing computational power are exploiting the continuously
growing seismological databases, allowing scientists to improve the
completeness for earthquake catalogs, systematically detecting smaller
magnitude earthquakes and reducing the errors introduced mainly by human
intervention. In this work, we introduce OKSP, a novel automatic earthquake
detection pipeline for seismic monitoring in Costa Rica. Using Kabre
supercomputer from the Costa Rica High Technology Center, we applied OKSP to
the day before and the first 5 days following the Puerto Armuelles, M6.5,
earthquake that occurred on 26 June, 2019, along the Costa Rica-Panama border
and found 1100 more earthquakes previously unidentified by the Volcanological
and Seismological Observatory of Costa Rica. From these events, a total of 23
earthquakes with magnitudes below 1.0 occurred a day to hours prior to the
mainshock, shedding light about the rupture initiation and earthquake
interaction leading to the occurrence of this productive seismic sequence. Our
observations show that for the study period, the model was 100% exhaustive and
82% precise, resulting in an F1 score of 0.90. This effort represents the very
first attempt for automatically detecting earthquakes in Costa Rica using deep
learning methods and demonstrates that, in the near future, earthquake
monitoring routines will be carried out entirely by AI algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik&#x27;s Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FastAudio: A Learnable Audio Front-End for Spoof Speech Detection. (arXiv:2109.02774v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02774">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Voice assistants, such as smart speakers, have exploded in popularity. It is
currently estimated that the smart speaker adoption rate has exceeded 35% in
the US adult population. Manufacturers have integrated speaker identification
technology, which attempts to determine the identity of the person speaking, to
provide personalized services to different members of the same family. Speaker
identification can also play an important role in controlling how the smart
speaker is used. For example, it is not critical to correctly identify the user
when playing music. However, when reading the user&#x27;s email out loud, it is
critical to correctly verify the speaker that making the request is the
authorized user. Speaker verification systems, which authenticate the speaker
identity, are therefore needed as a gatekeeper to protect against various
spoofing attacks that aim to impersonate the enrolled user. This paper compares
popular learnable front-ends which learn the representations of audio by joint
training with downstream tasks (End-to-End). We categorize the front-ends by
defining two generic architectures and then analyze the filtering stages of
both types in terms of learning constraints. We propose replacing fixed
filterbanks with a learnable layer that can better adapt to anti-spoofing
tasks. The proposed FastAudio front-end is then tested with two popular
back-ends to measure the performance on the LA track of the ASVspoof 2019
dataset. The FastAudio front-end achieves a relative improvement of 27% when
compared with fixed front-ends, outperforming all other learnable front-ends on
this task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ArGoT: A Glossary of Terms extracted from the arXiv. (arXiv:2109.02801v1 [cs.DL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce ArGoT, a data set of mathematical terms extracted from the
articles hosted on the arXiv website. A term is any mathematical concept
defined in an article. Using labels in the article&#x27;s source code and examples
from other popular math websites, we mine all the terms in the arXiv data and
compile a comprehensive vocabulary of mathematical terms. Each term can be then
organized in a dependency graph by using the term&#x27;s definitions and the arXiv&#x27;s
metadata. Using both hyperbolic and standard word embeddings, we demonstrate
how this structure is reflected in the text&#x27;s vector representation and how
they capture relations of entailment in mathematical concepts. This data set is
part of an ongoing effort to align natural mathematical text with existing
Interactive Theorem Prover Libraries (ITPs) of formally verified statements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting Mood Disorder Symptoms with Remotely Collected Videos Using an Interpretable Multimodal Dynamic Attention Fusion Network. (arXiv:2109.03029v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03029">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We developed a novel, interpretable multimodal classification method to
identify symptoms of mood disorders viz. depression, anxiety and anhedonia
using audio, video and text collected from a smartphone application. We used
CNN-based unimodal encoders to learn dynamic embeddings for each modality and
then combined these through a transformer encoder. We applied these methods to
a novel dataset - collected by a smartphone application - on 3002 participants
across up to three recording sessions. Our method demonstrated better
multimodal classification performance compared to existing methods that
employed static embeddings. Lastly, we used SHapley Additive exPlanations
(SHAP) to prioritize important features in our model that could serve as
potential digital markers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks. (arXiv:2109.02832v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02832">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most of existing statistical theories on deep neural networks have sample
complexities cursed by the data dimension and therefore cannot well explain the
empirical success of deep learning on high-dimensional data. To bridge this
gap, we propose to exploit low-dimensional geometric structures of the real
world data sets. We establish theoretical guarantees of convolutional residual
networks (ConvResNet) in terms of function approximation and statistical
estimation for binary classification. Specifically, given the data lying on a
$d$-dimensional manifold isometrically embedded in $\mathbb{R}^D$, we prove
that if the network architecture is properly chosen, ConvResNets can (1)
approximate Besov functions on manifolds with arbitrary accuracy, and (2) learn
a classifier by minimizing the empirical logistic risk, which gives an excess
risk in the order of $n^{-\frac{s}{2s+2(s\vee d)}}$, where $s$ is a smoothness
parameter. This implies that the sample complexity depends on the intrinsic
dimension $d$, instead of the data dimension $D$. Our results demonstrate that
ConvResNets are adaptive to low-dimensional structures of data sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimizing model-agnostic Random Subspace ensembles. (arXiv:2109.03099v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a model-agnostic ensemble approach for supervised
learning. The proposed approach alternates between (1) learning an ensemble of
models using a parametric version of the Random Subspace approach, in which
feature subsets are sampled according to Bernoulli distributions, and (2)
identifying the parameters of the Bernoulli distributions that minimize the
generalization error of the ensemble model. Parameter optimization is rendered
tractable by using an importance sampling approach able to estimate the
expected model output for any given parameter set, without the need to learn
new models. While the degree of randomization is controlled by a
hyper-parameter in standard Random Subspace, it has the advantage to be
automatically tuned in our parametric version. Furthermore, model-agnostic
feature importance scores can be easily derived from the trained ensemble
model. We show the good performance of the proposed approach, both in terms of
prediction and feature ranking, on simulated and real-world datasets. We also
show that our approach can be successfully used for the reconstruction of gene
regulatory networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots. (arXiv:2109.02724v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02724">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As machine learning systems become more ubiquitous, methods for understanding
and interpreting these models become increasingly important. In particular,
practitioners are often interested both in what features the model relies on
and how the model relies on them--the feature&#x27;s impact on model predictions.
Prior work on feature impact including partial dependence plots (PDPs) and
Individual Conditional Expectation (ICE) plots has focused on a visual
interpretation of feature impact. We propose a natural extension to ICE plots
with ICE feature impact, a model-agnostic, performance-agnostic feature impact
metric drawn out from ICE plots that can be interpreted as a close analogy to
linear regression coefficients. Additionally, we introduce an in-distribution
variant of ICE feature impact to vary the influence of out-of-distribution
points as well as heterogeneity and non-linearity measures to characterize
feature impact. Lastly, we demonstrate ICE feature impact&#x27;s utility in several
tasks using real-world data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trojan Signatures in DNN Weights. (arXiv:2109.02836v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02836">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks have been shown to be vulnerable to backdoor, or trojan,
attacks where an adversary has embedded a trigger in the network at training
time such that the model correctly classifies all standard inputs, but
generates a targeted, incorrect classification on any input which contains the
trigger. In this paper, we present the first ultra light-weight and highly
effective trojan detection method that does not require access to the
training/test data, does not involve any expensive computations, and makes no
assumptions on the nature of the trojan trigger. Our approach focuses on
analysis of the weights of the final, linear layer of the network. We
empirically demonstrate several characteristics of these weights that occur
frequently in trojaned networks, but not in benign networks. In particular, we
show that the distribution of the weights associated with the trojan target
class is clearly distinguishable from the weights associated with other
classes. Using this, we demonstrate the effectiveness of our proposed detection
method against state-of-the-art attacks across a variety of architectures,
datasets, and trigger types.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Individual Mobility Prediction via Attentive Marked Temporal Point Processes. (arXiv:2109.02715v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02715">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Individual mobility prediction is an essential task for transportation demand
management and traffic system operation. There exist a large body of works on
modeling location sequence and predicting the next location of users; however,
little attention is paid to the prediction of the next trip, which is governed
by the strong spatiotemporal dependencies between diverse attributes, including
trip start time $t$, origin $o$, and destination $d$. To fill this gap, in this
paper we propose a novel point process-based model -- Attentive Marked temporal
point processes (AMTPP) -- to model human mobility and predict the whole trip
$(t,o,d)$ in a joint manner. To encode the influence of history trips, AMTPP
employs the self-attention mechanism with a carefully designed positional
embedding to capture the daily/weekly periodicity and regularity in individual
travel behavior. Given the unique peaked nature of inter-event time in human
behavior, we use an asymmetric log-Laplace mixture distribution to precisely
model the distribution of trip start time $t$. Furthermore, an
origin-destination (OD) matrix learning block is developed to model the
relationship between every origin and destination pair. Experimental results on
two large metro trip datasets demonstrate the superior performance of AMTPP.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refinement of Hottopixx and its Postprocessing. (arXiv:2109.02863v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02863">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hottopixx, proposed by Bittorf et al. at NIPS 2012, is an algorithm for
solving nonnegative matrix factorization (NMF) problems under the separability
assumption. Separable NMFs have important applications, such as topic
extraction from documents and unmixing of hyperspectral images. In such
applications, the robustness of the algorithm to noise is the key to the
success. Hottopixx has been shown to be robust to noise, and its robustness can
be further enhanced through postprocessing. However, there is a drawback.
Hottopixx and its postprocessing require us to estimate the noise level
involved in the matrix we want to factorize before running, since they use it
as part of the input data. The noise-level estimation is not an easy task. In
this paper, we overcome this drawback. We present a refinement of Hottopixx and
its postprocessing that runs without prior knowledge of the noise level. We
show that the refinement has almost the same robustness to noise as the
original algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02752">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks may be challenging in real world data. Using
models as black-boxes, even with transfer learning, can result in poor
generalization or inconclusive results when it comes to small datasets or
specific applications. This tutorial covers the basic steps as well as more
recent options to improve models, in particular, but not restricted to,
supervised learning. It can be particularly useful in datasets that are not as
well-prepared as those in challenges, and also under scarce annotation and/or
small data. We describe basic procedures: as data preparation, optimization and
transfer learning, but also recent architectural choices such as use of
transformer modules, alternative convolutional layers, activation functions,
wide and deep networks, as well as training procedures including as curriculum,
contrastive and self-supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sequential Diagnosis Prediction with Transformer and Ontological Representation. (arXiv:2109.03069v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sequential diagnosis prediction on the Electronic Health Record (EHR) has
been proven crucial for predictive analytics in the medical domain. EHR data,
sequential records of a patient&#x27;s interactions with healthcare systems, has
numerous inherent characteristics of temporality, irregularity and data
insufficiency. Some recent works train healthcare predictive models by making
use of sequential information in EHR data, but they are vulnerable to
irregular, temporal EHR data with the states of admission/discharge from
hospital, and insufficient data. To mitigate this, we propose an end-to-end
robust transformer-based model called SETOR, which exploits neural ordinary
differential equation to handle both irregular intervals between a patient&#x27;s
visits with admitted timestamps and length of stay in each visit, to alleviate
the limitation of insufficient data by integrating medical ontology, and to
capture the dependencies between the patient&#x27;s visits by employing multi-layer
transformer blocks. Experiments conducted on two real-world healthcare datasets
show that, our sequential diagnoses prediction model SETOR not only achieves
better predictive results than previous state-of-the-art approaches,
irrespective of sufficient or insufficient training data, but also derives more
interpretable embeddings of medical codes. The experimental codes are available
at the GitHub repository (https://github.com/Xueping/SETOR).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Position-based Hash Embeddings For Scaling Graph Neural Networks. (arXiv:2109.00101v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00101">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph Neural Networks (GNNs) bring the power of deep representation learning
to graph and relational data and achieve state-of-the-art performance in many
applications. GNNs compute node representations by taking into account the
topology of the node&#x27;s ego-network and the features of the ego-network&#x27;s nodes.
When the nodes do not have high-quality features, GNNs learn an embedding layer
to compute node embeddings and use them as input features. However, the size of
the embedding layer is linear to the product of the number of nodes in the
graph and the dimensionality of the embedding and does not scale to big data
and graphs with hundreds of millions of nodes. To reduce the memory associated
with this embedding layer, hashing-based approaches, commonly used in
applications like NLP and recommender systems, can potentially be used.
However, a direct application of these ideas fails to exploit the fact that in
many real-world graphs, nodes that are topologically close will tend to be
related to each other (homophily) and as such their representations will be
similar.

In this work, we present approaches that take advantage of the nodes&#x27;
position in the graph to dramatically reduce the memory required, with minimal
if any degradation in the quality of the resulting GNN model. Our approaches
decompose a node&#x27;s embedding into two components: a position-specific component
and a node-specific component. The position-specific component models homophily
and the node-specific component models the node-to-node variation. Extensive
experiments using different datasets and GNN models show that our methods are
able to reduce the memory requirements by 88% to 97% while achieving, in nearly
all cases, better classification accuracy than other competing approaches,
including the full embeddings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.

In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02859">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>User purchasing prediction with multi-behavior information remains a
challenging problem for current recommendation systems. Various methods have
been proposed to address it via leveraging the advantages of graph neural
networks (GNNs) or multi-task learning. However, most existing works do not
take the complex dependencies among different behaviors of users into
consideration. They utilize simple and fixed schemes, like neighborhood
information aggregation or mathematical calculation of vectors, to fuse the
embeddings of different user behaviors to obtain a unified embedding to
represent a user&#x27;s behavioral patterns which will be used in downstream
recommendation tasks. To tackle the challenge, in this paper, we first propose
the concept of hyper meta-path to construct hyper meta-paths or hyper
meta-graphs to explicitly illustrate the dependencies among different behaviors
of a user. How to obtain a unified embedding for a user from hyper meta-paths
and avoid the previously mentioned limitations simultaneously is critical.
Thanks to the recent success of graph contrastive learning, we leverage it to
learn embeddings of user behavior patterns adaptively instead of assigning a
fixed scheme to understand the dependencies among different behaviors. A new
graph contrastive learning based framework is proposed by coupling with hyper
meta-paths, namely HMG-CR, which consistently and significantly outperforms all
baselines in extensive comparison experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training such inference models is challenging
due to the lack of paired (shape, program) data in most domains. A popular
approach is to pre-train a model on synthetic data and then fine-tune on real
shapes using slow, unstable reinforcement learning. In this paper, we argue
that self-training is a viable alternative for fine-tuning such models.
Self-training is a semi-supervised learning paradigm where a model assigns
pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)
pairs as the new ground truth. We show that for constructive solid geometry and
assembly-based modeling, self-training outperforms state-of-the-art
reinforcement learning approaches. Additionally, shape program inference has a
unique property that circumvents a potential downside of self-training
(incorrect pseudo-label assignment): inferred programs are executable. For a
given shape from our distribution of interest $\mathbf{x}^*$ and its predicted
program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape
$\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than
$(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution
self training (LEST). We demonstrate that self training infers shape programs
with higher shape reconstruction accuracy and converges significantly faster
than reinforcement learning approaches, and in some domains, LEST can further
improve this performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Early ICU Mortality Prediction and Survival Analysis for Respiratory Failure. (arXiv:2109.03048v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Respiratory failure is the one of major causes of death in critical care
unit. During the outbreak of COVID-19, critical care units experienced an
extreme shortage of mechanical ventilation because of respiratory failure
related syndromes. To help this, the early mortality risk prediction in
patients who suffer respiratory failure can provide timely support for clinical
treatment and resource management. In the study, we propose a dynamic modeling
approach for early mortality risk prediction of the respiratory failure
patients based on the first 24 hours ICU physiological data. Our proposed model
is validated on the eICU collaborate database. We achieved a high AUROC
performance (80-83%) and significantly improved AUCPR 4% on Day 5 since ICU
admission, compared to the state-of-art prediction models. In addition, we
illustrated that the survival curve includes the time-varying information for
the early ICU admission survival analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HMSG: Heterogeneous Graph Neural Network based on Metapath Subgraph Learning. (arXiv:2109.02868v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02868">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many real-world data can be represented as heterogeneous graphs with
different types of nodes and connections. Heterogeneous graph neural network
model aims to embed nodes or subgraphs into low-dimensional vector space for
various downstream tasks such as node classification, link prediction, etc.
Although several models were proposed recently, they either only aggregate
information from the same type of neighbors, or just indiscriminately treat
homogeneous and heterogeneous neighbors in the same way. Based on these
observations, we propose a new heterogeneous graph neural network model named
HMSG to comprehensively capture structural, semantic and attribute information
from both homogeneous and heterogeneous neighbors. Specifically, we first
decompose the heterogeneous graph into multiple metapath-based homogeneous and
heterogeneous subgraphs, and each subgraph associates specific semantic and
structural information. Then message aggregation methods are applied to each
subgraph independently, so that information can be learned in a more targeted
and efficient manner. Through a type-specific attribute transformation, node
attributes can also be transferred among different types of nodes. Finally, we
fuse information from subgraphs together to get the complete representation.
Extensive experiments on several datasets for node classification, node
clustering and link prediction tasks show that HMSG achieves the best
performance in all evaluation metrics than state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Motion Artifact Reduction In Photoplethysmography For Reliable Signal Selection. (arXiv:2109.02755v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02755">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Photoplethysmography (PPG) is a non-invasive and economical technique to
extract vital signs of the human body. Although it has been widely used in
consumer and research grade wrist devices to track a user&#x27;s physiology, the PPG
signal is very sensitive to motion which can corrupt the signal&#x27;s quality.
Existing Motion Artifact (MA) reduction techniques have been developed and
evaluated using either synthetic noisy signals or signals collected during
high-intensity activities - both of which are difficult to generalize for
real-life scenarios. Therefore, it is valuable to collect realistic PPG signals
while performing Activities of Daily Living (ADL) to develop practical signal
denoising and analysis methods. In this work, we propose an automatic pseudo
clean PPG generation process for reliable PPG signal selection. For each noisy
PPG segment, the corresponding pseudo clean PPG reduces the MAs and contains
rich temporal details depicting cardiac features. Our experimental results show
that 71% of the pseudo clean PPG collected from ADL can be considered as high
quality segment where the derived MAE of heart rate and respiration rate are
1.46 BPM and 3.93 BrPM, respectively. Therefore, our proposed method can
determine the reliability of the raw noisy PPG by considering quality of the
corresponding pseudo clean PPG signal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine Learning: Challenges, Limitations, and Compatibility for Audio Restoration Processes. (arXiv:2109.02692v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02692">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper machine learning networks are explored for their use in
restoring degraded and compressed speech audio. The project intent is to build
a new trained model from voice data to learn features of compression
artifacting distortion introduced by data loss from lossy compression and
resolution loss with an existing algorithm presented in SEGAN: Speech
Enhancement Generative Adversarial Network. The resulting generator from the
model was then to be used to restore degraded speech audio. This paper details
an examination of the subsequent compatibility and operational issues presented
by working with deprecated code, which obstructed the trained model from
successfully being developed. This paper further serves as an examination of
the challenges, limitations, and compatibility in the current state of machine
learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a Siamese Neural Network with Adaptive Sample Pair Formation. (arXiv:2109.02915v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02915">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Speech-based machine learning (ML) has been heralded as a promising solution
for tracking prosodic and spectrotemporal patterns in real-life that are
indicative of emotional changes, providing a valuable window into one&#x27;s
cognitive and mental state. Yet, the scarcity of labelled data in ambulatory
studies prevents the reliable training of ML models, which usually rely on
&quot;data-hungry&quot; distribution-based learning. Leveraging the abundance of labelled
speech data from acted emotions, this paper proposes a few-shot learning
approach for automatically recognizing emotion in spontaneous speech from a
small number of labelled samples. Few-shot learning is implemented via a metric
learning approach through a siamese neural network, which models the relative
distance between samples rather than relying on learning absolute patterns of
the corresponding distributions of each emotion. Results indicate the
feasibility of the proposed metric learning in recognizing emotions from
spontaneous speech in four datasets, even with a small amount of labelled
samples. They further demonstrate superior performance of the proposed metric
learning compared to commonly used adaptation methods, including network
fine-tuning and adversarial learning. Findings from this work provide a
foundation for the ambulatory tracking of human emotion in spontaneous speech
contributing to the real-life assessment of mental health degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the few-shot learning (FSL) problem, where a model learns to
recognize new objects with extremely few labeled training data per category.
Most of previous FSL approaches resort to the meta-learning paradigm, where the
model accumulates inductive bias through learning many training tasks so as to
solve a new unseen few-shot task. In contrast, we propose a simple approach to
exploit unlabeled data accompanying the few-shot task for improving few-shot
performance. Firstly, we propose a Dependency Maximization method based on the
Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the
statistical dependency between the embedded feature of those unlabeled data and
their label predictions, together with the supervised loss over the support
set. We then use the obtained model to infer the pseudo-labels for those
unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to
evaluate the credibility of each pseudo-labeled example and select the most
faithful ones into an augmented support set to retrain the model as in the
first step. We iterate the above process until the pseudo-labels for the
unlabeled data becomes stable. Following the standard transductive and
semi-supervised FSL setting, our experiments show that the proposed method
out-performs previous state-of-the-art methods on four widely used benchmarks,
including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02765">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep neural networks have achieved remarkable success in various
computer vision tasks, they often fail to generalize to new domains and subtle
variations of input images. Several defenses have been proposed to improve the
robustness against these variations. However, current defenses can only
withstand the specific attack used in training, and the models often remain
vulnerable to other input variations. Moreover, these methods often degrade
performance of the model on clean images and do not generalize to out-of-domain
samples. In this paper we present Generative Adversarial Training, an approach
to simultaneously improve the model&#x27;s generalization to the test set and
out-of-domain samples as well as its robustness to unseen adversarial attacks.
Instead of altering a low-level pre-defined aspect of images, we generate a
spectrum of low-level, mid-level and high-level changes using generative models
with a disentangled latent space. Adversarial training with these examples
enable the model to withstand a wide range of attacks by observing a variety of
input alterations during training. We show that our approach not only improves
performance of the model on clean images and out-of-domain samples but also
makes it robust against unforeseen attacks and outperforms prior work. We
validate effectiveness of our method by demonstrating results on various tasks
such as classification, segmentation and object detection.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-07">2021-09-07</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantum Natural Language Processing on Near-Term Quantum Computers. (arXiv:2005.04147v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04147">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we describe a full-stack pipeline for natural language
processing on near-term quantum computers, aka QNLP. The language-modelling
framework we employ is that of compositional distributional semantics
(DisCoCat), which extends and complements the compositional structure of
pregroup grammars. Within this model, the grammatical reduction of a sentence
is interpreted as a diagram, encoding a specific interaction of words according
to the grammar. It is this interaction which, together with a specific choice
of word embedding, realises the meaning (or &quot;semantics&quot;) of a sentence.
Building on the formal quantum-like nature of such interactions, we present a
method for mapping DisCoCat diagrams to quantum circuits. Our methodology is
compatible both with NISQ devices and with established Quantum Machine Learning
techniques, paving the way to near-term applications of quantum technology to
natural language processing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing technologies expand BERT from different perspectives, e.g. designing
different pre-training tasks, different semantic granularities and different
model architectures. Few models consider expanding BERT from different text
formats. In this paper, we propose a heterogeneous knowledge language model
(HKLM), a unified pre-trained language model (PLM) for all forms of text,
including unstructured text, semi-structured text and well-structured text. To
capture the corresponding relations among these multi-format knowledge, our
approach uses masked language model objective to learn word knowledge, uses
triple classification objective and title matching objective to learn entity
knowledge and topic knowledge respectively. To obtain the aforementioned
multi-format text, we construct a corpus in the tourism domain and conduct
experiments on 5 tourism NLP datasets. The results show that our approach
outperforms the pre-training of plain text using only 1/4 of the data. The
code, datasets, corpus and knowledge graph will be released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02176">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Coherence is an important aspect of text quality and is crucial for ensuring
its readability. It is essential desirable for outputs from text generation
systems like summarization, question answering, machine translation, question
generation, table-to-text, etc. An automated coherence scoring model is also
helpful in essay scoring or providing writing feedback. A large body of
previous work has leveraged entity-based methods, syntactic patterns, discourse
relations, and more recently traditional deep learning architectures for text
coherence assessment. Previous work suffers from drawbacks like the inability
to handle long-range dependencies, out-of-vocabulary words, or model sequence
information. We hypothesize that coherence assessment is a cognitively complex
task that requires deeper models and can benefit from other related tasks.
Accordingly, in this paper, we propose four different Transformer-based
architectures for the task: vanilla Transformer, hierarchical Transformer,
multi-task learning-based model, and a model with fact-based input
representation. Our experiments with popular benchmark datasets across multiple
domains on four different coherence assessment tasks demonstrate that our
models achieve state-of-the-art results outperforming existing models by a good
margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;&#x3D; 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What&#x27;s in your Head? Emergent Behaviour in Multi-Task Transformer Models. (arXiv:2104.06129v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06129">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The primary paradigm for multi-task training in natural language processing
is to represent the input with a shared pre-trained language model, and add a
small, thin network (head) per task. Given an input, a target head is the head
that is selected for outputting the final prediction. In this work, we examine
the behaviour of non-target heads, that is, the output of heads when given
input that belongs to a different task than the one they were trained for. We
find that non-target heads exhibit emergent behaviour, which may either explain
the target task, or generalize beyond their original task. For example, in a
numerical reasoning task, a span extraction head extracts from the input the
arguments to a computation that results in a number generated by a target
generative head. In addition, a summarization head that is trained with a
target question answering head, outputs query-based summaries when given a
question and a context from which the answer is to be extracted. This emergent
behaviour suggests that multi-task training leads to non-trivial extrapolation
of skills, which can be harnessed for interpretability and generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01850">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As the digital news industry becomes the main channel of information
dissemination, the adverse impact of fake news is explosively magnified. The
credibility of a news report should not be considered in isolation. Rather,
previously published news articles on the similar event could be used to assess
the credibility of a news report. Inspired by this, we propose a BERT-based
multimodal unreliable news detection framework, which captures both textual and
visual information from unreliable articles utilising the contrastive learning
strategy. The contrastive learner interacts with the unreliable news classifier
to push similar credible news (or similar unreliable news) closer while moving
news articles with similar content but opposite credibility labels away from
each other in the multimodal embedding space. Experimental results on a
COVID-19 related dataset, ReCOVery, show that our model outperforms a number of
competitive baseline in unreliable news detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v6 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer is a powerful model for text understanding. However, it is
inefficient due to its quadratic complexity to input sequence length. Although
there are many methods on Transformer acceleration, they are still either
inefficient on long sequences or not effective enough. In this paper, we
propose Fastformer, which is an efficient Transformer model based on additive
attention. In Fastformer, instead of modeling the pair-wise interactions
between tokens, we first use additive attention mechanism to model global
contexts, and then further transform each token representation based on its
interaction with global context representations. In this way, Fastformer can
achieve effective context modeling with linear complexity. Extensive
experiments on five datasets show that Fastformer is much more efficient than
many existing Transformer models and can meanwhile achieve comparable or even
better long text modeling performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-End Self-Debiasing Framework for Robust NLU Training. (arXiv:2109.02071v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02071">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing Natural Language Understanding (NLU) models have been shown to
incorporate dataset biases leading to strong performance on in-distribution
(ID) test sets but poor performance on out-of-distribution (OOD) ones. We
introduce a simple yet effective debiasing framework whereby the shallow
representations of the main model are used to derive a bias model and both
models are trained simultaneously. We demonstrate on three well studied NLU
tasks that despite its simplicity, our method leads to competitive OOD results.
It significantly outperforms other debiasing approaches on two tasks, while
still delivering high in-distribution performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser. (arXiv:2109.02297v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02297">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Considering the importance of building a good Visual Dialog (VD) Questioner,
many researchers study the topic under a Q-Bot-A-Bot image-guessing game
setting, where the Questioner needs to raise a series of questions to collect
information of an undisclosed image. Despite progress has been made in
Supervised Learning (SL) and Reinforcement Learning (RL), issues still exist.
Firstly, previous methods do not provide explicit and effective guidance for
Questioner to generate visually related and informative questions. Secondly,
the effect of RL is hampered by an incompetent component, i.e., the Guesser,
who makes image predictions based on the generated dialogs and assigns rewards
accordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced
Questioner (ReeQ) that generates questions under the guidance of related
entities and learns entity-based questioning strategy from human dialogs; 2) we
propose an Augmented Guesser (AugG) that is strong and is optimized for the VD
setting especially. Experimental results on the VisDial v1.0 dataset show that
our approach achieves state-of-theart performance on both image-guessing task
and question diversity. Human study further proves that our model generates
more visually related, informative and coherent questions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Putting a Spin on Language: A Quantum Interpretation of Unary Connectives for Linguistic Applications. (arXiv:2004.04128v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.04128">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Extended versions of the Lambek Calculus currently used in computational
linguistics rely on unary modalities to allow for the controlled application of
structural rules affecting word order and phrase structure. These controlled
structural operations give rise to derivational ambiguities that are missed by
the original Lambek Calculus or its pregroup simplification. Proposals for
compositional interpretation of extended Lambek Calculus in the compact closed
category of FVect and linear maps have been made, but in these proposals the
syntax-semantics mapping ignores the control modalities, effectively
restricting their role to the syntax. Our aim is to turn the modalities into
first-class citizens of the vectorial interpretation. Building on the
directional density matrix semantics, we extend the interpretation of the type
system with an extra spin density matrix space. The interpretation of proofs
then results in ambiguous derivations being tensored with orthogonal spin
states. Our method introduces a way of simultaneously representing co-existing
interpretations of ambiguous utterances, and provides a uniform framework for
the integration of lexical and derivational ambiguity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Re-entry Prediction for Online Conversations via Self-Supervised Learning. (arXiv:2109.02020v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, world business in online discussions and opinion sharing on
social media is booming. Re-entry prediction task is thus proposed to help
people keep track of the discussions which they wish to continue. Nevertheless,
existing works only focus on exploiting chatting history and context
information, and ignore the potential useful learning signals underlying
conversation data, such as conversation thread patterns and repeated engagement
of target users, which help better understand the behavior of target users in
conversations. In this paper, we propose three interesting and well-founded
auxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn
Authorship, as the self-supervised signals for re-entry prediction. These
auxiliary tasks are trained together with the main task in a multi-task manner.
Experimental results on two datasets newly collected from Twitter and Reddit
show that our method outperforms the previous state-of-the-arts with fewer
parameters and faster convergence. Extensive experiments and analysis show the
effectiveness of our proposed models and also point out some key ideas in
designing self-supervised tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nearest Neighbour Few-Shot Learning for Cross-lingual Classification. (arXiv:2109.02221v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have
led to significant performance gains on a wide range of cross-lingual NLP
tasks, success on many downstream tasks still relies on the availability of
sufficient annotated data. Traditional fine-tuning of pre-trained models using
only a few target samples can cause over-fitting. This can be quite limiting as
most languages in the world are under-resourced. In this work, we investigate
cross-lingual adaptation using a simple nearest neighbor few-shot (&lt;15 samples)
inference technique for classification tasks. We experiment using a total of 16
distinct languages across two NLP tasks- XNLI and PAWS-X. Our approach
consistently improves traditional fine-tuning using only a handful of labeled
samples in target locales. We also demonstrate its generalization capability
across tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training. (arXiv:2109.02284v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning multilingual and multi-domain translation model is challenging as
the heterogeneous and imbalanced data make the model converge inconsistently
over different corpora in real world. One common practice is to adjust the
share of each corpus in the training, so that the learning process is balanced
and low-resource cases can benefit from the high resource ones. However,
automatic balancing methods usually depend on the intra- and inter-dataset
characteristics, which is usually agnostic or requires human priors. In this
work, we propose an approach, MultiUAT, that dynamically adjusts the training
data usage based on the model&#x27;s uncertainty on a small set of trusted clean
data for multi-corpus machine translation. We experiments with two classes of
uncertainty measures on multilingual (16 languages with 4 settings) and
multi-domain settings (4 for in-domain and 2 for out-of-domain on
English-German translation) and demonstrate our approach MultiUAT substantially
outperforms its baselines, including both static and dynamic strategies. We
analyze the cross-domain transfer and show the deficiency of static and
similarity based methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13139">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dialogue summarization has drawn much attention recently. Especially in the
customer service domain, agents could use dialogue summaries to help boost
their works by quickly knowing customer&#x27;s issues and service progress. These
applications require summaries to contain the perspective of a single speaker
and have a clear topic flow structure, while neither are available in existing
datasets. Therefore, in this paper, we introduce a novel Chinese dataset for
Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive
summaries in two aspects: (1) In addition to the overall summary for the whole
dialogue, role-oriented summaries are also provided to acquire different
speakers&#x27; viewpoints. (2) All the summaries sum up each topic separately, thus
containing the topic-level structure of the dialogue. We define tasks in CSDS
as generating the overall summary and different role-oriented summaries for a
given dialogue. Next, we compare various summarization methods on CSDS, and
experiment results show that existing methods are prone to generate redundant
and incoherent summaries. Besides, the performance becomes much worse when
analyzing the performance on role-oriented summaries and topic structures. We
hope that this study could benchmark Chinese dialogue summarization and benefit
further studies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01982">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning hierarchical structures in sequential data -- from simple
algorithmic patterns to natural language -- in a reliable, generalizable way
remains a challenging problem for neural language models. Past work has shown
that recurrent neural networks (RNNs) struggle to generalize on held-out
algorithmic or syntactic patterns without supervision or some inductive bias.
To remedy this, many papers have explored augmenting RNNs with various
differentiable stacks, by analogy with finite automata and pushdown automata.
In this paper, we present a stack RNN model based on the recently proposed
Nondeterministic Stack RNN (NS-RNN) that achieves lower cross-entropy than all
previous stack RNNs on five context-free language modeling tasks (within 0.05
nats of the information-theoretic lower bound), including a task in which the
NS-RNN previously failed to outperform a deterministic stack RNN baseline. Our
model assigns arbitrary positive weights instead of probabilities to stack
actions, and we provide an analysis of why this improves training. We also
propose a restricted version of the NS-RNN that makes it practical to use for
language modeling on natural language and present results on the Penn Treebank
corpus.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identifying emotions from text is crucial for a variety of real world tasks.
We consider the two largest now-available corpora for emotion classification:
GoEmotions, with 58k messages labelled by readers, and Vent, with 33M
writer-labelled messages. We design a benchmark and evaluate several feature
spaces and learning algorithms, including two simple yet novel models on top of
BERT that outperform previous strong baselines on GoEmotions. Through an
experiment with human participants, we also analyze the differences between how
writers express emotions and how readers perceive them. Our results suggest
that emotions expressed by writers are harder to identify than emotions that
readers perceive. We share a public web interface for researchers to explore
our models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00840">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Though language model text embeddings have revolutionized NLP research, their
ability to capture high-level semantic information, such as relations between
entities in text, is limited. In this paper, we propose a novel contrastive
learning framework that trains sentence embeddings to encode the relations in a
graph structure. Given a sentence (unstructured text) and its graph, we use
contrastive learning to impose relation-related structure on the token-level
representations of the sentence obtained with a CharacterBERT (El Boukkouri et
al.,2020) model. The resulting relation-aware sentence embeddings achieve
state-of-the-art results on the relation extraction task using only a simple
KNN classifier, thereby demonstrating the success of the proposed method.
Additional visualization by a tSNE analysis shows the effectiveness of the
learned representation space compared to baselines. Furthermore, we show that
we can learn a different space for named entity recognition, again using a
contrastive learning objective, and demonstrate how to successfully combine
both representation spaces in an entity-relation task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks. (arXiv:2109.01958v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01958">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based pre-trained language models boost the performance of
open-domain dialogue systems. Prior works leverage Transformer-based
pre-trained language models to generate texts with desired attributes in two
general approaches: (1) gradient-based methods: updating all latent
representations of pre-trained models with gradients from attribute models; (2)
weighted-decoding methods: re-ranking beam candidates from pre-trained models
with attribute functions. However, gradient-based methods lead to high
computation cost and can easily get overfitted on small training sets, while
weighted-decoding methods are inherently constrained by the low-variance
high-bias pre-trained model. In this work, we propose a novel approach to
control the generation of Transformer-based pre-trained language models: the
SideControl framework, which leverages a novel control attributes loss to
incorporate useful control signals, and is shown to perform well with very
limited training samples. We evaluate our proposed method on two benchmark
open-domain dialogue datasets, and results show that the SideControl framework
has better controllability, higher generation quality and better
sample-efficiency than existing gradient-based and weighted-decoding baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LightTag: Text Annotation Platform. (arXiv:2109.02320v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02320">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text annotation tools assume that their user&#x27;s goal is to create a labeled
corpus. However, users view annotation as a necessary evil on the way to
deliver business value through NLP. Thus an annotation tool should optimize for
the throughput of the global NLP process, not only the productivity of
individual annotators. LightTag is a text annotation tool designed and built on
that principle. This paper shares our design rationale, data modeling choices,
and user interface decisions then illustrates how those choices serve the full
NLP lifecycle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction. (arXiv:2109.02099v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Distantly supervised relation extraction (RE) automatically aligns
unstructured text with relation instances in a knowledge base (KB). Due to the
incompleteness of current KBs, sentences implying certain relations may be
annotated as N/A instances, which causes the so-called false negative (FN)
problem. Current RE methods usually overlook this problem, inducing improper
biases in both training and testing procedures. To address this issue, we
propose a two-stage approach. First, it finds out possible FN samples by
heuristically leveraging the memory mechanism of deep neural networks. Then, it
aligns those unlabeled data with the training data into a unified feature space
by adversarial training to assign pseudo labels and further utilize the
information contained in them. Experiments on two wildly-used benchmark
datasets demonstrate the effectiveness of our approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models. (arXiv:2103.06678v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we explore the effects of language variants, data sizes, and
fine-tuning task types in Arabic pre-trained language models. To do so, we
build three pre-trained language models across three variants of Arabic: Modern
Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a
fourth language model which is pre-trained on a mix of the three. We also
examine the importance of pre-training data size by building additional models
that are pre-trained on a scaled-down set of the MSA variant. We compare our
different models to each other, as well as to eight publicly available models
by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest
that the variant proximity of pre-training data to fine-tuning data is more
important than the pre-training data size. We exploit this insight in defining
an optimized system selection model for the studied tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Linguistic entrainment is a phenomenon where people tend to mimic each other
in conversation. The core instrument to quantify entrainment is a linguistic
similarity measure between conversational partners. Most of the current
similarity measures are based on bag-of-words approaches that rely on
linguistic markers, ignoring the overall language structure and dialogue
context. To address this issue, we propose to use a neural network model to
perform the similarity measure for entrainment. Our model is context-aware, and
it further leverages a novel component to learn the shared high-level
linguistic features across dialogues. We first investigate the effectiveness of
our novel component. Then we use the model to perform similarity measure in a
corpus-based entrainment analysis. We observe promising results for both
evaluation tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14638">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is presented here a machine learning-based (ML) natural language
processing (NLP) approach capable to automatically recognize and extract
categorical and numerical parameters from a corpus of articles. The approach
(named a.RIX) operates with a concomitant/interchangeable use of ML models such
as neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes
classifiers (NBC), and a pattern recognition model using regular expression
(REGEX). A corpus of 7,873 scientific articles dealing with natural products
(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine
automatically extracts categorical and numerical parameters such as (i) the
plant species from which active molecules are extracted, (ii) the
microorganisms species for which active molecules can act against, and (iii)
the values of minimum inhibitory concentration (MIC) against these
microorganisms. The parameters are extracted without part-of-speech tagging
(POS) and named entity recognition (NER) approaches (i.e. without the need of
text annotation), and the models training is performed with unsupervised
approaches. In this way, a.RIX can be essentially used on articles from any
scientific field. Finally, it can potentially make obsolete the current article
reviewing process in some areas, especially those in which machine learning
models capture texts structure, text semantics, and latent knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Emotion dynamics is a framework for measuring how an individual&#x27;s emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one&#x27;s utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1106.0107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00270">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user&#x27;s
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user&#x27;s comments
conditioned on relevant user&#x27;s reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese. (arXiv:2103.10069v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rise of social media has led to the increasing of comments on online
forums. However, there still exists invalid comments which are not informative
for users. Moreover, those comments are also quite toxic and harmful to people.
In this paper, we create a dataset for constructive and toxic speech detection,
named UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)
with 10,000 human-annotated comments. For these tasks, we propose a system for
constructive and toxic speech detection with the state-of-the-art transfer
learning model in Vietnamese NLP as PhoBERT. With this system, we obtain
F1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,
respectively. Besides, we implement various baseline models as traditional
Machine Learning and Deep Neural Network-Based models to evaluate the dataset.
With the results, we can solve several tasks on the online discussions and
develop the framework for identifying constructiveness and toxicity of
Vietnamese social media comments automatically.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce Hocalarim (MyProfessors), the largest student review dataset
available for the Turkish language. It consists of over 5000 professor reviews
left online by students, with different aspects of education rated on a scale
of 1 to 5 stars. We investigate the properties of the dataset and present its
statistics. We examine the impact of students&#x27; institution type on their
ratings and the correlation of students&#x27; bias to give positive or negative
feedback.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks. (arXiv:2109.02237v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02237">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Biomedical entity linking is the task of linking entity mentions in a
biomedical document to referent entities in a knowledge base. Recently, many
BERT-based models have been introduced for the task. While these models have
achieved competitive results on many datasets, they are computationally
expensive and contain about 110M parameters. Little is known about the factors
contributing to their impressive performance and whether the
over-parameterization is needed. In this work, we shed some light on the inner
working mechanisms of these large BERT-based models. Through a set of probing
experiments, we have found that the entity linking performance only changes
slightly when the input word order is shuffled or when the attention scope is
limited to a fixed window size. From these observations, we propose an
efficient convolutional neural network with residual connections for biomedical
entity linking. Because of the sparse connectivity and weight sharing
properties, our model has a small number of parameters and is highly efficient.
On five public datasets, our model achieves comparable or even better linking
accuracy than the state-of-the-art BERT-based models while having about 60
times fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">STaCK: Sentence Ordering with Temporal Commonsense Knowledge. (arXiv:2109.02247v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02247">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sentence order prediction is the task of finding the correct order of
sentences in a randomly ordered document. Correctly ordering the sentences
requires an understanding of coherence with respect to the chronological
sequence of events described in the text. Document-level contextual
understanding and commonsense knowledge centered around these events are often
essential in uncovering this coherence and predicting the exact chronological
order. In this paper, we introduce STaCK -- a framework based on graph neural
networks and temporal commonsense knowledge to model global information and
predict the relative order of sentences. Our graph network accumulates temporal
evidence using knowledge of &#x60;past&#x27; and &#x60;future&#x27; and formulates sentence
ordering as a constrained edge classification problem. We report results on
five different datasets, and empirically show that the proposed method is
naturally suitable for order prediction. The implementation of this work is
publicly available at: https://github.com/declare-lab/sentence-ordering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. (arXiv:2011.14203v5 [cs.AR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14203">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based language models such as BERT provide significant accuracy
improvement for a multitude of natural language processing (NLP) tasks.
However, their hefty computational and memory demands make them challenging to
deploy to resource-constrained edge platforms with strict latency requirements.
We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware
energy optimization for multi-task NLP. EdgeBERT employs entropy-based early
exit predication in order to perform dynamic voltage-frequency scaling (DVFS),
at a sentence granularity, for minimal energy consumption while adhering to a
prescribed target latency. Computation and memory footprint overheads are
further alleviated by employing a calibrated combination of adaptive attention
span, selective network pruning, and floating-point quantization. Furthermore,
in order to maximize the synergistic benefits of these algorithms in always-on
and intermediate edge computing settings, we specialize a 12nm scalable
hardware accelerator system, integrating a fast-switching low-dropout voltage
regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,
high-density embedded non-volatile memories (eNVMs) wherein the sparse
floating-point bit encodings of the shared multi-task parameters are carefully
stored. Altogether, latency-aware multi-task NLP inference acceleration on the
EdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy
compared to the conventional inference without early stopping, the
latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson
Tegra X2 mobile GPU, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01560">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Question Paraphrase Identification (QPI) is a critical task for large-scale
Question-Answering forums. The purpose of QPI is to determine whether a given
pair of questions are semantically identical or not. Previous approaches for
this task have yielded promising results, but have often relied on complex
recurrence mechanisms that are expensive and time-consuming in nature. In this
paper, we propose a novel architecture combining a Bidirectional Transformer
Encoder with Convolutional Neural Networks for the QPI task. We produce the
predictions from the proposed architecture using two different inference
setups: Siamese and Matched Aggregation. Experimental results demonstrate that
our model achieves state-of-the-art performance on the Quora Question Pairs
dataset. We empirically prove that the addition of convolution layers to the
model architecture improves the results in both inference setups. We also
investigate the impact of partial and complete fine-tuning and analyze the
trade-off between computational power and accuracy in the process. Based on the
obtained results, we conclude that the Matched-Aggregation setup consistently
outperforms the Siamese setup. Our work provides insights into what
architecture combinations and setups are likely to produce better results for
the QPI task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations. (arXiv:2109.02254v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02254">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid growth in published clinical trials makes it difficult to maintain
up-to-date systematic reviews, which requires finding all relevant trials. This
leads to policy and practice decisions based on out-of-date, incomplete, and
biased subsets of available clinical evidence. Extracting and then normalising
Population, Intervention, Comparator, and Outcome (PICO) information from
clinical trial articles may be an effective way to automatically assign trials
to systematic reviews and avoid searching and screening - the two most
time-consuming systematic review processes. We propose and test a novel
approach to PICO span detection. The major difference between our proposed
method and previous approaches comes from detecting spans without needing
annotated span data and using only crowdsourced sentence-level annotations.
Experiments on two datasets show that PICO span detection results achieve much
higher results for recall when compared to fully supervised methods with PICO
sentence detection at least as good as human annotations. By removing the
reliance on expert annotations for span detection, this work could be used in
human-machine pipeline for turning low-quality crowdsourced, and sentence-level
PICO annotations into structured information that can be used to quickly assign
trials to relevant systematic reviews.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Planning with Learned Entity Prompts for Abstractive Summarization. (arXiv:2104.07606v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07606">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce a simple but flexible mechanism to learn an intermediate plan to
ground the generation of abstractive summaries. Specifically, we prepend (or
prompt) target summaries with entity chains -- ordered sequences of entities
mentioned in the summary. Transformer-based sequence-to-sequence models are
then trained to generate the entity chain and then continue generating the
summary conditioned on the entity chain and the input. We experimented with
both pretraining and finetuning with this content planning objective. When
evaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate
empirically that the grounded generation with the planning objective improves
entity specificity and planning in summaries for all datasets, and achieves
state-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we
demonstrate empirically that planning with entity chains provides a mechanism
to control hallucinations in abstractive summaries. By prompting the decoder
with a modified content plan that drops hallucinated entities, we outperform
state-of-the-art approaches for faithfulness when evaluated automatically and
by humans.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work claims, our auxiliary experiments suggest that relation
prediction is contributory to named entity prediction in a non-negligible way.
The source code can be found at https://github.com/Coopercoppers/PFN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02229">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Over the past few years, various word-level textual attack approaches have
been proposed to reveal the vulnerability of deep neural networks used in
natural language processing. Typically, these approaches involve an important
optimization step to determine which substitute to be used for each word in the
original input. However, current research on this step is still rather limited,
from the perspectives of both problem-understanding and problem-solving. In
this paper, we address these issues by uncovering the theoretical properties of
the problem and proposing an efficient local search algorithm (LS) to solve it.
We establish the first provable approximation guarantee on solving the problem
in general cases. Notably, for adversarial textual attack, it is even better
than the previous bound which only holds in special case. Extensive experiments
involving five NLP tasks, six datasets and eleven NLP models show that LS can
largely reduce the number of queries usually by an order of magnitude to
achieve high attack success rates. Further experiments show that the
adversarial examples crafted by LS usually have higher quality, exhibit better
transferability, and can bring more robustness improvement to victim models by
adversarial training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text. (arXiv:2109.02289v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02289">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Numerical reasoning skills are essential for complex question answering (CQA)
over text. It requires opertaions including counting, comparison, addition and
subtraction. A successful approach to CQA on text, Neural Module Networks
(NMNs), follows the programmer-interpreter paradigm and leverages specialised
modules to perform compositional reasoning. However, the NMNs framework does
not consider the relationship between numbers and entities in both questions
and paragraphs. We propose effective techniques to improve NMNs&#x27; numerical
reasoning capabilities by making the interpreter question-aware and capturing
the relationship between entities and numbers. On the same subset of the DROP
dataset for CQA on text, experimental results show that our additions
outperform the original NMNs by 3.0 points for the overall F1 score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Feed-Forward Layers Are Key-Value Memories. (arXiv:2012.14913v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14913">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Feed-forward layers constitute two-thirds of a transformer model&#x27;s
parameters, yet their role in the network remains under-explored. We show that
feed-forward layers in transformer-based language models operate as key-value
memories, where each key correlates with textual patterns in the training
examples, and each value induces a distribution over the output vocabulary. Our
experiments show that the learned patterns are human-interpretable, and that
lower layers tend to capture shallow patterns, while upper layers learn more
semantic ones. The values complement the keys&#x27; input patterns by inducing
output distributions that concentrate probability mass on tokens likely to
appear immediately after each pattern, particularly in the upper layers.
Finally, we demonstrate that the output of a feed-forward layer is a
composition of its memories, which is subsequently refined throughout the
model&#x27;s layers via residual connections to produce the final output
distribution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper demonstrates that by fine-tuning an autoregressive language model
(GPT-Neo) on appropriately structured step-by-step demonstrations, it is
possible to teach it to execute a mathematical task that has previously proved
difficult for Transformers - longhand modulo operations - with a relatively
small number of examples. Specifically, we fine-tune GPT-Neo to solve the
numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et
al. (arXiv:1904.01557) reported below 40% accuracy on this task with 2 million
training examples. We show that after fine-tuning on 200 appropriately
structured demonstrations of solving long division problems and reporting the
remainders, the smallest available GPT-Neo model achieves over 80% accuracy.
This is achieved by constructing an appropriate dataset for fine-tuning, with
no changes to the learning algorithm. These results suggest that fine-tuning
autoregressive language models on small sets of well-crafted demonstrations may
be a useful paradigm for enabling individuals without training in machine
learning to coax such models to perform some kinds of complex multi-step tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Citizen-generated counter speech is a promising way to fight hate speech and
promote peaceful, non-polarized discourse. However, there is a lack of
large-scale longitudinal studies of its effectiveness for reducing hate speech.
To this end, we perform an exploratory analysis of the effectiveness of counter
speech using several different macro- and micro-level measures to analyze
180,000 political conversations that took place on German Twitter over four
years. We report on the dynamic interactions of hate and counter speech over
time and provide insights into whether, as in &#x60;classic&#x27; bullying situations,
organized efforts are more effective than independent individuals in steering
online discourse. Taken together, our results build a multifaceted picture of
the dynamics of hate and counter speech online. While we make no causal claims
due to the complexity of discourse dynamics, our findings suggest that
organized hate speech is associated with changes in public discourse and that
counter speech -- especially when organized -- may help curb hateful rhetoric
in online discourse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Masked Segmental Language Model for Unsupervised Natural Language Segmentation. (arXiv:2104.07829v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07829">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmentation remains an important preprocessing step both in languages where
&quot;words&quot; or other important syntactic/semantic units (like morphemes) are not
clearly delineated by white space, as well as when dealing with continuous
speech data, where there is often no meaningful pause between words.
Near-perfect supervised methods have been developed for use in resource-rich
languages such as Chinese, but many of the world&#x27;s languages are both
morphologically complex, and have no large dataset of &quot;gold&quot; segmentations into
meaningful units. To solve this problem, we propose a new type of Segmental
Language Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)
for use in both unsupervised and lightly supervised segmentation tasks. We
introduce a Masked Segmental Language Model (MSLM) built on a span-masking
transformer architecture, harnessing the power of a bi-directional masked
modeling context and attention. In a series of experiments, our model
consistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation
quality, and performs similarly to the Recurrent model on English (PTB). We
conclude by discussing the different challenges posed in segmenting
phonemic-type writing systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Augmentation for Cross-Domain Named Entity Recognition. (arXiv:2109.01758v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01758">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current work in named entity recognition (NER) shows that data augmentation
techniques can produce more robust models. However, most existing techniques
focus on augmenting in-domain data in low-resource scenarios where annotated
data is quite limited. In contrast, we study cross-domain data augmentation for
the NER task. We investigate the possibility of leveraging data from
high-resource domains by projecting it into the low-resource domains.
Specifically, we propose a novel neural architecture to transform the data
representation from a high-resource to a low-resource domain by learning the
patterns (e.g. style, noise, abbreviations, etc.) in the text that
differentiate them and a shared feature space where both domains are aligned.
We experiment with diverse datasets and show that transforming the data to the
low-resource domain representation achieves significant improvements over only
using data from high-resource domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case. (arXiv:2109.01935v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contextualised word embeddings is a powerful tool to detect contextual
synonyms. However, most of the current state-of-the-art (SOTA) deep learning
concept extraction methods remain supervised and underexploit the potential of
the context. In this paper, we propose a self-supervised pre-training approach
which is able to detect contextual synonyms of concepts being training on the
data created by shallow matching. We apply our methodology in the sparse
multi-class setting (over 15,000 concepts) to extract phenotype information
from electronic health records. We further investigate data augmentation
techniques to address the problem of the class sparsity. Our approach achieves
a new SOTA for the unsupervised phenotype concept annotation on clinical text
on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and
4.0 absolute points, respectively. After fine-tuning with as little as 20\% of
the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic
evaluation on three ICU benchmarks also shows the benefit of using the
phenotypes annotated by our model as features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach. (arXiv:2109.01862v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, neural paraphrase generation based on Seq2Seq has achieved
superior performance, however, the generated paraphrase still has the problem
of lack of diversity. In this paper, we focus on improving the diversity
between the generated paraphrase and the original sentence, i.e., making
generated paraphrase different from the original sentence as much as possible.
We propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),
which leverages multi-round paraphrase generation to improve diversity and
employs back-translation to preserve semantic information. We evaluate BTmPG on
two benchmark datasets. Both automatic and human evaluation show BTmPG can
improve the diversity of paraphrase while preserving the semantics of the
original sentence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02051">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many endeavors have sought to develop countermeasure techniques as
enhancements on Automatic Speaker Verification (ASV) systems, in order to make
them more robust against spoof attacks. As evidenced by the latest ASVspoof
2019 countermeasure challenge, models currently deployed for the task of ASV
are, at their best, devoid of suitable degrees of generalization to unseen
attacks. Upon further investigation of the proposed methods, it appears that a
broader three-tiered view of the proposed systems. comprised of the classifier,
feature extraction phase, and model loss function, may to some extent lessen
the problem. Accordingly, the present study proposes the Efficient Attention
Branch Network (EABN) modular architecture with a combined loss function to
address the generalization problem...</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM), a self-supervised pretraining objective, is
widely used in natural language processing for learning text representations.
MLM trains a model to predict a random sample of input tokens that have been
replaced by a [MASK] placeholder in a multi-class setting over the entire
vocabulary. When pretraining, it is common to use alongside MLM other auxiliary
objectives on the token or sequence level to improve downstream performance
(e.g. next sentence prediction). However, no previous work so far has attempted
in examining whether other simpler linguistically intuitive or not objectives
can be used standalone as main pretraining objectives. In this paper, we
explore five simple pretraining objectives based on token-level classification
tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our
proposed methods achieve comparable or better performance to MLM using a
BERT-BASE architecture. We further validate our methods using smaller models,
showing that pretraining a model with 41% of the BERT-BASE&#x27;s parameters,
BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01754">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large-scale conversational assistants like Alexa, Siri, Cortana and Google
Assistant process every utterance using multiple models for domain, intent and
named entity recognition. Given the decoupled nature of model development and
large traffic volumes, it is extremely difficult to identify utterances
processed erroneously by such systems. We address this challenge to detect
domain classification errors using offline Transformer models. We combine
utterance encodings from a RoBERTa model with the Nbest hypothesis produced by
the production system. We then fine-tune end-to-end in a multitask setting
using a small dataset of humanannotated utterances with domain classification
errors. We tested our approach for detecting misclassifications from one domain
that accounts for &lt;0.5% of the traffic in a large-scale conversational AI
system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM
baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this
further by 2.2% to 32.2% by ensembling multiple models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Evaluation for Explainable AI. (arXiv:2109.01962v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01962">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While recent years have witnessed the emergence of various explainable
methods in machine learning, to what degree the explanations really represent
the reasoning process behind the model prediction -- namely, the faithfulness
of explanation -- is still an open problem. One commonly used way to measure
faithfulness is \textit{erasure-based} criteria. Though conceptually simple,
erasure-based criterion could inevitably introduce biases and artifacts. We
propose a new methodology to evaluate the faithfulness of explanations from the
\textit{counterfactual reasoning} perspective: the model should produce
substantially different outputs for the original input and its corresponding
counterfactual edited on a faithful feature. Specially, we introduce two
algorithms to find the proper counterfactuals in both discrete and continuous
scenarios and then use the acquired counterfactuals to measure faithfulness.
Empirical results on several datasets show that compared with existing metrics,
our proposed counterfactual evaluation method can achieve top correlation with
the ground truth under diffe</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01942">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained multilingual models have become a de facto default approach for
zero-shot cross-lingual transfer. Previous work has shown that these models are
able to achieve cross-lingual representations when pretrained on two or more
languages with shared parameters. In this work, we provide evidence that a
model can achieve language-agnostic representations even when pretrained on a
single language. That is, we find that monolingual models pretrained and
finetuned on different languages achieve competitive performance compared to
the ones that use the same target language. Surprisingly, the models show a
similar performance on a same task regardless of the pretraining language. For
example, models pretrained on distant languages such as German and Portuguese
perform similarly on English tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As a kind of new expression elements, Internet memes are popular and
extensively used in online chatting scenarios since they manage to make
dialogues vivid, moving, and interesting. However, most current dialogue
researches focus on text-only dialogue tasks. In this paper, we propose a new
task named as \textbf{M}eme incorporated \textbf{O}pen-domain \textbf{D}ialogue
(MOD). Compared to previous dialogue tasks, MOD is much more challenging since
it requires the model to understand the multimodal elements as well as the
emotions behind them. To facilitate the MOD research, we construct a
large-scale open-domain multimodal dialogue dataset incorporating abundant
Internet memes into utterances. The dataset consists of $\sim$45K Chinese
conversations with $\sim$606K utterances. Each conversation contains about $13$
utterances with about $4$ Internet memes on average and each utterance equipped
with an Internet meme is annotated with the corresponding emotion. In addition,
we present a simple and effective method, which utilizes a unified generation
network to solve the MOD task. Experimental results demonstrate that our method
trained on the proposed corpus is able to achieve expressive communication
including texts and memes. The corpus and models have been publicly available
at https://github.com/lizekang/DSTC10-MOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Active learning has emerged as a standard paradigm in areas with scarcity of
labeled training data, such as in the medical domain. Language models have
emerged as the prevalent choice of several natural language tasks due to the
performance boost offered by these models. However, in several domains, such as
medicine, the scarcity of labeled training data is a common issue. Also, these
models may not work well in cases where class imbalance is prevalent. Active
learning may prove helpful in these cases to boost the performance with a
limited label budget. To this end, we propose a novel method using sampling
techniques based on submodular optimization and optimal transport for active
learning in language models, dubbed ALLWAS. We construct a sampling strategy
based on submodular optimization of the designed objective in the gradient
domain. Furthermore, to enable learning from few samples, we propose a novel
strategy for sampling from the Wasserstein barycenters. Our empirical
evaluations on standard benchmark datasets for text classification show that
our methods perform significantly better (&gt;20% relative increase in some cases)
than existing approaches for active learning on language models.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13883">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The raw-RGB colors of a camera sensor vary due to the spectral sensitivity
differences across different sensor makes and models. This paper focuses on the
task of mapping between different sensor raw-RGB color spaces. Prior work
addressed this problem using a pairwise calibration to achieve accurate color
mapping. Although being accurate, this approach is less practical as it
requires: (1) capturing pair of images by both camera devices with a color
calibration object placed in each new scene; (2) accurate image alignment or
manual annotation of the color calibration object. This paper aims to tackle
color mapping in the raw space through a more practical setup. Specifically, we
present a semi-supervised raw-to-raw mapping method trained on a small set of
paired images alongside an unpaired set of images captured by each camera
device. Through extensive experiments, we show that our method achieves better
results compared to other domain adaptation alternatives in addition to the
single-calibration solution. We have generated a new dataset of raw images from
two different smartphone cameras as part of this effort. Our dataset includes
unpaired and paired sets for our semi-supervised training and evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13920">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image style transfer aims to manipulate the appearance of a source image, or
&quot;content&quot; image, to share similar texture and colors of a target &quot;style&quot; image.
Ideally, the style transfer manipulation should also preserve the semantic
content of the source image. A commonly used approach to assist in transferring
styles is based on Gram matrix optimization. One problem of Gram matrix-based
optimization is that it does not consider the correlation between colors and
their styles. Specifically, certain textures or structures should be associated
with specific colors. This is particularly challenging when the target style
image exhibits multiple style types. In this work, we propose a color-aware
multi-style transfer method that generates aesthetically pleasing results while
preserving the style-color correlation between style and generated images. We
achieve this desired outcome by introducing a simple but efficient modification
to classic Gram matrix-based style transfer optimization. A nice feature of our
method is that it enables the users to manually select the color associations
between the target style and content image for more transfer flexibility. We
validated our method with several qualitative comparisons, including a user
study conducted with 30 participants. In comparison with prior work, our method
is simple, easy to implement, and achieves visually appealing results when
targeting images that have multiple styles. Source code is available at
https://github.com/mahmoudnafifi/color-aware-style-transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03098">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a simple yet reliable bottom-up approach with a good trade-off
between accuracy and efficiency for the problem of multi-person pose
estimation. Given an image, we employ an Hourglass Network to infer all the
keypoints from different persons indiscriminately as well as the guiding
offsets connecting the adjacent keypoints belonging to the same persons. Then,
we greedily group the candidate keypoints into multiple human poses (if any),
utilizing the predicted guiding offsets. And we refer to this process as greedy
offset-guided keypoint grouping (GOG). Moreover, we revisit the
encoding-decoding method for the multi-person keypoint coordinates and reveal
some important facts affecting accuracy. Experiments have demonstrated the
obvious performance improvements brought by the introduced components. Our
approach is comparable to the state of the art on the challenging COCO dataset
under fair conditions. The source code and our pre-trained model are publicly
available online.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15475">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12709">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many works in the recent literature introduce semantic mapping methods that
use CNNs (Convolutional Neural Networks) to recognize semantic properties in
images. The types of properties (eg.: room size, place category, and objects)
and their classes (eg.: kitchen and bathroom, for place category) are usually
predefined and restricted to a specific task. Thus, all the visual data
acquired and processed during the construction of the maps are lost and only
the recognized semantic properties remain on the maps. In contrast, this work
introduces a topological semantic mapping method that uses deep visual features
extracted by a CNN (GoogLeNet), from 2D images captured in multiple views of
the environment as the robot operates, to create, through averages,
consolidated representations of the visual features acquired in the regions
covered by each topological node. These representations allow flexible
recognition of semantic properties of the regions and use in other visual
tasks. Experiments with a real-world indoor dataset showed that the method is
able to consolidate the visual features of regions and use them to recognize
objects and place categories as semantic properties, and to indicate the
topological location of images, with very promising results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Data augmentation is a widely used technique in classification to increase
data used in training. It improves generalization and reduces amount of
annotated human activity data needed for training which reduces labour and time
needed with the dataset. Sensor time-series data, unlike images, cannot be
augmented by computationally simple transformation algorithms. State of the art
models like Recurrent Generative Adversarial Networks (RGAN) are used to
generate realistic synthetic data. In this paper, transformer based generative
adversarial networks which have global attention on data, are compared on
PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer
approach provides improvements in time and savings in computational resources
needed for data augmentation than previous approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v6 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07770">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Video quality assessment (VQA) is now a fast-growing subject, maturing in the
full reference (FR) case, yet challenging in the exploding no reference (NR)
case. We investigate variants of the popular VMAF video quality assessment
algorithm for the FR case, using both support vector regression and feedforward
neural networks. We extend it to the NR case, using some different features but
similar learning, to develop a partially unified framework for VQA. When fully
trained, FR algorithms such as VMAF perform well on test datasets, with 90%+
match in PCC and SRCC; but for predicting performance in the wild, we
train/test from scratch for each database. With an 80/20 train/test split, we
still achieve 90%+ performance on average in both PCC and SRCC, with 8-9% gains
over VMAF. Moreover, we even get decent performance (~75%) if we ignore the
reference, treating FR as NR, partly justifying our attempts at unification. In
the true NR case, we reduce complexity vs. leading recent algorithms VIDEVAL,
RAPIQUE, yet achieve a stunning 90% in SRCC (~12% gain), while roughly matching
in PCC (78% vs. 79.6%). At lower complexities, we can still achieve 87% in
SRCC, 70% in PCC. In short, we find encouraging improvements in trainability in
both FR and NR, while also constraining computational complexity against
leading methods</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v12 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Random field and random cluster theory are used to describe certain
mathematical results concerning the probability distribution of image pixel
intensities characterized as generic $2D$ integer arrays. The size of the
smallest bounded region within an image is estimated for segmenting an image,
from which, the equilibrium distribution of intensities can be recovered. From
the estimated bounded regions, properties of the sub-optimal and equilibrium
distributions of intensities are derived, which leads to an image compression
methodology whereby only slightly more than half of all pixels are required for
a worst-case reconstruction of the original image. A custom deep belief network
and heuristic allows for the unsupervised segmentation, detection and
localization of objects in an image. An example illustrates the mathematical
results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment. (arXiv:2106.11776v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11776">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dietary studies showed that dietary-related problem such as obesity is
associated with other chronic diseases like hypertension, irregular blood sugar
levels, and increased risk of heart attacks. The primary cause of these
problems is poor lifestyle choices and unhealthy dietary habits, which are
manageable using interactive mHealth apps. However, traditional dietary
monitoring systems using manual food logging suffer from imprecision,
underreporting, time consumption, and low adherence. Recent dietary monitoring
systems tackle these challenges by automatic assessment of dietary intake
through machine learning methods. This survey discusses the most performing
methodologies that have been developed so far for automatic food recognition
and volume estimation. First, we will present the rationale of visual-based
methods for food recognition. The core of the paper is the presentation,
discussion and evaluation of these methods on popular food image databases.
Following that, we discussed the mobile applications that are implementing
these methods. The survey ends with a discussion of research gaps and open
issues in this area.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11272">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We introduce Neural Marching Cubes (NMC), a data-driven approach for
extracting a triangle mesh from a discretized implicit field. Classical MC is
defined by coarse tessellation templates isolated to individual cubes. While
more refined tessellations have been proposed, they all make heuristic
assumptions, such as trilinearity, when determining the vertex positions and
local mesh topologies in each cube. In principle, none of these approaches can
reconstruct geometric features that reveal coherence or dependencies between
nearby cubes (e.g., a sharp edge), as such information is unaccounted for,
resulting in poor estimates of the true underlying implicit field. To tackle
these challenges, we re-cast MC from a deep learning perspective, by designing
tessellation templates more apt at preserving geometric features, and learning
the vertex positions and mesh topologies from training meshes, to account for
contextual information from nearby cubes. We develop a compact per-cube
parameterization to represent the output triangle mesh, while being compatible
with neural processing, so that a simple 3D convolutional network can be
employed for the training. We show that all topological cases in each cube that
are applicable to our design can be easily derived using our representation,
and the resulting tessellations can also be obtained naturally and efficiently
by following a few design guidelines. In addition, our network learns local
features with limited receptive fields, hence it generalizes well to new shapes
and new datasets. We evaluate our neural MC approach by quantitative and
qualitative comparisons to all well-known MC variants. In particular, we
demonstrate the ability of our network to recover sharp features such as edges
and corners, a long-standing issue of MC and its variants. Our network also
reconstructs local mesh topologies more accurately than previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we tackle two important problems in low-rank learning, which
are partial singular value decomposition and numerical rank estimation of huge
matrices. By using the concepts of Krylov subspaces such as Golub-Kahan
bidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose
two methods for solving these problems in a fast and accurate way. Our
experiments show the advantages of the proposed methods compared to the
traditional and randomized singular value decomposition methods. The proposed
methods are appropriate for applications involving huge matrices where the
accuracy of the desired singular values and also all of their corresponding
singular vectors are essential. As a real application, we evaluate the
performance of our methods on the problem of Riemannian similarity learning
between two various image datasets of MNIST and USPS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents.
Inferring occupancy from agent behaviors is an inherently multimodal problem; a
driver may behave in the same manner for different occupancy patterns ahead of
them (e.g., a driver may move at constant speed in traffic or on an open road).
Past work, however, does not account for this multimodality, thus neglecting to
model this source of aleatoric uncertainty in the relationship between driver
behaviors and their environment. We propose an occlusion inference method that
characterizes observed behaviors of human agents as sensor measurements, and
fuses them with those from a standard sensor suite. To capture the aleatoric
uncertainty, we train a conditional variational autoencoder with a discrete
latent space to learn a multimodal mapping from observed driver trajectories to
an occupancy grid representation of the view ahead of the driver. Our method
handles multi-agent scenarios, combining measurements from multiple observed
drivers using evidential theory to solve the sensor fusion problem. Our
approach is validated on a real-world dataset, outperforming baselines and
demonstrating real-time capable performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection. (arXiv:2109.01770v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01770">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Weakly-supervised salient object detection (WSOD) aims to develop saliency
models using image-level annotations. Despite of the success of previous works,
explorations on an effective training strategy for the saliency network and
accurate matches between image-level annotations and salient objects are still
inadequate. In this work, 1) we propose a self-calibrated training strategy by
explicitly establishing a mutual calibration loop between pseudo labels and
network predictions, liberating the saliency network from error-prone
propagation caused by pseudo labels. 2) we prove that even a much smaller
dataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate
models to achieve better performance as well as generalizability. This sheds
new light on the development of WSOD and encourages more contributions to the
community. Comprehensive experiments demonstrate that our method outperforms
all the existing WSOD methods by adopting the self-calibrated strategy only.
Steady improvements are further achieved by training on the proposed dataset.
Additionally, our method achieves 94.7% of the performance of fully-supervised
methods on average. And what is more, the fully supervised models adopting our
predicted results as &quot;ground truths&quot; achieve successful results (95.6% for
BASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling
time for pixel-level annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10623">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting isocitrate dehydrogenase mutationstatus in glioma using structural brain networksand graph neural networks. (arXiv:2109.01854v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01854">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Glioma is a common malignant brain tumor that shows distinct survival among
patients. The isocitrate dehydrogenase (IDH) gene mutation status provides
critical diagnostic and prognostic value for glioma and is now accepted as the
standard of care. A non-invasive prediction of IDH mutation based on the
pre-treatment MRI has crucial clinical significance. Machine learning and deep
learning models show reasonable performance in predicting IDH mutation status.
However, most models neglect the systematic brain alterations caused by tumor
invasion, where the infiltration along white matter tracts throughout the brain
is identified as a hallmark of glioma. Structural brain network provides an
effective tool to characterise brain organisation, which could be captured by
the graph neural networks (GNN) for a more accurate prediction of IDH mutation
status.

Here we propose a method to predict the IDH mutation using GNN, based on the
structural brain network of patients. Specifically, we firstly construct a
network template of healthy subjects, which consists of atlases of edges (white
matter tracts) and nodes (cortical and subcortical brain regions) to provide
regions of interest (ROI). Next, we employ autoencoders to extract the latent
multi-modal MRI features from the ROIs of the edge and node in patients. These
features of edge and node of brain networks are used to train a GNN
architecture in predicting IDH mutation status. The results show that the
proposed method outperforms the baseline models using 3D-CNN and 3D-DenseNet.
In addition, the model interpretation suggests its ability to identify the
tracts infiltrated by tumor and corresponds to clinical prior knowledge. In
conclusion, integrating brain networks with GNN offers a new avenue to study
brain lesions using computational neuroscience and computer vision approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04563">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In order to extract knowledge from the large data collected by edge devices,
traditional cloud based approach that requires data upload may not be feasible
due to communication bandwidth limitation as well as privacy and security
concerns of end users. To address these challenges, a novel privacy preserving
edge computing framework is proposed in this paper for image classification.
Specifically, autoencoder will be trained unsupervised at each edge device
individually, then the obtained latent vectors will be transmitted to the edge
server for the training of a classifier. This framework would reduce the
communications overhead and protect the data of the end users. Comparing to
federated learning, the training of the classifier in the proposed framework
does not subject to the constraints of the edge devices, and the autoencoder
can be trained independently at each edge device without any server
involvement. Furthermore, the privacy of the end users&#x27; data is protected by
transmitting latent vectors without additional cost of encryption. Experimental
results provide insights on the image classification performance vs. various
design parameters such as the data compression ratio of the autoencoder and the
model complexity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1106.0107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RiWNet: A moving object instance segmentation Network being Robust in adverse Weather conditions. (arXiv:2109.01820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmenting each moving object instance in a scene is essential for many
applications. But like many other computer vision tasks, this task performs
well in optimal weather, but then adverse weather tends to fail. To be robust
in weather conditions, the usual way is to train network in data of given
weather pattern or to fuse multiple sensors. We focus on a new possibility,
that is, to improve its resilience to weather interference through the
network&#x27;s structural design. First, we propose a novel FPN structure called
RiWFPN with a progressive top-down interaction and attention refinement module.
RiWFPN can directly replace other FPN structures to improve the robustness of
the network in non-optimal weather conditions. Then we extend SOLOV2 to capture
temporal information in video to learn motion information, and propose a moving
object instance segmentation network with RiWFPN called RiWNet. Finally, in
order to verify the effect of moving instance segmentation in different weather
disturbances, we propose a VKTTI-moving dataset which is a moving instance
segmentation dataset based on the VKTTI dataset, taking into account different
weather scenes such as rain, fog, sunset, morning as well as overcast. The
experiment proves how RiWFPN improves the network&#x27;s resilience to adverse
weather effects compared to other FPN structures. We compare RiWNet to several
other state-of-the-art methods in some challenging datasets, and RiWNet shows
better performance especially under adverse weather conditions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05080">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While significant advancements have been made in the generation of deepfakes
using deep learning technologies, its misuse is a well-known issue now.
Deepfakes can cause severe security and privacy issues as they can be used to
impersonate a person&#x27;s identity in a video by replacing his/her face with
another person&#x27;s face. Recently, a new problem of generating synthesized human
voice of a person is emerging, where AI-based deep learning models can
synthesize any person&#x27;s voice requiring just a few seconds of audio. With the
emerging threat of impersonation attacks using deepfake audios and videos, a
new generation of deepfake detectors is needed to focus on both video and audio
collectively. A large amount of good quality datasets is typically required to
capture the real-world scenarios to develop a competent deepfake detector.
Existing deepfake datasets either contain deepfake videos or audios, which are
racially biased as well. Hence, there is a crucial need for creating a good
video as well as an audio deepfake dataset, which can be used to detect audio
and video deepfake simultaneously. To fill this gap, we propose a novel
Audio-Video Deepfake dataset (FakeAVCeleb) that contains not only deepfake
videos but also respective synthesized lip-synced fake audios. We generate this
dataset using the current most popular deepfake generation methods. We selected
real YouTube videos of celebrities with four racial backgrounds (Caucasian,
Black, East Asian, and South Asian) to develop a more realistic multimodal
dataset that addresses racial bias and further help develop multimodal deepfake
detectors. We performed several experiments using state-of-the-art detection
methods to evaluate our deepfake dataset and demonstrate the challenges and
usefulness of our multimodal Audio-Video deepfake dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery. (arXiv:2106.00880v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Detection of objects is extremely important in various aerial vision-based
applications. Over the last few years, the methods based on convolution neural
networks have made substantial progress. However, because of the large variety
of object scales, densities, and arbitrary orientations, the current detectors
struggle with the extraction of semantically strong features for small-scale
objects by a predefined convolution kernel. To address this problem, we propose
the rotation equivariant feature image pyramid network (REFIPN), an image
pyramid network based on rotation equivariance convolution. The proposed model
adopts single-shot detector in parallel with a lightweight image pyramid module
to extract representative features and generate regions of interest in an
optimization approach. The proposed network extracts feature in a wide range of
scales and orientations by using novel convolution filters. These features are
used to generate vector fields and determine the weight and angle of the
highest-scoring orientation for all spatial locations on an image. By this
approach, the performance for small-sized object detection is enhanced without
sacrificing the performance for large-sized object detection. The performance
of the proposed model is validated on two commonly used aerial benchmarks and
the results show our proposed model can achieve state-of-the-art performance
with satisfactory efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CTRL-C: Camera calibration TRansformer with Line-Classification. (arXiv:2109.02259v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02259">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Single image camera calibration is the task of estimating the camera
parameters from a single input image, such as the vanishing points, focal
length, and horizon line. In this work, we propose Camera calibration
TRansformer with Line-Classification (CTRL-C), an end-to-end neural
network-based approach to single image camera calibration, which directly
estimates the camera parameters from an image and a set of line segments. Our
network adopts the transformer architecture to capture the global structure of
an image with multi-modal inputs in an end-to-end manner. We also propose an
auxiliary task of line classification to train the network to extract the
global geometric information from lines effectively. Our experiments
demonstrate that CTRL-C outperforms the previous state-of-the-art methods on
the Google Street View and SUN360 benchmark datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review. (arXiv:2109.02322v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02322">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The optic nerve head represents the intraocular section of the optic nerve
(ONH), which is prone to damage by intraocular pressure. The advent of optical
coherence tomography (OCT) has enabled the evaluation of novel optic nerve head
parameters, namely the depth and curvature of the lamina cribrosa (LC).
Together with the Bruch&#x27;s membrane opening minimum-rim-width, these seem to be
promising optic nerve head parameters for diagnosis and monitoring of retinal
diseases such as glaucoma. Nonetheless, these optical coherence tomography
derived biomarkers are mostly extracted through manual segmentation, which is
time-consuming and prone to bias, thus limiting their usability in clinical
practice. The automatic segmentation of optic nerve head in OCT scans could
further improve the current clinical management of glaucoma and other diseases.

This review summarizes the current state-of-the-art in automatic segmentation
of the ONH in OCT. PubMed and Scopus were used to perform a systematic review.
Additional works from other databases (IEEE, Google Scholar and ARVO IOVS) were
also included, resulting in a total of 27 reviewed studies.

For each algorithm, the methods, the size and type of dataset used for
validation, and the respective results were carefully analyzed. The results
show that deep learning-based algorithms provide the highest accuracy,
sensitivity and specificity for segmenting the different structures of the ONH
including the LC. However, a lack of consensus regarding the definition of
segmented regions, extracted parameters and validation approaches has been
observed, highlighting the importance and need of standardized methodologies
for ONH segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition. (arXiv:2109.02171v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02171">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is
a crucial step for cardiac morphology and function analysis. However, automatic
RV segmentation from MRI is still challenging, mainly due to the heterogeneous
intensity, the complex variable shapes, and the unclear RV boundary. Moreover,
current methods for the RV segmentation tend to suffer from performance
degradation at the basal and apical slices of MRI. In this work, we propose an
automatic RV segmentation framework, where the information from long-axis (LA)
views is utilized to assist the segmentation of short-axis (SA) views via
information transition. Specifically, we employed the transformed segmentation
from LA views as a prior information, to extract the ROI from SA views for
better segmentation. The information transition aims to remove the surrounding
ambiguous regions in the SA views. %, such as the tricuspid valve regions. We
tested our model on a public dataset with 360 multi-center, multi-vendor and
multi-disease subjects that consist of both LA and SA MRIs. Our experimental
results show that including LA views can be effective to improve the accuracy
of the SA segmentation. Our model is publicly available at
https://github.com/NanYoMy/MMs-2.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.05248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural network pruning and quantization techniques have demonstrated it
is possible to achieve high levels of compression with surprisingly little
degradation to test set accuracy. However, this measure of performance conceals
significant differences in how different classes and images are impacted by
model compression techniques. We find that models with radically different
numbers of weights have comparable top-line performance metrics but diverge
considerably in behavior on a narrow subset of the dataset. This small subset
of data points, which we term Pruning Identified Exemplars (PIEs) are
systematically more impacted by the introduction of sparsity. Compression
disproportionately impacts model performance on the underrepresented long-tail
of the data distribution. PIEs over-index on atypical or noisy images that are
far more challenging for both humans and algorithms to classify. Our work
provides intuition into the role of capacity in deep neural networks and the
trade-offs incurred by compression. An understanding of this disparate impact
is critical given the widespread deployment of compressed models in the wild.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images. (arXiv:2109.01835v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Optical coherence tomography angiography (OCTA) performs non-invasive
visualization and characterization of microvasculature in research and clinical
applications mainly in ophthalmology and dermatology. A wide variety of
instruments, imaging protocols, processing methods and metrics have been used
to describe the microvasculature, such that comparing different study outcomes
is currently not feasible. With the goal of contributing to standardization of
OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA
(OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and
quantitative analysis of en face OCTA maximum intensity projection images in a
standardized workflow. We present each analysis step, including optimization of
filtering and choice of segmentation algorithm, and definition of metrics. We
perform quantitative analysis of OCTA images from different commercial and
non-commercial instruments and samples and show OCTAVA can accurately and
reproducibly determine metrics for characterization of microvasculature. Wide
adoption could enable studies and aggregation of data on a scale sufficient to
develop reliable microvascular biomarkers for early detection, and to guide
treatment, of microvascular disease.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences. (arXiv:2109.01132v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01132">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Purpose: Echocardiography is commonly used as a non-invasive imaging tool in
clinical practice for the assessment of cardiac function. However, delineation
of the left ventricle is challenging due to the inherent properties of
ultrasound imaging, such as the presence of speckle noise and the low
signal-to-noise ratio. Methods: We propose a semi-automated segmentation
algorithm for the delineation of the left ventricle in temporal 3D
echocardiography sequences. The method requires minimal user interaction and
relies on a diffeomorphic registration approach. Advantages of the method
include no dependence on prior geometrical information, training data, or
registration from an atlas. Results: The method was evaluated using
three-dimensional ultrasound scan sequences from 18 patients from the
Mazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual
delineations provided by an expert cardiologist and four other registration
algorithms. The segmentation approach yielded the following results over the
cardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff
distance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02).
Conclusions: The method performed well compared to the four other registration
algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Out-of-Distribution Detection for Automotive Perception. (arXiv:2011.01413v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks (NNs) are widely used for object classification in autonomous
driving. However, NNs can fail on input data not well represented by the
training dataset, known as out-of-distribution (OOD) data. A mechanism to
detect OOD samples is important for safety-critical applications, such as
automotive perception, to trigger a safe fallback mode. NNs often rely on
softmax normalization for confidence estimation, which can lead to high
confidences being assigned to OOD samples, thus hindering the detection of
failures. This paper presents a method for determining whether inputs are OOD,
which does not require OOD data during training and does not increase the
computational cost of inference. The latter property is especially important in
automotive applications with limited computational resources and real-time
constraints. Our proposed approach outperforms state-of-the-art methods on
real-world automotive datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-Camera Super-Resolution with Aligned Attention Modules. (arXiv:2109.01349v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01349">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present a novel approach to reference-based super-resolution (RefSR) with
the focus on dual-camera super-resolution (DCSR), which utilizes reference
images for high-quality and high-fidelity results. Our proposed method
generalizes the standard patch-based feature matching with spatial alignment
operations. We further explore the dual-camera super-resolution that is one
promising application of RefSR, and build a dataset that consists of 146 image
pairs from the main and telephoto cameras in a smartphone. To bridge the domain
gaps between real-world images and the training images, we propose a
self-supervised domain adaptation strategy for real-world images. Extensive
experiments on our dataset and a public benchmark demonstrate clear improvement
achieved by our method over state of the art in both quantitative evaluation
and visual comparisons.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose CX-ToM, short for counterfactual explanations with theory-of mind,
a new explainable AI (XAI) framework for explaining decisions made by a deep
convolutional neural network (CNN). In contrast to the current methods in XAI
that generate explanations as a single shot response, we pose explanation as an
iterative communication process, i.e. dialog, between the machine and human
user. More concretely, our CX-ToM framework generates sequence of explanations
in a dialog by mediating the differences between the minds of machine and human
user. To do this, we use Theory of Mind (ToM) which helps us in explicitly
modeling human&#x27;s intention, machine&#x27;s mind as inferred by the human as well as
human&#x27;s mind as inferred by the machine. Moreover, most state-of-the-art XAI
frameworks provide attention (or heat map) based explanations. In our work, we
show that these attention based explanations are not sufficient for increasing
human trust in the underlying CNN model. In CX-ToM, we instead use
counterfactual explanations called fault-lines which we define as follows:
given an input image I for which a CNN classification model M predicts class
c_pred, a fault-line identifies the minimal semantic-level features (e.g.,
stripes on zebra, pointed ears of dog), referred to as explainable concepts,
that need to be added to or deleted from I in order to alter the classification
category of I by M to another specified class c_alt. We argue that, due to the
iterative, conceptual and counterfactual nature of CX-ToM explanations, our
framework is practical and more natural for both expert and non-expert users to
understand the internal workings of complex deep learning models. Extensive
quantitative and qualitative experiments verify our hypotheses, demonstrating
that our CX-ToM significantly outperforms the state-of-the-art explainable AI
models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08212">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01181">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Targets are essential in problems such as object tracking in cluttered or
textureless environments, camera (and multi-sensor) calibration tasks, and
simultaneous localization and mapping (SLAM). Target shapes for these tasks
typically are symmetric (square, rectangular, or circular) and work well for
structured, dense sensor data such as pixel arrays (i.e., image). However,
symmetric shapes lead to pose ambiguity when using sparse sensor data such as
LiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.
This paper introduces the concept of optimizing target shape to remove pose
ambiguity for LiDAR point clouds. A target is designed to induce large
gradients at edge points under rotation and translation relative to the LiDAR
to ameliorate the quantization uncertainty associated with point cloud
sparseness. Moreover, given a target shape, we present a means that leverages
the target&#x27;s geometry to estimate the target&#x27;s vertices while globally
estimating the pose. Both the simulation and the experimental results (verified
by a motion capture system) confirm that by using the optimal shape and the
global solver, we achieve centimeter error in translation and a few degrees in
rotation even when a partially illuminated target is placed 30 meters away. All
the implementations and datasets are available at
https://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data. (arXiv:2106.00799v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00799">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This work proposes a multi-task fully convolutional architecture for tree
species mapping in dense forests from sparse and scarce polygon-level
annotations using hyperspectral UAV-borne data. Our model implements a partial
loss function that enables dense tree semantic labeling outcomes from non-dense
training samples, and a distance regression complementary task that enforces
tree crown boundary constraints and substantially improves the model
performance. Our multi-task architecture uses a shared backbone network that
learns common representations for both tasks and two task-specific decoders,
one for the semantic segmentation output and one for the distance map
regression. We report that introducing the complementary task boosts the
semantic segmentation performance compared to the single-task counterpart in up
to 11% reaching an average user&#x27;s accuracy of 88.63% and an average producer&#x27;s
accuracy of 88.59%, achieving state-of-art performance for tree species
classification in tropical forests.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.10837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Spiking neural network (SNN) is interesting both theoretically and
practically because of its strong bio-inspiration nature and potentially
outstanding energy efficiency. Unfortunately, its development has fallen far
behind the conventional deep neural network (DNN), mainly because of difficult
training and lack of widely accepted hardware experiment platforms. In this
paper, we show that a deep temporal-coded SNN can be trained easily and
directly over the benchmark datasets CIFAR10 and ImageNet, with testing
accuracy within 1% of the DNN of equivalent size and architecture. Training
becomes similar to DNN thanks to the closed-form solution to the spiking
waveform dynamics. Considering that SNNs should be implemented in practical
neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2
bits and with weights perturbed by random noise to demonstrate its robustness
in practical applications. In addition, we develop a phase-domain signal
processing circuit schematic to implement our spiking neuron with 90% gain of
energy efficiency over existing work. This paper demonstrates that the
temporal-coded deep SNN is feasible for applications with high performance and
high energy efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00636">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We learn an interactive vision-based driving policy from pre-recorded driving
logs via a model-based approach. A forward model of the world supervises a
driving policy that predicts the outcome of any potential driving trajectory.
To support learning from pre-recorded logs, we assume that the world is on
rails, meaning neither the agent nor its actions influence the environment.
This assumption greatly simplifies the learning problem, factorizing the
dynamics into a nonreactive world model and a low-dimensional and compact
forward model of the ego-vehicle. Our approach computes action-values for each
training trajectory using a tabular dynamic-programming evaluation of the
Bellman equations; these action-values in turn supervise the final vision-based
driving policy. Despite the world-on-rails assumption, the final driving policy
acts well in a dynamic and reactive world. At the time of writing, our method
ranks first on the CARLA leaderboard, attaining a 25% higher driving score
while using 40 times less data. Our method is also an order of magnitude more
sample-efficient than state-of-the-art model-free reinforcement learning
techniques on navigational tasks in the ProcGen benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Action Recognition Using Confidence Distillation. (arXiv:2109.02137v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Modern neural networks are powerful predictive models. However, when it comes
to recognizing that they may be wrong about their predictions, they perform
poorly. For example, for one of the most common activation functions, the ReLU
and its variants, even a well-calibrated model can produce incorrect but high
confidence predictions. In the related task of action recognition, most current
classification methods are based on clip-level classifiers that densely sample
a given video for non-overlapping, same-sized clips and aggregate the results
using an aggregation function - typically averaging - to achieve video level
predictions. While this approach has shown to be effective, it is sub-optimal
in recognition accuracy and has a high computational overhead. To mitigate both
these issues, we propose the confidence distillation framework to teach a
representation of uncertainty of the teacher to the student sampler and divide
the task of full video prediction between the student and the teacher models.
We conduct extensive experiments on three action recognition datasets and
demonstrate that our framework achieves significant improvements in action
recognition accuracy (up to 20%) and computational efficiency (more than 40%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12236">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Instance-level image retrieval is the task of searching in a large database
for images that match an object in a query image. To address this task, systems
usually rely on a retrieval step that uses global image descriptors, and a
subsequent step that performs domain-specific refinements or reranking by
leveraging operations such as geometric verification based on local features.
In this work, we propose Reranking Transformers (RRTs) as a general model to
incorporate both local and global features to rerank the matching images in a
supervised fashion and thus replace the relatively expensive process of
geometric verification. RRTs are lightweight and can be easily parallelized so
that reranking a set of top matching results can be performed in a single
forward-pass. We perform extensive experiments on the Revisited Oxford and
Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs
outperform previous reranking approaches while using much fewer local
descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs
can be optimized jointly with the feature extractor, which can lead to feature
representations tailored to downstream tasks and further accuracy improvements.
The code and trained models are publicly available at
https://github.com/uvavision/RerankingTransformer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07911">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the recent decade, high-throughput plant phenotyping techniques, which
combine non-invasive image analysis and machine learning, have been
successfully applied to identify and quantify plant health and diseases.
However, these techniques usually do not consider the progressive nature of
plant stress and often require images showing severe signs of stress to ensure
high confidence detection, thereby reducing the feasibility for early detection
and recovery of plants under stress. To overcome the problem mentioned above,
we propose a deep learning pipeline for the temporal analysis of the visual
changes induced in the plant due to stress and apply it to the specific water
stress identification case in Chickpea plant shoot images. For this, we have
considered an image dataset of two chickpea varieties JG-62 and Pusa-372, under
three water stress conditions; control, young seedling, and before flowering,
captured over five months. We have employed a variant of Convolutional Neural
Network - Long Short Term Memory (CNN-LSTM) network to learn spatio-temporal
patterns from the chickpea plant dataset and use them for water stress
classification. Our model has achieved ceiling level classification performance
of 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has
outperformed the best reported time-invariant technique by at least 14% for
both JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our
CNN-LSTM model has demonstrated robustness to noisy input, with a less than
2.5% dip in average model accuracy and a small standard deviation about the
mean for both species. Lastly, we have performed an ablation study to analyze
the performance of the CNN-LSTM model by decreasing the number of temporal
session data used for training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. (arXiv:2109.02244v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02244">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Supervised deep learning-based hash and vector quantization are enabling fast
and large-scale image retrieval systems. By fully exploiting label annotations,
they are achieving outstanding retrieval performances compared to the
conventional methods. However, it is painstaking to assign labels precisely for
a vast amount of training data, and also, the annotation process is
error-prone. To tackle these issues, we propose the first deep unsupervised
image retrieval method dubbed Self-supervised Product Quantization (SPQ)
network, which is label-free and trained in a self-supervised manner. We design
a Cross Quantized Contrastive learning strategy that jointly learns codewords
and deep visual descriptors by comparing individually transformed images
(views). Our method analyzes the image contents to extract descriptive
features, allowing us to understand image representations for accurate
retrieval. By conducting extensive experiments on benchmarks, we demonstrate
that the proposed method yields state-of-the-art results even without
supervised pretraining.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast Image-Anomaly Mitigation for Autonomous Mobile Robots. (arXiv:2109.01889v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Camera anomalies like rain or dust can severelydegrade image quality and its
related tasks, such as localizationand segmentation. In this work we address
this importantissue by implementing a pre-processing step that can
effectivelymitigate such artifacts in a real-time fashion, thus supportingthe
deployment of autonomous systems with limited computecapabilities. We propose a
shallow generator with aggregation,trained in an adversarial setting to solve
the ill-posed problemof reconstructing the occluded regions. We add an enhancer
tofurther preserve high-frequency details and image colorization.We also
produce one of the largest publicly available datasets1to train our
architecture and use realistic synthetic raindrops toobtain an improved
initialization of the model. We benchmarkour framework on existing datasets and
on our own imagesobtaining state-of-the-art results while enabling real-time
per-formance, with up to 40x faster inference time than existingapproaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Surprising Efficiency of Committee-based Models. (arXiv:2012.01988v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01988">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Committee-based models, i.e., model ensembles or cascades, are underexplored
in recent work on developing efficient models. While committee-based models
themselves are not new, there lacks a systematic understanding of their
efficiency in comparison with single models. To fill this gap, we conduct a
comprehensive analysis of the efficiency of committee-based models. We find
that committee-based models provide a complementary paradigm to achieve
superior efficiency without tuning the architecture: even the most simplistic
method for building ensembles or cascades from existing pre-trained networks
can attain a significant speedup and higher accuracy over state-of-the-art
single models, and also outperforms sophisticated neural architecture search
methods (e.g., BigNAS). The superior efficiency of committee-based models holds
true for several tasks, including image classification, video classification,
and semantic segmentation, and various architecture families, such as
EfficientNet, ResNet, MobileNetV2, and X3D.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02100">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged for their deployments to resource-limited
devices. Although recent studies have successfully discretized a full-precision
network, they still incur large quantization errors after training, thus giving
rise to a significant performance gap between a full-precision network and its
quantized counterpart. In this work, we propose a novel quantization method for
neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal
quantization grids while naturally encouraging the underlying full-precision
weights to gather around those quantization grids cohesively during training.
This property of CPQ is thanks to our two main ingredients that enable
differentiable quantization: i) the use of the categorical distribution
designed by a specific probabilistic parametrization in the forward pass and
ii) our proposed multi-class straight-through estimator (STE) in the backward
pass. Since our second component, multi-class STE, is intrinsically biased, we
additionally propose a new bit-drop technique, DropBits, that revises the
standard dropout regularization to randomly drop bits instead of neurons. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer by
imposing an additional regularization on DropBits. We experimentally validate
our method on various benchmark datasets and network architectures, and also
support a new hypothesis for quantization: learning heterogeneous quantization
levels outperforms the case using the same but fixed quantization levels from
scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness. (arXiv:2109.01945v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01945">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks have been shown to be highly effective at degrading the
performance of deep neural networks (DNNs). The most prominent defense is
adversarial training, a method for learning a robust model. Nevertheless,
adversarial training does not make DNNs immune to adversarial perturbations. We
propose a novel solution by adopting the recently suggested Predictive
Normalized Maximum Likelihood. Specifically, our defense performs adversarial
targeted attacks according to different hypotheses, where each hypothesis
assumes a specific label for the test sample. Then, by comparing the hypothesis
probabilities, we predict the label. Our refinement process corresponds to
recent findings of the adversarial subspace properties. We extensively evaluate
our approach on 16 adversarial attack benchmarks using ResNet-50,
WideResNet-28, and a2-layer ConvNet trained with ImageNet, CIFAR10, and MNIST,
showing a significant improvement of up to 5.7%, 3.7%, and 0.6% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10850">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Data hiding is the procedure of encoding desired information into the cover
image to resist potential noises while ensuring the embedded image has few
perceptual perturbations from the original one. Recently, with the tremendous
successes gained by deep neural networks in various fields, the researches of
data hiding with deep learning models have attracted an increasing number of
attentions. In the data hiding task, each pixel of cover images should be
treated differently since they have divergent tolerabilities. The neglect of
considering the sensitivity of each pixel will inevitably affect the model
robustness for information hiding. Targeting this problem, we propose a novel
deep data hiding scheme with Inverse Gradient Attention (IGA), combing the
ideas of adversarial learning and attention mechanism to endow different
sensitivities for different pixels. With the proposed component, the model can
spotlight pixels with more robustness for data hiding. Empirically, extensive
experiments show that the proposed model outperforms the state-of-the-art
methods on two prevalent datasets under multiple evaluations. Besides, we
further identify and discuss the connections between the proposed inverse
gradient attention and high-frequency regions within images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01879">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Moving object detection is a crucial task in computer vision. Event-based
cameras are bio-inspired cameras that work by mimicking the working of the
human eye. These cameras have multiple advantages over conventional frame-based
cameras, like reduced latency, HDR, reduced motion blur during high motion, low
power consumption, etc. However, these advantages come at a high cost, as
event-based cameras are noise sensitive and have low resolution. Moreover, the
task of moving object detection in these cameras is difficult, as event-based
sensors capture only the binary changes in brightness of a scene, lacking
useful visual features like texture and color. In this paper, we investigate
the application of the k-means clustering technique in detecting moving objects
in event-based data. Experimental results in publicly available datasets using
k-means show significant improvement in performance over the state-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00436">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mainly in object classification tasks. In this paper, we
make the first large-scale study concerning the task of Long-Tail Visual
Relationship Recognition (LTVRR). LTVRR aims at improving the learning of
structured visual relationships that come from the long-tail (e.g., &quot;rabbit
grazing on grass&quot;). In this setup, the subject, relation, and object classes
each follow a long-tail distribution. To begin our study and make a future
benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed
VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of several state-of-the-art
long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic
hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR
setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top
of existing models and despite being simple, our results show that they can
remarkably improve the performance, especially on tail classes. Benchmarks,
code, and models have been made available at:
https://github.com/Vision-CAIR/LTVRR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations. (arXiv:2006.10187v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10187">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Topology matters. Despite the recent success of point cloud processing with
geometric deep learning, it remains arduous to capture the complex topologies
of point cloud data with a learning model. Given a point cloud dataset
containing objects with various genera, or scenes with multiple objects, we
propose an autoencoder, TearingNet, which tackles the challenging task of
representing the point clouds using a fixed-length descriptor. Unlike existing
works directly deforming predefined primitives of genus zero (e.g., a 2D square
patch) to an object-level point cloud, our TearingNet is characterized by a
proposed Tearing network module and a Folding network module interacting with
each other iteratively. Particularly, the Tearing network module learns the
point cloud topology explicitly. By breaking the edges of a primitive graph, it
tears the graph into patches or with holes to emulate the topology of a target
point cloud, leading to faithful reconstructions. Experimentation shows the
superiority of our proposal in terms of reconstructing point clouds as well as
generating more topology-friendly representations than benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on Spark AR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on the enhanced data sets and showing that they outperform
equivalent systems trained on the original data sets (containing faces without
masks), when the test benchmark contains masked faces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Generate Scene Graph from Natural Language Supervision. (arXiv:2109.02227v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02227">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning from image-text data has demonstrated recent success for many
recognition tasks, yet is currently limited to visual features or individual
visual concepts such as objects. In this paper, we propose one of the first
methods that learn from image-sentence pairs to extract a graphical
representation of localized objects and their relationships within an image,
known as scene graph. To bridge the gap between images and texts, we leverage
an off-the-shelf object detector to identify and localize object instances,
match labels of detected regions to concepts parsed from captions, and thus
create &quot;pseudo&quot; labels for learning scene graph. Further, we design a
Transformer-based model to predict these &quot;pseudo&quot; labels via a masked token
prediction task. Learning from only image-sentence pairs, our model achieves
30% relative gain over a latest method trained with human-annotated unlocalized
scene graphs. Our model also shows strong results for weakly and fully
supervised scene graph generation. In addition, we explore an open-vocabulary
setting for detecting scene graphs, and present the first result for open-set
scene graph generation. Our code is available at
https://github.com/YiwuZhong/SGG_from_NLS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training (AT) has been demonstrated as one of the most promising
defense methods against various adversarial attacks. To our knowledge, existing
AT-based methods usually train with the locally most adversarial perturbed
points and treat all the perturbed points equally, which may lead to
considerably weaker adversarial robust generalization on test data. In this
work, we introduce a new adversarial training framework that considers the
diversity as well as characteristics of the perturbed points in the vicinity of
benign samples. To realize the framework, we propose a Regional Adversarial
Training (RAT) defense method that first utilizes the attack path generated by
the typical iterative attack method of projected gradient descent (PGD), and
constructs an adversarial region based on the attack path. Then, RAT samples
diverse perturbed training points efficiently inside this region, and utilizes
a distance-aware label smoothing mechanism to capture our intuition that
perturbed points at different locations should have different impact on the
model performance. Extensive experiments on several benchmark datasets show
that RAT consistently makes significant improvement on standard adversarial
training (SAT), and exhibits better robust generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01860">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid development of facial manipulation techniques has aroused public
concerns in recent years. Following the success of deep learning, existing
methods always formulate DeepFake video detection as a binary classification
problem and develop frame-based and video-based solutions. However, little
attention has been paid to capturing the spatial-temporal inconsistency in
forged videos. To address this issue, we term this task as a Spatial-Temporal
Inconsistency Learning (STIL) process and instantiate it into a novel STIL
block, which consists of a Spatial Inconsistency Module (SIM), a Temporal
Inconsistency Module (TIM), and an Information Supplement Module (ISM).
Specifically, we present a novel temporal modeling paradigm in TIM by
exploiting the temporal difference over adjacent frames along with both
horizontal and vertical directions. And the ISM simultaneously utilizes the
spatial information from SIM and temporal information from TIM to establish a
more comprehensive spatial-temporal representation. Moreover, our STIL block is
flexible and could be plugged into existing 2D CNNs. Extensive experiments and
visualizations are presented to demonstrate the effectiveness of our method
against the state-of-the-art competitors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Signal Processing for Geometric Data and Beyond: Theory and Applications. (arXiv:2008.01918v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D
point clouds, and 4D dynamic point clouds, have found a wide range of
applications including immersive telepresence, autonomous driving,
surveillance, etc. Due to irregular sampling patterns of most geometric data,
traditional image/video processing methodologies are limited, while Graph
Signal Processing (GSP) -- a fast-developing field in the signal processing
community -- enables processing signals that reside on irregular domains and
plays a critical role in numerous applications of geometric data from low-level
processing to high-level analysis. To further advance the research in this
field, we provide the first timely and comprehensive overview of GSP
methodologies for geometric data in a unified manner by bridging the
connections between geometric data and graphs, among the various geometric data
modalities, and with spectral/nodal graph filtering techniques. We also discuss
the recently developed Graph Neural Networks (GNNs) and interpret the operation
of these networks from the perspective of GSP. We conclude with a brief
discussion of open problems and challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As a kind of new expression elements, Internet memes are popular and
extensively used in online chatting scenarios since they manage to make
dialogues vivid, moving, and interesting. However, most current dialogue
researches focus on text-only dialogue tasks. In this paper, we propose a new
task named as \textbf{M}eme incorporated \textbf{O}pen-domain \textbf{D}ialogue
(MOD). Compared to previous dialogue tasks, MOD is much more challenging since
it requires the model to understand the multimodal elements as well as the
emotions behind them. To facilitate the MOD research, we construct a
large-scale open-domain multimodal dialogue dataset incorporating abundant
Internet memes into utterances. The dataset consists of $\sim$45K Chinese
conversations with $\sim$606K utterances. Each conversation contains about $13$
utterances with about $4$ Internet memes on average and each utterance equipped
with an Internet meme is annotated with the corresponding emotion. In addition,
we present a simple and effective method, which utilizes a unified generation
network to solve the MOD task. Experimental results demonstrate that our method
trained on the proposed corpus is able to achieve expressive communication
including texts and memes. The corpus and models have been publicly available
at https://github.com/lizekang/DSTC10-MOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Underwater 3D Reconstruction Using Light Fields. (arXiv:2109.02116v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02116">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Underwater 3D reconstruction is challenging due to the refraction of light at
the water-air interface (most electronic devices cannot be directly submerged
in water). In this paper, we present an underwater 3D reconstruction solution
using light field cameras. We first develop a light field camera calibration
algorithm that simultaneously estimates the camera parameters and the geometry
of the water-air interface. We then design a novel depth estimation algorithm
for 3D reconstruction. Specifically, we match correspondences on curved
epipolar lines caused by water refraction. We also observe that the
view-dependent specular reflection is very weak in the underwater environment,
resulting the angularly sampled rays in light field has uniform intensity. We
therefore propose an angular uniformity constraint for depth optimization. We
also develop a fast algorithm for locating the angular patches in presence of
non-linear light paths. Extensive synthetic and real experiments demonstrate
that our method can perform underwater 3D reconstruction with high accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04097">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Navigation is one of the fundamental features of a autonomous robot. And the
ability of long-term navigation with semantic instruction is a &#x60;holy grail&#x60;
goals of intelligent robots. The development of 3D simulation technology
provide a large scale of data to simulate the real-world environment. The deep
learning proves its ability to robustly learn various embodied navigation
tasks. However, deep learning on embodied navigation is still in its infancy
due to the unique challenges faced by the navigation exploration and learning
from partial observed visual input. Recently, deep learning in embodied
navigation has become even thriving, with numerous methods have been proposed
to tackle different challenges in this area. To give a promising direction for
future research, in this paper, we present a comprehensive review of embodied
navigation tasks and the recent progress in deep learning based methods. It
includes two major tasks: target-oriented navigation and the
instruction-oriented navigation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v4 [cs.PF] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.03459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10189">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Traffic accident anticipation aims to accurately and promptly predict the
occurrence of a future accident from dashcam videos, which is vital for a
safety-guaranteed self-driving system. To encourage an early and accurate
decision, existing approaches typically focus on capturing the cues of spatial
and temporal context before a future accident occurs. However, their
decision-making lacks visual explanation and ignores the dynamic interaction
with the environment. In this paper, we propose Deep ReInforced accident
anticipation with Visual Explanation, named DRIVE. The method simulates both
the bottom-up and top-down visual attention mechanism in a dashcam observation
environment so that the decision from the proposed stochastic multi-task agent
can be visually explained by attentive regions. Moreover, the proposed dense
anticipation reward and sparse fixation reward are effective in training the
DRIVE model with our improved reinforcement learning algorithm. Experimental
results show that the DRIVE model achieves state-of-the-art performance on
multiple real-world traffic accident datasets. Code and pre-trained model are
available at \url{https://www.rit.edu/actionlab/drive}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02344">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-view representation learning captures comprehensive information from
multiple views of a shared context. Recent works intuitively apply contrastive
learning (CL) to learn representations, regarded as a pairwise manner, which is
still scalable: view-specific noise is not filtered in learning view-shared
representations; the fake negative pairs, where the negative terms are actually
within the same class as the positive, and the real negative pairs are
coequally treated; and evenly measuring the similarities between terms might
interfere with optimization. Importantly, few works research the theoretical
framework of generalized self-supervised multi-view learning, especially for
more than two views. To this end, we rethink the existing multi-view learning
paradigm from the information theoretical perspective and then propose a novel
information theoretical framework for generalized multi-view learning. Guided
by it, we build a multi-view coding method with a three-tier progressive
architecture, namely Information theory-guided heuristic Progressive Multi-view
Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between
views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted
pools for contrasting, which utilizes a view filter to adaptively modify the
pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn
discriminative representations and reduce the gradient interference.
Theoretically and empirically, we demonstrate the superiority of IPMC over
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Audio-Visual Transformer Based Crowd Counting. (arXiv:2109.01926v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01926">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Crowd estimation is a very challenging problem. The most recent study tries
to exploit auditory information to aid the visual models, however, the
performance is limited due to the lack of an effective approach for feature
extraction and integration. The paper proposes a new audiovisual multi-task
network to address the critical challenges in crowd counting by effectively
utilizing both visual and audio inputs for better modalities association and
productive feature extraction. The proposed network introduces the notion of
auxiliary and explicit image patch-importance ranking (PIR) and patch-wise
crowd estimate (PCE) information to produce a third (run-time) modality. These
modalities (audio, visual, run-time) undergo a transformer-inspired
cross-modality co-attention mechanism to finally output the crowd estimate. To
acquire rich visual features, we propose a multi-branch structure with
transformer-style fusion in-between. Extensive experimental evaluations show
that the proposed scheme outperforms the state-of-the-art networks under all
evaluation settings with up to 33.8% improvement. We also analyze and compare
the vision-only variant of our network and empirically demonstrate its
superiority over previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatial Domain Feature Extraction Methods for Unconstrained Handwritten Malayalam Character Recognition. (arXiv:2109.02153v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02153">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Handwritten character recognition is an active research challenge,especially
for Indian scripts. This paper deals with handwritten Malayalam, with a
complete set of basic characters, vowel and consonant signs and compound
characters that may be present in the script. Spatial domain features suitable
for recognition are chosen in this work. For classification, k-NN, SVM and ELM
are employed</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05509">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The amount and quality of datasets and tools available in the research field
of hand pose and shape estimation act as evidence to the significant progress
that has been made.However, even the datasets of the highest quality, reported
to date, have shortcomings in annotation. We propose a refinement approach,
based on differentiable ray tracing,and demonstrate how a high-quality publicly
available, multi-camera dataset of hands(InterHand2.6M) can become an even
better dataset, with respect to annotation quality. Differentiable ray tracing
has not been employed so far to relevant problems and is hereby shown to be
superior to the approximative alternatives that have been employed in the past.
To tackle the lack of reliable ground truth, as far as quantitative evaluation
is concerned, we resort to realistic synthetic data, to show that the
improvement we induce is indeed significant. The same becomes evident in real
data through visual evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Representing human-made objects as a collection of base primitives has a long
history in computer vision and reverse engineering. In the case of
high-resolution point cloud scans, the challenge is to be able to detect both
large primitives as well as those explaining the detailed parts. While the
classical RANSAC approach requires case-specific parameter tuning,
state-of-the-art networks are limited by memory consumption of their backbone
modules such as PointNet++, and hence fail to detect the fine-scale primitives.
We present Cascaded Primitive Fitting Networks (CPFN) that relies on an
adaptive patch sampling network to assemble detection results of global and
local primitive detection networks. As a key enabler, we present a merging
formulation that dynamically aggregates the primitives across global and local
scales. Our evaluation demonstrates that CPFN improves the state-of-the-art
SPFN performance by 13-14% on high-resolution point cloud datasets and
specifically improves the detection of fine-scale primitives by 20-22%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Compression with Recurrent Neural Network and Generalized Divisive Normalization. (arXiv:2109.01999v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01999">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image compression is a method to remove spatial redundancy between adjacent
pixels and reconstruct a high-quality image. In the past few years, deep
learning has gained huge attention from the research community and produced
promising image reconstruction results. Therefore, recent methods focused on
developing deeper and more complex networks, which significantly increased
network complexity. In this paper, two effective novel blocks are developed:
analysis and synthesis block that employs the convolution layer and Generalized
Divisive Normalization (GDN) in the variable-rate encoder and decoder side. Our
network utilizes a pixel RNN approach for quantization. Furthermore, to improve
the whole network, we encode a residual image using LSTM cells to reduce
unnecessary information. Experimental results demonstrated that the proposed
variable-rate framework with novel blocks outperforms existing methods and
standard image codecs, such as George&#x27;s ~\cite{002} and JPEG in terms of image
similarity. The project page along with code and models are available at
https://khawar512.github.io/cvpr/</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09384">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hyperspectral images (HSI) consist of rich spatial and spectral information,
which can potentially be used for several applications. However, noise, band
correlations and high dimensionality restrict the applicability of such data.
This is recently addressed using creative deep learning network architectures
such as ResNet, SSRN, and A2S2K. However, the last layer, i.e the
classification layer, remains unchanged and is taken to be the softmax
classifier. In this article, we propose to use a watershed classifier.
Watershed classifier extends the watershed operator from Mathematical
Morphology for classification. In its vanilla form, the watershed classifier
does not have any trainable parameters. In this article, we propose a novel
approach to train deep learning networks to obtain representations suitable for
the watershed classifier. The watershed classifier exploits the connectivity
patterns, a characteristic of HSI datasets, for better inference. We show that
exploiting such characteristics allows the Triplet-Watershed to achieve
state-of-art results in supervised and semi-supervised contexts. These results
are validated on Indianpines (IP), University of Pavia (UP), Kennedy Space
Center (KSC) and University of Houston (UH) datasets, relying on simple convnet
architecture using a quarter of parameters compared to previous
state-of-the-art networks. The source code for reproducing the experiments and
supplementary material (high resolution images) is available at
https://github.com/ac20/TripletWatershed Code.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming. (arXiv:2009.05750v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>An effective perception system is a fundamental component for farming robots,
as it enables them to properly perceive the surrounding environment and to
carry out targeted operations. The most recent methods make use of
state-of-the-art machine learning techniques to learn a valid model for the
target task. However, those techniques need a large amount of labeled data for
training. A recent approach to deal with this issue is data augmentation
through Generative Adversarial Networks (GANs), where entire synthetic scenes
are added to the training data, thus enlarging and diversifying their
informative content. In this work, we propose an alternative solution with
respect to the common data augmentation methods, applying it to the fundamental
problem of crop/weed segmentation in precision farming. Starting from real
images, we create semi-artificial samples by replacing the most relevant object
classes (i.e., crop and weeds) with their synthesized counterparts. To do that,
we employ a conditional GAN (cGAN), where the generative model is trained by
conditioning the shape of the generated object. Moreover, in addition to RGB
data, we take into account also near-infrared (NIR) information, generating
four channel multi-spectral synthetic images. Quantitative experiments, carried
out on three publicly available datasets, show that (i) our model is capable of
generating realistic multi-spectral images of plants and (ii) the usage of such
synthetic images in the training process improves the segmentation performance
of state-of-the-art semantic segmentation convolutional networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Tightness of Semidefinite Relaxations for Rotation Estimation. (arXiv:2101.02099v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Why is it that semidefinite relaxations have been so successful in numerous
applications in computer vision and robotics for solving non-convex
optimization problems involving rotations? In studying the empirical
performance we note that there are few failure cases reported in the
literature, in particular for estimation problems with a single rotation,
motivating us to gain further theoretical understanding.

A general framework based on tools from algebraic geometry is introduced for
analyzing the power of semidefinite relaxations of problems with quadratic
objective functions and rotational constraints. Applications include
registration, hand-eye calibration and rotation averaging. We characterize the
extreme points, and show that there exist failure cases for which the
relaxation is not tight, even in the case of a single rotation. We also show
that some problem classes are always tight given an appropriate
parametrization. Our theoretical findings are accompanied with numerical
simulations, providing further evidence and understanding of the results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02123">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. (arXiv:2109.02226v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02226">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this manuscript, we introduce a semi-automatic scene graph annotation tool
for images, the GeneAnnotator. This software allows human annotators to
describe the existing relationships between participators in the visual scene
in the form of directed graphs, hence enabling the learning and reasoning on
visual relationships, e.g., image captioning, VQA and scene graph generation,
etc. The annotations for certain image datasets could either be merged in a
single VG150 data-format file to support most existing models for scene graph
learning or transformed into a separated annotation file for each single image
to build customized datasets. Moreover, GeneAnnotator provides a rule-based
relationship recommending algorithm to reduce the heavy annotation workload.
With GeneAnnotator, we propose Traffic Genome, a comprehensive scene graph
dataset with 1000 diverse traffic images, which in return validates the
effectiveness of the proposed software for scene graph annotation. The project
source code, with usage examples and sample data is available at
https://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph,
under the Apache open-source license.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing. (arXiv:2109.02281v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Compared with image scene parsing, video scene parsing introduces temporal
information, which can effectively improve the consistency and accuracy of
prediction. In this paper, we propose a Spatial-Temporal Semantic Consistency
method to capture class-exclusive context information. Specifically, we design
a spatial-temporal consistency loss to constrain the semantic consistency in
spatial and temporal dimensions. In addition, we adopt an pseudo-labeling
strategy to enrich the training dataset. We obtain the scores of 59.84% and
58.85% mIoU on development (test part 1) and testing set of VSPW, respectively.
And our method wins the 1st place on VSPW challenge at ICCV2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10423">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as &quot;review&quot; trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the &quot;review&quot; trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Segmentation plays a crucial role in diagnosis. Studying the retinal
vasculatures from fundus images help identify early signs of many crucial
illnesses such as diabetic retinopathy. Due to the varying shape, size, and
patterns of retinal vessels, along with artefacts and noises in fundus images,
no one-stage method can accurately segment retinal vessels. In this work, we
propose a multi-scale, multi-level attention embedded CNN architecture
((M)SLAe-Net) to address the issue of multi-stage processing for robust and
precise segmentation of retinal vessels. We do this by extracting features at
multiple scales and multiple levels of the network, enabling our model to
holistically extracts the local and global features. Multi-scale features are
extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We
also aggregate the features from all the network levels. These effectively
resolved the issues of varying shapes and artefacts and hence the need for
multiple stages. To assist in better pixel-level classification, we use the
Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and
Excitation(SE) module for segmentation tasks in our network to facilitate
pixel-group attention. Our unique network design and novel D-DPP module with
efficient task-specific loss function for thin vessels enabled our model for
better cross data performance. Exhaustive experimental results on DRIVE, STARE,
HRF, and CHASE-DB1 show the superiority of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The use of mobiles phones when driving have been a major factor when it comes
to road traffic incidents and the process of capturing such violations can be a
laborious task. Advancements in both modern object detection frameworks and
high-performance hardware has paved the way for a more automated approach when
it comes to video surveillance. In this work, we propose a custom-trained
state-of-the-art object detector to work with roadside cameras to capture
driver phone usage without the need for human intervention. The proposed
approach also addresses the issues caused by windscreen glare and introduces
the steps required to remedy this. Twelve pre-trained models are fine-tuned
with our custom dataset using four popular object detection methods: YOLO, SSD,
Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO
yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to
~30 FPS. DeepSort object tracking algorithm is also integrated into the
best-performing model to collect records of only the unique violations, and
enable the proposed approach to count the number of vehicles. The proposed
automated system will collect the output images of the identified violations,
timestamps of each violation, and total vehicle count. Data can be accessed via
a purpose-built user interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical. (arXiv:2109.02219v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02219">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we investigate the problem of facial kinship verification by
learning hierarchical reasoning graph networks. Conventional methods usually
focus on learning discriminative features for each facial image of a paired
sample and neglect how to fuse the obtained two facial image features and
reason about the relations between them. To address this, we propose a
Star-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a
star-shaped graph where each surrounding node encodes the information of
comparisons in a feature dimension and the central node is employed as the
bridge for the interaction of surrounding nodes. Then we perform relational
reasoning on this star graph with iterative message passing. The proposed S-RGN
uses only one central node to analyze and process information from all
surrounding nodes, which limits its reasoning capacity. We further develop a
Hierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and
flexible capacity. More specifically, our H-RGN introduces a set of latent
reasoning nodes and constructs a hierarchical graph with them. Then bottom-up
comparative information abstraction and top-down comprehensive signal
propagation are iteratively performed on the hierarchical graph to update the
node features. Extensive experimental results on four widely used kinship
databases show that the proposed methods achieve very competitive results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the problem of achieving fair classification in Federated
Learning (FL) under data heterogeneity. Most of the approaches proposed for
fair classification require diverse data that represent the different
demographic groups involved. In contrast, it is common for each client to own
data that represents only a single demographic group. Hence the existing
approaches cannot be adopted for fair classification models at the client
level. To resolve this challenge, we propose several aggregation techniques. We
empirically validate these techniques by comparing the resulting fairness
metrics and accuracy on CelebA, UTK, and FairFace datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hierarchical Object-to-Zone Graph for Object Navigation. (arXiv:2109.02066v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02066">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The goal of object navigation is to reach the expected objects according to
visual information in the unseen environments. Previous works usually implement
deep models to train an agent to predict actions in real-time. However, in the
unseen environment, when the target object is not in egocentric view, the agent
may not be able to make wise decisions due to the lack of guidance. In this
paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent
in a coarse-to-fine manner, and an online-learning mechanism is also proposed
to update HOZ according to the real-time observation in new environments. In
particular, the HOZ graph is composed of scene nodes, zone nodes and object
nodes. With the pre-learned HOZ graph, the real-time observation and the target
goal, the agent can constantly plan an optimal path from zone to zone. In the
estimated path, the next potential zone is regarded as sub-goal, which is also
fed into the deep reinforcement learning model for action prediction. Our
methods are evaluated on the AI2-Thor simulator. In addition to widely used
evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE
that focuses on the effective action rate. Experimental results demonstrate the
effectiveness and efficiency of our proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02283">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery. (arXiv:2109.02342v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02342">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Purpose: Static cardiac imaging such as late gadolinium enhancement, mapping,
or 3-D coronary angiography require prior information, e.g., the phase during a
cardiac cycle with least motion, called resting phase (RP). The purpose of this
work is to propose a fully automated framework that allows the detection of the
right coronary artery (RCA) RP within CINE series. Methods: The proposed
prototype system consists of three main steps. First, the localization of the
regions of interest (ROI) is performed. Second, as CINE series are
time-resolved, the cropped ROI series over all time points are taken for
tracking motions quantitatively. Third, the output motion values are used to
classify RPs. In this work, we focused on the detection of the area with the
outer edge of the cross-section of the RCA as our target. The proposed
framework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The
automatically classified RPs were compared with the ground truth RPs annotated
manually by a medical expert for testing the robustness and feasibility of the
framework. Results: The predicted RCA RPs showed high agreement with the
experts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0%
specificity for the unseen study dataset. The mean absolute difference of the
start and end RP was 13.6 ${\pm}$ 18.6 ms for the validation study dataset
(n&#x3D;102). Conclusion: In this work, automated RP detection has been introduced
by the proposed framework and demonstrated feasibility, robustness, and
applicability for diverse static imaging acquisitions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02220">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Model compression techniques are recently gaining explosive attention for
obtaining efficient AI models for various real-time applications. Channel
pruning is one important compression strategy and is widely used in slimming
various DNNs. Previous gate-based or importance-based pruning methods aim to
remove channels whose importance is smallest. However, it remains unclear what
criteria the channel importance should be measured on, leading to various
channel selection heuristics. Some other sampling-based pruning methods deploy
sampling strategies to train sub-nets, which often causes the training
instability and the compressed model&#x27;s degraded performance. In view of the
research gaps, we present a new module named Gates with Differentiable
Polarization (GDP), inspired by principled optimization ideas. GDP can be
plugged before convolutional layers without bells and whistles, to control the
on-and-off of each channel or whole layer block. During the training process,
the polarization effect will drive a subset of gates to smoothly decrease to
exact zero, while other gates gradually stay away from zero by a large margin.
When training terminates, those zero-gated channels can be painlessly removed,
while other non-zero gates can be absorbed into the succeeding convolution
kernel, causing completely no interruption to training nor damage to the
trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show
that the proposed GDP algorithm achieves the state-of-the-art performance on
various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to
DeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose
test performance sees no drop (even slightly improved) with over 60% FLOPs
saving.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis. (arXiv:2109.02081v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep person generation has attracted extensive research attention due to its
wide applications in virtual agents, video conferencing, online shopping and
art/movie production. With the advancement of deep learning, visual appearances
(face, pose, cloth) of a person image can be easily generated or manipulated on
demand. In this survey, we first summarize the scope of person generation, and
then systematically review recent progress and technical trends in deep person
generation, covering three major tasks: talking-head generation (face),
pose-guided person generation (pose) and garment-oriented person generation
(cloth). More than two hundred papers are covered for a thorough overview, and
the milestone works are highlighted to witness the major technical
breakthrough. Based on these fundamental tasks, a number of applications are
investigated, e.g., virtual fitting, digital human, generative data
augmentation. We hope this survey could shed some light on the future prospects
of deep person generation, and provide a helpful foundation for full
applications towards digital human.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Investigating Customization Strategies and Convergence Behaviors of Task-specific ADMM. (arXiv:1909.10819v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.10819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Alternating Direction Method of Multiplier (ADMM) has been a popular
algorithmic framework for separable optimization problems with linear
constraints. For numerical ADMM fail to exploit the particular structure of the
problem at hand nor the input data information, leveraging task-specific
modules (e.g., neural networks and other data-driven architectures) to extend
ADMM is a significant but challenging task. This work focuses on designing a
flexible algorithmic framework to incorporate various task-specific modules
(with no additional constraints) to improve the performance of ADMM in
real-world applications. Specifically, we propose Guidance from Optimality
(GO), a new customization strategy, to embed task-specific modules into ADMM
(GO-ADMM). By introducing an optimality-based criterion to guide the
propagation, GO-ADMM establishes an updating scheme agnostic to the choice of
additional modules. The existing task-specific methods just plug their
task-specific modules into the numerical iterations in a straightforward
manner. Even with some restrictive constraints on the plug-in modules, they can
only obtain some relatively weaker convergence properties for the resulted ADMM
iterations. Fortunately, without any restrictions on the embedded modules, we
prove the convergence of GO-ADMM regarding objective values and constraint
violations, and derive the worst-case convergence rate measured by iteration
complexity. Extensive experiments are conducted to verify the theoretical
results and demonstrate the efficiency of GO-ADMM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
spatial-temporal representation of a dynamic scene from a set of RGB images.
Key to our approach is the use of a neural implicit representation that learns
to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
consistency across different modalities, our representation enables multi-view
rendering in diverse dynamic scenes, including water pouring, robotic
interaction, and real images, outperforming state-of-the-art methods for
spatial-temporal view synthesis. Our approach works even when inputs images are
captured with only one camera. We further demonstrate that the learned
representation can serve as an implicit scene prior, enabling video processing
tasks such as image super-resolution and de-noising without any additional
supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we focus on landscape animation, which aims to generate
time-lapse videos from a single landscape image. Motion is crucial for
landscape animation as it determines how objects move in videos. Existing
methods are able to generate appealing videos by learning motion from real
time-lapse videos. However, current methods suffer from inaccurate motion
generation, which leads to unrealistic video results. To tackle this problem,
we propose a model named FGLA to generate high-quality and realistic videos by
learning Fine-Grained motion embedding for Landscape Animation. Our model
consists of two parts: (1) a motion encoder which embeds time-lapse motion in a
fine-grained way. (2) a motion generator which generates realistic motion to
animate input images. To train and evaluate on diverse time-lapse videos, we
build the largest high-resolution Time-lapse video dataset with Diverse scenes,
namely Time-lapse-D, which includes 16,874 video clips with over 10 million
frames. Quantitative and qualitative experimental results demonstrate the
superiority of our method. In particular, our method achieves relative
improvements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art
methods on our dataset. A user study carried out with 700 human subjects shows
that our approach visually outperforms existing methods by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dissecting Image Crops. (arXiv:2011.11831v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11831">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The elementary operation of cropping underpins nearly every computer vision
system, ranging from data augmentation and translation invariance to
computational photography and representation learning. This paper investigates
the subtle traces introduced by this operation. For example, despite
refinements to camera optics, lenses will leave behind certain clues, notably
chromatic aberration and vignetting. Photographers also leave behind other
clues relating to image aesthetics and scene composition. We study how to
detect these traces, and investigate the impact that cropping has on the image
distribution. While our aim is to dissect the fundamental impact of spatial
crops, there are also a number of practical implications to our work, such as
revealing faulty photojournalism and equipping neural network researchers with
a better understanding of shortcut learning. Code is available at
https://github.com/basilevh/dissecting-image-crops.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Mixture of Experts (MoE) with sparse conditional computation has been proved
an effective architecture for scaling attention-based models to more parameters
with comparable computation cost. In this paper, we propose Sparse-MLP, scaling
the recent MLP-Mixer model with sparse MoE layers, to achieve a more
computation-efficient architecture. We replace a subset of dense MLP blocks in
the MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two
stages of MoE layers: one with MLP experts mixing information within channels
along image patch dimension, one with MLP experts mixing information within
patches along the channel dimension. Besides, to reduce computational cost in
routing and improve experts capacity, we design Re-represent layers in each
Sparse block. These layers are to re-scale image representations by two simple
but effective linear transformations. By pre-training on ImageNet-1k with MoCo
v3 algorithm, our models can outperform dense MLP models with comparable
parameters and less computational cost on several downstream image
classification tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1710.03113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multidiversified ensemble clustering approach. In particular, we create a
large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Square Root Marginalization for Sliding-Window Bundle Adjustment. (arXiv:2109.02182v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02182">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we propose a novel square root sliding-window bundle adjustment
suitable for real-time odometry applications. The square root formulation
pervades three major aspects of our optimization-based sliding-window
estimator: for bundle adjustment we eliminate landmark variables with nullspace
projection; to store the marginalization prior we employ a matrix square root
of the Hessian; and when marginalizing old poses we avoid forming normal
equations and update the square root prior directly with a specialized QR
decomposition. We show that the proposed square root marginalization is
algebraically equivalent to the conventional use of Schur complement (SC) on
the Hessian. Moreover, it elegantly deals with rank-deficient Jacobians
producing a prior equivalent to SC with Moore-Penrose inverse. Our evaluation
of visual and visual-inertial odometry on real-world datasets demonstrates that
the proposed estimator is 36% faster than the baseline. It furthermore shows
that in single precision, conventional Hessian-based marginalization leads to
numeric failures and reduced accuracy. We analyse numeric properties of the
marginalization prior to explain why our square root form does not suffer from
the same effect and therefore entails superior performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02015">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The shortage of workforce and increasing cost of maintenance has forced many
farm industrialists to shift towards automated and mechanized approaches. The
key component for autonomous systems is the path planning techniques used.
Coverage path planning (CPP) algorithm is used for navigating over farmlands to
perform various agricultural operations such as seeding, ploughing, or spraying
pesticides and fertilizers. This report paper compares novel algorithms for
autonomous navigation of farmlands. For reduction of navigational constraints,
a high-resolution grid map representation is taken into consideration specific
to Indian environments. The free space is covered by distinguishing the grid
cells as covered, unexplored, partially explored and presence of an obstacle.
The performance of the compared algorithms is evaluated with metrics such as
time efficiency, space efficiency, accuracy, and robustness to changes in the
environment. Robotic Operating System (ROS), Dassault Systemes Experience
Platform (3DS Experience), MATLAB along Python were used for the simulation of
the compared algorithms. The results proved the applicability of the algorithms
for autonomous field navigation and feasibility with robotic path planning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stimuli-Aware Visual Emotion Analysis. (arXiv:2109.01812v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01812">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Visual emotion analysis (VEA) has attracted great attention recently, due to
the increasing tendency of expressing and understanding emotions through images
on social networks. Different from traditional vision tasks, VEA is inherently
more challenging since it involves a much higher level of complexity and
ambiguity in human cognitive process. Most of the existing methods adopt deep
learning techniques to extract general features from the whole image,
disregarding the specific features evoked by various emotional stimuli.
Inspired by the \textit{Stimuli-Organism-Response (S-O-R)} emotion model in
psychological theory, we proposed a stimuli-aware VEA method consisting of
three stages, namely stimuli selection (S), feature extraction (O) and emotion
prediction (R). First, specific emotional stimuli (i.e., color, object, face)
are selected from images by employing the off-the-shelf tools. To the best of
our knowledge, it is the first time to introduce stimuli selection process into
VEA in an end-to-end network. Then, we design three specific networks, i.e.,
Global-Net, Semantic-Net and Expression-Net, to extract distinct emotional
features from different stimuli simultaneously. Finally, benefiting from the
inherent structure of Mikel&#x27;s wheel, we design a novel hierarchical
cross-entropy loss to distinguish hard false examples from easy ones in an
emotion-specific manner. Experiments demonstrate that the proposed method
consistently outperforms the state-of-the-art approaches on four public visual
emotion datasets. Ablation study and visualizations further prove the validity
and interpretability of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01668">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The recent achievements of Deep Learning rely on the test data being similar
in distribution to the training data. In an ideal case, Deep Learning models
would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make
predictions on out-of-distribution data. Yet in practice, models usually fail
to generalize well when facing a shift in distribution. Several methods were
thereby designed to improve the robustness of the features learned by a model
through Regularization- or Domain-Prediction-based schemes. Segmenting medical
images such as MRIs of the hippocampus is essential for the diagnosis and
treatment of neuropsychiatric disorders. But these brain images often suffer
from distribution shift due to the patient&#x27;s age and various pathologies
affecting the shape of the organ. In this work, we evaluate OoD Generalization
solutions for the problem of hippocampus segmentation in MR data using both
fully- and semi-supervised training. We find that no method performs reliably
in all experiments. Only the V-REx loss stands out as it remains easy to tune,
while it outperforms a standard U-Net in most cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN. (arXiv:2109.02103v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02103">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the
Chest (CXR) as a complementary screening technique to RT-PCR testing grows to
its clinical use for respiratory complaints. Many new deep learning approaches
have developed as a consequence. The goal of this research is to assess the
convolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray
images of chest. The performance of CNN with one, three, and four convolution
layers has been evaluated in this research. A dataset of 13,808 CXR photographs
are used in this research. When evaluated on X-ray images with three splits of
the dataset, our preliminary experimental results show that the CNN model with
three convolution layers can reliably detect with 96 percent accuracy
(precision being 96 percent). This fact indicates the commitment of our
suggested model for reliable screening of COVID-19.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">F3S: Free Flow Fever Screening. (arXiv:2109.01733v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identification of people with elevated body temperature can reduce or
dramatically slow down the spread of infectious diseases like COVID-19. We
present a novel fever-screening system, F3S, that uses edge machine learning
techniques to accurately measure core body temperatures of multiple individuals
in a free-flow setting. F3S performs real-time sensor fusion of visual camera
with thermal camera data streams to detect elevated body temperature, and it
has several unique features: (a) visual and thermal streams represent very
different modalities, and we dynamically associate semantically-equivalent
regions across visual and thermal frames by using a new, dynamic alignment
technique that analyzes content and context in real-time, (b) we track people
through occlusions, identify the eye (inner canthus), forehead, face and head
regions where possible, and provide an accurate temperature reading by using a
prioritized refinement algorithm, and (c) we robustly detect elevated body
temperature even in the presence of personal protective equipment like masks,
or sunglasses or hats, all of which can be affected by hot weather and lead to
spurious temperature readings. F3S has been deployed at over a dozen large
commercial establishments, providing contact-less, free-flow, real-time fever
screening for thousands of employees and customers in indoors and outdoor
settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation. (arXiv:2109.01801v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Event cameras are novel sensors that perceive the per-pixel intensity changes
and output asynchronous event streams with high dynamic range and less motion
blur. It has been shown that events alone can be used for end-task learning,
\eg, semantic segmentation, based on encoder-decoder-like networks. However, as
events are sparse and mostly reflect edge information, it is difficult to
recover original details merely relying on the decoder. Moreover, most methods
resort to pixel-wise loss alone for supervision, which might be insufficient to
fully exploit the visual details from sparse events, thus leading to less
optimal performance. In this paper, we propose a simple yet flexible two-stream
framework named Dual Transfer Learning (DTL) to effectively enhance the
performance on the end-tasks without adding extra inference cost. The proposed
approach consists of three parts: event to end-task learning (EEL) branch,
event to image translation (EIT) branch, and transfer learning (TL) module that
simultaneously explores the feature-level affinity information and pixel-level
knowledge from the EIT branch to improve the EEL branch. This simple yet novel
method leads to strong representation learning from events and is evidenced by
the significant performance boost on the end-tasks such as semantic
segmentation and depth estimation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sleep stage classification is essential for sleep assessment and disease
diagnosis. Although previous attempts to classify sleep stages have achieved
high classification performance, several challenges remain open: 1) How to
effectively utilize time-varying spatial and temporal features from
multi-channel brain signals remains challenging. Prior works have not been able
to fully utilize the spatial topological information among brain regions. 2)
Due to the many differences found in individual biological signals, how to
overcome the differences of subjects and improve the generalization of deep
neural networks is important. 3) Most deep learning methods ignore the
interpretability of the model to the brain. To address the above challenges, we
propose a multi-view spatial-temporal graph convolutional networks (MSTGCN)
with domain generalization for sleep stage classification. Specifically, we
construct two brain view graphs for MSTGCN based on the functional connectivity
and physical distance proximity of the brain regions. The MSTGCN consists of
graph convolutions for extracting spatial features and temporal convolutions
for capturing the transition rules among sleep stages. In addition, attention
mechanism is employed for capturing the most relevant spatial-temporal
information for sleep stage classification. Finally, domain generalization and
MSTGCN are integrated into a unified framework to extract subject-invariant
sleep features. Experiments on two public datasets demonstrate that the
proposed model outperforms the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. (arXiv:2109.01723v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D hand-mesh reconstruction from RGB images facilitates many applications,
including augmented reality (AR). However, this requires not only real-time
speed and accurate hand pose and shape but also plausible mesh-image alignment.
While existing works already achieve promising results, meeting all three
requirements is very challenging. This paper presents a novel pipeline by
decoupling the hand-mesh reconstruction task into three stages: a joint stage
to predict hand joints and segmentation; a mesh stage to predict a rough hand
mesh; and a refine stage to fine-tune it with an offset mesh for mesh-image
alignment. With careful design in the network structure and in the loss
functions, we can promote high-quality finger-level mesh-image alignment and
drive the models together to deliver real-time predictions. Extensive
quantitative and qualitative results on benchmark datasets demonstrate that the
quality of our results outperforms the state-of-the-art methods on
hand-mesh/pose precision and hand-image alignment. In the end, we also showcase
several real-time AR scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping. (arXiv:2109.00900v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In a complex urban scene, observation from a single sensor unavoidably leads
to voids in observations, failing to describe urban objects in a comprehensive
manner. In this paper, we propose a spatio-temporal-spectral-angular
observation model to integrate observations from UAV and mobile mapping vehicle
platform, realizing a joint, coordinated observation operation from both air
and ground. We develop a multi-source remote sensing data acquisition system to
effectively acquire multi-angle data of complex urban scenes. Multi-source data
fusion solves the missing data problem caused by occlusion and achieves
accurate, rapid, and complete collection of holographic spatial and temporal
information in complex urban scenes. We carried out an experiment on Baisha
Town, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and
mobile mapping vehicle. We first extracted the point cloud from UAV and then
integrated the UAV and mobile mapping vehicle point cloud. The integrated
results combined both the characteristic of UAV and mobile mapping vehicle
point cloud, confirming the practicability of the proposed joint data
acquisition platform and the effectiveness of spatio-temporal-spectral-angular
observation model. Compared with the observation from UAV or mobile mapping
vehicle alone, the integrated system provides an effective data acquisition
solution towards comprehensive urban monitoring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Seam Carving Detection and Localization using Two-Stage Deep Neural Networks. (arXiv:2109.01764v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01764">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Seam carving is a method to resize an image in a content aware fashion.
However, this method can also be used to carve out objects from images. In this
paper, we propose a two-step method to detect and localize seam carved images.
First, we build a detector to detect small patches in an image that has been
seam carved. Next, we compute a heatmap on an image based on the patch
detector&#x27;s output. Using these heatmaps, we build another detector to detect if
a whole image is seam carved or not. Our experimental results show that our
approach is effective in detecting and localizing seam carved images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01844">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Catastrophic forgetting of previously learned knowledge while learning new
tasks is a widely observed limitation of contemporary neural networks. Although
many continual learning methods are proposed to mitigate this drawback, the
main question remains unanswered: what is the root cause of catastrophic
forgetting? In this work, we aim at answering this question by posing and
validating a set of research hypotheses related to the specificity of
representations built internally by neural models. More specifically, we design
a set of empirical evaluations that compare the robustness of representations
in discriminative and generative models against catastrophic forgetting. We
observe that representations learned by discriminative models are more prone to
catastrophic forgetting than their generative counterparts, which sheds new
light on the advantages of developing generative models for continual learning.
Finally, our work opens new research pathways and possibilities to adopt
generative models in continual learning beyond mere replay mechanisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PR-Net: Preference Reasoning for Personalized Video Highlight Detection. (arXiv:2109.01799v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01799">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Personalized video highlight detection aims to shorten a long video to
interesting moments according to a user&#x27;s preference, which has recently raised
the community&#x27;s attention. Current methods regard the user&#x27;s history as
holistic information to predict the user&#x27;s preference but negating the inherent
diversity of the user&#x27;s interests, resulting in vague preference
representation. In this paper, we propose a simple yet efficient preference
reasoning framework (PR-Net) to explicitly take the diverse interests into
account for frame-level highlight prediction. Specifically, distinct
user-specific preferences for each input query frame are produced, presented as
the similarity weighted sum of history highlights to the corresponding query
frame. Next, distinct comprehensive preferences are formed by the user-specific
preferences and a learnable generic preference for more overall highlight
measurement. Lastly, the degree of highlight and non-highlight for each query
frame is calculated as semantic similarity to its comprehensive and
non-highlight preferences, respectively. Besides, to alleviate the ambiguity
due to the incomplete annotation, a new bi-directional contrastive loss is
proposed to ensure a compact and differentiable metric space. In this way, our
method significantly outperforms state-of-the-art methods with a relative
improvement of 12% in mean accuracy precision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01071">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Disease-aware image editing by means of generative adversarial networks
(GANs) constitutes a promising avenue for advancing the use of AI in the
healthcare sector. Here, we present a proof of concept of this idea. While
GAN-based techniques have been successful in generating and manipulating
natural images, their application to the medical domain, however, is still in
its infancy. Working with the CheXpert data set, we show that StyleGAN can be
trained to generate realistic chest X-rays. Inspired by the Cyclic Reverse
Generator (CRG) framework, we train an encoder that allows for faithfully
inverting the generator on synthetic X-rays and provides organ-level
reconstructions of real ones. Employing a guided manipulation of latent codes,
we confer the medical condition of cardiomegaly (increased heart size) onto
real X-rays from healthy patients. This work was presented in the Medical
Imaging meets Neurips Workshop 2020, which was held as part of the 34th
Conference on Neural Information Processing Systems (NeurIPS 2020) in
Vancouver, Canada</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Personalized Image Semantic Segmentation. (arXiv:2107.13978v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation models trained on public datasets have achieved great
success in recent years. However, these models didn&#x27;t consider the
personalization issue of segmentation though it is important in practice. In
this paper, we address the problem of personalized image segmentation. The
objective is to generate more accurate segmentation results on unlabeled
personalized images by investigating the data&#x27;s personalized traits. To open up
future research in this area, we collect a large dataset containing various
users&#x27; personalized images called PIS (Personalized Image Semantic
Segmentation). We also survey some recent researches related to this problem
and report their performance on our dataset. Furthermore, by observing the
correlation among a user&#x27;s personalized images, we propose a baseline method
that incorporates the inter-image context when segmenting certain images.
Extensive experiments show that our method outperforms the existing methods on
the proposed dataset. The code and the PIS dataset will be made publicly
available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02288">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging
task. The recordings are mostly sparse and contain only small objects. In this
work, we propose a simple tiling method that improves the detection capability
in the remote sensing case without modifying the model itself. By reducing the
background bias and enabling the usage of higher image resolutions during
training, our method can improve the performance of models substantially. The
procedure was validated on three different data sets and outperformed similar
approaches in performance and speed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02093">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We unveil a long-standing problem in the prevailing co-saliency detection
systems: there is indeed inconsistency between training and testing.
Constructing a high-quality co-saliency detection dataset involves
time-consuming and labor-intensive pixel-level labeling, which has forced most
recent works to rely instead on semantic segmentation or saliency detection
datasets for training. However, the lack of proper co-saliency and the absence
of multiple foreground objects in these datasets can lead to spurious
variations and inherent biases learned by models. To tackle this, we introduce
the idea of counterfactual training through context adjustment, and propose a
&quot;cost-free&quot; group-cut-paste (GCP) procedure to leverage images from
off-the-shelf saliency detection datasets and synthesize new samples. Following
GCP, we collect a novel dataset called Context Adjustment Training (CAT). CAT
consists of 33,500 images, making it four times larger than the current
co-saliency detection datasets. All images are automatically annotated with
high-quality mask annotations, object categories, and edge maps. Extensive
experiments with state-of-the-art models are conducted to demonstrate the
superiority of our dataset. We hope that the scale, diversity, and quality of
our dataset can benefit researchers in this area and beyond. The dataset and
benchmark toolkit will be publicly accessible through our project page.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study. (arXiv:2109.02252v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02252">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Radiomics is an active area of research focusing on high throughput feature
extraction from medical images with a wide array of applications in clinical
practice, such as clinical decision support in oncology. However, noise in low
dose computed tomography (CT) scans can impair the accurate extraction of
radiomic features. In this article, we investigate the possibility of using
deep learning generative models to improve the performance of radiomics from
low dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics
and LIDC-IDRI - as test datasets for two tasks - pre-treatment survival
prediction and lung cancer diagnosis. We used encoder-decoder networks and
conditional generative adversarial networks (CGANs) trained in a previous study
as generative models to transform low dose CT images into full dose CT images.
Radiomic features extracted from the original and improved CT scans were used
to build two classifiers - a support vector machine (SVM) and a deep attention
based multiple instance learning model - for survival prediction and lung
cancer diagnosis respectively. Finally, we compared the performance of the
models derived from the original and improved CT scans. Encoder-decoder
networks and CGANs improved the area under the curve (AUC) of survival
prediction from 0.52 to 0.57 (p-value&lt;0.01). On the other hand, Encoder-decoder
network and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88
and 0.89 respectively (p-value&lt;0.01). Moreover, there are no statistically
significant differences in improving AUC by using encoder-decoder network and
CGAN (p-value&#x3D;0.34) when networks trained at 75 and 100 epochs. Generative
models can improve the performance of low dose CT-based radiomics in different
tasks. Hence, denoising using generative models seems to be a necessary
pre-processing step for calculating radiomic features from low dose CTs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging
method providing multiplex molecular and functional information from the rodent
brain. It can be greatly augmented by magnetic resonance imaging (MRI) that
offers excellent soft-tissue contrast and high-resolution brain anatomy.
Nevertheless, registration of multi-modal images remains challenging, chiefly
due to the entirely different image contrast rendered by these modalities.
Previously reported registration algorithms mostly relied on manual
user-dependent brain segmentation, which compromised data interpretation and
accurate quantification. Here we propose a fully automated registration method
for MSOT-MRI multimodal imaging empowered by deep learning. The automated
workflow includes neural network-based image segmentation to generate suitable
masks, which are subsequently registered using an additional neural network.
Performance of the algorithm is showcased with datasets acquired by
cross-sectional MSOT and high-field MRI preclinical scanners. The automated
registration method is further validated with manual and half-automated
registration, demonstrating its robustness and accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v3 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dynamic imaging is a beneficial tool for interventions to assess
physiological changes. Nonetheless during dynamic MRI, while achieving a high
temporal resolution, the spatial resolution is compromised. To overcome this
spatio-temporal trade-off, this research presents a super-resolution (SR) MRI
reconstruction with prior knowledge based fine-tuning to maximise spatial
information while reducing the required scan-time for dynamic MRIs. An U-Net
based network with perceptual loss is trained on a benchmark dataset and
fine-tuned using one subject-specific static high resolution MRI as prior
knowledge to obtain high resolution dynamic images during the inference stage.
3D dynamic data for three subjects were acquired with different parameters to
test the generalisation capabilities of the network. The method was tested for
different levels of in-plane undersampling for dynamic MRI. The reconstructed
dynamic SR results after fine-tuning showed higher similarity with the high
resolution ground-truth, while quantitatively achieving statistically
significant improvement. The average SSIM of the lowest resolution experimented
during this research (6.25~\% of the k-space) before and after fine-tuning were
0.939 $\pm$ 0.008 and 0.957 $\pm$ 0.006 respectively. This could theoretically
result in an acceleration factor of 16, which can potentially be acquired in
less than half a second. The proposed approach shows that the super-resolution
MRI reconstruction with prior-information can alleviate the spatio-temporal
trade-off in dynamic MRI, even for high acceleration factors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation is a classic computer vision task with multiple
applications, which includes medical and remote sensing image analysis. Despite
recent advances with deep-based approaches, labeling samples (pixels) for
training models is laborious and, in some cases, unfeasible. In this paper, we
present two novel meta learning methods, named WeaSeL and ProtoSeg, for the
few-shot semantic segmentation task with sparse annotations. We conducted
extensive evaluation of the proposed methods in different applications (12
datasets) in medical imaging and agricultural remote sensing, which are very
distinct fields of knowledge and usually subject to data scarcity. The results
demonstrated the potential of our method, achieving suitable results for
segmenting both coffee/orange crops and anatomical parts of the human body in
comparison with full dense annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Ranking Models in Unlabeled New Environments. (arXiv:2108.10310v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10310">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Consider a scenario where we are supplied with a number of ready-to-use
models trained on a certain source domain and hope to directly apply the most
appropriate ones to different target domains based on the models&#x27; relative
performance. Ideally we should annotate a validation set for model performance
assessment on each new target environment, but such annotations are often very
expensive. Under this circumstance, we introduce the problem of ranking models
in unlabeled new environments. For this problem, we propose to adopt a proxy
dataset that 1) is fully labeled and 2) well reflects the true model rankings
in a given target environment, and use the performance rankings on the proxy
sets as surrogates. We first select labeled datasets as the proxy.
Specifically, datasets that are more similar to the unlabeled target domain are
found to better preserve the relative performance rankings. Motivated by this,
we further propose to search the proxy set by sampling images from various
datasets that have similar distributions as the target. We analyze the problem
and its solutions on the person re-identification (re-ID) task, for which
sufficient datasets are publicly available, and show that a carefully
constructed proxy set effectively captures relative performance ranking in new
environments. Code is available at \url{https://github.com/sxzrt/Proxy-Set}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ISyNet: Convolutional Neural Networks design for AI accelerator. (arXiv:2109.01932v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years Deep Learning reached significant results in many practical
problems, such as computer vision, natural language processing, speech
recognition and many others. For many years the main goal of the research was
to improve the quality of models, even if the complexity was impractically
high. However, for the production solutions, which often require real-time
work, the latency of the model plays a very important role. Current
state-of-the-art architectures are found with neural architecture search (NAS)
taking model complexity into account. However, designing of the search space
suitable for specific hardware is still a challenging task. To address this
problem we propose a measure of hardware efficiency of neural architecture
search space - matrix efficiency measure (MEM); a search space comprising of
hardware-efficient operations; a latency-aware scaling method; and ISyNet - a
set of architectures designed to be fast on the specialized neural processing
unit (NPU) hardware and accurate at the same time. We show the advantage of the
designed architectures for the NPU devices on ImageNet and the generalization
ability for the downstream classification and detection tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sensor Data Augmentation with Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v1 [cs.HC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02054">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Human activity recognition plays an increasingly important role not only in
our daily lives, but also in the medical and rehabilitation fields. The
development of deep learning has also contributed to the advancement of human
activity recognition, but the large amount of data annotation work required to
train deep learning models is a major obstacle to the development of human
activity recognition. Contrastive learning has started to be used in the field
of sensor-based human activity recognition due to its ability to avoid the cost
of labeling large datasets and its ability to better distinguish between sample
representations of different instances. Among them, data augmentation, an
important part of contrast learning, has a significant impact on model
effectiveness, but current data augmentation methods do not perform too
successfully in contrast learning frameworks for wearable sensor-based activity
recognition. To optimize the effect of contrast learning models, in this paper,
we investigate the sampling frequency of sensors and propose a resampling data
augmentation method. In addition, we also propose a contrast learning framework
based on human activity recognition and apply the resampling augmentation
method to the data augmentation phase of contrast learning. The experimental
results show that the resampling augmentation method outperforms supervised
learning by 9.88% on UCI HAR and 7.69% on Motion Sensor in the fine-tuning
evaluation of contrast learning with a small amount of labeled data, and also
reveal that not all data augmentation methods will have positive effects in the
contrast learning framework. Finally, we explored the influence of the
combination of different augmentation methods on contrastive learning, and the
experimental results showed that the effect of most combination augmentation
methods was better than that of single augmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06911">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper considers online object-level mapping using partial point-cloud
observations obtained online in an unknown environment. We develop and approach
for fully Convolutional Object Retrieval and Symmetry-AIded Registration
(CORSAIR). Our model extends the Fully Convolutional Geometric Features model
to learn a global object-shape embedding in addition to local point-wise
features from the point-cloud observations. The global feature is used to
retrieve a similar object from a category database, and the local features are
used for robust pose registration between the observed and the retrieved
object. Our formulation also leverages symmetries, present in the object
shapes, to obtain promising local-feature pairs from different symmetry classes
for matching. We present results from synthetic and real-world datasets with
different object categories to verify the robustness of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02167">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>GAN-based techniques that generate and synthesize realistic faces have caused
severe social concerns and security problems. Existing methods for detecting
GAN-generated faces can perform well on limited public datasets. However,
images from existing public datasets do not represent real-world scenarios well
enough in terms of view variations and data distributions (where real faces
largely outnumber synthetic faces). The state-of-the-art methods do not
generalize well in real-world problems and lack the interpretability of
detection results. Performance of existing GAN-face detection models degrades
significantly when facing imbalanced data distributions. To address these
shortcomings, we propose a robust, attentive, end-to-end network that can spot
GAN-generated faces by analyzing their eye inconsistencies. Specifically, our
model learns to identify inconsistent eye components by localizing and
comparing the iris artifacts between the two eyes automatically. Our deep
network addresses the imbalance learning issues by considering the AUC loss and
the traditional cross-entropy loss jointly. Comprehensive evaluations of the
FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the
superiority of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SHAD3S: A model to Sketch, Shade and Shadow. (arXiv:2011.06822v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hatching is a common method used by artists to accentuate the third dimension
of a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete
with a human at hatching generic three-dimensional (3D) shapes, and also tries
to assist her in a form exploration exercise. The novelty of our approach lies
in the fact that we make no assumptions about the input other than that it
represents a 3D shape, and yet, given a contextual information of illumination
and texture, we synthesise an accurate hatch pattern over the sketch, without
access to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet
effective method to synthesise a sufficiently large high fidelity dataset,
pertinent to task; b) creating a pipeline with conditional generative
adversarial network (CGAN); and c) creating an interactive utility with GIMP,
that is a tool for artists to engage with automated hatching or a
form-exploration exercise. User evaluation of the tool suggests that the model
performance does generalise satisfactorily over diverse input, both in terms of
style as well as shape. A simple comparison of inception scores suggest that
the generated distribution is as diverse as the ground truth.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos. (arXiv:2104.12671v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12671">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multimodal self-supervised learning is getting more and more attention as it
allows not only to train large networks without human supervision but also to
search and retrieve data across various modalities. In this context, this paper
proposes a self-supervised training framework that learns a common multimodal
embedding space that, in addition to sharing representations across different
modalities, enforces a grouping of semantically similar instances. To this end,
we extend the concept of instance-level contrastive learning with a multimodal
clustering step in the training pipeline to capture semantic similarities
across modalities. The resulting embedding space enables retrieval of samples
across all modalities, even from unseen datasets and different domains. To
evaluate our approach, we train our model on the HowTo100M dataset and evaluate
its zero-shot retrieval capabilities in two challenging domains, namely
text-to-video retrieval, and temporal action localization, showing
state-of-the-art results on four different datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset. (arXiv:2109.01841v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01841">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a privacy-preserving image-retrieval scheme using a
codebook generated by using a plain-image dataset. Encryption-then-compression
(EtC) images, which were proposed for EtC systems, have been used in
conventional privacy-preserving image-retrieval schemes, in which a codebook is
generated from EtC images uploaded by image owners, and extended SIMPLE
descriptors are then calculated as image descriptors by using the codebook. In
contrast, in the proposed scheme, a codebook is generated from a dataset
independent of uploaded images. The use of an independent dataset enables us
not only to use a codebook that does not require recalculation but also to
constantly provide a high retrieval accuracy. In an experiment, the proposed
scheme is demonstrated to maintain a high retrieval performance, even if
codebooks are generated from a plain image dataset independent of image owners&#x27;
encrypted images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image recognition via Vietoris-Rips complex. (arXiv:2109.02231v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02231">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Extracting informative features from images has been of capital importance in
computer vision. In this paper, we propose a way to extract such features from
images by a method based on algebraic topology. To that end, we construct a
weighted graph from an image, which extracts local information of an image. By
considering this weighted graph as a pseudo-metric space, we construct a
Vietoris-Rips complex with a parameter $\varepsilon$ by a well-known process of
algebraic topology. We can extract information of complexity of the image and
can detect a sub-image with a relatively high concentration of information from
this Vietoris-Rips complex. The parameter $\varepsilon$ of the Vietoris-Rips
complex produces robustness to noise. We empirically show that the extracted
feature captures well images&#x27; characteristics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Realistic Single-View 3D Object Reconstructionwith Unsupervised Learning from Multiple Images. (arXiv:2109.02288v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02288">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recovering the 3D structure of an object from a single image is a challenging
task due to its ill-posed nature. One approach is to utilize the plentiful
photos of the same object category to learn a strong 3D shape prior for the
object. This approach has successfully been demonstrated by a recent work of Wu
et al. (2020), which obtained impressive 3D reconstruction networks with
unsupervised learning. However, their algorithm is only applicable to symmetric
objects. In this paper, we eliminate the symmetry requirement with a novel
unsupervised algorithm that can learn a 3D reconstruction network from a
multi-image dataset. Our algorithm is more general and covers the
symmetry-required scenario as a special case. Besides, we employ a novel albedo
loss that improves the reconstructed details and realisticity. Our method
surpasses the previous work in both quality and robustness, as shown in
experiments on datasets of various structures, including single-view,
multi-view, image-collection, and video sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse Spatial Attention Network for Semantic Segmentation. (arXiv:2109.01915v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01915">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The spatial attention mechanism captures long-range dependencies by
aggregating global contextual information to each query location, which is
beneficial for semantic segmentation. In this paper, we present a sparse
spatial attention network (SSANet) to improve the efficiency of the spatial
attention mechanism without sacrificing the performance. Specifically, a sparse
non-local (SNL) block is proposed to sample a subset of key and value elements
for each query element to capture long-range relations adaptively and generate
a sparse affinity matrix to aggregate contextual information efficiently.
Experimental results show that the proposed approach outperforms other context
aggregation methods and achieves state-of-the-art performance on the
Cityscapes, PASCAL Context and ADE20K datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parsing Table Structures in the Wild. (arXiv:2109.02199v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02199">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper tackles the problem of table structure parsing (TSP) from images
in the wild. In contrast to existing studies that mainly focus on parsing
well-aligned tabular images with simple layouts from scanned PDF documents, we
aim to establish a practical table structure parsing system for real-world
scenarios where tabular input images are taken or scanned with severe
deformation, bending or occlusions. For designing such a system, we propose an
approach named Cycle-CenterNet on the top of CenterNet with a novel
cycle-pairing module to simultaneously detect and group tabular cells into
structured tables. In the cycle-pairing module, a new pairing loss function is
proposed for the network training. Alongside with our Cycle-CenterNet, we also
present a large-scale dataset, named Wired Table in the Wild (WTW), which
includes well-annotated structure parsing of multiple style tables in several
scenes like the photo, scanning files, web pages, \emph{etc.}. In experiments,
we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy
of table structure parsing on the new WTW dataset by 24.6\% absolute
improvement evaluated by the TEDS metric. A more comprehensive experimental
analysis also validates the advantages of our proposed methods for the TSP
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02357">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In practice, and more especially when training deep neural networks, visual
recognition rules are often learned based on various sources of information. On
the other hand, the recent deployment of facial recognition systems with uneven
predictive performances on different population segments highlights the
representativeness issues possibly induced by a naive aggregation of image
datasets. Indeed, sampling bias does not vanish simply by considering larger
datasets, and ignoring its impact may completely jeopardize the generalization
capacity of the learned prediction rules. In this paper, we show how biasing
models, originally introduced for nonparametric estimation in (Gill et al.,
1988), and recently revisited from the perspective of statistical learning
theory in (Laforgue and Cl\&#x27;emen\c{c}on, 2019), can be applied to remedy these
problems in the context of visual recognition. Based on the (approximate)
knowledge of the biasing mechanisms at work, our approach consists in
reweighting the observations, so as to form a nearly debiased estimator of the
target distribution. One key condition for our method to be theoretically valid
is that the supports of the distributions generating the biased datasets at
disposal must overlap, and cover the support of the target distribution. In
order to meet this requirement in practice, we propose to use a low dimensional
image representation, shared across the image databases. Finally, we provide
numerical experiments highlighting the relevance of our approach whenever the
biasing functions are appropriately chosen.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Inpainting Transformer for Anomaly Detection. (arXiv:2104.13897v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Anomaly detection in computer vision is the task of identifying images which
deviate from a set of normal images. A common approach is to train deep
convolutional autoencoders to inpaint covered parts of an image and compare the
output with the original image. By training on anomaly-free samples only, the
model is assumed to not being able to reconstruct anomalous regions properly.
For anomaly detection by inpainting we suggest it to be beneficial to
incorporate information from potentially distant regions. In particular we pose
anomaly detection as a patch-inpainting problem and propose to solve it with a
purely self-attention based approach discarding convolutions. The proposed
Inpainting Transformer (InTra) is trained to inpaint covered patches in a large
sequence of image patches, thereby integrating information across large regions
of the input image. When training from scratch, in comparison to other methods
not using extra training data, InTra achieves results on par with the current
state-of-the-art on the MVTec AD dataset for detection and surpassing them on
segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This research project investigates the application of deep learning to timbre
transfer, where the timbre of a source audio can be converted to the timbre of
a target audio with minimal loss in quality. The adopted approach combines
Variational Autoencoders with Generative Adversarial Networks to construct
meaningful representations of the source audio and produce realistic
generations of the target audio and is applied to the Flickr 8k Audio dataset
for transferring the vocal timbre between speakers and the URMP dataset for
transferring the musical timbre between instruments. Furthermore, variations of
the adopted approach are trained, and generalised performance is compared using
the metrics SSIM (Structural Similarity Index) and FAD (Frech\&#x27;et Audio
Distance). It was found that a many-to-many approach supersedes a one-to-one
approach in terms of reconstructive capabilities, and that the adoption of a
basic over a bottleneck residual block design is more suitable for enriching
content information about a latent space. It was also found that the decision
on whether cyclic loss takes on a variational autoencoder or vanilla
autoencoder approach does not have a significant impact on reconstructive and
adversarial translation aspects of the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. (arXiv:2109.02079v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hyperspectral image has become increasingly crucial due to its abundant
spectral information. However, It has poor spatial resolution with the
limitation of the current imaging mechanism. Nowadays, many convolutional
neural networks have been proposed for the hyperspectral image super-resolution
problem. However, convolutional neural network (CNN) based methods only
consider the local information instead of the global one with the limited
kernel size of receptive field in the convolution operation. In this paper, we
design a network based on the transformer for fusing the low-resolution
hyperspectral images and high-resolution multispectral images to obtain the
high-resolution hyperspectral images. Thanks to the representing ability of the
transformer, our approach is able to explore the intrinsic relationships of
features globally. Furthermore, considering the LR-HSIs hold the main spectral
structure, the network focuses on the spatial detail estimation releasing from
the burden of reconstructing the whole data. It reduces the mapping space of
the proposed network, which enhances the final performance. Various experiments
and quality indexes show our approach&#x27;s superiority compared with other
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation. (arXiv:2109.02303v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02303">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D human shape and pose estimation is the essential task for human motion
analysis, which is widely used in many 3D applications. However, existing
methods cannot simultaneously capture the relations at multiple levels,
including spatial-temporal level and human joint level. Therefore they fail to
make accurate predictions in some hard scenarios when there is cluttered
background, occlusion, or extreme pose. To this end, we propose Multi-level
Attention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder
(STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in
a unified framework. STE consists of a series of cascaded blocks based on
Multi-Head Self-Attention, and each block uses two parallel branches to learn
spatial and temporal attention respectively. Meanwhile, KTD aims at modeling
the joint level attention. It regards pose estimation as a top-down
hierarchical process similar to SMPL kinematic tree. With the training set of
3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4
mm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and
Human3.6M respectively. Our code is available at
https://github.com/ziniuwan/maed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image. (arXiv:1911.12721v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12721">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In multi-object detection using neural networks, the fundamental problem is,
&quot;How should the network learn a variable number of bounding boxes in different
input images?&quot;. Previous methods train a multi-object detection network through
a procedure that directly assigns the ground truth bounding boxes to the
specific locations of the network&#x27;s output. However, this procedure makes the
training of a multi-object detection network too heuristic and complicated. In
this paper, we reformulate the multi-object detection task as a problem of
density estimation of bounding boxes. Instead of assigning each ground truth to
specific locations of network&#x27;s output, we train a network by estimating the
probability density of bounding boxes in an input image using a mixture model.
For this purpose, we propose a novel network for object detection called
Mixture Density Object Detector (MDOD), and the corresponding objective
function for the density-estimation-based training. We applied MDOD to MS COCO
dataset. Our proposed method not only deals with multi-object detection
problems in a new approach, but also improves detection performances through
MDOD. The code is available: https://github.com/yoojy31/MDOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Large pre-trained models such as CLIP offer consistent accuracy across a
range of data distributions when performing zero-shot inference (i.e., without
fine-tuning on a specific dataset). Although existing fine-tuning approaches
substantially improve accuracy in-distribution, they also reduce
out-of-distribution robustness. We address this tension by introducing a simple
and effective method for improving robustness: ensembling the weights of the
zero-shot and fine-tuned models. Compared to standard fine-tuning, the
resulting weight-space ensembles provide large accuracy improvements
out-of-distribution, while matching or improving in-distribution accuracy. On
ImageNet and five derived distribution shifts, weight-space ensembles improve
out-of-distribution accuracy by 2 to 10 percentage points while increasing
in-distribution accuracy by nearly 1 percentage point relative to standard
fine-tuning. These improvements come at no additional computational cost during
fine-tuning or inference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion. (arXiv:2109.01800v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01800">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role
in UAV defense systems. Deep learning is widely adopted for UAV object
detection whereas researches on this topic are limited by the amount of dataset
and small scale of UAV. To tackle these problems, a novel comprehensive
approach that combines transfer learning based on simulation data and adaptive
fusion is proposed. Firstly, the open-source plugin AirSim proposed by
Microsoft is used to generate mass realistic simulation data. Secondly,
transfer learning is applied to obtain a pre-trained YOLOv5 model on the
simulated dataset and fine-tuned model on the real-world dataset. Finally, an
adaptive fusion mechanism is proposed to further improve small object detection
performance. Experiment results demonstrate the effectiveness of
simulation-based transfer learning which leads to a 2.7% performance increase
on UAV object detection. Furthermore, with transfer learning and adaptive
fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5
model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01980">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Using only a model that was trained to predict where people look at images,
and no additional training data, we can produce a range of powerful editing
effects for reducing distraction in images. Given an image and a mask
specifying the region to edit, we backpropagate through a state-of-the-art
saliency model to parameterize a differentiable editing operator, such that the
saliency within the masked region is reduced. We demonstrate several operators,
including: a recoloring operator, which learns to apply a color transform that
camouflages and blends distractors into their surroundings; a warping operator,
which warps less salient image regions to cover distractors, gradually
collapsing objects into themselves and effectively removing them (an effect
akin to inpainting); a GAN operator, which uses a semantic prior to fully
replace image regions with plausible, less salient alternatives. The resulting
effects are consistent with cognitive research on the human visual system
(e.g., since color mismatch is salient, the recoloring operator learns to
harmonize objects&#x27; colors with their surrounding to reduce their saliency),
and, importantly, are all achieved solely through the guidance of the
pretrained saliency model, with no additional supervision. We present results
on a variety of natural images and conduct a perceptual study to evaluate and
validate the changes in viewers&#x27; eye-gaze between the original images and our
edited results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01878">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For the MIDOG mitosis detection challenge, we created a cascade algorithm
consisting of a Mask-RCNN detector, followed by a classification ensemble
consisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.
The MIDOG training data consists of 200 frames originating from four scanners,
three of which are annotated for mitotic instances with centroid annotations.
Our main algorithmic choices are as follows: first, to enhance the
generalizability of our detector and classification networks, we use a
state-of-the-art residual Cycle-GAN to transform each scanner domain to every
other scanner domain. During training, we then randomly load, for each image,
one of the four domains. In this way, our networks can learn from the fourth
non-annotated scanner domain even if we don&#x27;t have annotations for it. Second,
for training the detector network, rather than using centroid-based fixed-size
bounding boxes, we create mitosis-specific bounding boxes. We do this by
manually annotating a small selection of mitoses, training a Mask-RCNN on this
small dataset, and applying it to the rest of the data to obtain full
annotations. We trained the follow-up classification ensemble using only the
challenge-provided positive and hard-negative examples. On the preliminary test
set, the algorithm scores an F1 score of 0.7578, putting us as the second-place
team on the leaderboard.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.
correlation clustering) problem, a classical graph clustering problem widely
used in machine learning and computer vision. Our algorithm consists of three
steps executed recursively: (1) Finding conflicted cycles that correspond to
violated inequalities of the underlying multicut relaxation, (2) Performing
message passing between the edges and cycles to optimize the Lagrange
relaxation coming from the found violated cycles producing reduced costs and
(3) Contracting edges with high reduced costs through matrix-matrix
multiplications.

Our algorithm produces primal solutions and dual lower bounds that estimate
the distance to optimum. We implement our algorithm on GPUs and show resulting
one to two order-of-magnitudes improvements in execution speed without
sacrificing solution quality compared to traditional serial algorithms that run
on CPUs. We can solve very large scale benchmark problems with up to
$\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We
make our code available at https://github.com/pawelswoboda/RAMA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01732">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents a computational method of analysis that draws from
machine learning, library science, and literary studies to map the visual
layouts of multi-ethnic newspapers from the late 19th and early 20th century
United States. This work departs from prior approaches to newspapers that focus
on individual pieces of textual and visual content. Our method combines
Chronicling America&#x27;s MARC data and the Newspaper Navigator machine learning
dataset to identify the visual patterns of newspaper page layouts. By analyzing
high-dimensional visual similarity, we aim to better understand how editors
spoke and protested through the layout of their papers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01664">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Super-resolving the Magnetic Resonance (MR) image of a target contrast under
the guidance of the corresponding auxiliary contrast, which provides additional
anatomical information, is a new and effective solution for fast MR imaging.
However, current multi-contrast super-resolution (SR) methods tend to
concatenate different contrasts directly, ignoring their relationships in
different clues, \eg, in the foreground and background. In this paper, we
propose a separable attention network (comprising a foreground priority
attention and background separation attention), named SANet. Our method can
explore the foreground and background areas in the forward and reverse
directions with the help of the auxiliary contrast, enabling it to learn
clearer anatomical structures and edge information for the SR of a
target-contrast MR image. SANet provides three appealing benefits: (1) It is
the first model to explore a separable attention mechanism that uses the
auxiliary contrast to predict the foreground and background regions, diverting
more attention to refining any uncertain details between these regions and
correcting the fine areas in the reconstructed results. (2) A multi-stage
integration module is proposed to learn the response of multi-contrast fusion
at different stages, obtain the dependency between the fused features, and
improve their representation ability. (3) Extensive experiments with various
state-of-the-art multi-contrast SR methods on fastMRI and clinical \textit{in
vivo} datasets demonstrate the superiority of our model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01696">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A recent work from Bello shows that training and scaling strategies may be
more significant than model architectures for visual recognition. This short
note studies effective training and scaling strategies for video recognition
models. We propose a simple scaling strategy for 3D ResNets, in combination
with improved training strategies and minor architectural changes. The
resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0
on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained
on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on
Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated
in a self-supervised setup using contrastive learning, demonstrating improved
performance. Code is available at:
https://github.com/tensorflow/models/tree/master/official.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robotic Waste Sorter with Agile Manipulation and Quickly Trainable Detector. (arXiv:2104.01260v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01260">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Owing to human labor shortages, the automation of labor-intensive manual
waste-sorting is needed. The goal of automating waste-sorting is to replace the
human role of robust detection and agile manipulation of waste items with
robots. To achieve this, we propose three methods. First, we provide a combined
manipulation method using graspless push-and-drop and pick-and-release
manipulation. Second, we provide a robotic system that can automatically
collect object images to quickly train a deep neural-network model. Third, we
provide a method to mitigate the differences in the appearance of target
objects from two scenes: one for dataset collection and the other for waste
sorting in a recycling factory. If differences exist, the performance of a
trained waste detector may decrease. We address differences in illumination and
background by applying object scaling, histogram matching with histogram
equalization, and background synthesis to the source target-object images. Via
experiments in an indoor experimental workplace for waste-sorting, we confirm
that the proposed methods enable quick collection of the training image sets
for three classes of waste items (i.e., aluminum can, glass bottle, and plastic
bottle) and detection with higher performance than the methods that do not
consider the differences. We also confirm that the proposed method enables the
robot quickly manipulate the objects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10118">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper explores the potential for performing temporal semantic
segmentation in the context of agricultural robotics without temporally
labelled data. We achieve this by proposing to generate virtual temporal
samples from labelled still images. By exploiting the relatively static scene
and assuming that the robot (camera) moves we are able to generate virtually
labelled temporal sequences with no extra annotation effort. Normally, to train
a recurrent neural network (RNN), labelled samples from a video (temporal)
sequence are required which is laborious and has stymied work in this
direction. By generating virtual temporal samples, we demonstrate that it is
possible to train a lightweight RNN to perform semantic segmentation on two
challenging agricultural datasets. Our results show that by training a temporal
semantic segmenter using virtual samples we can increase the performance by an
absolute amount of $4.6$ and $4.9$ on sweet pepper and sugar beet datasets,
respectively. This indicates that our virtual data augmentation technique is
able to accurately classify agricultural images temporally without the use of
complicated synthetic data generation techniques nor with the overhead of
labelling large amounts of temporal sequences.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering. (arXiv:2109.01847v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Implicit neural rendering techniques have shown promising results for novel
view synthesis. However, existing methods usually encode the entire scene as a
whole, which is generally not aware of the object identity and limits the
ability to the high-level editing tasks such as moving or adding furniture. In
this paper, we present a novel neural scene rendering system, which learns an
object-compositional neural radiance field and produces realistic rendering
with editing capability for a clustered and real-world scene. Specifically, we
design a novel two-pathway architecture, in which the scene branch encodes the
scene geometry and appearance, and the object branch encodes each standalone
object conditioned on learnable object activation codes. To survive the
training in heavily cluttered scenes, we propose a scene-guided training
strategy to solve the 3D space ambiguity in the occluded regions and learn
sharp boundaries for each object. Extensive experiments demonstrate that our
system not only achieves competitive performance for static scene novel-view
synthesis, but also produces realistic rendering for object-level editing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To understand the black-box characteristics of deep networks, counterfactual
explanation that deduces not only the important features of an input space but
also how those features should be modified to classify input as a target class
has gained an increasing interest. The patterns that deep networks have learned
from a training dataset can be grasped by observing the feature variation among
various classes. However, current approaches perform the feature modification
to increase the classification probability for the target class irrespective of
the internal characteristics of deep networks. This often leads to unclear
explanations that deviate from real-world data distributions. To address this
problem, we propose a counterfactual explanation method that exploits the
statistics learned from a training dataset. Especially, we gradually construct
an explanation by iterating over masking and composition steps. The masking
step aims to select an important feature from the input data to be classified
as a target class. Meanwhile, the composition step aims to optimize the
previously selected feature by ensuring that its output score is close to the
logit space of the training data that are classified as the target class.
Experimental results show that our method produces human-friendly
interpretations on various classification datasets and verify that such
interpretations can be achieved with fewer feature modification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01111">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scene graph is a structured representation of a scene that can clearly
express the objects, attributes, and relationships between objects in the
scene. As computer vision technology continues to develop, people are no longer
satisfied with simply detecting and recognizing objects in images; instead,
people look forward to a higher level of understanding and reasoning about
visual scenes. For example, given an image, we want to not only detect and
recognize objects in the image, but also know the relationship between objects
(visual relationship detection), and generate a text description (image
captioning) based on the image content. Alternatively, we might want the
machine to tell us what the little girl in the image is doing (Visual Question
Answering (VQA)), or even remove the dog from the image and find similar images
(image editing and retrieval), etc. These tasks require a higher level of
understanding and reasoning for image vision tasks. The scene graph is just
such a powerful tool for scene understanding. Therefore, scene graphs have
attracted the attention of a large number of researchers, and related research
is often cross-modal, complex, and rapidly developing. However, no relatively
systematic survey of scene graphs exists at present. To this end, this survey
conducts a comprehensive investigation of the current scene graph research.
More specifically, we first summarized the general definition of the scene
graph, then conducted a comprehensive and systematic discussion on the
generation method of the scene graph (SGG) and the SGG with the aid of prior
knowledge. We then investigated the main applications of scene graphs and
summarized the most commonly used datasets. Finally, we provide some insights
into the future development of scene graphs. We believe this will be a very
helpful foundation for future research on scene graphs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we introduce pixel wise tensor normalization, which is inserted
after rectifier linear units and, together with batch normalization, provides a
significant improvement in the accuracy of modern deep neural networks. In
addition, this work deals with the robustness of networks. We show that the
factorized superposition of images from the training set and the reformulation
of the multi class problem into a multi-label problem yields significantly more
robust networks. The reformulation and the adjustment of the multi class log
loss also improves the results compared to the overlay with only one class as
label.
https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p&#x3D;%2FTNandFDT&amp;mode&#x3D;list</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GOHOME: Graph-Oriented Heatmap Output forfuture Motion Estimation. (arXiv:2109.01827v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01827">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose GOHOME, a method leveraging graph representations
of the High Definition Map and sparse projections to generate a heatmap output
representing the future position probability distribution for a given agent in
a traffic scene. This heatmap output yields an unconstrained 2D grid
representation of agent future possible locations, allowing inherent
multimodality and a measure of the uncertainty of the prediction. Our
graph-oriented model avoids the high computation burden of representing the
surrounding context as squared images and processing it with classical CNNs,
but focuses instead only on the most probable lanes where the agent could end
up in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion
Forecasting Benchmark on the MissRate$_6$ metric while achieving significant
speed-up and memory burden diminution compared to 1$^{st}$ place method HOME.
We also highlight that heatmap output enables multimodal ensembling and improve
1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our best ensemble.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We compare the machine readability of pictograms found on Austrian and German
traffic signs. To that end, we train classification models on synthetic data
sets and evaluate their classification accuracy in a controlled setting. In
particular, we focus on differences between currently deployed pictograms in
the two countries, and a set of new pictograms designed to increase human
readability. Besides other results, we find that machine-learning models
generalize poorly to data sets with pictogram designs they have not been
trained on. We conclude that manufacturers of advanced driver-assistance
systems (ADAS) must take special care to properly address small visual
differences between current and newly designed traffic sign pictograms, as well
as between pictograms from different countries.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Retrieval-based Conversational Recommendation. (arXiv:2109.02311v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02311">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational recommender systems have attracted immense attention recently.
The most recent approaches rely on neural models trained on recorded dialogs
between humans, implementing an end-to-end learning process. These systems are
commonly designed to generate responses given the user&#x27;s utterances in natural
language. One main challenge is that these generated responses both have to be
appropriate for the given dialog context and must be grammatically and
semantically correct. An alternative to such generation-based approaches is to
retrieve responses from pre-recorded dialog data and to adapt them if needed.
Such retrieval-based approaches were successfully explored in the context of
general conversational systems, but have received limited attention in recent
years for CRS. In this work, we re-assess the potential of such approaches and
design and evaluate a novel technique for response retrieval and ranking. A
user study (N&#x3D;90) revealed that the responses by our system were on average of
higher quality than those of two recent generation-based systems. We
furthermore found that the quality ranking of the two generation-based
approaches is not aligned with the results from the literature, which points to
open methodological questions. Overall, our research underlines that
retrieval-based approaches should be considered an alternative or complement to
language generation approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Patent-KG: A Patent Knowledge Graph for Engineering Design. (arXiv:2108.11899v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11899">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To facilitate the knowledge reuse in engineering design, several dataset
approaches have been proposed and applied by designers. This paper builds a
patent-based knowledge graph, patent-KG, to represent the knowledge facts in
patents for engineering design. The arising patent-KG approach proposes a new
unsupervised mechanism to extract the knowledge facts in patent, by searching
the attention graph in language models. This method avoids using expensive
labelled data in supervised learning or listing complex syntactic rules in
rule-based extraction. The extracted entities are compared with other
benchmarks and shows a higher coverage of engineering words. The extracted
relationships are also compared with other benchmarks, and the result shows
meaningful advantages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models. We use a multi-objective approach to introduce
temporal context into traditionally time-unaware recommender systems and
confirm its advantage via the proposed evaluation protocol. Finally, we
validate that the Pareto Fronts obtained with the added objective dominate
those produced by state-of-the-art models that are only optimized for accuracy
on three real-world publicly available datasets. The results show that
including our temporal objective can improve recall@20 by up to 20%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal set prediction is becoming increasingly important as many companies
employ recommender systems in their online businesses, e.g., personalized
purchase prediction of shopping baskets. While most previous techniques have
focused on leveraging a user&#x27;s history, the study of combining it with others&#x27;
histories remains untapped potential. This paper proposes Global-Local Item
Embedding (GLOIE) that learns to utilize the temporal properties of sets across
whole users as well as within a user by coining the names as global and local
information to distinguish the two temporal patterns. GLOIE uses Variational
Autoencoder (VAE) and dynamic graph-based model to capture global and local
information and then applies attention to integrate resulting item embeddings.
Additionally, we propose to use Tweedie output for the decoder of VAE as it can
easily model zero-inflated and long-tailed distribution, which is more suitable
for several real-world data distributions than Gaussian or multinomial
counterparts. When evaluated on three public benchmarks, our algorithm
consistently outperforms previous state-of-the-art methods in most ranking
metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adherence and Constancy in LIME-RS Explanations for Recommendation. (arXiv:2109.00818v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Explainable Recommendation has attracted a lot of attention due to a renewed
interest in explainable artificial intelligence. In particular, post-hoc
approaches have proved to be the most easily applicable ones to increasingly
complex recommendation models, which are then treated as black-boxes. The most
recent literature has shown that for post-hoc explanations based on local
surrogate models, there are problems related to the robustness of the approach
itself. This consideration becomes even more relevant in human-related tasks
like recommendation. The explanation also has the arduous task of enhancing
increasingly relevant aspects of user experience such as transparency or
trustworthiness. This paper aims to show how the characteristics of a classical
post-hoc model based on surrogates is strongly model-dependent and does not
prove to be accountable for the explanations generated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.

In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedMatch: Federated Learning Over Heterogeneous Question Answering Data. (arXiv:2108.05069v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Question Answering (QA), a popular and promising technique for intelligent
information access, faces a dilemma about data as most other AI techniques. On
one hand, modern QA methods rely on deep learning models which are typically
data-hungry. Therefore, it is expected to collect and fuse all the available QA
datasets together in a common site for developing a powerful QA model. On the
other hand, real-world QA datasets are typically distributed in the form of
isolated islands belonging to different parties. Due to the increasing
awareness of privacy security, it is almost impossible to integrate the data
scattered around, or the cost is prohibited. A possible solution to this
dilemma is a new approach known as federated learning, which is a
privacy-preserving machine learning technique over distributed datasets. In
this work, we propose to adopt federated learning for QA with the special
concern on the statistical heterogeneity of the QA data. Here the heterogeneity
refers to the fact that annotated QA data are typically with non-identical and
independent distribution (non-IID) and unbalanced sizes in practice.
Traditional federated learning methods may sacrifice the accuracy of individual
models under the heterogeneous situation. To tackle this problem, we propose a
novel Federated Matching framework for QA, named FedMatch, with a
backbone-patch architecture. The shared backbone is to distill the common
knowledge of all the participants while the private patch is a compact and
efficient module to retain the domain information for each participant. To
facilitate the evaluation, we build a benchmark collection based on several QA
datasets from different domains to simulate the heterogeneous situation in
practice. Empirical studies demonstrate that our model can achieve significant
improvements against the baselines over all the datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Recommendation. (arXiv:2109.02046v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02046">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To alleviate data sparsity and cold-start problems of traditional recommender
systems (RSs), incorporating knowledge graphs (KGs) to supplement auxiliary
information has attracted considerable attention recently. However, simply
integrating KGs in current KG-based RS models is not necessarily a guarantee to
improve the recommendation performance, which may even weaken the holistic
model capability. This is because the construction of these KGs is independent
of the collection of historical user-item interactions; hence, information in
these KGs may not always be helpful for recommendation to all users.

In this paper, we propose attentive Knowledge-aware Graph convolutional
networks with Collaborative Guidance for personalized Recommendation (CG-KGR).
CG-KGR is a novel knowledge-aware recommendation model that enables ample and
coherent learning of KGs and user-item interactions, via our proposed
Collaborative Guidance Mechanism. Specifically, CG-KGR first encapsulates
historical interactions to interactive information summarization. Then CG-KGR
utilizes it as guidance to extract information out of KGs, which eventually
provides more precise personalized recommendation. We conduct extensive
experiments on four real-world datasets over two recommendation tasks, i.e.,
Top-K recommendation and Click-Through rate (CTR) prediction. The experimental
results show that the CG-KGR model significantly outperforms recent
state-of-the-art models by 4.0-53.2% and 0.4-3.2%, in terms of Recall metric on
Top-K recommendation and AUC on CTR prediction, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Researchers in Machine Learning based on Author-Topic Model. (arXiv:2109.02022v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The aim of this paper is to uncover the researchers in machine learning using
the author-topic model (ATM). We collect 16,855 scientific papers from six top
journals in the field of machine learning published from 1997 to 2016 and
analyze them using ATM. The dataset is broken down into 4 intervals to identify
the top researchers and find similar researchers using their similarity score.
The similarity score is calculated using Hellinger distance. The researchers
are plotted using t-SNE, which reduces the dimensionality of the data while
keeping the same distance between the points. The analysis of our study helps
the upcoming researchers to find the top researchers in their area of interest.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01732">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a computational method of analysis that draws from
machine learning, library science, and literary studies to map the visual
layouts of multi-ethnic newspapers from the late 19th and early 20th century
United States. This work departs from prior approaches to newspapers that focus
on individual pieces of textual and visual content. Our method combines
Chronicling America&#x27;s MARC data and the Newspaper Navigator machine learning
dataset to identify the visual patterns of newspaper page layouts. By analyzing
high-dimensional visual similarity, we aim to better understand how editors
spoke and protested through the layout of their papers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this article, we propose a systematic approach for fire station location
planning. We develop a machine learning model, based on Random Forest, for
demand prediction and utilize the model further to define a generalized index
to measure quality of fire service in urban settings. Our model is built upon
spatial data collected from multiple different sources. Efficacy of proper
facility planning depends on choice of candidates where fire stations can be
located along with existing stations, if any. Also, the travel time from these
candidates to demand locations need to be taken care of to maintain fire safety
standard. Here, we propose a travel time based clustering technique to identify
suitable candidates. Finally, we develop an optimization problem to select best
locations to install new fire stations. Our optimization problem is built upon
maximum coverage problem, based on integer programming. We present a detailed
experimental study of our proposed approach in collaboration with city of
Victoria Fire Department, MN, USA. Our demand prediction model achieves true
positive rate of 70% and false positive rate of 22% approximately. We aid
Victoria Fire Department to select a location for a new fire station using our
approach. We present detailed results on improvement statistics by locating a
new facility, as suggested by our methodology, in the city of Victoria.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08888">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Maintaining the long-term exploration capability of the agent remains one of
the critical challenges in deep reinforcement learning. A representative
solution is to leverage reward shaping to provide intrinsic rewards for the
agent to encourage exploration. However, most existing methods suffer from
vanishing intrinsic rewards, which cannot provide sustainable exploration
incentives. Moreover, they rely heavily on complex models and additional memory
to record learning procedures, resulting in high computational complexity and
low robustness. To tackle this problem, entropy-based methods are proposed to
evaluate the global exploration performance, encouraging the agent to visit the
state space more equitably. However, the sample complexity of estimating the
state visitation entropy is prohibitive when handling environments with
high-dimensional observations. In this paper, we introduce a novel metric
entitled Jain&#x27;s fairness index (JFI) to replace the entropy regularizer, which
solves the exploration problem from a brand new perspective. In sharp contrast
to the entropy regularizer, JFI is more computable and robust and can be easily
applied generalized into arbitrary tasks. Furthermore, we leverage a
variational auto-encoder (VAE) model to capture the life-long novelty of
states, which is combined with the global JFI score to form multimodal
intrinsic rewards. Finally, extensive simulation results demonstrate that our
multimodal reward shaping (MMRS) method can achieve higher performance than
other benchmark schemes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11272">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We introduce Neural Marching Cubes (NMC), a data-driven approach for
extracting a triangle mesh from a discretized implicit field. Classical MC is
defined by coarse tessellation templates isolated to individual cubes. While
more refined tessellations have been proposed, they all make heuristic
assumptions, such as trilinearity, when determining the vertex positions and
local mesh topologies in each cube. In principle, none of these approaches can
reconstruct geometric features that reveal coherence or dependencies between
nearby cubes (e.g., a sharp edge), as such information is unaccounted for,
resulting in poor estimates of the true underlying implicit field. To tackle
these challenges, we re-cast MC from a deep learning perspective, by designing
tessellation templates more apt at preserving geometric features, and learning
the vertex positions and mesh topologies from training meshes, to account for
contextual information from nearby cubes. We develop a compact per-cube
parameterization to represent the output triangle mesh, while being compatible
with neural processing, so that a simple 3D convolutional network can be
employed for the training. We show that all topological cases in each cube that
are applicable to our design can be easily derived using our representation,
and the resulting tessellations can also be obtained naturally and efficiently
by following a few design guidelines. In addition, our network learns local
features with limited receptive fields, hence it generalizes well to new shapes
and new datasets. We evaluate our neural MC approach by quantitative and
qualitative comparisons to all well-known MC variants. In particular, we
demonstrate the ability of our network to recover sharp features such as edges
and corners, a long-standing issue of MC and its variants. Our network also
reconstructs local mesh topologies more accurately than previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v2 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12996">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by cutting-edge applications like cryo-electron microscopy
(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an
unknown signal from repeated measurements of its images under the latent action
of a group of isometries and additive noise of magnitude $\sigma$. Despite
significant interest, a clear picture for understanding rates of estimation in
this model has emerged only recently, particularly in the high-noise regime
$\sigma \gg 1$ that is highly relevant in applications. Recent investigations
have revealed a remarkable asymptotic sample complexity of order $\sigma^6$ for
certain signals whose Fourier transforms have full support, in stark contrast
to the traditional $\sigma^2$ that arise in regular models. Often prohibitively
large in practice, these results have prompted the investigation of variations
around the MRA model where better sample complexity may be achieved. In this
paper, we show that \emph{sparse} signals exhibit an intermediate $\sigma^4$
sample complexity even in the classical MRA model. Our results explore and
exploit connections of the MRA estimation problem with two classical topics in
applied mathematics: the \textit{beltway problem} from combinatorial
optimization, and \textit{uniform uncertainty principles} from harmonic
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08399">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Measuring the predictability and complexity of time series is an essential
tool in designing and controlling the nonlinear system. Different entropy
measures exist in the literature to analyze the predictability and complexity
of time series. However, the existed methods have some drawbacks related to a
strong dependence of entropy on the parameters of the methods, as well as on
the length and amplitude of the time series. To overcome these difficulties,
this study proposes a new method for estimating the entropy of a time series
using the LogNNet neural network model. The LogNNet reservoir matrix is filled
with the time series elements according to our algorithm. The network is
trained on MNIST-10 dataset and the classification accuracy is calculated. The
accuracy is considered as the entropy measure and denoted by NNetEn. The
novelty of entropy calculation is that the time series is involved in mixing
the input information in the reservoir. The greater complexity of the time
series leads to the better ability of the neural network to learn, and to the
higher classification accuracy and NNetEn values. The epochs number in the
training process of LogNNet is considered as the control parameter. We
introduce a new time series characteristic, called time series learning
inertia, that determines the learning rate of the neural network. The
robustness and efficiency of the method is verified on chaotic, periodic,
random, binary and constant time series. The comparison of NNetEn with other
methods of entropy estimation demonstrates that our method is more robust and
accurate and can be widely used in practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03869">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A state-of-the-art systematic review on XAI applied to Prognostic and Health
Management (PHM) of industrial asset is presented. This work provides an
overview of the general trend of XAI in PHM, answers the question of accuracy
versus explainability, the extent of human involvement, the explanation
assessment and uncertainty quantification in PHM-XAI domain. Research articles
associated with the subject, from 2015 to 2021 were selected from five known
databases following PRISMA guidelines. Data was then extracted from the
selected articles and examined. Several findings were synthesized. Firstly,
while the discipline is still young, the analysis indicated the growing
acceptance of XAI in PHM domain. Secondly, XAI functions as a double edge
sword, where it is assimilated as a tool to execute PHM tasks as well as a mean
of explanation, particularly in diagnostic and anomaly detection activities,
implying a real need for XAI in PHM. Thirdly, the review showed that PHM-XAI
papers produce either good or excellent result in general, suggesting that PHM
performance is unaffected by XAI. Fourthly, human role, evaluation metrics and
uncertainty management are areas requiring further attention by the PHM
community. Adequate assessment metrics to cater for PHM need are urgently
needed.Finally, most case study featured on the accepted articles are based on
real, industrial data, indicating that the available PHM-XAI blends are fit to
solve complex,real-world challenges, increasing the confidence in AI adoption
in the industry.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v5 [cs.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.08962">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>This paper studies a class of constrained restless multi-armed bandits
(CRMAB). The constraints are in the form of time varying set of actions (set of
available arms). This variation can be either stochastic or semi-deterministic.
Given a set of arms, a fixed number of them can be chosen to be played in each
decision interval. The play of each arm yields a state dependent reward. The
current states of arms are partially observable through binary feedback signals
from arms that are played. The current availability of arms is fully
observable. The objective is to maximize long term cumulative reward. The
uncertainty about future availability of arms along with partial state
information makes this objective challenging. Applications for CRMAB can be
found in resource allocation in cyber-physical systems involving components
with time varying availability.

First, this optimization problem is analyzed using Whittle&#x27;s index policy. To
this end, a constrained restless single-armed bandit is studied. It is shown to
admit a threshold-type optimal policy and is also indexable. An algorithm to
compute Whittle&#x27;s index is presented. An alternate solution method with lower
complexity is also presented in the form of an online rollout policy. A
detailed discussion on the complexity of both these schemes is also presented,
which suggests that online rollout policy with short look ahead is simpler to
implement than Whittle&#x27;s index computation. Further, upper bounds on the value
function are derived in order to estimate the degree of sub-optimality of
various solutions. The simulation study compares the performance of Whittle&#x27;s
index, online rollout, myopic and modified Whittle&#x27;s index policies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05406">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a black-box reduction that turns a certain reinforcement learning
algorithm with optimal regret in a (near-)stationary environment into another
algorithm with optimal dynamic regret in a non-stationary environment,
importantly without any prior knowledge on the degree of non-stationarity. By
plugging different algorithms into our black-box, we provide a list of examples
showing that our approach not only recovers recent results for (contextual)
multi-armed bandits achieved by very specialized algorithms, but also
significantly improves the state of the art for (generalized) linear bandits,
episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most
cases our algorithm achieves the optimal dynamic regret
$\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is
the number of rounds and $L$ and $\Delta$ are the number and amount of changes
of the world respectively, while previous works only obtain suboptimal bounds
and/or require the knowledge of $L$ and $\Delta$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v4 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01034">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Intergovernmental Panel on Climate Change proposes different mitigation
strategies to achieve the net emissions reductions that would be required to
follow a pathway that limits global warming to 1.5{\deg}C with no or limited
overshoot. The transition towards a carbon-free society goes through an
inevitable increase in the share of renewable generation in the energy mix and
a drastic decrease in the total consumption of fossil fuels. Therefore, this
thesis studies the integration of renewables in power systems by investigating
forecasting and decision-making tools. Indeed, in contrast to conventional
power plants, renewable energy is subject to uncertainty. Most of the
generation technologies based on renewable sources are non-dispatchable, and
their production is stochastic and complex to predict in advance. A high share
of renewables is challenging for power systems that have been designed and
sized for dispatchable units. In this context, probabilistic forecasts, which
aim at modeling the distribution of all possible future realizations, have
become a vital tool to equip decision-makers, hopefully leading to better
decisions in energy applications. This thesis focuses on two main research
questions: (1) How to produce reliable probabilistic renewable generation
forecasts, consumption, and electricity prices? (2) How to make decisions with
uncertainty using probabilistic forecasts? The thesis perimeter is the energy
management of &quot;small&quot; systems such as microgrids at a residential scale on a
day-ahead basis. It is divided into two main parts to propose directions to
address both research questions (1) a forecasting part; (2) a planning and
control part.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Critical Connectivity Radius for Segmenting Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v8 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1602.03822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by a $2$-dimensional (unsupervised) image segmentation task whereby
local regions of pixels are clustered via edge detection methods, a more
general probabilistic mathematical framework is devised. Critical thresholds
are calculated that indicate strong correlation between randomly-generated,
high dimensional data points that have been projected into structures in a
partition of a bounded, $2$-dimensional area, of which, an image is a special
case. A neighbor concept for structures in the partition is defined and a
critical radius is uncovered. Measured from a central structure in localized
regions of the partition, the radius indicates strong, long and short range
correlation in the count of occupied structures. The size of a short interval
of radii is estimated upon which the transition from short-to-long range
correlation is virtually assured, which defines a demarcation of when an image
ceases to be &quot;interesting&quot;.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we tackle two important problems in low-rank learning, which
are partial singular value decomposition and numerical rank estimation of huge
matrices. By using the concepts of Krylov subspaces such as Golub-Kahan
bidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose
two methods for solving these problems in a fast and accurate way. Our
experiments show the advantages of the proposed methods compared to the
traditional and randomized singular value decomposition methods. The proposed
methods are appropriate for applications involving huge matrices where the
accuracy of the desired singular values and also all of their corresponding
singular vectors are essential. As a real application, we evaluate the
performance of our methods on the problem of Riemannian similarity learning
between two various image datasets of MNIST and USPS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using BART to Perform Pareto Optimization and Quantify its Uncertainties. (arXiv:2101.02558v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02558">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Techniques to reduce the energy burden of an industrial ecosystem often
require solving a multiobjective optimization problem. However, collecting
experimental data can often be either expensive or time-consuming. In such
cases, statistical methods can be helpful. This article proposes Pareto Front
(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression
Trees (BART), which is a non-parametric model whose assumptions are typically
less restrictive than popular alternatives, such as Gaussian Processes (GPs).
These less restrictive assumptions allow BART to handle scenarios (e.g.
high-dimensional input spaces, nonsmooth responses, large datasets) that GPs
find difficult. The performance of our BART-based method is compared to a
GP-based method using analytic test functions, demonstrating convincing
advantages. Finally, our BART-based methodology is applied to a motivating
engineering problem. Supplementary materials, which include a theorem proof,
algorithms, and R code, for this article are available online.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning with Holographic Reduced Representations. (arXiv:2109.02157v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02157">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Holographic Reduced Representations (HRR) are a method for performing
symbolic AI on top of real-valued vectors \cite{Plate1995} by associating each
vector with an abstract concept, and providing mathematical operations to
manipulate vectors as if they were classic symbolic objects. This method has
seen little use outside of older symbolic AI work and cognitive science. Our
goal is to revisit this approach to understand if it is viable for enabling a
hybrid neural-symbolic approach to learning as a differentiable component of a
deep learning architecture. HRRs today are not effective in a differentiable
solution due to numerical instability, a problem we solve by introducing a
projection step that forces the vectors to exist in a well behaved point in
space. In doing so we improve the concept retrieval efficacy of HRRs by over
$100\times$. Using multi-label classification we demonstrate how to leverage
the symbolic HRR properties to develop an output layer and loss function that
is able to learn effectively, and allows us to investigate some of the pros and
cons of an HRR neuro-symbolic learning approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RDFFrames: Knowledge Graph Access for Machine Learning Tools. (arXiv:2002.03614v4 [cs.DB] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03614">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Knowledge graphs represented as RDF datasets are integral to many machine
learning applications. RDF is supported by a rich ecosystem of data management
systems and tools, most notably RDF database systems that provide a SPARQL
query interface. Surprisingly, machine learning tools for knowledge graphs do
not use SPARQL, despite the obvious advantages of using a database system. This
is due to the mismatch between SPARQL and machine learning tools in terms of
data model and programming style. Machine learning tools work on data in
tabular format and process it using an imperative programming style, while
SPARQL is declarative and has as its basic operation matching graph patterns to
RDF triples. We posit that a good interface to knowledge graphs from a machine
learning software stack should use an imperative, navigational programming
paradigm based on graph traversal rather than the SPARQL query paradigm based
on graph patterns. In this paper, we present RDFFrames, a framework that
provides such an interface. RDFFrames provides an imperative Python API that
gets internally translated to SPARQL, and it is integrated with the PyData
machine learning software stack. RDFFrames enables the user to make a sequence
of Python calls to define the data to be extracted from a knowledge graph
stored in an RDF database system, and it translates these calls into a compact
SPQARL query, executes it on the database system, and returns the results in a
standard tabular format. Thus, RDFFrames is a useful tool for data preparation
that combines the usability of PyData with the flexibility and performance of
RDF database systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence in Dry Eye Disease. (arXiv:2109.01658v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01658">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dry eye disease (DED) has a prevalence of between 5 and 50\%, depending on
the diagnostic criteria used and population under study. However, it remains
one of the most underdiagnosed and undertreated conditions in ophthalmology.
Many tests used in the diagnosis of DED rely on an experienced observer for
image interpretation, which may be considered subjective and result in
variation in diagnosis. Since artificial intelligence (AI) systems are capable
of advanced problem solving, use of such techniques could lead to more
objective diagnosis. Although the term &#x60;AI&#x27; is commonly used, recent success in
its applications to medicine is mainly due to advancements in the sub-field of
machine learning, which has been used to automatically classify images and
predict medical outcomes. Powerful machine learning techniques have been
harnessed to understand nuances in patient data and medical images, aiming for
consistent diagnosis and stratification of disease severity. This is the first
literature review on the use of AI in DED. We provide a brief introduction to
AI, report its current use in DED research and its potential for application in
the clinic. Our review found that AI has been employed in a wide range of DED
clinical tests and research applications, primarily for interpretation of
interferometry, slit-lamp and meibography images. While initial results are
promising, much work is still needed on model development, clinical testing and
standardisation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM), a self-supervised pretraining objective, is
widely used in natural language processing for learning text representations.
MLM trains a model to predict a random sample of input tokens that have been
replaced by a [MASK] placeholder in a multi-class setting over the entire
vocabulary. When pretraining, it is common to use alongside MLM other auxiliary
objectives on the token or sequence level to improve downstream performance
(e.g. next sentence prediction). However, no previous work so far has attempted
in examining whether other simpler linguistically intuitive or not objectives
can be used standalone as main pretraining objectives. In this paper, we
explore five simple pretraining objectives based on token-level classification
tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our
proposed methods achieve comparable or better performance to MLM using a
BERT-BASE architecture. We further validate our methods using smaller models,
showing that pretraining a model with 41% of the BERT-BASE&#x27;s parameters,
BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees. (arXiv:2002.10940v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10940">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning (FL) has emerged as a prominent distributed learning
paradigm. FL entails some pressing needs for developing novel parameter
estimation approaches with theoretical guarantees of convergence, which are
also communication efficient, differentially private and Byzantine resilient in
the heterogeneous data distribution settings. Quantization-based SGD solvers
have been widely adopted in FL and the recently proposed SIGNSGD with majority
vote shows a promising direction. However, no existing methods enjoy all the
aforementioned properties. In this paper, we propose an intuitively-simple yet
theoretically-sound method based on SIGNSGD to bridge the gap. We present
Stochastic-Sign SGD which utilizes novel stochastic-sign based gradient
compressors enabling the aforementioned properties in a unified framework. We
also present an error-feedback variant of the proposed Stochastic-Sign SGD
which further improves the learning performance in FL. We test the proposed
method with extensive experiments using deep neural networks on the MNIST
dataset and the CIFAR-10 dataset. The experimental results corroborate the
effectiveness of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-label Classification via Adaptive Resonance Theory-based Clustering. (arXiv:2103.01511v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01511">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a multi-label classification algorithm capable of
continual learning by applying an Adaptive Resonance Theory (ART)-based
clustering algorithm and the Bayesian approach for label probability
computation. The ART-based clustering algorithm adaptively and continually
generates prototype nodes corresponding to given data, and the generated nodes
are used as classifiers. The label probability computation independently counts
the number of label appearances for each class and calculates the Bayesian
probabilities. Thus, the label probability computation can cope with an
increase in the number of labels. Experimental results with synthetic and
real-world multi-label datasets show that the proposed algorithm has
competitive classification performance to other well-known algorithms while
realizing continual learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An empirical evaluation of attention-based multi-head models for improved turbofan engine remaining useful life prediction. (arXiv:2109.01761v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01761">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A single unit (head) is the conventional input feature extractor in deep
learning architectures trained on multivariate time series signals. The
importance of the fixed-dimensional vector representation generated by the
single-head network has been demonstrated for industrial machinery condition
monitoring and predictive maintenance. However, processing heterogeneous sensor
signals with a single head may result in a model that cannot explicitly account
for the diversity in time-varying multivariate inputs. This work extends the
conventional single-head deep learning models to a more robust form by
developing context-specific heads to independently capture the inherent pattern
of each sensor reading in multivariate time series signals. Using the turbofan
aircraft engine benchmark dataset (CMAPSS), an extensive experiment is
performed to verify the effectiveness and benefits of multi-head fully
connected neurons, recurrent networks, convolution network, the
transformer-style stand-alone attention network, and their variants for
remaining useful life estimation. Moreover, the effect of different attention
mechanisms on the multi-head models is also evaluated. In addition, each
architecture&#x27;s relative advantage and computational overhead are analyzed.
Results show that utilizing the attention layer is task-sensitive and
model-dependent, as it does not provide consistent improvement across the
models investigated. The result is further compared with five state-of-the-art
models, and the comparison shows that a relatively simple multi-head
architecture performs better than the state-of-the-art models. The results
presented in this study demonstrate the importance of multi-head models and
attention mechanisms to improved understanding of the remaining useful life of
industrial assets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sleep stage classification is essential for sleep assessment and disease
diagnosis. Although previous attempts to classify sleep stages have achieved
high classification performance, several challenges remain open: 1) How to
effectively utilize time-varying spatial and temporal features from
multi-channel brain signals remains challenging. Prior works have not been able
to fully utilize the spatial topological information among brain regions. 2)
Due to the many differences found in individual biological signals, how to
overcome the differences of subjects and improve the generalization of deep
neural networks is important. 3) Most deep learning methods ignore the
interpretability of the model to the brain. To address the above challenges, we
propose a multi-view spatial-temporal graph convolutional networks (MSTGCN)
with domain generalization for sleep stage classification. Specifically, we
construct two brain view graphs for MSTGCN based on the functional connectivity
and physical distance proximity of the brain regions. The MSTGCN consists of
graph convolutions for extracting spatial features and temporal convolutions
for capturing the transition rules among sleep stages. In addition, attention
mechanism is employed for capturing the most relevant spatial-temporal
information for sleep stage classification. Finally, domain generalization and
MSTGCN are integrated into a unified framework to extract subject-invariant
sleep features. Experiments on two public datasets demonstrate that the
proposed model outperforms the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12827">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vehicular fog computing (VFC) pushes the cloud computing capability to the
distributed fog nodes at the edge of the Internet, enabling compute-intensive
and latency-sensitive computing services for vehicles through task offloading.
However, a heterogeneous mobility environment introduces uncertainties in terms
of resource supply and demand, which are inevitable bottlenecks for the optimal
offloading decision. Also, these uncertainties bring extra challenges to task
offloading under the oblivious adversary attack and data privacy risks. In this
article, we develop a new adversarial online learning algorithm with bandit
feedback based on the adversarial multi-armed bandit theory, to enable scalable
and low-complexity offloading decision making. Specifically, we focus on
optimizing fog node selection with the aim of minimizing the offloading service
costs in terms of delay and energy. The key is to implicitly tune the
exploration bonus in the selection process and the assessment rules of the
designed algorithm, taking into account volatile resource supply and demand. We
theoretically prove that the input-size dependent selection rule allows to
choose a suitable fog node without exploring the sub-optimal actions, and also
an appropriate score patching rule allows to quickly adapt to evolving
circumstances, which reduce variance and bias simultaneously, thereby achieving
a better exploitation-exploration balance. Simulation results verify the
effectiveness and robustness of the proposed algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization. (arXiv:2109.02038v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02038">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent advances on Out-of-Distribution (OoD) generalization reveal the
robustness of deep learning models against distribution shifts. However,
existing works focus on OoD algorithms, such as invariant risk minimization,
domain generalization, or stable learning, without considering the influence of
deep model architectures on OoD generalization, which may lead to sub-optimal
performance. Neural Architecture Search (NAS) methods search for architecture
based on its performance on the training data, which may result in poor
generalization for OoD tasks. In this work, we propose robust Neural
Architecture Search for OoD generalization (NAS-OoD), which optimizes the
architecture with respect to its performance on generated OoD data by gradient
descent. Specifically, a data generator is learned to synthesize OoD data by
maximizing losses computed by different neural architectures, while the goal
for architecture search is to find the optimal architecture parameters that
minimize the synthetic OoD data losses. The data generator and the neural
architecture are jointly optimized in an end-to-end manner, and the minimax
training process effectively discovers robust architectures that generalize
well for different distribution shifts. Extensive experimental results show
that NAS-OoD achieves superior performance on various OoD generalization
benchmarks with deep models having a much fewer number of parameters. In
addition, on a real industry dataset, the proposed NAS-OoD method reduces the
error rate by more than 70% compared with the state-of-the-art method,
demonstrating the proposed method&#x27;s practicality for real applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04563">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In order to extract knowledge from the large data collected by edge devices,
traditional cloud based approach that requires data upload may not be feasible
due to communication bandwidth limitation as well as privacy and security
concerns of end users. To address these challenges, a novel privacy preserving
edge computing framework is proposed in this paper for image classification.
Specifically, autoencoder will be trained unsupervised at each edge device
individually, then the obtained latent vectors will be transmitted to the edge
server for the training of a classifier. This framework would reduce the
communications overhead and protect the data of the end users. Comparing to
federated learning, the training of the classifier in the proposed framework
does not subject to the constraints of the edge devices, and the autoencoder
can be trained independently at each edge device without any server
involvement. Furthermore, the privacy of the end users&#x27; data is protected by
transmitting latent vectors without additional cost of encryption. Experimental
results provide insights on the image classification performance vs. various
design parameters such as the data compression ratio of the autoencoder and the
model complexity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v2 [cs.CR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10243">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Over the recent years, Federated machine learning continues to gain interest
and momentum where there is a need to draw insights from data while preserving
the data provider&#x27;s privacy. However, one among other existing challenges in
the adoption of federated learning has been the lack of fair, transparent and
universally agreed incentivization schemes for rewarding the federated learning
contributors. Smart contracts on a blockchain network provide transparent,
immutable and independently verifiable proofs by all participants of the
network. We leverage this open and transparent nature of smart contracts on a
blockchain to define incentivization rules for the contributors, which is based
on a novel scalar quantity - federated contribution. Such a smart contract
based reward-driven model has the potential to revolutionize the federated
learning adoption in enterprises. Our contribution is two-fold: first is to
show how smart contract based blockchain can be a very natural communication
channel for federated learning. Second, leveraging this infrastructure, we can
show how an intuitive measure of each agents&#x27; contribution can be built and
integrated with the life cycle of the training and reward process.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02344">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-view representation learning captures comprehensive information from
multiple views of a shared context. Recent works intuitively apply contrastive
learning (CL) to learn representations, regarded as a pairwise manner, which is
still scalable: view-specific noise is not filtered in learning view-shared
representations; the fake negative pairs, where the negative terms are actually
within the same class as the positive, and the real negative pairs are
coequally treated; and evenly measuring the similarities between terms might
interfere with optimization. Importantly, few works research the theoretical
framework of generalized self-supervised multi-view learning, especially for
more than two views. To this end, we rethink the existing multi-view learning
paradigm from the information theoretical perspective and then propose a novel
information theoretical framework for generalized multi-view learning. Guided
by it, we build a multi-view coding method with a three-tier progressive
architecture, namely Information theory-guided heuristic Progressive Multi-view
Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between
views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted
pools for contrasting, which utilizes a view filter to adaptively modify the
pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn
discriminative representations and reduce the gradient interference.
Theoretically and empirically, we demonstrate the superiority of IPMC over
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging
method providing multiplex molecular and functional information from the rodent
brain. It can be greatly augmented by magnetic resonance imaging (MRI) that
offers excellent soft-tissue contrast and high-resolution brain anatomy.
Nevertheless, registration of multi-modal images remains challenging, chiefly
due to the entirely different image contrast rendered by these modalities.
Previously reported registration algorithms mostly relied on manual
user-dependent brain segmentation, which compromised data interpretation and
accurate quantification. Here we propose a fully automated registration method
for MSOT-MRI multimodal imaging empowered by deep learning. The automated
workflow includes neural network-based image segmentation to generate suitable
masks, which are subsequently registered using an additional neural network.
Performance of the algorithm is showcased with datasets acquired by
cross-sectional MSOT and high-field MRI preclinical scanners. The automated
registration method is further validated with manual and half-automated
registration, demonstrating its robustness and accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating the probabilities of causation via deep monotonic twin networks. (arXiv:2109.01904v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01904">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There has been much recent work using machine learning to answer causal
queries. Most focus on interventional queries, such as the conditional average
treatment effect. However, as noted by Pearl, interventional queries only form
part of a larger hierarchy of causal queries, with counterfactuals sitting at
the top. Despite this, our community has not fully succeeded in adapting
machine learning tools to answer counterfactual queries. This work addresses
this challenge by showing how to implement twin network counterfactual
inference -- an alternative to abduction, action, &amp; prediction counterfactual
inference -- with deep learning to estimate counterfactual queries. We show how
the graphical nature of twin networks makes them particularly amenable to deep
learning, yielding simple neural network architectures that, when trained, are
capable of counterfactual inference. Importantly, we show how to enforce known
identifiability constraints during training, ensuring the answer to each
counterfactual query is uniquely determined. We demonstrate our approach by
using it to accurately estimate the probabilities of causation -- important
counterfactual queries that quantify the degree to which one event was a
necessary or sufficient cause of another -- on both synthetic and real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Differentiable Programming for Flexible Statistical Modeling. (arXiv:2012.05722v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05722">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Differentiable programming has recently received much interest as a paradigm
that facilitates taking gradients of computer programs. While the corresponding
flexible gradient-based optimization approaches so far have been used
predominantly for deep learning or enriching the latter with modeling
components, we want to demonstrate that they can also be useful for statistical
modeling per se, e.g., for quick prototyping when classical maximum likelihood
approaches are challenging or not feasible. In an application from a COVID-19
setting, we utilize differentiable programming to quickly build and optimize a
flexible prediction model adapted to the data quality challenges at hand.
Specifically, we develop a regression model, inspired by delay differential
equations, that can bridge temporal gaps of observations in the central German
registry of COVID-19 intensive care cases for predicting future demand. With
this exemplary modeling challenge, we illustrate how differentiable programming
can enable simple gradient-based optimization of the model by automatic
differentiation. This allowed us to quickly prototype a model under time
pressure that outperforms simpler benchmark models. We thus exemplify the
potential of differentiable programming also outside deep learning
applications, to provide more options for flexible applied statistical
modeling.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bacteriophage classification for assembled contigs using Graph Convolutional Network. (arXiv:2102.03746v2 [q-bio.GN] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03746">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivation: Bacteriophages (aka phages), which mainly infect bacteria, play
key roles in the biology of microbes. As the most abundant biological entities
on the planet, the number of discovered phages is only the tip of the iceberg.
Recently, many new phages have been revealed using high throughput sequencing,
particularly metagenomic sequencing. Compared to the fast accumulation of
phage-like sequences, there is a serious lag in taxonomic classification of
phages. High diversity, abundance, and limited known phages pose great
challenges for taxonomic analysis. In particular, alignment-based tools have
difficulty in classifying fast accumulating contigs assembled from metagenomic
data. Results: In this work, we present a novel semi-supervised learning model,
named PhaGCN, to conduct taxonomic classification for phage contigs. In this
learning model, we construct a knowledge graph by combining the DNA sequence
features learned by convolutional neural network (CNN) and protein sequence
similarity gained from gene-sharing network. Then we apply graph convolutional
network (GCN) to utilize both the labeled and unlabeled samples in training to
enhance the learning ability. We tested PhaGCN on both simulated and real
sequencing data. The results clearly show that our method competes favorably
against available phage classification tools.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trainable Discrete Feature Embeddings for Variational Quantum Classifier. (arXiv:2106.09415v2 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09415">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum classifiers provide sophisticated embeddings of input data in Hilbert
space promising quantum advantage. The advantage stems from quantum feature
maps encoding the inputs into quantum states with variational quantum circuits.
A recent work shows how to map discrete features with fewer quantum bits using
Quantum Random Access Coding (QRAC), an important primitive to encode binary
strings into quantum states. We propose a new method to embed discrete features
with trainable quantum circuits by combining QRAC and a recently proposed
strategy for training quantum feature map called quantum metric learning. We
show that the proposed trainable embedding requires not only as few qubits as
QRAC but also overcomes the limitations of QRAC to classify inputs whose
classes are based on hard Boolean functions. We numerically demonstrate its use
in variational quantum classifiers to achieve better performances in
classifying real-world datasets, and thus its possibility to leverage near-term
quantum computers for quantum machine learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model retraining and information sharing in a supply chain with long-term fluctuating demands. (arXiv:2109.01784v1 [physics.soc-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Demand forecasting based on empirical data is a viable approach for
optimizing a supply chain. However, in this approach, a model constructed from
past data occasionally becomes outdated due to long-term changes in the
environment, in which case the model should be updated (i.e., retrained) using
the latest data. In this study, we examine the effects of updating models in a
supply chain using a minimal setting. We demonstrate that when each party in
the supply chain has its own forecasting model, uncoordinated model retraining
causes the bullwhip effect even if a very simple replenishment policy is
applied. Our results also indicate that sharing the forecasting model among the
parties involved significantly reduces the bullwhip effect.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Customer 360-degree Insights in Predicting Chronic Diabetes. (arXiv:2109.01863v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01863">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Chronic diseases such as diabetes are quite prevalent in the world and are
responsible for a significant number of deaths per year. In addition,
treatments for such chronic diseases account for a high healthcare cost.
However, research has shown that diabetes can be proactively managed and
prevented while lowering these healthcare costs. We have mined a sample of ten
million customers&#x27; 360-degree data representing the state of Texas, USA, with
attributes current as of late 2018. The sample received from a market research
data vendor has over 1000 customer attributes consisting of demography,
lifestyle, and in some cases self-reported chronic conditions. In this study,
we have developed a classification model to predict chronic diabetes with an
accuracy of 80%. We demonstrate a use case where a large volume of 360-degree
customer data can be useful to predict and hence proactively prevent chronic
diseases such as diabetes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large pre-trained models such as CLIP offer consistent accuracy across a
range of data distributions when performing zero-shot inference (i.e., without
fine-tuning on a specific dataset). Although existing fine-tuning approaches
substantially improve accuracy in-distribution, they also reduce
out-of-distribution robustness. We address this tension by introducing a simple
and effective method for improving robustness: ensembling the weights of the
zero-shot and fine-tuned models. Compared to standard fine-tuning, the
resulting weight-space ensembles provide large accuracy improvements
out-of-distribution, while matching or improving in-distribution accuracy. On
ImageNet and five derived distribution shifts, weight-space ensembles improve
out-of-distribution accuracy by 2 to 10 percentage points while increasing
in-distribution accuracy by nearly 1 percentage point relative to standard
fine-tuning. These improvements come at no additional computational cost during
fine-tuning or inference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient On-Chip Learning for Optical Neural Networks Through Power-Aware Sparse Zeroth-Order Optimization. (arXiv:2012.11148v3 [cs.ET] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11148">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Optical neural networks (ONNs) have demonstrated record-breaking potential in
high-performance neuromorphic computing due to their ultra-high execution speed
and low energy consumption. However, current learning protocols fail to provide
scalable and efficient solutions to photonic circuit optimization in practical
applications. In this work, we propose a novel on-chip learning framework to
release the full potential of ONNs for power-efficient in situ training.
Instead of deploying implementation-costly back-propagation, we directly
optimize the device configurations with computation budgets and power
constraints. We are the first to model the ONN on-chip learning as a
resource-constrained stochastic noisy zeroth-order optimization problem, and
propose a novel mixed-training strategy with two-level sparsity and power-aware
dynamic pruning to offer a scalable on-chip training solution in practical ONN
deployment. Compared with previous methods, we are the first to optimize over
2,500 optical components on chip. We can achieve much better optimization
stability, 3.7x-7.6x higher efficiency, and save &gt;90% power under practical
device variations and thermal crosstalk.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Critical Review of the state-of-the-art on Deep Neural Networks for Blood Glucose Prediction in Patients with Diabetes. (arXiv:2109.02178v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02178">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This article compares ten recently proposed neural networks and proposes two
ensemble neural network-based models for blood glucose prediction. All of them
are tested under the same dataset, preprocessing workflow, and tools using the
OhioT1DM Dataset at three different prediction horizons: 30, 60, and 120
minutes. We compare their performance using the most common metrics in blood
glucose prediction and rank the best-performing ones using three methods
devised for the statistical comparison of the performance of multiple
algorithms: scmamp, model confidence set, and superior predictive ability. Our
analysis highlights those models with the highest probability of being the best
predictors, estimates the increase in error of the models that perform more
poorly with respect to the best ones, and provides a guide for their use in
clinical practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Bayesian Networks Under Sparsity Constraints: A Parameterized Complexity Analysis. (arXiv:2004.14724v3 [cs.DS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14724">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study the problem of learning the structure of an optimal Bayesian network
when additional constraints are posed on the network or on its moralized graph.
More precisely, we consider the constraint that the network or its moralized
graph are close, in terms of vertex or edge deletions, to a sparse graph class
$\Pi$. For example, we show that learning an optimal network whose moralized
graph has vertex deletion distance at most $k$ from a graph with maximum degree
1 can be computed in polynomial time when $k$ is constant. This extends
previous work that gave an algorithm with such a running time for the vertex
deletion distance to edgeless graphs [Korhonen &amp; Parviainen, NIPS 2015]. We
then show that further extensions or improvements are presumably impossible.
For example, we show that learning optimal networks where the network or its
moralized graph have maximum degree $2$ or connected components of size at most
$c$, $c\ge 3$, is NP-hard. Finally, we show that learning an optimal network
with at most $k$ edges in the moralized graph presumably has no $f(k)\cdot
|I|^{O(1)}$-time algorithm and that, in contrast, an optimal network with at
most $k$ arcs can be computed in $2^{O(k)}\cdot |I|^{O(1)}$ time where $|I|$ is
the total input size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1710.03113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multidiversified ensemble clustering approach. In particular, we create a
large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Open-set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments. (arXiv:2002.11561v7 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11561">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The problem of training with a small set of positive samples is known as
few-shot learning (FSL). It is widely known that traditional deep learning (DL)
algorithms usually show very good performance when trained with large datasets.
However, in many applications, it is not possible to obtain such a high number
of samples. In the image domain, typical FSL applications include those related
to face recognition. In the audio domain, music fraud or speaker recognition
can be clearly benefited from FSL methods. This paper deals with the
application of FSL to the detection of specific and intentional acoustic events
given by different types of sound alarms, such as door bells or fire alarms,
using a limited number of samples. These sounds typically occur in domestic
environments where many events corresponding to a wide variety of sound classes
take place. Therefore, the detection of such alarms in a practical scenario can
be considered an open-set recognition (OSR) problem. To address the lack of a
dedicated public dataset for audio FSL, researchers usually make modifications
on other available datasets. This paper is aimed at poviding the audio
recognition community with a carefully annotated dataset
(https://zenodo.org/record/3689288) for FSL in an OSR context comprised of 1360
clips from 34 classes divided into pattern sounds} and unwanted sounds. To
facilitate and promote research on this area, results with state-of-the-art
baseline systems based on transfer learning are also presented.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Transformer-based Model to Detect Phishing URLs. (arXiv:2109.02138v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02138">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Phishing attacks are among emerging security issues that recently draws
significant attention in the cyber security community. There are numerous
existing approaches for phishing URL detection. However, malicious URL
detection is still a research hotspot because attackers can bypass newly
introduced detection mechanisms by changing their tactics. This paper will
introduce a transformer-based malicious URL detection model, which has
significant accuracy and outperforms current detection methods. We conduct
experiments and compare them with six existing classical detection models.
Experiments demonstrate that our transformer-based model is the best performing
model from all perspectives among the seven models and achieves 97.3 % of
detection accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards high-accuracy deep learning inference of compressible turbulent flows over aerofoils. (arXiv:2109.02183v1 [physics.flu-dyn])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02183">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The present study investigates the accurate inference of Reynolds-averaged
Navier-Stokes solutions for the compressible flow over aerofoils in two
dimensions with a deep neural network. Our approach yields networks that learn
to generate precise flow fields for varying body-fitted, structured grids by
providing them with an encoding of the corresponding mapping to a canonical
space for the solutions. We apply the deep neural network model to a benchmark
case of incompressible flow at randomly given angles of attack and Reynolds
numbers and achieve an improvement of more than an order of magnitude compared
to previous work. Further, for transonic flow cases, the deep neural network
model accurately predicts complex flow behaviour at high Reynolds numbers, such
as shock wave/boundary layer interaction, and quantitative distributions like
pressure coefficient, skin friction coefficient as well as wake total pressure
profiles downstream of aerofoils. The proposed deep learning method
significantly speeds up the predictions of flow fields and shows promise for
enabling fast aerodynamic designs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Event-Based Communication in Multi-Agent Distributed Q-Learning. (arXiv:2109.01417v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01417">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present in this work an approach to reduce the communication of
information needed on a multi-agent learning system inspired by Event Triggered
Control (ETC) techniques. We consider a baseline scenario of a distributed
Q-learning problem on a Markov Decision Process (MDP). Following an event-based
approach, N agents explore the MDP and communicate experiences to a central
learner only when necessary, which performs updates of the actor Q functions.
We analyse the convergence guarantees retained with respect to a regular
Q-learning algorithm, and present experimental results showing that event-based
communication results in a substantial reduction of data transmission rates in
such distributed systems. Additionally, we discuss what effects (desired and
undesired) these event-based approaches have on the learning processes studied,
and how they can be applied to more complex multi-agent learning systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal transport weights for causal inference. (arXiv:2109.01991v1 [stat.ME])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01991">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Weighting methods are a common tool to de-bias estimates of causal effects.
And though there are an increasing number of seemingly disparate methods, many
of them can be folded into one unifying regime: causal optimal transport. This
new method directly targets distributional balance by minimizing optimal
transport distances between treatment and control groups or, more generally,
between a source and target population. Our approach is model-free but can also
incorporate moments or any other important functions of covariates that the
researcher desires to balance. We find that the causal optimal transport
outperforms competitor methods when both the propensity score and outcome
models are misspecified, indicating it is a robust alternative to common
weighting methods. Finally, we demonstrate the utility of our method in an
external control study examining the effect of misoprostol versus oxytocin for
treatment of post-partum hemorrhage.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Physics Informed Neural Networks: the role of quadratures and test functions. (arXiv:2109.02035v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02035">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work we analyze how Gaussian or Newton-Cotes quadrature rules of
different precisions and piecewise polynomial test functions of different
degrees affect the convergence rate of Variational Physics Informed Neural
Networks (VPINN) with respect to mesh refinement, while solving elliptic
boundary-value problems. Using a Petrov-Galerkin framework relying on an
inf-sup condition, we derive an a priori error estimate in the energy norm
between the exact solution and a suitable high-order piecewise interpolant of a
computed neural network. Numerical experiments confirm the theoretical
predictions, and also indicate that the error decay follows the same behavior
when the neural network is not interpolated. Our results suggest, somehow
counterintuitively, that for smooth solutions the best strategy to achieve a
high decay rate of the error consists in choosing test functions of the lowest
polynomial degree, while using quadrature formulas of suitably high precision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the problem of achieving fair classification in Federated
Learning (FL) under data heterogeneity. Most of the approaches proposed for
fair classification require diverse data that represent the different
demographic groups involved. In contrast, it is common for each client to own
data that represents only a single demographic group. Hence the existing
approaches cannot be adopted for fair classification models at the client
level. To resolve this challenge, we propose several aggregation techniques. We
empirically validate these techniques by comparing the resulting fairness
metrics and accuracy on CelebA, UTK, and FairFace datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anomaly Detection on IT Operation Series via Online Matrix Profile. (arXiv:2108.12093v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12093">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Anomaly detection on time series is a fundamental task in monitoring the Key
Performance Indicators (KPIs) of IT systems. Many of the existing approaches in
the literature show good performance while requiring a lot of training
resources. In this paper, the online matrix profile, which requires no
training, is proposed to address this issue. The anomalies are detected by
referring to the past subsequence that is the closest to the current one. The
distance significance is introduced based on the online matrix profile, which
demonstrates a prominent pattern when an anomaly occurs. Another training-free
approach spectral residual is integrated into our approach to further enhance
the detection accuracy. Moreover, the proposed approach is sped up by at least
four times for long time series by the introduced cache strategy. In comparison
to the existing approaches, the online matrix profile makes a good trade-off
between accuracy and efficiency. More importantly, it is generic to various
types of time series in the sense that it works without the constraint from any
trained model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Data augmentation is a widely used technique in classification to increase
data used in training. It improves generalization and reduces amount of
annotated human activity data needed for training which reduces labour and time
needed with the dataset. Sensor time-series data, unlike images, cannot be
augmented by computationally simple transformation algorithms. State of the art
models like Recurrent Generative Adversarial Networks (RGAN) are used to
generate realistic synthetic data. In this paper, transformer based generative
adversarial networks which have global attention on data, are compared on
PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer
approach provides improvements in time and savings in computational resources
needed for data augmentation than previous approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hindsight Reward Tweaking via Conditional Deep Reinforcement Learning. (arXiv:2109.02332v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02332">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Designing optimal reward functions has been desired but extremely difficult
in reinforcement learning (RL). When it comes to modern complex tasks,
sophisticated reward functions are widely used to simplify policy learning yet
even a tiny adjustment on them is expensive to evaluate due to the drastically
increasing cost of training. To this end, we propose a hindsight reward
tweaking approach by designing a novel paradigm for deep reinforcement learning
to model the influences of reward functions within a near-optimal space. We
simply extend the input observation with a condition vector linearly correlated
with the effective environment reward parameters and train the model in a
conventional manner except for randomizing reward configurations, obtaining a
hyper-policy whose characteristics are sensitively regulated over the condition
space. We demonstrate the feasibility of this approach and study one of its
potential application in policy performance boosting with multiple MuJoCo
tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models. We use a multi-objective approach to introduce
temporal context into traditionally time-unaware recommender systems and
confirm its advantage via the proposed evaluation protocol. Finally, we
validate that the Pareto Fronts obtained with the added objective dominate
those produced by state-of-the-art models that are only optimized for accuracy
on three real-world publicly available datasets. The results show that
including our temporal objective can improve recall@20 by up to 20%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Inferring feature importance with uncertainties in high-dimensional data. (arXiv:2109.00855v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00855">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Estimating feature importance is a significant aspect of explaining
data-based models. Besides explaining the model itself, an equally relevant
question is which features are important in the underlying data generating
process. We present a Shapley value based framework for inferring the
importance of individual features, including uncertainty in the estimator. We
build upon the recently published feature importance measure of SAGE (Shapley
additive global importance) and introduce sub-SAGE which can be estimated
without resampling for tree-based models. We argue that the uncertainties can
be estimated from bootstrapping and demonstrate the approach for tree ensemble
methods. The framework is exemplified on synthetic data as well as
high-dimensional genomics data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmentation plays a crucial role in diagnosis. Studying the retinal
vasculatures from fundus images help identify early signs of many crucial
illnesses such as diabetic retinopathy. Due to the varying shape, size, and
patterns of retinal vessels, along with artefacts and noises in fundus images,
no one-stage method can accurately segment retinal vessels. In this work, we
propose a multi-scale, multi-level attention embedded CNN architecture
((M)SLAe-Net) to address the issue of multi-stage processing for robust and
precise segmentation of retinal vessels. We do this by extracting features at
multiple scales and multiple levels of the network, enabling our model to
holistically extracts the local and global features. Multi-scale features are
extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We
also aggregate the features from all the network levels. These effectively
resolved the issues of varying shapes and artefacts and hence the need for
multiple stages. To assist in better pixel-level classification, we use the
Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and
Excitation(SE) module for segmentation tasks in our network to facilitate
pixel-group attention. Our unique network design and novel D-DPP module with
efficient task-specific loss function for thin vessels enabled our model for
better cross data performance. Exhaustive experimental results on DRIVE, STARE,
HRF, and CHASE-DB1 show the superiority of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.

In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Citizen-generated counter speech is a promising way to fight hate speech and
promote peaceful, non-polarized discourse. However, there is a lack of
large-scale longitudinal studies of its effectiveness for reducing hate speech.
To this end, we perform an exploratory analysis of the effectiveness of counter
speech using several different macro- and micro-level measures to analyze
180,000 political conversations that took place on German Twitter over four
years. We report on the dynamic interactions of hate and counter speech over
time and provide insights into whether, as in &#x60;classic&#x27; bullying situations,
organized efforts are more effective than independent individuals in steering
online discourse. Taken together, our results build a multifaceted picture of
the dynamics of hate and counter speech online. While we make no causal claims
due to the complexity of discourse dynamics, our findings suggest that
organized hate speech is associated with changes in public discourse and that
counter speech -- especially when organized -- may help curb hateful rhetoric
in online discourse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">High-Dimensional Sparse Linear Bandits. (arXiv:2011.04020v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Stochastic linear bandits with high-dimensional sparse features are a
practical model for a variety of domains, including personalized medicine and
online advertising. We derive a novel $\Omega(n^{2/3})$ dimension-free minimax
regret lower bound for sparse linear bandits in the data-poor regime where the
horizon is smaller than the ambient dimension and where the feature vectors
admit a well-conditioned exploration distribution. This is complemented by a
nearly matching upper bound for an explore-then-commit algorithm showing that
that $\Theta(n^{2/3})$ is the optimal rate in the data-poor regime. The results
complement existing bounds for the data-rich regime and provide another example
where carefully balancing the trade-off between information and regret is
necessary. Finally, we prove a dimension-free $O(\sqrt{n})$ regret upper bound
under an additional assumption on the magnitude of the signal for relevant
features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BEAUTY Powered BEAST. (arXiv:2103.00674v3 [stat.ME] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00674">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study nonparametric dependence detection with the proposed binary
expansion approximation of uniformity (BEAUTY) approach, which generalizes the
celebrated Euler&#x27;s formula, and approximates the characteristic function of any
copula with a linear combination of expectations of binary interactions from
marginal binary expansions. This novel theory enables a unification of many
important tests through approximations from some quadratic forms of symmetry
statistics, where the deterministic weight matrix characterizes the power
properties of each test. To achieve a robust power, we study test statistics
with data-adaptive weights, referred to as the binary expansion adaptive
symmetry test (BEAST). By utilizing the properties of the binary expansion
filtration, we show that the Neyman-Pearson test of uniformity can be
approximated by an oracle weighted sum of symmetry statistics. The BEAST with
this oracle provides a benchmark of feasible power against any alternative by
leading all existing tests with a substantial margin. To approach this oracle
power, we develop the BEAST through a regularized resampling approximation of
the oracle test. The BEAST improves the empirical power of many existing tests
against a wide spectrum of common alternatives while providing clear
interpretation of the form of dependency upon rejection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online mirror descent and dual averaging: keeping pace in the dynamic case. (arXiv:2006.02585v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.02585">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online mirror descent (OMD) and dual averaging (DA) -- two fundamental
algorithms for online convex optimization -- are known to have very similar
(and sometimes identical) performance guarantees when used with a fixed
learning rate. Under dynamic learning rates, however, OMD is provably inferior
to DA and suffers a linear regret, even in common settings such as prediction
with expert advice. We modify the OMD algorithm through a simple technique that
we call stabilization. We give essentially the same abstract regret bound for
OMD with stabilization and for DA by modifying the classical OMD convergence
analysis in a careful and modular way that allows for straightforward and
flexible proofs. Simple corollaries of these bounds show that OMD with
stabilization and DA enjoy the same performance guarantees in many applications
-- even under dynamic learning rates. We also shed light on the similarities
between OMD and DA and show simple conditions under which stabilized-OMD and DA
generate the same iterates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">From Static to Dynamic Prediction: Wildfire Risk Assessment Based on Multiple Environmental Factors. (arXiv:2103.10901v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Wildfire is one of the biggest disasters that frequently occurs on the west
coast of the United States. Many efforts have been made to understand the
causes of the increases in wildfire intensity and frequency in recent years. In
this work, we propose static and dynamic prediction models to analyze and
assess the areas with high wildfire risks in California by utilizing a
multitude of environmental data including population density, Normalized
Difference Vegetation Index (NDVI), Palmer Drought Severity Index (PDSI), tree
mortality area, tree mortality number, and altitude. Moreover, we focus on a
better understanding of the impacts of different factors so as to inform
preventive actions. To validate our models and findings, we divide the land of
California into 4,242 grids of 0.1 degrees $\times$ 0.1 degrees in latitude and
longitude, and compute the risk of each grid based on spatial and temporal
conditions. To verify the generalizability of our models, we further expand the
scope of wildfire risk assessment from California to Washington without any
fine tuning. By performing counterfactual analysis, we uncover the effects of
several possible methods on reducing the number of high risk wildfires. Taken
together, our study has the potential to estimate, monitor, and reduce the
risks of wildfires across diverse areas provided that such environment data is
available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02357">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In practice, and more especially when training deep neural networks, visual
recognition rules are often learned based on various sources of information. On
the other hand, the recent deployment of facial recognition systems with uneven
predictive performances on different population segments highlights the
representativeness issues possibly induced by a naive aggregation of image
datasets. Indeed, sampling bias does not vanish simply by considering larger
datasets, and ignoring its impact may completely jeopardize the generalization
capacity of the learned prediction rules. In this paper, we show how biasing
models, originally introduced for nonparametric estimation in (Gill et al.,
1988), and recently revisited from the perspective of statistical learning
theory in (Laforgue and Cl\&#x27;emen\c{c}on, 2019), can be applied to remedy these
problems in the context of visual recognition. Based on the (approximate)
knowledge of the biasing mechanisms at work, our approach consists in
reweighting the observations, so as to form a nearly debiased estimator of the
target distribution. One key condition for our method to be theoretically valid
is that the supports of the distributions generating the biased datasets at
disposal must overlap, and cover the support of the target distribution. In
order to meet this requirement in practice, we propose to use a low dimensional
image representation, shared across the image databases. Finally, we provide
numerical experiments highlighting the relevance of our approach whenever the
biasing functions are appropriately chosen.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Compression. (arXiv:2105.10059v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10059">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With time, machine learning models have increased in their scope,
functionality and size. Consequently, the increased functionality and size of
such models requires high-end hardware to both train and provide inference
after the fact. This paper aims to explore the possibilities within the domain
of model compression, discuss the efficiency of combining various levels of
pruning and quantization, while proposing a quality measurement metric to
objectively decide which combination is best in terms of minimizing the
accuracy delta and maximizing the size reduction factor.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Importance Sampling for Error Estimation in the Context of Optimal Bayesian Transfer Learning. (arXiv:2109.02150v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Classification has been a major task for building intelligent systems as it
enables decision-making under uncertainty. Classifier design aims at building
models from training data for representing feature-label distributions--either
explicitly or implicitly. In many scientific or clinical settings, training
data are typically limited, which makes designing accurate classifiers and
evaluating their classification error extremely challenging. While transfer
learning (TL) can alleviate this issue by incorporating data from relevant
source domains to improve learning in a different target domain, it has
received little attention for performance assessment, notably in error
estimation. In this paper, we fill this gap by investigating knowledge
transferability in the context of classification error estimation within a
Bayesian paradigm. We introduce a novel class of Bayesian minimum mean-square
error (MMSE) estimators for optimal Bayesian transfer learning (OBTL), which
enables rigorous evaluation of classification error under uncertainty in a
small-sample setting. Using Monte Carlo importance sampling, we employ the
proposed estimator to evaluate the classification accuracy of a broad family of
classifiers that span diverse learning capabilities. Experimental results based
on both synthetic data as well as real-world RNA sequencing (RNA-seq) data show
that our proposed OBTL error estimation scheme clearly outperforms standard
error estimators, especially in a small-sample setting, by tapping into the
data from other relevant domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Memory-Efficient Neural Networks via Multi-Level in situ Generation. (arXiv:2108.11430v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11430">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks (DNN) have shown superior performance in a variety of
tasks. As they rapidly evolve, their escalating computation and memory demands
make it challenging to deploy them on resource-constrained edge devices. Though
extensive efficient accelerator designs, from traditional electronics to
emerging photonics, have been successfully demonstrated, they are still
bottlenecked by expensive memory accesses due to tremendous gaps between the
bandwidth/power/latency of electrical memory and computing cores. Previous
solutions fail to fully-leverage the ultra-fast computational speed of emerging
DNN accelerators to break through the critical memory bound. In this work, we
propose a general and unified framework to trade expensive memory transactions
with ultra-fast on-chip computations, directly translating to performance
improvement. We are the first to jointly explore the intrinsic correlations and
bit-level redundancy within DNN kernels and propose a multi-level in situ
generation mechanism with mixed-precision bases to achieve on-the-fly recovery
of high-resolution parameters with minimum hardware overhead. Extensive
experiments demonstrate that our proposed joint method can boost the memory
efficiency by 10-20x with comparable accuracy over four state-of-the-art
designs, when benchmarked on ResNet-18/DenseNet-121/MobileNetV2/V3 with various
tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data science and Machine learning in the Clouds: A Perspective for the Future. (arXiv:2109.01661v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01661">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As we are fast approaching the beginning of a paradigm shift in the field of
science, Data driven science (the so called fourth science paradigm) is going
to be the driving force in research and innovation. From medicine to
biodiversity and astronomy to geology, all these terms are somehow going to be
affected by this paradigm shift. The huge amount of data to be processed under
this new paradigm will be a major concern in the future and one will strongly
require cloud based services in all the aspects of these computations (from
storage to compute and other services). Another aspect will be energy
consumption and performance of prediction jobs and tasks within such a
scientific paradigm which will change the way one sees computation. Data
science has heavily impacted or rather triggered the emergence of Machine
Learning, Signal/Image/Video processing related algorithms, Artificial
intelligence, Robotics, health informatics, geoinformatics, and many more such
areas of interest. Hence, we envisage an era where Data science can deliver its
promises with the help of the existing cloud based platforms and services with
the addition of new services. In this article, we discuss about data driven
science and Machine learning and how they are going to be linked through cloud
based services in the future. It also discusses the rise of paradigms like
approximate computing, quantum computing and many more in recent times and
their applicability in big data processing, data science, analytics, prediction
and machine learning in the cloud environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Meta-Surrogate Model for Transferable Adversarial Attack. (arXiv:2109.01983v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01983">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider adversarial attacks to a black-box model when no queries are
allowed. In this setting, many methods directly attack surrogate models and
transfer the obtained adversarial examples to fool the target model. Plenty of
previous works investigated what kind of attacks to the surrogate model can
generate more transferable adversarial examples, but their performances are
still limited due to the mismatches between surrogate models and the target
model. In this paper, we tackle this problem from a novel angle -- instead of
using the original surrogate models, can we obtain a Meta-Surrogate Model (MSM)
such that attacks to this model can be easier transferred to other models? We
show that this goal can be mathematically formulated as a well-posed
(bi-level-like) optimization problem and design a differentiable attacker to
make training feasible. Given one or a set of surrogate models, our method can
thus obtain an MSM such that adversarial examples generated on MSM enjoy
eximious transferability. Comprehensive experiments on Cifar-10 and ImageNet
demonstrate that by attacking the MSM, we can obtain stronger transferable
adversarial examples to fool black-box models including adversarially trained
ones, with much higher success rates than existing methods. The proposed method
reveals significant security challenges of deep models and is promising to be
served as a state-of-the-art benchmark for evaluating the robustness of deep
models in the black-box setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01668">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The recent achievements of Deep Learning rely on the test data being similar
in distribution to the training data. In an ideal case, Deep Learning models
would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make
predictions on out-of-distribution data. Yet in practice, models usually fail
to generalize well when facing a shift in distribution. Several methods were
thereby designed to improve the robustness of the features learned by a model
through Regularization- or Domain-Prediction-based schemes. Segmenting medical
images such as MRIs of the hippocampus is essential for the diagnosis and
treatment of neuropsychiatric disorders. But these brain images often suffer
from distribution shift due to the patient&#x27;s age and various pathologies
affecting the shape of the organ. In this work, we evaluate OoD Generalization
solutions for the problem of hippocampus segmentation in MR data using both
fully- and semi-supervised training. We find that no method performs reliably
in all experiments. Only the V-REx loss stands out as it remains easy to tune,
while it outperforms a standard U-Net in most cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pointspectrum: Equivariance Meets Laplacian Filtering for Graph Representation Learning. (arXiv:2109.02358v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02358">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph Representation Learning (GRL) has become essential for modern graph
data mining and learning tasks. GRL aims to capture the graph&#x27;s structural
information and exploit it in combination with node and edge attributes to
compute low-dimensional representations. While Graph Neural Networks (GNNs)
have been used in state-of-the-art GRL architectures, they have been shown to
suffer from over smoothing when many GNN layers need to be stacked. In a
different GRL approach, spectral methods based on graph filtering have emerged
addressing over smoothing; however, up to now, they employ traditional neural
networks that cannot efficiently exploit the structure of graph data. Motivated
by this, we propose PointSpectrum, a spectral method that incorporates a set
equivariant network to account for a graph&#x27;s structure. PointSpectrum enhances
the efficiency and expressiveness of spectral methods, while it outperforms or
competes with state-of-the-art GRL methods. Overall, PointSpectrum addresses
over smoothing by employing a graph filter and captures a graph&#x27;s structure
through set equivariance, lying on the intersection of GNNs and spectral
methods. Our findings are promising for the benefits and applicability of this
architectural shift for spectral methods and GRL.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.
correlation clustering) problem, a classical graph clustering problem widely
used in machine learning and computer vision. Our algorithm consists of three
steps executed recursively: (1) Finding conflicted cycles that correspond to
violated inequalities of the underlying multicut relaxation, (2) Performing
message passing between the edges and cycles to optimize the Lagrange
relaxation coming from the found violated cycles producing reduced costs and
(3) Contracting edges with high reduced costs through matrix-matrix
multiplications.

Our algorithm produces primal solutions and dual lower bounds that estimate
the distance to optimum. We implement our algorithm on GPUs and show resulting
one to two order-of-magnitudes improvements in execution speed without
sacrificing solution quality compared to traditional serial algorithms that run
on CPUs. We can solve very large scale benchmark problems with up to
$\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We
make our code available at https://github.com/pawelswoboda/RAMA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tolerating Adversarial Attacks and Byzantine Faults in Distributed Machine Learning. (arXiv:2109.02018v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02018">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks attempt to disrupt the training, retraining and utilizing
of artificial intelligence and machine learning models in large-scale
distributed machine learning systems. This causes security risks on its
prediction outcome. For example, attackers attempt to poison the model by
either presenting inaccurate misrepresentative data or altering the models&#x27;
parameters. In addition, Byzantine faults including software, hardware, network
issues occur in distributed systems which also lead to a negative impact on the
prediction outcome. In this paper, we propose a novel distributed training
algorithm, partial synchronous stochastic gradient descent (ParSGD), which
defends adversarial attacks and/or tolerates Byzantine faults. We demonstrate
the effectiveness of our algorithm under three common adversarial attacks again
the ML models and a Byzantine fault during the training phase. Our results show
that using ParSGD, ML models can still produce accurate predictions as if it is
not being attacked nor having failures at all when almost half of the nodes are
being compromised or failed. We will report the experimental evaluations of
ParSGD in comparison with other algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on Spark AR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on the enhanced data sets and showing that they outperform
equivalent systems trained on the original data sets (containing faces without
masks), when the test benchmark contains masked faces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Online Multi-Source Domain Adaptation. (arXiv:2109.01996v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01996">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Knowledge transfer across several streaming processes remain challenging
problem not only because of different distributions of each stream but also
because of rapidly changing and never-ending environments of data streams.
Albeit growing research achievements in this area, most of existing works are
developed for a single source domain which limits its resilience to exploit
multi-source domains being beneficial to recover from concept drifts quickly
and to avoid the negative transfer problem. An online domain adaptation
technique under multisource streaming processes, namely automatic online
multi-source domain adaptation (AOMSDA), is proposed in this paper. The online
domain adaptation strategy of AOMSDA is formulated under a coupled generative
and discriminative approach of denoising autoencoder (DAE) where the central
moment discrepancy (CMD)-based regularizer is integrated to handle the
existence of multi-source domains thereby taking advantage of complementary
information sources. The asynchronous concept drifts taking place at different
time periods are addressed by a self-organizing structure and a node
re-weighting strategy. Our numerical study demonstrates that AOMSDA is capable
of outperforming its counterparts in 5 of 8 study cases while the ablation
study depicts the advantage of each learning component. In addition, AOMSDA is
general for any number of source streams. The source code of AOMSDA is shared
publicly in https://github.com/Renchunzi-Xie/AOMSDA.git.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Detection of COVID-19 Symptoms using Deep Learning &amp; Probability-based Weighting of Modes. (arXiv:2109.01669v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01669">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic is one of the most challenging healthcare crises during
the 21st century. As the virus continues to spread on a global scale, the
majority of efforts have been on the development of vaccines and the mass
immunization of the public. While the daily case numbers were following a
decreasing trend, the emergent of new virus mutations and variants still pose a
significant threat. As economies start recovering and societies start opening
up with people going back into office buildings, schools, and malls, we still
need to have the ability to detect and minimize the spread of COVID-19.
Individuals with COVID-19 may show multiple symptoms such as cough, fever, and
shortness of breath. Many of the existing detection techniques focus on
symptoms having the same equal importance. However, it has been shown that some
symptoms are more prevalent than others. In this paper, we present a multimodal
method to predict COVID-19 by incorporating existing deep learning classifiers
using convolutional neural networks and our novel probability-based weighting
function that considers the prevalence of each symptom. The experiments were
performed on an existing dataset with respect to the three considered modes of
coughs, fever, and shortness of breath. The results show considerable
improvements in the detection of COVID-19 using our weighting function when
compared to an equal weighting function.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VARGAN: Variance Enforcing Network Enhanced GAN. (arXiv:2109.02117v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02117">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative adversarial networks (GANs) are one of the most widely used
generative models. GANs can learn complex multi-modal distributions, and
generate real-like samples. Despite the major success of GANs in generating
synthetic data, they might suffer from unstable training process, and mode
collapse. In this paper, we introduce a new GAN architecture called variance
enforcing GAN (VARGAN), which incorporates a third network to introduce
diversity in the generated samples. The third network measures the diversity of
the generated samples, which is used to penalize the generator&#x27;s loss for low
diversity samples. The network is trained on the available training data and
undesired distributions with limited modality. On a set of synthetic and
real-world image data, VARGAN generates a more diverse set of samples compared
to the recent state-of-the-art models. High diversity and low computational
complexity, as well as fast convergence, make VARGAN a promising model to
alleviate mode collapse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.05248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural network pruning and quantization techniques have demonstrated it
is possible to achieve high levels of compression with surprisingly little
degradation to test set accuracy. However, this measure of performance conceals
significant differences in how different classes and images are impacted by
model compression techniques. We find that models with radically different
numbers of weights have comparable top-line performance metrics but diverge
considerably in behavior on a narrow subset of the dataset. This small subset
of data points, which we term Pruning Identified Exemplars (PIEs) are
systematically more impacted by the introduction of sparsity. Compression
disproportionately impacts model performance on the underrepresented long-tail
of the data distribution. PIEs over-index on atypical or noisy images that are
far more challenging for both humans and algorithms to classify. Our work
provides intuition into the role of capacity in deep neural networks and the
trade-offs incurred by compression. An understanding of this disparate impact
is critical given the widespread deployment of compressed models in the wild.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on H\&quot;older Class. (arXiv:2103.00542v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00542">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we construct neural networks with ReLU, sine and $2^x$ as
activation functions. For general continuous $f$ defined on $[0,1]^d$ with
continuity modulus $\omega_f(\cdot)$, we construct ReLU-sine-$2^x$ networks
that enjoy an approximation rate
$\mathcal{O}(\omega_f(\sqrt{d})\cdot2^{-M}+\omega_{f}\left(\frac{\sqrt{d}}{N}\right))$,
where $M,N\in \mathbb{N}^{+}$ denote the hyperparameters related to widths of
the networks. As a consequence, we can construct ReLU-sine-$2^x$ network with
the depth $5$ and width
$\max\left\{\left\lceil2d^{3/2}\left(\frac{3\mu}{\epsilon}\right)^{1/{\alpha}}\right\rceil,2\left\lceil\log_2\frac{3\mu
d^{\alpha/2}}{2\epsilon}\right\rceil+2\right\}$ that approximates $f\in
\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$ within a given tolerance $\epsilon &gt;0$
measured in $L^p$ norm $p\in[1,\infty)$, where
$\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$ denotes the H\&quot;older continuous function
class defined on $[0,1]^d$ with order $\alpha \in (0,1]$ and constant $\mu &gt;
0$. Therefore, the ReLU-sine-$2^x$ networks overcome the curse of
dimensionality on $\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$. In addition to its
supper expressive power, functions implemented by ReLU-sine-$2^x$ networks are
(generalized) differentiable, enabling us to apply SGD to train.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cohort Characteristics and Factors Associated with Cannabis Use among Adolescents in Canada Using Pattern Discovery and Disentanglement Method. (arXiv:2109.01739v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01739">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>COMPASS is a longitudinal, prospective cohort study collecting data annually
from students attending high school in jurisdictions across Canada. We aimed to
discover significant frequent/rare associations of behavioral factors among
Canadian adolescents related to cannabis use. We use a subset of COMPASS
dataset which contains 18,761 records of students in grades 9 to 12 with 31
selected features (attributes) involving various characteristics, from living
habits to academic performance. We then used the Pattern Discovery and
Disentanglement (PDD) algorithm that we have developed to detect strong and
rare (yet statistically significant) associations from the dataset. PDD used
the criteria derived from disentangled statistical spaces (known as
Re-projected Adjusted-Standardized Residual Vector Spaces, notated as RARV). It
outperformed methods using other criteria (i.e. support and confidence) popular
as reported in the literature. Association results showed that PDD can
discover: i) a smaller set of succinct significant associations in clusters;
ii) frequent and rare, yet significant, patterns supported by population health
relevant study; iii) patterns from a dataset with extremely imbalanced groups
(majority class: minority class &#x3D; 88.3%: 11.7%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Soft Hierarchical Graph Recurrent Networks for Many-Agent Partially Observable Environments. (arXiv:2109.02032v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02032">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The recent progress in multi-agent deep reinforcement learning(MADRL) makes
it more practical in real-world tasks, but its relatively poor scalability and
the partially observable constraints raise challenges to its performance and
deployment. Based on our intuitive observation that the human society could be
regarded as a large-scale partially observable environment, where each
individual has the function of communicating with neighbors and remembering its
own experience, we propose a novel network structure called hierarchical graph
recurrent network(HGRN) for multi-agent cooperation under partial
observability. Specifically, we construct the multi-agent system as a graph,
use the hierarchical graph attention network(HGAT) to achieve communication
between neighboring agents, and exploit GRU to enable agents to record
historical information. To encourage exploration and improve robustness, we
design a maximum-entropy learning method to learn stochastic policies of a
configurable target action entropy. Based on the above technologies, we
proposed a value-based MADRL algorithm called Soft-HGRN and its actor-critic
variant named SAC-HRGN. Experimental results based on three homogeneous tasks
and one heterogeneous environment not only show that our approach achieves
clear improvements compared with four baselines, but also demonstrates the
interpretability, scalability, and transferability of the proposed model.
Ablation studies prove the function and necessity of each component.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Active learning has emerged as a standard paradigm in areas with scarcity of
labeled training data, such as in the medical domain. Language models have
emerged as the prevalent choice of several natural language tasks due to the
performance boost offered by these models. However, in several domains, such as
medicine, the scarcity of labeled training data is a common issue. Also, these
models may not work well in cases where class imbalance is prevalent. Active
learning may prove helpful in these cases to boost the performance with a
limited label budget. To this end, we propose a novel method using sampling
techniques based on submodular optimization and optimal transport for active
learning in language models, dubbed ALLWAS. We construct a sampling strategy
based on submodular optimization of the designed objective in the gradient
domain. Furthermore, to enable learning from few samples, we propose a novel
strategy for sampling from the Wasserstein barycenters. Our empirical
evaluations on standard benchmark datasets for text classification show that
our methods perform significantly better (&gt;20% relative increase in some cases)
than existing approaches for active learning on language models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Communication Efficient Tensor Factorization for Decentralized Healthcare Networks. (arXiv:2109.01718v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01718">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tensor factorization has been proved as an efficient unsupervised learning
approach for health data analysis, especially for computational phenotyping,
where the high-dimensional Electronic Health Records (EHRs) with patients
history of medical procedures, medications, diagnosis, lab tests, etc., are
converted to meaningful and interpretable medical concepts. Federated tensor
factorization distributes the tensor computation to multiple workers under the
coordination of a central server, which enables jointly learning the phenotypes
across multiple hospitals while preserving the privacy of the patient
information. However, existing federated tensor factorization algorithms
encounter the single-point-failure issue with the involvement of the central
server, which is not only easily exposed to external attacks, but also limits
the number of clients sharing information with the server under restricted
uplink bandwidth. In this paper, we propose CiderTF, a communication-efficient
decentralized generalized tensor factorization, which reduces the uplink
communication cost by leveraging a four-level communication reduction strategy
designed for a generalized tensor factorization, which has the flexibility of
modeling different tensor distribution with multiple kinds of loss functions.
Experiments on two real-world EHR datasets demonstrate that CiderTF achieves
comparable convergence with the communication reduction up to 99.99%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Node Feature Kernels Increase Graph Convolutional Network Robustness. (arXiv:2109.01785v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The robustness of the much-used Graph Convolutional Networks (GCNs) to
perturbations of their input is becoming a topic of increasing importance. In
this paper, the random GCN is introduced for which a random matrix theory
analysis is possible. This analysis suggests that if the graph is sufficiently
perturbed, or in the extreme case random, then the GCN fails to benefit from
the node features. It is furthermore observed that enhancing the message
passing step in GCNs by adding the node feature kernel to the adjacency matrix
of the graph structure solves this problem. An empirical study of a GCN
utilised for node classification on six real datasets further confirms the
theoretical findings and demonstrates that perturbations of the graph structure
can result in GCNs performing significantly worse than Multi-Layer Perceptrons
run on the node features alone. In practice, adding a node feature kernel to
the message passing of perturbed graphs results in a significant improvement of
the GCN&#x27;s performance, thereby rendering it more robust to graph perturbations.
Our code is publicly available at:https://github.com/ChangminWu/RobustGCN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Acceleration Method for Learning Fine-Layered Optical Neural Networks. (arXiv:2109.01731v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01731">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>An optical neural network (ONN) is a promising system due to its high-speed
and low-power operation. Its linear unit performs a multiplication of an input
vector and a weight matrix in optical analog circuits. Among them, a circuit
with a multiple-layered structure of programmable Mach-Zehnder interferometers
(MZIs) can realize a specific class of unitary matrices with a limited number
of MZIs as its weight matrix. The circuit is effective for balancing the number
of programmable MZIs and ONN performance. However, it takes a lot of time to
learn MZI parameters of the circuit with a conventional automatic
differentiation (AD), which machine learning platforms are equipped with. To
solve the time-consuming problem, we propose an acceleration method for
learning MZI parameters. We create customized complex-valued derivatives for an
MZI, exploiting Wirtinger derivatives and a chain rule. They are incorporated
into our newly developed function module implemented in C++ to collectively
calculate their values in a multi-layered structure. Our method is simple,
fast, and versatile as well as compatible with the conventional AD. We
demonstrate that our method works 20 times faster than the conventional AD when
a pixel-by-pixel MNIST task is performed in a complex-valued recurrent neural
network with an MZI-based hidden unit.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge Learning. (arXiv:2109.02353v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated edge learning (FEEL) has emerged as a revolutionary paradigm to
develop AI services at the edge of 6G wireless networks as it supports
collaborative model training at a massive number of mobile devices. However,
model communication over wireless channels, especially in uplink model
uploading of FEEL, has been widely recognized as a bottleneck that critically
limits the efficiency of FEEL. Although over-the-air computation can alleviate
the excessive cost of radio resources in FEEL model uploading, practical
implementations of over-the-air FEEL still suffer from several challenges,
including strong straggler issues, large communication overheads, and potential
privacy leakage. In this article, we study these challenges in over-the-air
FEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of
future wireless systems, to address these challenges. We study the
state-of-the-art solutions on RIS-empowered FEEL and explore the promising
research opportunities for adopting RIS to enhance FEEL performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS. (arXiv:2109.01818v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, convolutional neural networks (CNNs) have experienced an
increasing interest for their ability to perform fast approximation of
effective hydrodynamic parameters in porous media research and applications.
This paper presents a novel methodology for permeability prediction from
micro-CT scans of geological rock samples. The training data set for CNNs
dedicated to permeability prediction consists of permeability labels that are
typically generated by classical lattice Boltzmann methods (LBM) that simulate
the flow through the pore space of the segmented image data. We instead perform
direct numerical simulation (DNS) by solving the stationary Stokes equation in
an efficient and distributed-parallel manner. As such, we circumvent the
convergence issues of LBM that frequently are observed on complex pore
geometries, and therefore, improve on the generality and accuracy of our
training data set. Using the DNS-computed permeabilities, a physics-informed
CNN PhyCNN) is trained by additionally providing a tailored characteristic
quantity of the pore space. More precisely, by exploiting the connection to
flow problems on a graph representation of the pore space, additional
information about confined structures is provided to the network in terms of
the maximum flow value, which is the key innovative component of our workflow.
As a result, unprecedented prediction accuracy and robustness are observed for
a variety of sandstone samples from archetypal rock formations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Kernel Correntropy for Robust Learning. (arXiv:1905.10115v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.10115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As a novel similarity measure that is defined as the expectation of a kernel
function between two random variables, correntropy has been successfully
applied in robust machine learning and signal processing to combat large
outliers. The kernel function in correntropy is usually a zero-mean Gaussian
kernel. In a recent work, the concept of mixture correntropy (MC) was proposed
to improve the learning performance, where the kernel function is a mixture
Gaussian kernel, namely a linear combination of several zero-mean Gaussian
kernels with different widths. In both correntropy and mixture correntropy, the
center of the kernel function is, however, always located at zero. In the
present work, to further improve the learning performance, we propose the
concept of multi-kernel correntropy (MKC), in which each component of the
mixture Gaussian kernel can be centered at a different location. The properties
of the MKC are investigated and an efficient approach is proposed to determine
the free parameters in MKC. Experimental results show that the learning
algorithms under the maximum multi-kernel correntropy criterion (MMKCC) can
outperform those under the original maximum correntropy criterion (MCC) and the
maximum mixture correntropy criterion (MMCC).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02549">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hierarchical 3D Feature Learning for Pancreas Segmentation. (arXiv:2109.01667v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01667">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a novel 3D fully convolutional deep network for automated pancreas
segmentation from both MRI and CT scans. More specifically, the proposed model
consists of a 3D encoder that learns to extract volume features at different
scales; features taken at different points of the encoder hierarchy are then
sent to multiple 3D decoders that individually predict intermediate
segmentation maps. Finally, all segmentation maps are combined to obtain a
unique detailed segmentation mask. We test our model on both CT and MRI imaging
data: the publicly available NIH Pancreas-CT dataset (consisting of 82
contrast-enhanced CTs) and a private MRI dataset (consisting of 40 MRI scans).
Experimental results show that our model outperforms existing methods on CT
pancreas segmentation, obtaining an average Dice score of about 88%, and yields
promising segmentation performance on a very challenging MRI data set (average
Dice score is about 77%). Additional control experiments demonstrate that the
achieved performance is due to the combination of our 3D fully-convolutional
deep network and the hierarchical representation decoding, thus substantiating
our architectural design.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nonasymptotic one-and two-sample tests in high dimension with unknown covariance structure. (arXiv:2109.01730v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01730">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Let $\mathbf{X} &#x3D; (X_i)_{1\leq i \leq n}$ be an i.i.d. sample of
square-integrable variables in $\mathbb{R}^d$, with common expectation $\mu$
and covariance matrix $\Sigma$, both unknown. We consider the problem of
testing if $\mu$ is $\eta$-close to zero, i.e. $\|\mu\| \leq \eta $ against
$\|\mu\| \geq (\eta + \delta)$; we also tackle the more general two-sample mean
closeness testing problem. The aim of this paper is to obtain nonasymptotic
upper and lower bounds on the minimal separation distance $\delta$ such that we
can control both the Type I and Type II errors at a given level. The main
technical tools are concentration inequalities, first for a suitable estimator
of $\|\mu\|^2$ used a test statistic, and secondly for estimating the operator
and Frobenius norms of $\Sigma$ coming into the quantiles of said test
statistic. These properties are obtained for Gaussian and bounded
distributions. A particular attention is given to the dependence in the
pseudo-dimension $d_*$ of the distribution, defined as $d_* :&#x3D;
\|\Sigma\|_2^2/\|\Sigma\|_\infty^2$. In particular, for $\eta&#x3D;0$, the minimum
separation distance is ${\Theta}(d_*^{\frac{1}{4}}\sqrt{\|\Sigma\|_\infty/n})$,
in contrast with the minimax estimation distance for $\mu$, which is
${\Theta}(d_e^{\frac{1}{2}}\sqrt{\|\Sigma\|_\infty/n})$ (where
$d_e:&#x3D;\|\Sigma\|_1/\|\Sigma\|_\infty$). This generalizes a phenomenon spelled
out in particular by Baraud (2002).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Temporal Quantum Tomography. (arXiv:2103.13973v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantifying and verifying the control level in preparing a quantum state are
central challenges in building quantum devices. The quantum state is
characterized from experimental measurements, using a procedure known as
tomography, which requires a vast number of resources. Furthermore, the
tomography for a quantum device with temporal processing, which is
fundamentally different from the standard tomography, has not been formulated.
We develop a practical and approximate tomography method using a recurrent
machine learning framework for this intriguing situation. The method is based
on repeated quantum interactions between a system called quantum reservoir with
a stream of quantum states. Measurement data from the reservoir are connected
to a linear readout to train a recurrent relation between quantum channels
applied to the input stream. We demonstrate our algorithms for quantum learning
tasks followed by the proposal of a quantum short-term memory capacity to
evaluate the temporal processing ability of near-term quantum devices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attentive Neural Controlled Differential Equations for Time-series Classification and Forecasting. (arXiv:2109.01876v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks inspired by differential equations have proliferated for the
past several years. Neural ordinary differential equations (NODEs) and neural
controlled differential equations (NCDEs) are two representative examples of
them. In theory, NCDEs provide better representation learning capability for
time-series data than NODEs. In particular, it is known that NCDEs are suitable
for processing irregular time-series data. Whereas NODEs have been successfully
extended after adopting attention, however, it had not been studied yet how to
integrate attention into NCDEs. To this end, we present the method of Attentive
Neural Controlled Differential Equations (ANCDEs) for time-series
classification and forecasting, where dual NCDEs are used: one for generating
attention values, and the other for evolving hidden vectors for a downstream
machine learning task. We conduct experiments with three real-world time-series
datasets and 10 baselines. After dropping some values, we also conduct
irregular time-series experiments. Our method consistently shows the best
accuracy in all cases by non-trivial margins. Our visualizations also show that
the presented attention mechanism works as intended by focusing on crucial
information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Mixture of Experts (MoE) with sparse conditional computation has been proved
an effective architecture for scaling attention-based models to more parameters
with comparable computation cost. In this paper, we propose Sparse-MLP, scaling
the recent MLP-Mixer model with sparse MoE layers, to achieve a more
computation-efficient architecture. We replace a subset of dense MLP blocks in
the MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two
stages of MoE layers: one with MLP experts mixing information within channels
along image patch dimension, one with MLP experts mixing information within
patches along the channel dimension. Besides, to reduce computational cost in
routing and improve experts capacity, we design Re-represent layers in each
Sparse block. These layers are to re-scale image representations by two simple
but effective linear transformations. By pre-training on ImageNet-1k with MoCo
v3 algorithm, our models can outperform dense MLP models with comparable
parameters and less computational cost on several downstream image
classification tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mobility Functional Areas and COVID-19 Spread. (arXiv:2103.16894v2 [stat.AP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16894">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work introduces a new concept of functional areas called Mobility
Functional Areas (MFAs), i.e., the geographic zones highly interconnected
according to the analysis of mobile positioning data. The MFAs do not coincide
necessarily with administrative borders as they are built observing natural
human mobility and, therefore, they can be used to inform, in a bottom-up
approach, local transportation, spatial planning, health and economic policies.
After presenting the methodology behind the MFAs, this study focuses on the
link between the COVID-19 pandemic and the MFAs in Austria. It emerges that the
MFAs registered an average number of infections statistically larger than the
areas in the rest of the country, suggesting the usefulness of the MFAs in the
context of targeted re-escalation policy responses to this health crisis. The
MFAs dataset is openly available to other scholars for further analyses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Perceptually-Validated Metric for Crowd Trajectory Quality Evaluation. (arXiv:2108.12346v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12346">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Simulating crowds requires controlling a very large number of trajectories
and is usually performed using crowd motion algorithms for which appropriate
parameter values need to be found. The study of the relation between parametric
values for simulation techniques and the quality of the resulting trajectories
has been studied either through perceptual experiments or by comparison with
real crowd trajectories. In this paper, we integrate both strategies. A quality
metric, QF, is proposed to abstract from reference data while capturing the
most salient features that affect the perception of trajectory realism. QF
weights and combines cost functions that are based on several individual, local
and global properties of trajectories. These trajectory features are selected
from the literature and from interviews with experts. To validate the capacity
of QF to capture perceived trajectory quality, we conduct an online experiment
that demonstrates the high agreement between the automatic quality score and
non-expert users. To further demonstrate the usefulness of QF, we use it in a
data-free parameter tuning application able to tune any parametric microscopic
crowd simulation model that outputs independent trajectories for characters.
The learnt parameters for the tuned crowd motion model maintain the influence
of the reference data which was used to weight the terms of QF.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. (arXiv:2108.12284v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, many datasets have been proposed to test the systematic
generalization ability of neural networks. The companion baseline Transformers,
typically trained with default hyper-parameters from standard tasks, are shown
to fail dramatically. Here we demonstrate that by revisiting model
configurations as basic as scaling of embeddings, early stopping, relative
positional embedding, and Universal Transformer variants, we can drastically
improve the performance of Transformers on systematic generalization. We report
improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics
dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity
split, and from 35% to 81% on COGS. On SCAN, relative positional embedding
largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%
accuracy on the length split with a cutoff at 26. Importantly, performance
differences between these models are typically invisible on the IID data split.
This calls for proper generalization validation sets for developing neural
networks that generalize systematically. We publicly release the code to
reproduce our results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment. (arXiv:2108.10828v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10828">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Considerable research has been devoted to deep learning-based predictive
models for system prognostics and health management in the reliability and
safety community. However, there is limited study on the utilization of deep
learning for system reliability assessment. This paper aims to bridge this gap
and explore this new interface between deep learning and system reliability
assessment by exploiting the recent advances of physics-informed deep learning.
Particularly, we present an approach to frame system reliability assessment in
the context of physics-informed deep learning and discuss the potential value
of physics-informed generative adversarial networks for the uncertainty
quantification and measurement data incorporation in system reliability
assessment. The proposed approach is demonstrated by three numerical examples
involving a dual-processor computing system. The results indicate the potential
value of physics-informed deep learning to alleviate computational challenges
and combine measurement data and mathematical models for system reliability
assessment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-Driven Learning of 3-Point Correlation Functions as Microstructure Representations. (arXiv:2109.02255v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02255">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper considers the open challenge of identifying complete, concise, and
explainable quantitative microstructure representations for disordered
heterogeneous material systems. Completeness and conciseness have been achieved
through existing data-driven methods, e.g., deep generative models, which,
however, do not provide mathematically explainable latent representations. This
study investigates representations composed of three-point correlation
functions, which are a special type of spatial convolutions. We show that a
variety of microstructures can be characterized by a concise subset of
three-point correlations, and the identification of such subsets can be
achieved by Bayesian optimization. Lastly, we show that the proposed
representation can directly be used to compute material properties based on the
effective medium theory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Variational Approach to Privacy and Fairness. (arXiv:2006.06332v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06332">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this article, we propose a new variational approach to learn private
and/or fair representations. This approach is based on the Lagrangians of a new
formulation of the privacy and fairness optimization problems that we propose.
In this formulation, we aim to generate representations of the data that keep a
prescribed level of the relevant information that is not shared by the private
or sensitive data, while minimizing the remaining information they keep. The
proposed approach (i) exhibits the similarities of the privacy and fairness
problems, (ii) allows us to control the trade-off between utility and privacy
or fairness through the Lagrange multiplier parameter, and (iii) can be
comfortably incorporated to common representation learning algorithms such as
the VAE, the $\beta$-VAE, the VIB, or the nonlinear IB.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Phonexia VoxCeleb Speaker Recognition Challenge 2021 System Description. (arXiv:2109.02052v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02052">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We describe the Phonexia submission for the VoxCeleb Speaker Recognition
Challenge 2021 (VoxSRC-21) in the unsupervised speaker verification track. Our
solution was very similar to IDLab&#x27;s winning submission for VoxSRC-20. An
embedding extractor was bootstrapped using momentum contrastive learning, with
input augmentations as the only source of supervision. This was followed by
several iterations of clustering to assign pseudo-speaker labels that were then
used for supervised embedding extractor training. Finally, a score fusion was
done, by averaging the zt-normalized cosine scores of five different embedding
extractors. We briefly also describe unsuccessful solutions involving i-vectors
instead of DNN embeddings and PLDA instead of cosine scoring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Approximate information state for approximate planning and reinforcement learning in partially observed systems. (arXiv:2010.08843v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a theoretical framework for approximate planning and learning in
partially observed systems. Our framework is based on the fundamental notion of
information state. We provide two equivalent definitions of information state
-- i) a function of history which is sufficient to compute the expected reward
and predict its next value; ii) equivalently, a function of the history which
can be recursively updated and is sufficient to compute the expected reward and
predict the next observation. An information state always leads to a dynamic
programming decomposition. Our key result is to show that if a function of the
history (called approximate information state (AIS)) approximately satisfies
the properties of the information state, then there is a corresponding
approximate dynamic program. We show that the policy computed using this is
approximately optimal with bounded loss of optimality. We show that several
approximations in state, observation and action spaces in literature can be
viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A
salient feature of AIS is that it can be learnt from data. We present AIS based
multi-time scale policy gradient algorithms. and detailed numerical experiments
with low, moderate and high dimensional environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games. (arXiv:2109.01795v1 [cs.GT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01795">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Similar to the role of Markov decision processes in reinforcement learning,
Stochastic Games (SGs) lay the foundation for the study of multi-agent
reinforcement learning (MARL) and sequential agent interactions. In this paper,
we derive that computing an approximate Markov Perfect Equilibrium (MPE) in a
finite-state discounted Stochastic Game within the exponential precision is
\textbf{PPAD}-complete. We adopt a function with a polynomially bounded
description in the strategy space to convert the MPE computation to a
fixed-point problem, even though the stochastic game may demand an exponential
number of pure strategies, in the number of states, for each agent. The
completeness result follows the reduction of the fixed-point problem to {\sc
End of the Line}. Our results indicate that finding an MPE in SGs is highly
unlikely to be \textbf{NP}-hard unless \textbf{NP}&#x3D;\textbf{co-NP}. Our work
offers confidence for MARL research to study MPE computation on general-sum SGs
and to develop fruitful algorithms as currently on zero-sum SGs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Structural Optimization Makes Graph Classification Simpler and Better. (arXiv:2109.02027v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In deep neural networks, better results can often be obtained by increasing
the complexity of previously developed basic models. However, it is unclear
whether there is a way to boost performance by decreasing the complexity of
such models. Here, based on an optimization method, we investigate the
feasibility of improving graph classification performance while simplifying the
model learning process. Inspired by progress in structural information
assessment, we optimize the given data sample from graphs to encoding trees. In
particular, we minimize the structural entropy of the transformed encoding tree
to decode the key structure underlying a graph. This transformation is denoted
as structural optimization. Furthermore, we propose a novel feature combination
scheme, termed hierarchical reporting, for encoding trees. In this scheme,
features are transferred from leaf nodes to root nodes by following the
hierarchical structures of encoding trees. We then present an implementation of
the scheme in a tree kernel and a convolutional network to perform graph
classification. The tree kernel follows label propagation in the
Weisfeiler-Lehman (WL) subtree kernel, but it has a lower runtime complexity
$O(n)$. The convolutional network is a special implementation of our tree
kernel in the deep learning field and is called Encoding Tree Learning (ETL).
We empirically validate our tree kernel and convolutional network with several
graph classification benchmarks and demonstrate that our methods achieve better
performance and lower computational consumption than competing approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01887">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents an automatic algorithm for the segmentation of areas
affected by an acute stroke on the non-contrast computed tomography brain
images. The proposed algorithm is designed for learning in a weakly supervised
scenario when some images are labeled accurately, and some images are labeled
inaccurately. Wrong labels appear as a result of inaccuracy made by a
radiologist in the process of manual annotation of computed tomography images.
We propose methods for solving the segmentation problem in the case of
inaccurately labeled training data. We use the U-Net neural network
architecture with several modifications. Experiments on real computed
tomography scans show that the proposed methods increase the segmentation
accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation is a classic computer vision task with multiple
applications, which includes medical and remote sensing image analysis. Despite
recent advances with deep-based approaches, labeling samples (pixels) for
training models is laborious and, in some cases, unfeasible. In this paper, we
present two novel meta learning methods, named WeaSeL and ProtoSeg, for the
few-shot semantic segmentation task with sparse annotations. We conducted
extensive evaluation of the proposed methods in different applications (12
datasets) in medical imaging and agricultural remote sensing, which are very
distinct fields of knowledge and usually subject to data scarcity. The results
demonstrated the potential of our method, achieving suitable results for
segmenting both coffee/orange crops and anatomical parts of the human body in
comparison with full dense annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01980">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Using only a model that was trained to predict where people look at images,
and no additional training data, we can produce a range of powerful editing
effects for reducing distraction in images. Given an image and a mask
specifying the region to edit, we backpropagate through a state-of-the-art
saliency model to parameterize a differentiable editing operator, such that the
saliency within the masked region is reduced. We demonstrate several operators,
including: a recoloring operator, which learns to apply a color transform that
camouflages and blends distractors into their surroundings; a warping operator,
which warps less salient image regions to cover distractors, gradually
collapsing objects into themselves and effectively removing them (an effect
akin to inpainting); a GAN operator, which uses a semantic prior to fully
replace image regions with plausible, less salient alternatives. The resulting
effects are consistent with cognitive research on the human visual system
(e.g., since color mismatch is salient, the recoloring operator learns to
harmonize objects&#x27; colors with their surrounding to reduce their saliency),
and, importantly, are all achieved solely through the guidance of the
pretrained saliency model, with no additional supervision. We present results
on a variety of natural images and conduct a perceptual study to evaluate and
validate the changes in viewers&#x27; eye-gaze between the original images and our
edited results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing the Knowledge State of Online Students -- New Data, New Approaches, Improved Accuracy. (arXiv:2109.01753v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the problem of assessing the changing knowledge state of
individual students as they go through online courses. This student performance
(SP) modeling problem, also known as knowledge tracing, is a critical step for
building adaptive online teaching systems. Specifically, we conduct a study of
how to utilize various types and large amounts of students log data to train
accurate machine learning models that predict the knowledge state of future
students. This study is the first to use four very large datasets made
available recently from four distinct intelligent tutoring systems. Our results
include a new machine learning approach that defines a new state of the art for
SP modeling, improving over earlier methods in several ways: First, we achieve
improved accuracy by introducing new features that can be easily computed from
conventional question-response logs (e.g., the pattern in the student&#x27;s most
recent answers). Second, we take advantage of features of the student history
that go beyond question-response pairs (e.g., which video segments the student
watched, or skipped) as well as information about prerequisite structure in the
curriculum. Third, we train multiple specialized modeling models for different
aspects of the curriculum (e.g., specializing in early versus later segments of
the student history), then combine these specialized models to create a group
prediction of student knowledge. Taken together, these innovations yield an
average AUC score across these four datasets of 0.807 compared to the previous
best logistic regression approach score of 0.766, and also outperforming
state-of-the-art deep neural net approaches. Importantly, we observe consistent
improvements from each of our three methodological innovations, in each
dataset, suggesting that our methods are of general utility and likely to
produce improvements for other online tutoring systems as well.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Providing an Approach to Predicting Customer Quality in E-Commerce Social Networks Based on Big Data and Unsupervised Learning Method. (arXiv:2109.02080v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02080">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the goals of every business enterprise is to increase customer
loyalty. The degree of customer loyalty is called customer quality which its
forecasting will affect strategic marketing practices. The purpose of this
study is to predict the quality of customers of large e-commerce social
networks by big data algorithms and unsupervised learning. For this purpose, a
graph-based social network analysis framework was used for community detection
in the Stanford Network Analysis Platform (SNAP). Then in the found
communities, the quality of customers was predicted. The results showed that
various visits with an impact of 37.13% can have the greatest impact on
customer quality and the order of impact of other parameters were from highest
to lowest: number of frequent customer visits (28.56%), role in social networks
(28.37%), Indirect transactions (26.74%), activity days (25.62%) and customer
social network size (25.06%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00436">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mainly in object classification tasks. In this paper, we
make the first large-scale study concerning the task of Long-Tail Visual
Relationship Recognition (LTVRR). LTVRR aims at improving the learning of
structured visual relationships that come from the long-tail (e.g., &quot;rabbit
grazing on grass&quot;). In this setup, the subject, relation, and object classes
each follow a long-tail distribution. To begin our study and make a future
benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed
VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of several state-of-the-art
long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic
hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR
setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top
of existing models and despite being simple, our results show that they can
remarkably improve the performance, especially on tail classes. Benchmarks,
code, and models have been made available at:
https://github.com/Vision-CAIR/LTVRR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">User-Oriented Smart General AI System under Causal Inference. (arXiv:2103.14561v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14561">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>General AI system solves a wide range of tasks with high performance in an
automated fashion. The best general AI algorithm designed by one individual is
different from that devised by another. The best performance records achieved
by different users are also different. An inevitable component of general AI is
tacit knowledge that depends upon user-specific comprehension of task
information and individual model design preferences that are related to user
technical experiences. Tacit knowledge affects model performance but cannot be
automatically optimized in general AI algorithms. In this paper, we propose
User-Oriented Smart General AI System under Causal Inference, abbreviated as
UOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means
under the framework of causal inference. User characteristics that have a
significant influence upon tacit knowledge can be extracted from observed model
training experiences of many users in external memory modules. Under the
framework of causal inference, we manage to identify the optimal value of user
characteristics that are connected with the best model performance designed by
users. We make suggestions to users about how different user characteristics
can improve the best model performance achieved by users. By recommending
updating user characteristics associated with individualized tacit knowledge
comprehension and technical preferences, UOGAS helps users design models with
better performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Closed Loop Gradient Descent Algorithm applied to Rosenbrock&#x27;s function. (arXiv:2108.12883v3 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12883">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce a novel adaptive damping technique for an inertial gradient
system which finds application as a gradient descent algorithm for
unconstrained optimisation. In an example using the non-convex Rosenbrock&#x27;s
function, we show an improvement on existing momentum-based gradient
optimisation methods. Also using Lyapunov stability analysis, we demonstrate
the performance of the continuous-time version of the algorithm. Using
numerical simulations, we consider the performance of its discrete-time
counterpart obtained by using the symplectic Euler method of discretisation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Agents using Upside-Down Reinforcement Learning. (arXiv:1912.02877v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.02877">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We develop Upside-Down Reinforcement Learning (UDRL), a method for learning
to act using only supervised learning techniques. Unlike traditional
algorithms, UDRL does not use reward prediction or search for an optimal
policy. Instead, it trains agents to follow commands such as &quot;obtain so much
total reward in so much time.&quot; Many of its general principles are outlined in a
companion report; the goal of this paper is to develop a practical learning
algorithm and show that this conceptually simple perspective on agent training
can produce a range of rewarding behaviors for multiple episodic environments.
Experiments show that on some tasks UDRL&#x27;s performance can be surprisingly
competitive with, and even exceed that of some traditional baseline algorithms
developed over decades of research. Based on these results, we suggest that
alternative approaches to expected reward maximization have an important role
to play in training useful autonomous agents.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10623">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00636">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We learn an interactive vision-based driving policy from pre-recorded driving
logs via a model-based approach. A forward model of the world supervises a
driving policy that predicts the outcome of any potential driving trajectory.
To support learning from pre-recorded logs, we assume that the world is on
rails, meaning neither the agent nor its actions influence the environment.
This assumption greatly simplifies the learning problem, factorizing the
dynamics into a nonreactive world model and a low-dimensional and compact
forward model of the ego-vehicle. Our approach computes action-values for each
training trajectory using a tabular dynamic-programming evaluation of the
Bellman equations; these action-values in turn supervise the final vision-based
driving policy. Despite the world-on-rails assumption, the final driving policy
acts well in a dynamic and reactive world. At the time of writing, our method
ranks first on the CARLA leaderboard, attaining a 25% higher driving score
while using 40 times less data. Our method is also an order of magnitude more
sample-efficient than state-of-the-art model-free reinforcement learning
techniques on navigational tasks in the ProcGen benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Novel Multi-Centroid Template Matching Algorithm and Its Application to Cough Detection. (arXiv:2109.00630v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00630">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Cough is a major symptom of respiratory-related diseases. There exists a
tremendous amount of work in detecting coughs from audio but there has been no
effort to identify coughs from solely inertial measurement unit (IMU). Coughing
causes motion across the whole body and especially on the neck and head.
Therefore, head motion data during coughing captured by a head-worn IMU sensor
could be leveraged to detect coughs using a template matching algorithm. In
time series template matching problems, K-Nearest Neighbors (KNN) combined with
elastic distance measurement (esp. Dynamic Time Warping (DTW)) achieves
outstanding performance. However, it is often regarded as prohibitively
time-consuming. Nearest Centroid Classifier is thereafter proposed. But the
accuracy is comprised of only one centroid obtained for each class.
Centroid-based Classifier performs clustering and averaging for each cluster,
but requires manually setting the number of clusters. We propose a novel
self-tuning multi-centroid template-matching algorithm, which can automatically
adjust the number of clusters to balance accuracy and inference time. Through
experiments conducted on synthetic datasets and a real-world earbud-based cough
dataset, we demonstrate the superiority of our proposed algorithm and present
the result of cough detection with a single accelerometer sensor on the earbuds
platform.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02100">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged for their deployments to resource-limited
devices. Although recent studies have successfully discretized a full-precision
network, they still incur large quantization errors after training, thus giving
rise to a significant performance gap between a full-precision network and its
quantized counterpart. In this work, we propose a novel quantization method for
neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal
quantization grids while naturally encouraging the underlying full-precision
weights to gather around those quantization grids cohesively during training.
This property of CPQ is thanks to our two main ingredients that enable
differentiable quantization: i) the use of the categorical distribution
designed by a specific probabilistic parametrization in the forward pass and
ii) our proposed multi-class straight-through estimator (STE) in the backward
pass. Since our second component, multi-class STE, is intrinsically biased, we
additionally propose a new bit-drop technique, DropBits, that revises the
standard dropout regularization to randomly drop bits instead of neurons. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer by
imposing an additional regularization on DropBits. We experimentally validate
our method on various benchmark datasets and network architectures, and also
support a new hypothesis for quantization: learning heterogeneous quantization
levels outperforms the case using the same but fixed quantization levels from
scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks. (arXiv:2006.13866v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13866">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sampling methods (e.g., node-wise, layer-wise, or subgraph) has become an
indispensable strategy to speed up training large-scale Graph Neural Networks
(GNNs). However, existing sampling methods are mostly based on the graph
structural information and ignore the dynamicity of optimization, which leads
to high variance in estimating the stochastic gradients. The high variance
issue can be very pronounced in extremely large graphs, where it results in
slow convergence and poor generalization. In this paper, we theoretically
analyze the variance of sampling methods and show that, due to the composite
structure of empirical risk, the variance of any sampling method can be
decomposed into \textit{embedding approximation variance} in the forward stage
and \textit{stochastic gradient variance} in the backward stage that
necessities mitigating both types of variance to obtain faster convergence
rate. We propose a decoupled variance reduction strategy that employs
(approximate) gradient information to adaptively sample nodes with minimal
variance, and explicitly reduces the variance introduced by embedding
approximation. We show theoretically and empirically that the proposed method,
even with smaller mini-batch sizes, enjoys a faster convergence rate and
entails a better generalization compared to the existing methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To understand the black-box characteristics of deep networks, counterfactual
explanation that deduces not only the important features of an input space but
also how those features should be modified to classify input as a target class
has gained an increasing interest. The patterns that deep networks have learned
from a training dataset can be grasped by observing the feature variation among
various classes. However, current approaches perform the feature modification
to increase the classification probability for the target class irrespective of
the internal characteristics of deep networks. This often leads to unclear
explanations that deviate from real-world data distributions. To address this
problem, we propose a counterfactual explanation method that exploits the
statistics learned from a training dataset. Especially, we gradually construct
an explanation by iterating over masking and composition steps. The masking
step aims to select an important feature from the input data to be classified
as a target class. Meanwhile, the composition step aims to optimize the
previously selected feature by ensuring that its output score is close to the
logit space of the training data that are classified as the target class.
Experimental results show that our method produces human-friendly
interpretations on various classification datasets and verify that such
interpretations can be achieved with fewer feature modification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scalable Feature Selection for (Multitask) Gradient Boosted Trees. (arXiv:2109.01965v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01965">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking
and relevance models in search and recommendation. Considerations such as
latency and interpretability dictate the use of as few features as possible to
train these models. Feature selection in GBDT models typically involves
heuristically ranking the features by importance and selecting the top few, or
by performing a full backward feature elimination routine. On-the-fly feature
selection methods proposed previously scale suboptimally with the number of
features, which can be daunting in high dimensional settings. We develop a
scalable forward feature selection variant for GBDT, via a novel group testing
procedure that works well in high dimensions, and enjoys favorable theoretical
performance and computational guarantees. We show via extensive experiments on
both public and proprietary datasets that the proposed method offers
significant speedups in training time, while being as competitive as existing
GBDT methods in terms of model performance metrics. We also extend the method
to the multitask setting, allowing the practitioner to select common features
across tasks, as well as selecting task-specific features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Gradient Normalization for Generative Adversarial Networks. (arXiv:2109.02235v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02235">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a novel normalization method called gradient
normalization (GN) to tackle the training instability of Generative Adversarial
Networks (GANs) caused by the sharp gradient space. Unlike existing work such
as gradient penalty and spectral normalization, the proposed GN only imposes a
hard 1-Lipschitz constraint on the discriminator function, which increases the
capacity of the discriminator. Moreover, the proposed gradient normalization
can be applied to different GAN architectures with little modification.
Extensive experiments on four datasets show that GANs trained with gradient
normalization outperform existing methods in terms of both Frechet Inception
Distance and Inception Score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pandemic Drugs at Pandemic Speed: Infrastructure for Accelerating COVID-19 Drug Discovery with Hybrid Machine Learning- and Physics-based Simulations on High Performance Computers. (arXiv:2103.02843v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The race to meet the challenges of the global pandemic has served as a
reminder that the existing drug discovery process is expensive, inefficient and
slow. There is a major bottleneck screening the vast number of potential small
molecules to shortlist lead compounds for antiviral drug development. New
opportunities to accelerate drug discovery lie at the interface between machine
learning methods, in this case developed for linear accelerators, and
physics-based methods. The two in silico methods, each have their own
advantages and limitations which, interestingly, complement each other. Here,
we present an innovative infrastructural development that combines both
approaches to accelerate drug discovery. The scale of the potential resulting
workflow is such that it is dependent on supercomputing to achieve extremely
high throughput. We have demonstrated the viability of this workflow for the
study of inhibitors for four COVID-19 target proteins and our ability to
perform the required large-scale calculations to identify lead antiviral
compounds through repurposing on a variety of supercomputers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training (AT) has been demonstrated as one of the most promising
defense methods against various adversarial attacks. To our knowledge, existing
AT-based methods usually train with the locally most adversarial perturbed
points and treat all the perturbed points equally, which may lead to
considerably weaker adversarial robust generalization on test data. In this
work, we introduce a new adversarial training framework that considers the
diversity as well as characteristics of the perturbed points in the vicinity of
benign samples. To realize the framework, we propose a Regional Adversarial
Training (RAT) defense method that first utilizes the attack path generated by
the typical iterative attack method of projected gradient descent (PGD), and
constructs an adversarial region based on the attack path. Then, RAT samples
diverse perturbed training points efficiently inside this region, and utilizes
a distance-aware label smoothing mechanism to capture our intuition that
perturbed points at different locations should have different impact on the
model performance. Extensive experiments on several benchmark datasets show
that RAT consistently makes significant improvement on standard adversarial
training (SAT), and exhibits better robust generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v3 [q-bio.PE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08143">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>SARS-CoV-2, like any other virus, continues to mutate as it spreads,
according to an evolutionary process. Unlike any other virus, the number of
currently available sequences of SARS-CoV-2 in public databases such as GISAID
is already several million. This amount of data has the potential to uncover
the evolutionary dynamics of a virus like never before. However, a million is
already several orders of magnitude beyond what can be processed by the
traditional methods designed to reconstruct a virus&#x27;s evolutionary history,
such as those that build a phylogenetic tree. Hence, new and scalable methods
will need to be devised in order to make use of the ever increasing number of
viral sequences being collected.

Since identifying variants is an important part of understanding the
evolution of a virus, in this paper, we propose an approach based on clustering
sequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer
based feature vector generation and efficient feature selection methods, our
approach is effective in identifying variants, as well as being efficient and
scalable to millions of sequences. Such a clustering method allows us to show
the relative proportion of each variant over time, giving the rate of spread of
each variant in different locations -- something which is important for vaccine
development and distribution. We also compute the importance of each amino acid
position of the spike protein in identifying a given variant in terms of
information gain. Positions of high variant-specific importance tend to agree
with those reported by the USA&#x27;s Centers for Disease Control and Prevention
(CDC), further demonstrating our approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep State-Space Gaussian Processes. (arXiv:2008.04733v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper is concerned with a state-space approach to deep Gaussian process
(DGP) regression. We construct the DGP by hierarchically putting transformed
Gaussian process (GP) priors on the length scales and magnitudes of the next
level of Gaussian processes in the hierarchy. The idea of the state-space
approach is to represent the DGP as a non-linear hierarchical system of linear
stochastic differential equations (SDEs), where each SDE corresponds to a
conditional GP. The DGP regression problem then becomes a state estimation
problem, and we can estimate the state efficiently with sequential methods by
using the Markov property of the state-space DGP. The computational complexity
scales linearly with respect to the number of measurements. Based on this, we
formulate state-space MAP as well as Bayesian filtering and smoothing solutions
to the DGP regression problem. We demonstrate the performance of the proposed
models and methods on synthetic non-stationary signals and apply the
state-space DGP to detection of the gravitational waves from LIGO measurements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this article, we propose a systematic approach for fire station location
planning. We develop a machine learning model, based on Random Forest, for
demand prediction and utilize the model further to define a generalized index
to measure quality of fire service in urban settings. Our model is built upon
spatial data collected from multiple different sources. Efficacy of proper
facility planning depends on choice of candidates where fire stations can be
located along with existing stations, if any. Also, the travel time from these
candidates to demand locations need to be taken care of to maintain fire safety
standard. Here, we propose a travel time based clustering technique to identify
suitable candidates. Finally, we develop an optimization problem to select best
locations to install new fire stations. Our optimization problem is built upon
maximum coverage problem, based on integer programming. We present a detailed
experimental study of our proposed approach in collaboration with city of
Victoria Fire Department, MN, USA. Our demand prediction model achieves true
positive rate of 70% and false positive rate of 22% approximately. We aid
Victoria Fire Department to select a location for a new fire station using our
approach. We present detailed results on improvement statistics by locating a
new facility, as suggested by our methodology, in the city of Victoria.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantifying the Reproducibility of Graph Neural Networks using Multigraph Brain Data. (arXiv:2109.02248v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph neural networks (GNNs) have witnessed an unprecedented proliferation in
tackling several problems in computer vision, computer-aided diagnosis, and
related fields. While prior studies have focused on boosting the model
accuracy, quantifying the reproducibility of the most discriminative features
identified by GNNs is still an intact problem that yields concerns about their
reliability in clinical applications in particular. Specifically, the
reproducibility of biological markers across clinical datasets and distribution
shifts across classes (e.g., healthy and disordered brains) is of paramount
importance in revealing the underpinning mechanisms of diseases as well as
propelling the development of personalized treatment. Motivated by these
issues, we propose, for the first time, reproducibility-based GNN selection
(RG-Select), a framework for GNN reproducibility assessment via the
quantification of the most discriminative features (i.e., biomarkers) shared
between different models. To ascertain the soundness of our framework, the
reproducibility assessment embraces variations of different factors such as
training strategies and data perturbations. Despite these challenges, our
framework successfully yielded replicable conclusions across different training
strategies and various clinical datasets. Our findings could thus pave the way
for the development of biomarker trustworthiness and reliability assessment
methods for computer-aided diagnosis and prognosis tasks. RG-Select code is
available on GitHub at https://github.com/basiralab/RG-Select.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
spatial-temporal representation of a dynamic scene from a set of RGB images.
Key to our approach is the use of a neural implicit representation that learns
to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
consistency across different modalities, our representation enables multi-view
rendering in diverse dynamic scenes, including water pouring, robotic
interaction, and real images, outperforming state-of-the-art methods for
spatial-temporal view synthesis. Our approach works even when inputs images are
captured with only one camera. We further demonstrate that the learned
representation can serve as an implicit scene prior, enabling video processing
tasks such as image super-resolution and de-noising without any additional
supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating Leaf Water Content using Remotely Sensed Hyperspectral Data. (arXiv:2109.02250v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02250">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Plant water stress may occur due to the limited availability of water to the
roots/soil or due to increased transpiration. These factors adversely affect
plant physiology and photosynthetic ability to the extent that it has been
shown to have inhibitory effects in both growth and yield [18]. Early
identification of plant water stress status enables suitable corrective
measures to be applied to obtain the expected crop yield. Further, improving
crop yield through precision agriculture methods is a key component of climate
policy and the UN sustainable development goals [1]. Leaf water content (LWC)
is a measure that can be used to estimate water content and identify stressed
plants. LWC during the early crop growth stages is an important indicator of
plant productivity and yield. The effect of water stress can be instantaneous
[15], affecting gaseous exchange or long-term, significantly reducing [9, 18,
22]. It is thus necessary to identify potential plant water stress during the
early stages of growth [15] to introduce corrective irrigation and alleviate
stress. LWC is also useful for identifying plant genotypes that are tolerant to
water stress and salinity by measuring the stability of LWC even under
artificially induced water stress [18, 25]. Such experiments generally employ
destructive procedures to obtain the LWC, which is time-consuming and labor
intensive. Accordingly, this research has developed a non-destructive method to
estimate LWC from UAV-based hyperspectral data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Communication Efficient Federated Learning with Energy Awareness over Wireless Networks. (arXiv:2004.07351v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.07351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In federated learning (FL), reducing the communication overhead is one of the
most critical challenges since the parameter server and the mobile devices
share the training parameters over wireless links. With such consideration, we
adopt the idea of SignSGD in which only the signs of the gradients are
exchanged. Moreover, most of the existing works assume Channel State
Information (CSI) available at both the mobile devices and the parameter
server, and thus the mobile devices can adopt fixed transmission rates dictated
by the channel capacity. In this work, only the parameter server side CSI is
assumed, and channel capacity with outage is considered. In this case, an
essential problem for the mobile devices is to select appropriate local
processing and communication parameters (including the transmission rates) to
achieve a desired balance between the overall learning performance and their
energy consumption. Two optimization problems are formulated and solved, which
optimize the learning performance given the energy consumption requirement, and
vice versa. Furthermore, considering that the data may be distributed across
the mobile devices in a highly uneven fashion in FL, a stochastic sign-based
algorithm is proposed. Extensive simulations are performed to demonstrate the
effectiveness of the proposed methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Questioning the AI: Informing Design Practices for Explainable AI User Experiences. (arXiv:2001.02478v3 [cs.HC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02478">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A surge of interest in explainable AI (XAI) has led to a vast collection of
algorithmic work on the topic. While many recognize the necessity to
incorporate explainability features in AI systems, how to address real-world
user needs for understanding AI remains an open question. By interviewing 20 UX
and design practitioners working on various AI products, we seek to identify
gaps between the current XAI algorithmic work and practices to create
explainable AI products. To do so, we develop an algorithm-informed XAI
question bank in which user needs for explainability are represented as
prototypical questions users might ask about the AI, and use it as a study
probe. Our work contributes insights into the design space of XAI, informs
efforts to support design practices in this space, and identifies opportunities
for future XAI work. We also provide an extended XAI question bank and discuss
how it can be used for creating user-centered XAI.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Select Cuts for Efficient Mixed-Integer Programming. (arXiv:2105.13645v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Cutting plane methods play a significant role in modern solvers for tackling
mixed-integer programming (MIP) problems. Proper selection of cuts would remove
infeasible solutions in the early stage, thus largely reducing the
computational burden without hurting the solution accuracy. However, the major
cut selection approaches heavily rely on heuristics, which strongly depend on
the specific problem at hand and thus limit their generalization capability. In
this paper, we propose a data-driven and generalizable cut selection approach,
named Cut Ranking, in the settings of multiple instance learning. To measure
the quality of the candidate cuts, a scoring function, which takes the
instance-specific cut features as inputs, is trained and applied in cut ranking
and selection. In order to evaluate our method, we conduct extensive
experiments on both synthetic datasets and real-world datasets. Compared with
commonly used heuristics for cut selection, the learning-based policy has shown
to be more effective, and is capable of generalizing over multiple problems
with different properties. Cut Ranking has been deployed in an industrial
solver for large-scale MIPs. In the online A/B testing of the product planning
problems with more than $10^7$ variables and constraints daily, Cut Ranking has
achieved the average speedup ratio of 12.42% over the production solver without
any accuracy loss of solution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-agent online learning in time-varying games. (arXiv:1809.03066v3 [cs.GT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.03066">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We examine the long-run behavior of multi-agent online learning in games that
evolve over time. Specifically, we focus on a wide class of policies based on
mirror descent, and we show that the induced sequence of play (a) converges to
Nash equilibrium in time-varying games that stabilize in the long run to a
strictly monotone limit; and (b) it stays asymptotically close to the evolving
equilibrium of the sequence of stage games (assuming they are strongly
monotone). Our results apply to both gradient-based and payoff-based feedback -
i.e., the &quot;bandit feedback&quot; case where players only get to observe the payoffs
of their chosen actions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-Euclidean Analysis of Joint Variations in Multi-Object Shapes. (arXiv:2109.02230v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02230">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper considers joint analysis of multiple functionally related
structures in classification tasks. In particular, our method developed is
driven by how functionally correlated brain structures vary together between
autism and control groups. To do so, we devised a method based on a novel
combination of (1) non-Euclidean statistics that can faithfully represent
non-Euclidean data in Euclidean spaces and (2) a non-parametric integrative
analysis method that can decompose multi-block Euclidean data into joint,
individual, and residual structures. We find that the resulting joint structure
is effective, robust, and interpretable in recognizing the underlying patterns
of the joint variation of multi-block non-Euclidean data. We verified the
method in classifying the structural shape data collected from cases that
developed and did not develop into Autistic Spectrum Disorder (ASD).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The use of mobiles phones when driving have been a major factor when it comes
to road traffic incidents and the process of capturing such violations can be a
laborious task. Advancements in both modern object detection frameworks and
high-performance hardware has paved the way for a more automated approach when
it comes to video surveillance. In this work, we propose a custom-trained
state-of-the-art object detector to work with roadside cameras to capture
driver phone usage without the need for human intervention. The proposed
approach also addresses the issues caused by windscreen glare and introduces
the steps required to remedy this. Twelve pre-trained models are fine-tuned
with our custom dataset using four popular object detection methods: YOLO, SSD,
Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO
yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to
~30 FPS. DeepSort object tracking algorithm is also integrated into the
best-performing model to collect records of only the unique violations, and
enable the proposed approach to count the number of vehicles. The proposed
automated system will collect the output images of the identified violations,
timestamps of each violation, and total vehicle count. Data can be accessed via
a purpose-built user interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01696">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A recent work from Bello shows that training and scaling strategies may be
more significant than model architectures for visual recognition. This short
note studies effective training and scaling strategies for video recognition
models. We propose a simple scaling strategy for 3D ResNets, in combination
with improved training strategies and minor architectural changes. The
resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0
on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained
on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on
Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated
in a self-supervised setup using contrastive learning, demonstrating improved
performance. Code is available at:
https://github.com/tensorflow/models/tree/master/official.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01754">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Large-scale conversational assistants like Alexa, Siri, Cortana and Google
Assistant process every utterance using multiple models for domain, intent and
named entity recognition. Given the decoupled nature of model development and
large traffic volumes, it is extremely difficult to identify utterances
processed erroneously by such systems. We address this challenge to detect
domain classification errors using offline Transformer models. We combine
utterance encodings from a RoBERTa model with the Nbest hypothesis produced by
the production system. We then fine-tune end-to-end in a multitask setting
using a small dataset of humanannotated utterances with domain classification
errors. We tested our approach for detecting misclassifications from one domain
that accounts for &lt;0.5% of the traffic in a large-scale conversational AI
system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM
baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this
further by 2.2% to 32.2% by ensembling multiple models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Graph Neural Networks by Graphon Estimation. (arXiv:2109.01918v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we propose to train a graph neural network via resampling from
a graphon estimate obtained from the underlying network data. More
specifically, the graphon or the link probability matrix of the underlying
network is first obtained from which a new network will be resampled and used
during the training process at each layer. Due to the uncertainty induced from
the resampling, it helps mitigate the well-known issue of over-smoothing in a
graph neural network (GNN) model. Our framework is general, computationally
efficient, and conceptually simple. Another appealing feature of our method is
that it requires minimal additional tuning during the training process.
Extensive numerical results show that our approach is competitive with and in
many cases outperform the other over-smoothing reducing GNN training methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents.
Inferring occupancy from agent behaviors is an inherently multimodal problem; a
driver may behave in the same manner for different occupancy patterns ahead of
them (e.g., a driver may move at constant speed in traffic or on an open road).
Past work, however, does not account for this multimodality, thus neglecting to
model this source of aleatoric uncertainty in the relationship between driver
behaviors and their environment. We propose an occlusion inference method that
characterizes observed behaviors of human agents as sensor measurements, and
fuses them with those from a standard sensor suite. To capture the aleatoric
uncertainty, we train a conditional variational autoencoder with a discrete
latent space to learn a multimodal mapping from observed driver trajectories to
an occupancy grid representation of the view ahead of the driver. Our method
handles multi-agent scenarios, combining measurements from multiple observed
drivers using evidential theory to solve the sensor fusion problem. Our
approach is validated on a real-world dataset, outperforming baselines and
demonstrating real-time capable performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose CX-ToM, short for counterfactual explanations with theory-of mind,
a new explainable AI (XAI) framework for explaining decisions made by a deep
convolutional neural network (CNN). In contrast to the current methods in XAI
that generate explanations as a single shot response, we pose explanation as an
iterative communication process, i.e. dialog, between the machine and human
user. More concretely, our CX-ToM framework generates sequence of explanations
in a dialog by mediating the differences between the minds of machine and human
user. To do this, we use Theory of Mind (ToM) which helps us in explicitly
modeling human&#x27;s intention, machine&#x27;s mind as inferred by the human as well as
human&#x27;s mind as inferred by the machine. Moreover, most state-of-the-art XAI
frameworks provide attention (or heat map) based explanations. In our work, we
show that these attention based explanations are not sufficient for increasing
human trust in the underlying CNN model. In CX-ToM, we instead use
counterfactual explanations called fault-lines which we define as follows:
given an input image I for which a CNN classification model M predicts class
c_pred, a fault-line identifies the minimal semantic-level features (e.g.,
stripes on zebra, pointed ears of dog), referred to as explainable concepts,
that need to be added to or deleted from I in order to alter the classification
category of I by M to another specified class c_alt. We argue that, due to the
iterative, conceptual and counterfactual nature of CX-ToM explanations, our
framework is practical and more natural for both expert and non-expert users to
understand the internal workings of complex deep learning models. Extensive
quantitative and qualitative experiments verify our hypotheses, demonstrating
that our CX-ToM significantly outperforms the state-of-the-art explainable AI
models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal Aware Deep Reinforcement Learning. (arXiv:2109.02145v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02145">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The function approximators employed by traditional image based Deep
Reinforcement Learning (DRL) algorithms usually lack a temporal learning
component and instead focus on learning the spatial component. We propose a
technique wherein both temporal as well as spatial components are jointly
learned. Our tested was tested with a generic DQN and it outperformed it in
terms of maximum rewards as well as sample complexity. This algorithm has
implications in the robotics as well as sequential decision making domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Growing Cosine Unit: A Novel Oscillatory Activation Function That Can Speedup Training and Reduce Parameters in Convolutional Neural Networks. (arXiv:2108.12943v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12943">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Convolution neural networks have been successful in solving many socially
important and economically significant problems. Their ability to learn complex
high-dimensional functions hierarchically can be attributed to the use of
nonlinear activation functions. A key discovery that made training deep
networks feasible was the adoption of the Rectified Linear Unit (ReLU)
activation function to alleviate the vanishing gradient problem caused by using
saturating activation functions. Since then many improved variants of the ReLU
activation have been proposed. However a majority of activation functions used
today are non-oscillatory and monotonically increasing due to their biological
plausibility. This paper demonstrates that oscillatory activation functions can
improve gradient flow and reduce network size. It is shown that oscillatory
activation functions allow neurons to switch classification (sign of output)
within the interior of neuronal hyperplane positive and negative half-spaces
allowing complex decisions with fewer neurons. A new oscillatory activation
function C(z) &#x3D; z cos z that outperforms Sigmoids, Swish, Mish and ReLU on a
variety of architectures and benchmarks is presented. This new activation
function allows even single neurons to exhibit nonlinear decision boundaries.
This paper presents a single neuron solution to the famous XOR problem.
Experimental results indicate that replacing the activation function in the
convolutional layers with C(z) significantly improves performance on CIFAR-10,
CIFAR-100 and Imagenette.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reinforcement Learning for Battery Energy Storage Dispatch augmented with Model-based Optimizer. (arXiv:2109.01659v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01659">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Reinforcement learning has been found useful in solving optimal power flow
(OPF) problems in electric power distribution systems. However, the use of
largely model-free reinforcement learning algorithms that completely ignore the
physics-based modeling of the power grid compromises the optimizer performance
and poses scalability challenges. This paper proposes a novel approach to
synergistically combine the physics-based models with learning-based algorithms
using imitation learning to solve distribution-level OPF problems.
Specifically, we propose imitation learning based improvements in deep
reinforcement learning (DRL) methods to solve the OPF problem for a specific
case of battery storage dispatch in the power distribution systems. The
proposed imitation learning algorithm uses the approximate optimal solutions
obtained from a linearized model-based OPF solver to provide a good initial
policy for the DRL algorithms while improving the training efficiency. The
effectiveness of the proposed approach is demonstrated using IEEE 34-bus and
123-bus distribution feeders with numerous distribution-level battery storage
systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets. (arXiv:2108.05935v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The quality of training data has a huge impact on the efficiency, accuracy
and complexity of machine learning tasks. Various tools and techniques are
available that assess data quality with respect to general cleaning and
profiling checks. However these techniques are not applicable to detect data
issues in the context of machine learning tasks, like noisy labels, existence
of overlapping classes etc. We attempt to re-look at the data quality issues in
the context of building a machine learning pipeline and build a tool that can
detect, explain and remediate issues in the data, and systematically and
automatically capture all the changes applied to the data. We introduce the
Data Quality Toolkit for machine learning as a library of some key quality
metrics and relevant remediation techniques to analyze and enhance the
readiness of structured training datasets for machine learning projects. The
toolkit can reduce the turn-around times of data preparation pipelines and
streamline the data quality assessment process. Our toolkit is publicly
available via IBM API Hub [1] platform, any developer can assess the data
quality using the IBM&#x27;s Data Quality for AI apis [2]. Detailed tutorials are
also available on IBM Learning Path [3].</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributed Learning with Dependent Samples. (arXiv:2002.03757v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03757">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper focuses on learning rate analysis of distributed kernel ridge
regression for strong mixing sequences. Using a recently developed integral
operator approach and a classical covariance inequality for Banach-valued
strong mixing sequences, we succeed in deriving optimal learning rate for
distributed kernel ridge regression. As a byproduct, we also deduce a
sufficient condition for the mixing property to guarantee the optimal learning
rates for kernel ridge regression. Our results extend the applicable range of
distributed learning from i.i.d. samples to non-i.i.d. sequences.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Multi-view Multi-task Learning Framework for Multi-variate Time Series Forecasting. (arXiv:2109.01657v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01657">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-variate time series (MTS) data is a ubiquitous class of data
abstraction in the real world. Any instance of MTS is generated from a hybrid
dynamical system and their specific dynamics are usually unknown. The hybrid
nature of such a dynamical system is a result of complex external attributes,
such as geographic location and time of day, each of which can be categorized
into either spatial attributes or temporal attributes. Therefore, there are two
fundamental views which can be used to analyze MTS data, namely the spatial
view and the temporal view. Moreover, from each of these two views, we can
partition the set of data samples of MTS into disjoint forecasting tasks in
accordance with their associated attribute values. Then, samples of the same
task will manifest similar forthcoming pattern, which is less sophisticated to
be predicted in comparison with the original single-view setting. Considering
this insight, we propose a novel multi-view multi-task (MVMT) learning
framework for MTS forecasting. Instead of being explicitly presented in most
scenarios, MVMT information is deeply concealed in the MTS data, which severely
hinders the model from capturing it naturally. To this end, we develop two
kinds of basic operations, namely task-wise affine transformation and task-wise
normalization, respectively. Applying these two operations with prior knowledge
on the spatial and temporal view allows the model to adaptively extract MVMT
information while predicting. Extensive experiments on three datasets are
conducted to illustrate that canonical architectures can be greatly enhanced by
the MVMT learning framework in terms of both effectiveness and efficiency. In
addition, we design rich case studies to reveal the properties of
representations produced at different phases in the entire prediction
procedure.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Linguistic entrainment is a phenomenon where people tend to mimic each other
in conversation. The core instrument to quantify entrainment is a linguistic
similarity measure between conversational partners. Most of the current
similarity measures are based on bag-of-words approaches that rely on
linguistic markers, ignoring the overall language structure and dialogue
context. To address this issue, we propose to use a neural network model to
perform the similarity measure for entrainment. Our model is context-aware, and
it further leverages a novel component to learn the shared high-level
linguistic features across dialogues. We first investigate the effectiveness of
our novel component. Then we use the model to perform similarity measure in a
corpus-based entrainment analysis. We observe promising results for both
evaluation tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08212">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy. (arXiv:2012.02972v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Growing use of machine learning in policy and social impact settings have
raised concerns for fairness implications, especially for racial minorities.
These concerns have generated considerable interest among machine learning and
artificial intelligence researchers, who have developed new methods and
established theoretical bounds for improving fairness, focusing on the source
data, regularization and model training, or post-hoc adjustments to model
scores. However, little work has studied the practical trade-offs between
fairness and accuracy in real-world settings to understand how these bounds and
methods translate into policy choices and impact on society. Our empirical
study fills this gap by investigating the impact of mitigating disparities on
accuracy, focusing on the common context of using machine learning to inform
benefit allocation in resource-constrained programs across education, mental
health, criminal justice, and housing safety. Here we describe applied work in
which we find fairness-accuracy trade-offs to be negligible in practice. In
each setting studied, explicitly focusing on achieving equity and using our
proposed post-hoc disparity mitigation methods, fairness was substantially
improved without sacrificing accuracy. This observation was robust across
policy contexts studied, scale of resources available for intervention, time,
and relative size of the protected groups. These empirical results challenge a
commonly held assumption that reducing disparities either requires accepting an
appreciable drop in accuracy or the development of novel, complex methods,
making reducing disparities in these applications more practical.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation. (arXiv:2002.11187v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11187">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several scalable sample-based methods to compute the Kullback Leibler (KL)
divergence between two distributions have been proposed and applied in
large-scale machine learning models. While they have been found to be unstable,
the theoretical root cause of the problem is not clear. In this paper, we study
a generative adversarial network based approach that uses a neural network
discriminator to estimate KL divergence. We argue that, in such case, high
fluctuations in the estimates are a consequence of not controlling the
complexity of the discriminator function space. We provide a theoretical
underpinning and remedy for this problem by first constructing a discriminator
in the Reproducing Kernel Hilbert Space (RKHS). This enables us to leverage
sample complexity and mean embedding to theoretically relate the error
probability bound of the KL estimates to the complexity of the discriminator in
RKHS. Based on this theory, we then present a scalable way to control the
complexity of the discriminator for a reliable estimation of KL divergence. We
support both our proposed theory and method to control the complexity of the
RKHS discriminator through controlled experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10423">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as &quot;review&quot; trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the &quot;review&quot; trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Postulating Exoplanetary Habitability via a Novel Anomaly Detection Method. (arXiv:2109.02273v1 [astro-ph.EP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02273">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A profound shift in the study of cosmology came with the discovery of
thousands of exoplanets and the possibility of the existence of billions of
them in our Galaxy. The biggest goal in these searches is whether there are
other life-harbouring planets. However, the question which of these detected
planets are habitable, potentially-habitable, or maybe even inhabited, is still
not answered. Some potentially habitable exoplanets have been hypothesized, but
since Earth is the only known habitable planet, measures of habitability are
necessarily determined with Earth as the reference. Several recent works
introduced new habitability metrics based on optimization methods.
Classification of potentially habitable exoplanets using supervised learning is
another emerging area of study. However, both modeling and supervised learning
approaches suffer from drawbacks. We propose an anomaly detection method, the
Multi-Stage Memetic Algorithm (MSMA), to detect anomalies and extend it to an
unsupervised clustering algorithm MSMVMCA to use it to detect potentially
habitable exoplanets as anomalies. The algorithm is based on the postulate that
Earth is an anomaly, with the possibility of existence of few other anomalies
among thousands of data points. We describe an MSMA-based clustering approach
with a novel distance function to detect habitable candidates as anomalies
(including Earth). The results are cross-matched with the habitable exoplanet
catalog (PHL-HEC) of the Planetary Habitability Laboratory (PHL) with both
optimistic and conservative lists of potentially habitable exoplanets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FBCNN: A Deep Neural Network Architecture for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02165">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Objective: To propose a novel deep neural network (DNN) architecture -- the
filter bank convolutional neural network (FBCNN) -- to improve SSVEP
classification in single-channel BCIs with small data lengths.

Methods: We propose two models: the FBCNN-2D and the FBCNN-3D. The FBCNN-2D
utilizes a filter bank to create sub-band components of the
electroencephalography (EEG) signal, which it transforms using the fast Fourier
transform (FFT) and analyzes with a 2D CNN. The FBCNN-3D utilizes the same
filter bank, but it transforms the sub-band components into spectrograms via
short-time Fourier transform (STFT), and analyzes them with a 3D CNN. We made
use of transfer learning. To train the FBCNN-3D, we proposed a new technique,
called inter-dimensional transfer learning, to transfer knowledge from a 2D DNN
to a 3D DNN. Our BCI was conceived so as not to require calibration from the
final user: therefore, the test subject data was separated from training and
validation.

Results: The mean test accuracy was 85.7% for the FBCCA-2D and 85% for the
FBCCA-3D. Mean F1-Scores were 0.858 and 0.853. Alternative classification
methods, SVM, FBCCA and a CNN, had mean accuracy of 79.2%, 80.1% and 81.4%,
respectively.

Conclusion: The FBCNNs surpassed traditional SSVEP classification methods in
our simulated BCI, by a considerable margin (about 5% higher accuracy).
Transfer learning and inter-dimensional transfer learning made training much
faster and more predictable.

Significance: We proposed a new and flexible type of DNN, which had a better
performance than standard methods in SSVEP classification for portable and fast
BCIs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02283">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SEC4SR: A Security Analysis Platform for Speaker Recognition. (arXiv:2109.01766v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01766">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks have been expanded to speaker recognition (SR). However,
existing attacks are often assessed using different SR models, recognition
tasks and datasets, and only few adversarial defenses borrowed from computer
vision are considered. Yet,these defenses have not been thoroughly evaluated
against adaptive attacks. Thus, there is still a lack of quantitative
understanding about the strengths and limitations of adversarial attacks and
defenses. More effective defenses are also required for securing SR systems. To
bridge this gap, we present SEC4SR, the first platform enabling researchers to
systematically and comprehensively evaluate adversarial attacks and defenses in
SR. SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses
including our novel feature-level transformations. It also contains techniques
for mounting adaptive attacks. Using SEC4SR, we conduct thus far the
largest-scale empirical study on adversarial attacks and defenses in SR,
involving 23 defenses, 15 attacks and 4 attack settings. Our study provides
lots of useful findings that may advance future research: such as (1) all the
transformations slightly degrade accuracy on benign examples and their
effectiveness vary with attacks; (2) most transformations become less effective
under adaptive attacks, but some transformations become more effective; (3) few
transformations combined with adversarial training yield stronger defenses over
some but not all attacks, while our feature-level transformation combined with
adversarial training yields the strongest defense over all the attacks.
Extensive experiments demonstrate capabilities and advantages of SEC4SR which
can benefit future research in SR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identifying emotions from text is crucial for a variety of real world tasks.
We consider the two largest now-available corpora for emotion classification:
GoEmotions, with 58k messages labelled by readers, and Vent, with 33M
writer-labelled messages. We design a benchmark and evaluate several feature
spaces and learning algorithms, including two simple yet novel models on top of
BERT that outperform previous strong baselines on GoEmotions. Through an
experiment with human participants, we also analyze the differences between how
writers express emotions and how readers perceive them. Our results suggest
that emotions expressed by writers are harder to identify than emotions that
readers perceive. We share a public web interface for researchers to explore
our models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We compare the machine readability of pictograms found on Austrian and German
traffic signs. To that end, we train classification models on synthetic data
sets and evaluate their classification accuracy in a controlled setting. In
particular, we focus on differences between currently deployed pictograms in
the two countries, and a set of new pictograms designed to increase human
readability. Besides other results, we find that machine-learning models
generalize poorly to data sets with pictogram designs they have not been
trained on. We conclude that manufacturers of advanced driver-assistance
systems (ADAS) must take special care to properly address small visual
differences between current and newly designed traffic sign pictograms, as well
as between pictograms from different countries.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervised DKRC with Images for Offline System Identification. (arXiv:2109.02241v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02241">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Koopman spectral theory has provided a new perspective in the field of
dynamical systems in recent years. Modern dynamical systems are becoming
increasingly non-linear and complex, and there is a need for a framework to
model these systems in a compact and comprehensive representation for
prediction and control. The central problem in applying Koopman theory to a
system of interest is that the choice of finite-dimensional basis functions is
typically done apriori, using expert knowledge of the systems dynamics. Our
approach learns these basis functions using a supervised learning approach
where a combination of autoencoders and deep neural networks learn the basis
functions for any given system. We demonstrate this approach on a simple
pendulum example in which we obtain a linear representation of the non-linear
system and then predict the future state trajectories given some initial
conditions. We also explore how changing the input representation of the
dynamic systems time series data can impact the quality of learned basis
functions. This alternative representation is compared to the traditional raw
time series data approach to determine which method results in lower
reconstruction and prediction error of the true non-linear dynamics of the
system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;&#x3D; 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MLCTR: A Fast Scalable Coupled Tensor Completion Based on Multi-Layer Non-Linear Matrix Factorization. (arXiv:2109.01773v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Firms earning prediction plays a vital role in investment decisions,
dividends expectation, and share price. It often involves multiple
tensor-compatible datasets with non-linear multi-way relationships,
spatiotemporal structures, and different levels of sparsity. Current non-linear
tensor completion algorithms tend to learn noisy embedding and incur
overfitting. This paper focuses on the embedding learning aspect of the tensor
completion problem and proposes a new multi-layer neural network architecture
for tensor factorization and completion (MLCTR). The network architecture
entails multiple advantages: a series of low-rank matrix factorizations (MF)
building blocks to minimize overfitting, interleaved transfer functions in each
layer for non-linearity, and by-pass connections to reduce the gradient
diminishing problem and increase the depths of neural networks. Furthermore,
the model employs Stochastic Gradient Descent(SGD) based optimization for fast
convergence in training. Our algorithm is highly efficient for imputing missing
values in the EPS data. Experiments confirm that our strategy of incorporating
non-linearity in factor matrices demonstrates impressive performance in
embedding learning and end-to-end tensor models, and outperforms approaches
with non-linearity in the phase of reconstructing tensors from factor matrices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Barycenteric distribution alignment and manifold-restricted invertibility for domain generalization. (arXiv:2109.01902v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01902">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>For the Domain Generalization (DG) problem where the hypotheses are composed
of a common representation function followed by a labeling function, we point
out a shortcoming in existing approaches that fail to explicitly optimize for a
term, appearing in a well-known and widely adopted upper bound to the risk on
the unseen domain, that is dependent on the representation to be learned. To
this end, we first derive a novel upper bound to the prediction risk. We show
that imposing a mild assumption on the representation to be learned, namely
manifold restricted invertibility, is sufficient to deal with this issue.
Further, unlike existing approaches, our novel upper bound doesn&#x27;t require the
assumption of Lipschitzness of the loss function. In addition, the
distributional discrepancy in the representation space is handled via the
Wasserstein-2 barycenter cost. In this context, we creatively leverage old and
recent transport inequalities, which link various optimal transport metrics, in
particular the $L^1$ distance (also known as the total variation distance) and
the Wasserstein-2 distances, with the Kullback-Liebler divergence. These
analyses and insights motivate a new representation learning cost for DG that
additively balances three competing objectives: 1) minimizing classification
error across seen domains via cross-entropy, 2) enforcing domain-invariance in
the representation space via the Wasserstein-2 barycenter cost, and 3)
promoting non-degenerate, nearly-invertible representation via one of two
mechanisms, viz., an autoencoder-based reconstruction loss or a mutual
information loss. It is to be noted that the proposed algorithms completely
bypass the use of any adversarial training mechanism that is typical of many
current domain generalization approaches. Simulation results on several
standard datasets demonstrate superior performance compared to several
well-known DG algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters. (arXiv:2109.01313v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01313">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Modern GPU datacenters are critical for delivering Deep Learning (DL) models
and services in both the research community and industry. When operating a
datacenter, optimization of resource scheduling and management can bring
significant financial benefits. Achieving this goal requires a deep
understanding of the job features and user behaviors. We present a
comprehensive study about the characteristics of DL jobs and resource
management. First, we perform a large-scale analysis of real-world job traces
from SenseTime. We uncover some interesting conclusions from the perspectives
of clusters, jobs and users, which can facilitate the cluster system designs.
Second, we introduce a general-purpose framework, which manages resources based
on historical data. As case studies, we design: a Quasi-Shortest-Service-First
scheduling service, which can minimize the cluster-wide average job completion
time by up to 6.5x; and a Cluster Energy Saving service, which improves overall
cluster utilization by up to 13%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Temporal set prediction is becoming increasingly important as many companies
employ recommender systems in their online businesses, e.g., personalized
purchase prediction of shopping baskets. While most previous techniques have
focused on leveraging a user&#x27;s history, the study of combining it with others&#x27;
histories remains untapped potential. This paper proposes Global-Local Item
Embedding (GLOIE) that learns to utilize the temporal properties of sets across
whole users as well as within a user by coining the names as global and local
information to distinguish the two temporal patterns. GLOIE uses Variational
Autoencoder (VAE) and dynamic graph-based model to capture global and local
information and then applies attention to integrate resulting item embeddings.
Additionally, we propose to use Tweedie output for the decoder of VAE as it can
easily model zero-inflated and long-tailed distribution, which is more suitable
for several real-world data distributions than Gaussian or multinomial
counterparts. When evaluated on three public benchmarks, our algorithm
consistently outperforms previous state-of-the-art methods in most ranking
metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08481">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The classical development of neural networks has primarily focused on
learning mappings between finite dimensional Euclidean spaces or finite sets.
We propose a generalization of neural networks tailored to learn operators
mapping between infinite dimensional function spaces. We formulate the
approximation of operators by composition of a class of linear integral
operators and nonlinear activation functions, so that the composed operator can
approximate complex nonlinear operators. We prove a universal approximation
theorem for our construction. Furthermore, we introduce four classes of
operator parameterizations: graph-based operators, low-rank operators,
multipole graph-based operators, and Fourier operators and describe efficient
algorithms for computing with each one. The proposed neural operators are
resolution-invariant: they share the same network parameters between different
discretizations of the underlying function spaces and can be used for zero-shot
super-resolutions. Numerically, the proposed models show superior performance
compared to existing machine learning based methodologies on Burgers&#x27; equation,
Darcy flow, and the Navier-Stokes equation, while being several order of
magnitude faster compared to conventional PDE solvers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning. (arXiv:2109.02355v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02355">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.

Understanding learning in this overparameterized regime requires new theory
and foundational empirical studies, even for the simplest case of the linear
model. The underpinnings of this understanding have been laid in very recent
analyses of overparameterized linear regression and related statistical
learning tasks, which resulted in precise analytic characterizations of double
descent. This paper provides a succinct overview of this emerging theory of
overparameterized ML (henceforth abbreviated as TOPML) that explains these
recent findings through a statistical signal processing perspective. We
emphasize the unique aspects that define the TOPML research area as a subfield
of modern ML theory and outline interesting open questions that remain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning. (arXiv:1910.01723v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.01723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is notoriously difficult to control the behavior of reinforcement learning
agents. Agents often learn to exploit the environment or reward signal and need
to be retrained multiple times. The multi-objective reinforcement learning
(MORL) framework separates a reward function into several objectives. An ideal
MORL agent learns to generalize to novel combinations of objectives allowing
for better control of an agent&#x27;s behavior without requiring retraining. Many
MORL approaches use a weight vector to parameterize the importance of each
objective. However, this approach suffers from lack of expressiveness and
interpretability. We propose using propositional logic to specify the
importance of multiple objectives. By using a logic where predicates correspond
directly to objectives, specifications are inherently more interpretable.
Additionally the set of specifications that can be expressed with formal
languages is a superset of what can be expressed by weight vectors. In this
paper, we define a formal language based on propositional logic with
quantitative semantics. We encode logical specifications using a recurrent
neural network and show that MORL agents parameterized by these encodings are
able to generalize to novel specifications over objectives and achieve
performance comparable to single objective baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Thompson Sampling for Bandits with Clustered Arms. (arXiv:2109.01656v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01656">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose algorithms based on a multi-level Thompson sampling scheme, for
the stochastic multi-armed bandit and its contextual variant with linear
expected rewards, in the setting where arms are clustered. We show, both
theoretically and empirically, how exploiting a given cluster structure can
significantly improve the regret and computational cost compared to using
standard Thompson sampling. In the case of the stochastic multi-armed bandit we
give upper bounds on the expected cumulative regret showing how it depends on
the quality of the clustering. Finally, we perform an empirical evaluation
showing that our algorithms perform well compared to previously proposed
algorithms for bandits with clustered arms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sense and Learn: Self-Supervision for Omnipresent Sensors. (arXiv:2009.13233v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13233">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning general-purpose representations from multisensor data produced by
the omnipresent sensing systems (or IoT in general) has numerous applications
in diverse use cases. Existing purely supervised end-to-end deep learning
techniques depend on the availability of a massive amount of well-curated data,
acquiring which is notoriously difficult but required to achieve a sufficient
level of generalization on a task of interest. In this work, we leverage the
self-supervised learning paradigm towards realizing the vision of continual
learning from unlabeled inputs. We present a generalized framework named Sense
and Learn for representation or feature learning from raw sensory data. It
consists of several auxiliary tasks that can learn high-level and broadly
useful features entirely from unannotated data without any human involvement in
the tedious labeling process. We demonstrate the efficacy of our approach on
several publicly available datasets from different domains and in various
settings, including linear separability, semi-supervised or few shot learning,
and transfer learning. Our methodology achieves results that are competitive
with the supervised approaches and close the gap through fine-tuning a network
while learning the downstream tasks in most cases. In particular, we show that
the self-supervised network can be utilized as initialization to significantly
boost the performance in a low-data regime with as few as 5 labeled instances
per class, which is of high practical importance to real-world problems.
Likewise, the learned representations with self-supervision are found to be
highly transferable between related datasets, even when few labeled instances
are available from the target domains. The self-learning nature of our
methodology opens up exciting possibilities for on-device continual learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Connecting GANs, MFGs, and OT. (arXiv:2002.04112v4 [cs.GT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.04112">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative adversarial networks (GANs) have enjoyed tremendous success in
image generation and processing, and have recently attracted growing interests
in financial modelings. This paper analyzes GANs from the perspectives of
mean-field games (MFGs) and optimal transport. More specifically, from the game
theoretical perspective, GANs are interpreted as MFGs under Pareto Optimality
criterion or mean-field controls; from the optimal transport perspective, GANs
are to minimize the optimal transport cost indexed by the generator from the
known latent distribution to the unknown true distribution of data. The MFGs
perspective of GANs leads to a GAN-based computational method (MFGANs) to solve
MFGs: one neural network for the backward Hamilton-Jacobi-Bellman equation and
one neural network for the forward Fokker-Planck equation, with the two neural
networks trained in an adversarial way. Numerical experiments demonstrate
superior performance of this proposed algorithm, especially in the higher
dimensional case, when compared with existing neural network approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. (arXiv:2010.05073v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05073">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Access to high-quality data repositories and benchmarks have been
instrumental in advancing the state of the art in many experimental research
domains. While advanced analytics tasks over time series data have been gaining
lots of attention, lack of such community resources severely limits scientific
progress. In this paper, we present Exathlon, the first comprehensive public
benchmark for explainable anomaly detection over high-dimensional time series
data. Exathlon has been systematically constructed based on real data traces
from repeated executions of large-scale stream processing jobs on an Apache
Spark cluster. Some of these executions were intentionally disturbed by
introducing instances of six different types of anomalous events (e.g.,
misbehaving inputs, resource contention, process failures). For each of the
anomaly instances, ground truth labels for the root cause interval as well as
those for the extended effect interval are provided, supporting the development
and evaluation of a wide range of anomaly detection (AD) and explanation
discovery (ED) tasks. We demonstrate the practical utility of Exathlon&#x27;s
dataset, evaluation methodology, and end-to-end data science pipeline design
through an experimental study with three state-of-the-art AD and ED techniques.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">High-quality Thermal Gibbs Sampling with Quantum Annealing Hardware. (arXiv:2109.01690v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01690">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantum Annealing (QA) was originally intended for accelerating the solution
of combinatorial optimization tasks that have natural encodings as Ising
models. However, recent experiments on QA hardware platforms have demonstrated
that, in the operating regime corresponding to weak interactions, the QA
hardware behaves like a noisy Gibbs sampler at a hardware-specific effective
temperature. This work builds on those insights and identifies a class of small
hardware-native Ising models that are robust to noise effects and proposes a
novel procedure for executing these models on QA hardware to maximize Gibbs
sampling performance. Experimental results indicate that the proposed protocol
results in high-quality Gibbs samples from a hardware-specific effective
temperature and that the QA annealing time can be used to adjust the effective
temperature of the output distribution. The procedure proposed in this work
provides a new approach to using QA hardware for Ising model sampling
presenting potential new opportunities for applications in machine learning and
physics simulation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards a Common Testing Terminology for Software Engineering and Artificial Intelligence Experts. (arXiv:2108.13837v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Analytical quality assurance, especially testing, is an integral part of
software-intensive system development. With the increased usage of Artificial
Intelligence (AI) and Machine Learning (ML) as part of such systems, this
becomes more difficult as well-understood software testing approaches cannot be
applied directly to the AI-enabled parts of the system. The required adaptation
of classical testing approaches and development of new concepts for AI would
benefit from a deeper understanding and exchange between AI and software
engineering experts. A major obstacle on this way, we see in the different
terminologies used in the two communities. As we consider a mutual
understanding of the testing terminology as a key, this paper contributes a
mapping between the most important concepts from classical software testing and
AI testing. In the mapping, we highlight differences in relevance and naming of
the mapped concepts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This research project investigates the application of deep learning to timbre
transfer, where the timbre of a source audio can be converted to the timbre of
a target audio with minimal loss in quality. The adopted approach combines
Variational Autoencoders with Generative Adversarial Networks to construct
meaningful representations of the source audio and produce realistic
generations of the target audio and is applied to the Flickr 8k Audio dataset
for transferring the vocal timbre between speakers and the URMP dataset for
transferring the musical timbre between instruments. Furthermore, variations of
the adopted approach are trained, and generalised performance is compared using
the metrics SSIM (Structural Similarity Index) and FAD (Frech\&#x27;et Audio
Distance). It was found that a many-to-many approach supersedes a one-to-one
approach in terms of reconstructive capabilities, and that the adoption of a
basic over a bottleneck residual block design is more suitable for enriching
content information about a latent space. It was also found that the decision
on whether cyclic loss takes on a variational autoencoder or vanilla
autoencoder approach does not have a significant impact on reconstructive and
adversarial translation aspects of the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast Hypergraph Regularized Nonnegative Tensor Ring Factorization Based on Low-Rank Approximation. (arXiv:2109.02314v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For the high dimensional data representation, nonnegative tensor ring (NTR)
decomposition equipped with manifold learning has become a promising model to
exploit the multi-dimensional structure and extract the feature from tensor
data. However, the existing methods such as graph regularized tensor ring
decomposition (GNTR) only models the pair-wise similarities of objects. For
tensor data with complex manifold structure, the graph can not exactly
construct similarity relationships. In this paper, in order to effectively
utilize the higher-dimensional and complicated similarities among objects, we
introduce hypergraph to the framework of NTR to further enhance the feature
extraction, upon which a hypergraph regularized nonnegative tensor ring
decomposition (HGNTR) method is developed. To reduce the computational
complexity and suppress the noise, we apply the low-rank approximation trick to
accelerate HGNTR (called LraHGNTR). Our experimental results show that compared
with other state-of-the-art algorithms, the proposed HGNTR and LraHGNTR can
achieve higher performance in clustering tasks, in addition, LraHGNTR can
greatly reduce running time without decreasing accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Faster Convergence of Scaled Sign Gradient Descent. (arXiv:2109.01806v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Communication has been seen as a significant bottleneck in industrial
applications over large-scale networks. To alleviate the communication burden,
sign-based optimization algorithms have gained popularity recently in both
industrial and academic communities, which is shown to be closely related to
adaptive gradient methods, such as Adam. Along this line, this paper
investigates faster convergence for a variant of sign-based gradient descent,
called scaled signGD, in three cases: 1) the objective function is strongly
convex; 2) the objective function is nonconvex but satisfies the
Polyak-Lojasiewicz (PL) inequality; 3) the gradient is stochastic, called
scaled signGD in this case. For the first two cases, it can be shown that the
scaled signGD converges at a linear rate. For case 3), the algorithm is shown
to converge linearly to a neighborhood of the optimal value when a constant
learning rate is employed, and the algorithm converges at a rate of $O(1/k)$
when using a diminishing learning rate, where $k$ is the iteration number. The
results are also extended to the distributed setting by majority vote in a
parameter-server framework. Finally, numerical experiments on logistic
regression are performed to corroborate the theoretical findings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms. (arXiv:2109.01768v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01768">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With AlphaGo defeats top human players, reinforcement learning(RL) algorithms
have gradually become the code-base of building stronger artificial
intelligence(AI). The RL algorithm design firstly needs to adapt to the
specific environment, so the designed environment guides the rapid and profound
development of RL algorithms. However, the existing environments, which can be
divided into real world games and customized toy environments, have obvious
shortcomings. For real world games, it is designed for human entertainment, and
too much difficult for most of RL researchers. For customized toy environments,
there is no widely accepted unified evaluation standard for all RL algorithms.
Therefore, we introduce the first virtual user-friendly environment framework
for RL. In this framework, the environment can be easily configured to realize
all kinds of RL tasks in the mainstream research. Then all the mainstream
state-of-the-art(SOTA) RL algorithms can be conveniently evaluated and
compared. Therefore, our contributions mainly includes the following aspects:
1.single configured environment for all classification of SOTA RL algorithms;
2.combined environment of more than one classification RL algorithms; 3.the
evaluation standard for all kinds of RL algorithms. With all these efforts, a
possibility for breeding an AI with capability of general competency in a
variety of tasks is provided, and maybe it will open up a new chapter for AI.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01844">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Catastrophic forgetting of previously learned knowledge while learning new
tasks is a widely observed limitation of contemporary neural networks. Although
many continual learning methods are proposed to mitigate this drawback, the
main question remains unanswered: what is the root cause of catastrophic
forgetting? In this work, we aim at answering this question by posing and
validating a set of research hypotheses related to the specificity of
representations built internally by neural models. More specifically, we design
a set of empirical evaluations that compare the robustness of representations
in discriminative and generative models against catastrophic forgetting. We
observe that representations learned by discriminative models are more prone to
catastrophic forgetting than their generative counterparts, which sheds new
light on the advantages of developing generative models for continual learning.
Finally, our work opens new research pathways and possibilities to adopt
generative models in continual learning beyond mere replay mechanisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mean-Variance Efficient Reinforcement Learning by Expected Quadratic Utility Maximization. (arXiv:2010.01404v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01404">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Risk management is critical in decision making, and mean-variance (MV)
trade-off is one of the most common criteria. However, in reinforcement
learning (RL) for sequential decision making under uncertainty, most of the
existing methods for MV control suffer from computational difficulties caused
by the double sampling problem. In this paper, in contrast to strict MV
control, we consider learning MV efficient policies that achieve Pareto
efficiency regarding MV trade-off. To achieve this purpose, we train an agent
to maximize the expected quadratic utility function, a common objective of risk
management in finance and economics. We call our approach direct expected
quadratic utility maximization (EQUM). The EQUM does not suffer from the double
sampling issue because it does not include gradient estimation of variance. We
confirm that the maximizer of the objective in the EQUM directly corresponds to
an MV efficient policy under a certain condition. We conduct experiments with
benchmark settings to demonstrate the effectiveness of the EQUM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we introduce pixel wise tensor normalization, which is inserted
after rectifier linear units and, together with batch normalization, provides a
significant improvement in the accuracy of modern deep neural networks. In
addition, this work deals with the robustness of networks. We show that the
factorized superposition of images from the training set and the reformulation
of the multi class problem into a multi-label problem yields significantly more
robust networks. The reformulation and the adjustment of the multi class log
loss also improves the results compared to the overlay with only one class as
label.
https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p&#x3D;%2FTNandFDT&amp;mode&#x3D;list</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spike and slab variational Bayes for high dimensional logistic regression. (arXiv:2010.11665v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11665">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Variational Bayes (VB) is a popular scalable alternative to Markov chain
Monte Carlo for Bayesian inference. We study a mean-field spike and slab VB
approximation of widely used Bayesian model selection priors in sparse
high-dimensional logistic regression. We provide non-asymptotic theoretical
guarantees for the VB posterior in both $\ell_2$ and prediction loss for a
sparse truth, giving optimal (minimax) convergence rates. Since the VB
algorithm does not depend on the unknown truth to achieve optimality, our
results shed light on effective prior choices. We confirm the improved
performance of our VB algorithm over common sparse VB approaches in a numerical
study.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Provably Correct Training of Neural Network Controllers Using Reachability Analysis. (arXiv:2102.10806v2 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we consider the problem of training neural network (NN)
controllers for nonlinear dynamical systems that are guaranteed to satisfy
safety and liveness (e.g., reach-avoid) properties. Our approach is to combine
model-based design methodologies for dynamical systems with data-driven
approaches to achieve this target. We confine our attention to NNs with
Rectifier Linear Unit (ReLU) nonlinearity which are known to represent
Continuous Piece-Wise Affine (CPWA) functions. Given a mathematical model of
the dynamical system, we compute a finite-state abstract model that captures
the closed-loop behavior under all possible CPWA controllers. Using this
finite-state abstract model, our framework identifies a family of CPWA
functions guaranteed to satisfy the safety requirements. We augment the
learning algorithm with a NN weight projection operator during training that
enforces the resulting NN to represent a CPWA function from the provably safe
family of CPWA functions. Moreover, the proposed framework uses the
finite-state abstract model to identify candidate CPWA functions that may
satisfy the liveness properties. Using such candidate CPWA functions, the
proposed framework biases the NN training to achieve the liveness
specification. We show the efficacy of the proposed framework both in
simulation and on an actual robotic vehicle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the stability properties of Gated Recurrent Units neural networks. (arXiv:2011.06806v5 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The goal of this paper is to provide sufficient conditions for guaranteeing
the Input-to-State Stability (ISS) and the Incremental Input-to-State Stability
({\delta}ISS) of Gated Recurrent Units (GRUs) neural networks. These
conditions, devised for both single-layer and multi-layer architectures,
consist of nonlinear inequalities on network&#x27;s weights. They can be employed to
check the stability of trained networks, or can be enforced as constraints
during the training procedure of a GRU. The resulting training procedure is
tested on a Quadruple Tank nonlinear benchmark system, showing satisfactory
modeling performances.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nonparametric Extrema Analysis in Time Series for Envelope Extraction, Peak Detection and Clustering. (arXiv:2109.02082v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02082">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a nonparametric approach that can be used in
envelope extraction, peak-burst detection and clustering in time series. Our
problem formalization results in a naturally defined splitting/forking of the
time series. With a possibly hierarchical implementation, it can be used for
various applications in machine learning, signal processing and mathematical
finance. From an incoming input signal, our iterative procedure sequentially
creates two signals (one upper bounding and one lower bounding signal) by
minimizing the cumulative $L_1$ drift. We show that a solution can be
efficiently calculated by use of a Viterbi-like path tracking algorithm
together with an optimal elimination rule. We consider many interesting
settings, where our algorithm has near-linear time complexities.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>

  <footer>
    <time id="build-timestamp" datetime="2021-09-08T07:20:13.098Z">2021-09-08T07:20:13.098Z</time>
    <span><a class="footer-link" href="https://github.com/osmoscraft/osmosfeed">osmosfeed 1.11.0</a></span>
  </footer>
  <script src="index.js"></script>
  <!-- %before-body-end.html% -->
</body>

</html>