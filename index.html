<!DOCTYPE html>
<html lang="en">

<head>
  <title>osmos::feed</title>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="noindex, nofollow" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />
  <link rel="alternate" type="application/rss+xml" title="osmos::feed" href="feed.atom" />
  <link href="index.css" rel="stylesheet" />
  <!-- %before-head-end.html% -->
</head>

<body>
<!-- %after-body-begin.html% -->
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-14">2021-09-14</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The cross-lingual language models are typically pretrained with masked
language modeling on multilingual text or parallel sentences. In this paper, we
introduce denoising word alignment as a new cross-lingual pre-training task.
Specifically, the model first self-labels word alignments for parallel
sentences. Then we randomly mask tokens in a bitext pair. Given a masked token,
the model uses a pointer network to predict the aligned token in the other
language. We alternately perform the above two steps in an
expectation-maximization manner. Experimental results show that our method
improves cross-lingual transferability on various datasets, especially on the
token-level tasks, such as question answering, and structured prediction.
Moreover, the model can serve as a pretrained word aligner, which achieves
reasonably low error rates on the alignment benchmarks. The code and pretrained
parameters are available at https://github.com/CZWin32768/XLM-Align.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Representations from large pretrained models such as BERT encode a range of
features into monolithic vectors, affording strong predictive accuracy across a
multitude of downstream tasks. In this paper we explore whether it is possible
to learn disentangled representations by identifying existing subnetworks
within pretrained models that encode distinct, complementary aspect
representations. Concretely, we learn binary masks over transformer weights or
hidden units to uncover subsets of features that correlate with a specific
factor of variation; this eliminates the need to train a disentangled model
from scratch for a particular task. We evaluate this method with respect to its
ability to disentangle representations of sentiment from genre in movie
reviews, &quot;toxicity&quot; from dialect in Tweets, and syntax from semantics.

By combining masking with magnitude pruning we find that we can identify
sparse subnetworks within BERT that strongly encode particular aspects (e.g.,
toxicity) while only weakly encoding others (e.g., race). Moreover, despite
only learning masks, we find that disentanglement-via-masking performs as well
as -- and often better than -- previously proposed methods based on variational
autoencoders and adversarial training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent neural text-to-SQL models can effectively translate natural language
questions to corresponding SQL queries on unseen databases. Working mostly on
the Spider dataset, researchers have proposed increasingly sophisticated
solutions to the problem. Contrary to this trend, in this paper we focus on
simplifications. We begin by building DuoRAT, a re-implementation of the
state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware
or vanilla transformers as the building blocks. We perform several ablation
experiments using DuoRAT as the baseline model. Our experiments confirm the
usefulness of some techniques and point out the redundancy of others, including
structural SQL features and features that link the question with the schema.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12885">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Paraphrase generation has benefited extensively from recent progress in the
designing of training objectives and model architectures. However, previous
explorations have largely focused on supervised methods, which require a large
amount of labeled data that is costly to collect. To address this drawback, we
adopt a transfer learning approach and propose a training pipeline that enables
pre-trained language models to generate high-quality paraphrases in an
unsupervised setting. Our recipe consists of task-adaptation, self-supervision,
and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a
surface form dissimilar from the input, whenever the language model emits a
token contained in the source sequence, DB prevents the model from outputting
the subsequent source token for the next generation step. We show with
automatic and human evaluations that our approach achieves state-of-the-art
performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and
is robust to domain shift between the two datasets of distinct distributions.
We also demonstrate that our model transfers to paraphrasing in other languages
without any additional finetuning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08692">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive
monolingual texts, which has shown promising results on many cross-lingual
tasks. In this paper, we improve multilingual text-to-text transfer Transformer
with translation pairs (mT6). Specifically, we explore three cross-lingual
text-to-text pre-training tasks, namely, machine translation, translation pair
span corruption, and translation span corruption. In addition, we propose a
partially non-autoregressive objective for text-to-text pre-training. We
evaluate the methods on eight multilingual benchmark datasets, including
sentence classification, named entity recognition, question answering, and
abstractive summarization. Experimental results show that the proposed mT6
improves cross-lingual transferability over mT5.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Can NLI Models Verify QA Systems&#x27; Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08731">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To build robust question answering systems, we need the ability to verify
whether answers to questions are truly correct, not just &quot;good enough&quot; in the
context of imperfect QA datasets. We explore the use of natural language
inference (NLI) as a way to achieve this goal, as NLI inherently requires the
premise (document context) to contain all necessary information to support the
hypothesis (proposed answer to the question). We leverage large pre-trained
models and recent prior datasets to construct powerful question converter and
decontextualization modules, which can reformulate QA instances as
premise-hypothesis pairs with very high reliability. Then, by combining
standard NLI datasets with NLI examples automatically derived from QA training
data, we can train NLI models to judge the correctness of QA models&#x27; proposed
answers. We show that our NLI approach can generally improve the confidence
estimation of a QA model across different domains, evaluated in a selective QA
setting. Careful manual analysis over the predictions of our NLI model shows
that it can further identify cases where the QA model produces the right answer
for the wrong reason, or where the answer cannot be verified as addressing all
aspects of the question.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07467">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Stance detection concerns the classification of a writer&#x27;s viewpoint towards
a target. There are different task variants, e.g., stance of a tweet vs. a full
article, or stance with respect to a claim vs. an (implicit) topic. Moreover,
task definitions vary, which includes the label inventory, the data collection,
and the annotation protocol. All these aspects hinder cross-domain studies, as
they require changes to standard domain adaptation approaches. In this paper,
we perform an in-depth analysis of 16 stance detection datasets, and we explore
the possibility for cross-domain learning from them. Moreover, we propose an
end-to-end unsupervised framework for out-of-domain prediction of unseen,
user-defined labels. In particular, we combine domain adaptation techniques
such as mixture of experts and domain-adversarial training with label
embeddings, and we demonstrate sizable performance gains over strong baselines,
both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for
unseen targets. Finally, we perform an exhaustive analysis of the cross-domain
results, and we highlight the important factors influencing the model
performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07571">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The goal of question answering (QA) is to answer any question. However, major
QA datasets have skewed distributions over gender, profession, and nationality.
Despite that skew, model accuracy analysis reveals little evidence that
accuracy is lower for people based on gender or nationality; instead, there is
more variation on professions (question topic). But QA&#x27;s lack of representation
could itself hide evidence of bias, necessitating QA datasets that better
represent global diversity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05460">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Successful conversational search systems can present natural, adaptive and
interactive shopping experience for online shopping customers. However,
building such systems from scratch faces real word challenges from both
imperfect product schema/knowledge and lack of training dialog data.In this
work we first propose ConvSearch, an end-to-end conversational search system
that deeply combines the dialog system with search. It leverages the text
profile to retrieve products, which is more robust against imperfect product
schema/knowledge compared with using product attributes alone. We then address
the lack of data challenges by proposing an utterance transfer approach that
generates dialogue utterances by using existing dialog from other domains, and
leveraging the search behavior data from e-commerce retailer. With utterance
transfer, we introduce a new conversational search dataset for online shopping.
Experiments show that our utterance transfer method can significantly improve
the availability of training dialogue data without crowd-sourcing, and the
conversational search system significantly outperformed the best tested
baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08366">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Pre-trained models for programming language have achieved dramatic empirical
improvements on a variety of code-related tasks such as code search, code
completion, code summarization, etc. However, existing pre-trained models
regard a code snippet as a sequence of tokens, while ignoring the inherent
structure of code, which provides crucial code semantics and would enhance the
code understanding process. We present GraphCodeBERT, a pre-trained model for
programming language that considers the inherent structure of code. Instead of
taking syntactic-level structure of code like abstract syntax tree (AST), we
use data flow in the pre-training stage, which is a semantic-level structure of
code that encodes the relation of &quot;where-the-value-comes-from&quot; between
variables. Such a semantic-level structure is neat and does not bring an
unnecessarily deep hierarchy of AST, the property of which makes the model more
efficient. We develop GraphCodeBERT based on Transformer. In addition to using
the task of masked language modeling, we introduce two structure-aware
pre-training tasks. One is to predict code structure edges, and the other is to
align representations between source code and code structure. We implement the
model in an efficient way with a graph-guided masked attention function to
incorporate the code structure. We evaluate our model on four tasks, including
code search, clone detection, code translation, and code refinement. Results
show that code structure and newly introduced pre-training tasks can improve
GraphCodeBERT and achieves state-of-the-art performance on the four downstream
tasks. We further show that the model prefers structure-level attentions over
token-level attentions in the task of code search.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02397">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, &gt;, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.

We restrict our attention on CCSPs with addition constraints ($x + y &#x3D; z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y &#x3D; z$), squaring constraints ($x^2 &#x3D; y$),
or inversion constraints ($x\cdot y &#x3D; 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) &#x3D; 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.

We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05611">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a novel scheme to use the Levenshtein Transformer to perform the
task of word-level quality estimation. A Levenshtein Transformer is a natural
fit for this task: trained to perform decoding in an iterative manner, a
Levenshtein Transformer can learn to post-edit without explicit supervision. To
further minimize the mismatch between the translation task and the word-level
QE task, we propose a two-stage transfer learning procedure on both augmented
data and human post-editing data. We also propose heuristics to construct
reference labels that are compatible with subword-level finetuning and
inference. Results on WMT 2020 QE shared task dataset show that our proposed
method has superior data efficiency under the data-constrained setting and
competitive performance under the unconstrained setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.03760">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Disentanglement is a problem in which multiple conversations occur in the
same channel simultaneously, and the listener should decide which utterance is
part of the conversation he will respond to. We propose a new model, named
Dialogue BERT (DialBERT), which integrates local and global semantics in a
single stream of messages to disentangle the conversations that mixed together.
We employ BERT to capture the matching information in each utterance pair at
the utterance-level, and use a BiLSTM to aggregate and incorporate the
context-level information. With only a 3% increase in parameters, a 12%
improvement has been attained in comparison to BERT, based on the F1-Score. The
model achieves a state-of-the-art result on the a new dataset proposed by IBM
and surpasses previous work by a substantial margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06239">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Discontinuous constituent parsers have always lagged behind continuous
approaches in terms of accuracy and speed, as the presence of constituents with
discontinuous yield introduces extra complexity to the task. However, a
discontinuous tree can be converted into a continuous variant by reordering
tokens. Based on that, we propose to reduce discontinuous parsing to a
continuous problem, which can then be directly solved by any off-the-shelf
continuous parser. To that end, we develop a Pointer Network capable of
accurately generating the continuous token arrangement for a given input
sentence and define a bijective function to recover the original order.
Experiments on the main benchmarks with two continuous parsers prove that our
approach is on par in accuracy with purely discontinuous state-of-the-art
algorithms, but considerably faster.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05620">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To audit the robustness of named entity recognition (NER) models, we propose
RockNER, a simple yet effective method to create natural adversarial examples.
Specifically, at the entity level, we replace target entities with other
entities of the same semantic class in Wikidata; at the context level, we use
pre-trained language models (e.g., BERT) to generate word substitutions.
Together, the two levels of attack produce natural adversarial examples that
result in a shifted distribution from the training data on which our target
models have been trained. We apply the proposed method to the OntoNotes dataset
and create a new benchmark named OntoRock for evaluating the robustness of
existing NER models via a systematic evaluation protocol. Our experiments and
analysis reveal that even the best model has a significant performance drop,
and these models seem to memorize in-domain entity patterns instead of
reasoning from the context. Our work also studies the effects of a few simple
data augmentation methods to improve the robustness of NER models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14541">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
https://github.com/IBM/yaso-tsa.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05463">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Post-hoc interpretation aims to explain a trained model and reveal how the
model arrives at a decision. Though research on post-hoc interpretations has
developed rapidly, one growing pain in this field is the difficulty in
evaluating interpretations. There are some crucial logic traps behind existing
evaluation methods, which are ignored by most works. In this opinion piece, we
summarize four kinds evaluation methods and point out the corresponding logic
traps behind them. We argue that we should be clear about these traps rather
than ignore them and draw conclusions assertively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The research community has proposed copious modifications to the Transformer
architecture since it was introduced over three years ago, relatively few of
which have seen widespread adoption. In this paper, we comprehensively evaluate
many of these modifications in a shared experimental setting that covers most
of the common uses of the Transformer in natural language processing.
Surprisingly, we find that most modifications do not meaningfully improve
performance. Furthermore, most of the Transformer variants we found beneficial
were either developed in the same codebase that we used or are relatively minor
changes. We conjecture that performance improvements may strongly depend on
implementation details and correspondingly make some recommendations for
improving the generality of experimental results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05602">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper asks whether extrapolating the hidden space distribution of text
examples from one class onto another is a valid inductive bias for data
augmentation. To operationalize this question, I propose a simple data
augmentation protocol called &quot;good-enough example extrapolation&quot; (GE3). GE3 is
lightweight and has no hyperparameters. Applied to three text classification
datasets for various data imbalance scenarios, GE3 improves performance more
than upsampling and other hidden-space data augmentation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12289">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present and make available MedLatinEpi and MedLatinLit, two datasets of
medieval Latin texts to be used in research on computational authorship
analysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,
respectively, labelled by author; MedLatinEpi texts are of epistolary nature,
while MedLatinLit texts consist of literary comments and treatises about
various subjects. As such, these two datasets lend themselves to supporting
research in authorship analysis tasks, such as authorship attribution,
authorship verification, or same-author verification. Along with the datasets
we provide experimental results, obtained on these datasets, for the authorship
verification task, i.e., the task of predicting whether a text of unknown
authorship was written by a candidate author or not. We also make available the
source code of the authorship verification system we have used, thus allowing
our experiments to be reproduced, and to be used as baselines, by other
researchers. We also describe the application of the above authorship
verification system, using these datasets as training data, for investigating
the authorship of two medieval epistles whose authorship has been disputed by
scholars.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02339">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing the number of unique tokens) is a key factor for improved
performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08154">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Developing a unified multilingual model has long been a pursuit for machine
translation. However, existing approaches suffer from performance degradation
-- a single multilingual model is inferior to separately trained bilingual ones
on rich-resource languages. We conjecture that such a phenomenon is due to
interference caused by joint training with multiple languages. To accommodate
the issue, we propose CIAT, an adapted Transformer model with a small parameter
overhead for multilingual machine translation. We evaluate CIAT on multiple
benchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that
CIAT consistently outperforms strong multilingual baselines on 64 of total 66
language directions, 42 of which see above 0.5 BLEU improvement. Our code is
available at \url{https://github.com/Yaoming95/CIAT}~.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03416">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Ad-dressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus con-firming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01477">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several studies have been carried out on revealing linguistic features
captured by BERT. This is usually achieved by training a diagnostic classifier
on the representations obtained from different layers of BERT. The subsequent
classification accuracy is then interpreted as the ability of the model in
encoding the corresponding linguistic property. Despite providing insights,
these studies have left out the potential role of token representations. In
this paper, we provide a more in-depth analysis on the representation space of
BERT in search for distinct and meaningful subspaces that can explain the
reasons behind these probing results. Based on a set of probing tasks and with
the help of attribution methods we show that BERT tends to encode meaningful
knowledge in specific token representations (which are often ignored in
standard classification setups), allowing the model to detect syntactic and
semantic abnormalities, and to distinctively separate grammatical number and
tense subspaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14830">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Building ASR models across many languages is a challenging multi-task
learning problem due to large variations and heavily unbalanced data. Existing
work has shown positive transfer from high resource to low resource languages.
However, degradations on high resource languages are commonly observed due to
interference from the heterogeneous multilingual data and reduction in
per-language capacity. We conduct a capacity study on a 15-language task, with
the amount of data per language varying from 7.6K to 53.5K hours. We adopt
GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that
(1) scaling the number of model parameters is an effective way to solve the
capacity bottleneck - our 500M-param model already outperforms monolingual
baselines and scaling it to 1B and 10B brought further quality gains; (2)
larger models are not only more data efficient, but also more efficient in
terms of training cost as measured in TPU days - the 1B-param model reaches the
same accuracy at 34% of training time as the 500M-param model; (3) given a
fixed capacity budget, adding depth works better than width and large encoders
do better than large decoders; (4) with continuous training, they can be
adapted to new languages and domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Fine-tuning pre-trained models have achieved impressive performance on
standard natural language processing benchmarks. However, the resultant model
generalizability remains poorly understood. We do not know, for example, how
excellent performance can lead to the perfection of generalization models. In
this study, we analyze a fine-tuned BERT model from different perspectives
using relation extraction. We also characterize the differences in
generalization techniques according to our proposed improvements. From
empirical experimentation, we find that BERT suffers a bottleneck in terms of
robustness by way of randomizations, adversarial and counterfactual tests, and
biases (i.e., selection and semantic). These findings highlight opportunities
for future improvements. Our open-sourced testbed DiagnoseRE is available in
\url{https://github.com/zjunlp/DiagnoseRE}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07814">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper introduces WeChat AI&#x27;s participation in WMT 2021 shared news
translation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and
English-&gt;German. Our systems are based on the Transformer (Vaswani et al.,
2017) with several novel and effective variants. In our experiments, we employ
data filtering, large-scale synthetic data generation (i.e., back-translation,
knowledge distillation, forward-translation, iterative in-domain knowledge
transfer), advanced finetuning approaches, and boosted Self-BLEU based model
ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3
case-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese,
Japanese-&gt;English and English-&gt;German, respectively. The BLEU scores of
English-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among
all submissions, and that of English-&gt;German is the highest among all
constrained submissions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00055">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Representation learning for text via pretraining a language model on a large
corpus has become a standard starting point for building NLP systems. This
approach stands in contrast to autoencoders, also trained on raw text, but with
the objective of learning to encode each input as a vector that allows full
reconstruction. Autoencoders are attractive because of their latent space
structure and generative properties. We therefore explore the construction of a
sentence-level autoencoder from a pretrained, frozen transformer language
model. We adapt the masked language modeling objective as a generative,
denoising one, while only training a sentence bottleneck and a single-layer
modified transformer decoder. We demonstrate that the sentence representations
discovered by our model achieve better quality than previous methods that
extract representations from pretrained transformers on text similarity tasks,
style transfer (an example of controlled generation), and single-sentence
classification tasks in the GLUE benchmark, while using fewer parameters than
large pretrained models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05776">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The effectiveness of a language model is influenced by its token
representations, which must encode contextual information and handle the same
word form having a plurality of meanings (polysemy). Currently, none of the
common language modelling architectures explicitly model polysemy. We propose a
language model which not only predicts the next word, but also its sense in
context. We argue that this higher prediction granularity may be useful for end
tasks such as assistive writing, and allow for more a precise linking of
language models with knowledge bases. We find that multi-sense language
modelling requires architectures that go beyond standard language models, and
here propose a structured prediction framework that decomposes the task into a
word followed by a sense prediction task. To aid sense prediction, we utilise a
Graph Attention Network, which encodes definitions and example uses of word
senses. Overall, we find that multi-sense language modelling is a highly
challenging task, and suggest that future work focus on the creation of more
annotated training datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05477">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The ability of a dialog system to express consistent language style during
conversations has a direct, positive impact on its usability and on user
satisfaction. Although previous studies have demonstrated that style transfer
is feasible with a large amount of parallel data, it is often impossible to
collect such data for different styles. In this paper, instead of manually
constructing conversation data with a certain style, we propose a flexible
framework that adapts a generic retrieval-based dialogue system to mimic the
language style of a specified persona without any parallel data. Our approach
is based on automatic generation of stylized data by learning the usage of
jargon, and then rewriting the generic conversations to a stylized one by
incorporating the jargon. In experiments we implemented dialogue systems with
five distinct language styles, and the result shows our framework significantly
outperforms baselines in terms of the average score of responses&#x27; relevance and
style degree, and content diversity. A/B testing on a commercial chatbot shows
that users are more satisfied with our system. This study demonstrates the
feasibility of building stylistic dialogue systems by simple data augmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08676">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing system dealing with online complaint provides a final decision
without explanations. We propose to analyse the complaint text of internet
fraud in a fine-grained manner. Considering the complaint text includes
multiple clauses with various functions, we propose to identify the role of
each clause and classify them into different types of fraud element. We
construct a large labeled dataset originated from a real finance service
platform. We build an element identification model on top of BERT and propose
additional two modules to utilize the context of complaint text for better
element label classification, namely, global context encoder and label refiner.
Experimental results show the effectiveness of our model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based
on propositional logic, which has demonstrated competitive performance in many
Natural Language Processing (NLP) tasks, including sentiment analysis, text
classification, and Word Sense Disambiguation. To obtain human-level
interpretability, legacy TM employs Boolean input features such as bag-of-words
(BOW). However, the BOW representation makes it difficult to use any
pre-trained information, for instance, word2vec and GloVe word representations.
This restriction has constrained the performance of TM compared to deep neural
networks (DNNs) in NLP. To reduce the performance gap, in this paper, we
propose a novel way of using pre-trained word representations for TM. The
approach significantly enhances the performance and interpretability of TM. We
achieve this by extracting semantically related words from pre-trained word
representations as input features to the TM. Our experiments show that the
accuracy of the proposed approach is significantly higher than the previous
BOW-based TM, reaching the level of DNN-based models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00544">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training, a method for learning robust deep neural networks,
constructs adversarial examples during training. However, recent methods for
generating NLP adversarial examples involve combinatorial search and expensive
sentence encoders for constraining the generated instances. As a result, it
remains challenging to use vanilla adversarial training to improve NLP models&#x27;
performance, and the benefits are mainly uninvestigated. This paper proposes a
simple and improved vanilla adversarial training process for NLP models, which
we name Attacking to Training (A2T). The core part of A2T is a new and cheaper
word substitution attack optimized for vanilla adversarial training. We use A2T
to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI
datasets. Our results empirically show that it is possible to train robust NLP
models using a much cheaper adversary. We demonstrate that vanilla adversarial
training with A2T can improve an NLP model&#x27;s robustness to the attack it was
originally trained with and also defend the model against other types of word
substitution attacks. Furthermore, we show that A2T can improve NLP models&#x27;
standard accuracy, cross-domain generalization, and interpretability. Code is
available at https://github.com/QData/Textattack-A2T .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01371">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent advances in text autoencoders have significantly improved the quality
of the latent space, which enables models to generate grammatical and
consistent text from aggregated latent vectors. As a successful application of
this property, unsupervised opinion summarization models generate a summary by
decoding the aggregated latent vectors of inputs. More specifically, they
perform the aggregation via simple average. However, little is known about how
the vector aggregation step affects the generation quality. In this study, we
revisit the commonly used simple average approach by examining the latent space
and generated summaries. We found that text autoencoders tend to generate
overly generic summaries from simply averaged latent vectors due to an
unexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we
refer to as summary vector degeneration. To overcome this issue, we develop a
framework Coop, which searches input combinations for the latent vector
aggregation using input-output word overlap. Experimental results show that
Coop successfully alleviates the summary vector degeneration issue and
establishes new state-of-the-art performance on two opinion summarization
benchmarks. Code is available at \url{https://github.com/megagonlabs/coop}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08350">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Understanding how events are semantically related to each other is the
essence of reading comprehension. Recent event-centric reading comprehension
datasets focus mostly on event arguments or temporal relations. While these
tasks partially evaluate machines&#x27; ability of narrative understanding,
human-like reading comprehension requires the capability to process event-based
information beyond arguments and temporal reasoning. For example, to understand
causality between events, we need to infer motivation or purpose; to establish
event hierarchy, we need to understand the composition of events. To facilitate
these tasks, we introduce ESTER, a comprehensive machine reading comprehension
(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages
natural language queries to reason about the five most common event semantic
relations, provides more than 6K questions and captures 10.1K event relation
pairs. Experimental results show that the current SOTA systems achieve 22.1%,
63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,
which are all significantly below human performances (36.0%, 79.6%, 100%
respectively), highlighting our dataset as a challenging benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05527">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Detecting events and their evolution through time is a crucial task in
natural language understanding. Recent neural approaches to event temporal
relation extraction typically map events to embeddings in the Euclidean space
and train a classifier to detect temporal relations between event pairs.
However, embeddings in the Euclidean space cannot capture richer asymmetric
relations such as event temporal relations. We thus propose to embed events
into hyperbolic spaces, which are intrinsically oriented at modeling
hierarchical structures. We introduce two approaches to encode events and their
temporal relations in hyperbolic spaces. One approach leverages hyperbolic
embeddings to directly infer event relations through simple geometrical
operations. In the second one, we devise an end-to-end architecture composed of
hyperbolic neural units tailored for the temporal relation extraction task.
Thorough experimental assessments on widely used datasets have shown the
benefits of revisiting the tasks on a different geometrical space, resulting in
state-of-the-art performance on several standard metrics. Finally, the ablation
study and several qualitative analyses highlighted the rich event semantics
implicitly encoded into hyperbolic spaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00225">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In industry NLP application, our manually labeled data has a certain number
of noisy data. We present a simple method to find the noisy data and relabel
them manually, meanwhile we collect the correction information. Then we present
novel method to incorporate the human correction information into deep learning
model. Human know how to correct noisy data. So the correction information can
be inject into deep learning model. We do the experiment on our own text
classification dataset, which is manually labeled, because we relabel the noisy
data in our dataset for our industry application. The experiment result shows
that our method improve the classification accuracy from 91.7% to 92.5%. The
91.7% baseline is based on BERT training on the corrected dataset, which is
hard to surpass.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11715">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>End-to-end automatic speech recognition (ASR) systems are increasingly
popular due to their relative architectural simplicity and competitive
performance. However, even though the average accuracy of these systems may be
high, the performance on rare content words often lags behind hybrid ASR
systems. To address this problem, second-pass rescoring is often applied
leveraging upon language modeling. In this paper, we propose a second-pass
system with multi-task learning, utilizing semantic targets (such as intent and
slot prediction) to improve speech recognition performance. We show that our
rescoring model trained with these additional tasks outperforms the baseline
rescoring model, trained with only the language modeling task, by 1.4% on a
general test and by 2.6% on a rare word test set in terms of word-error-rate
relative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR
deduction compared with RNN Transducer only ASR baseline for rare words
recognition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15283">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While pre-trained language models (PTLMs) have achieved noticeable success on
many NLP tasks, they still struggle for tasks that require event temporal
reasoning, which is essential for event-centric applications. We present a
continual pre-training approach that equips PTLMs with targeted knowledge about
event temporal relations. We design self-supervised learning objectives to
recover masked-out event and temporal indicators and to discriminate sentences
from their corrupted counterparts (where event or temporal indicators got
replaced). By further pre-training a PTLM with these objectives jointly, we
reinforce its attention to event and temporal information, yielding enhanced
capability on event temporal reasoning. This effective continual pre-training
framework for event temporal reasoning (ECONET) improves the PTLMs&#x27; fine-tuning
performances across five relation extraction and question answering tasks and
achieves new or on-par state-of-the-art performances in most of our downstream
tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05395">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Single-table text-to-SQL aims to transform a natural language question into a
SQL query according to one single table. Recent work has made promising
progress on this task by pre-trained language models and a multi-submodule
framework. However, zero-shot table, that is, the invisible table in the
training set, is currently the most critical bottleneck restricting the
application of existing approaches to real-world scenarios. Although some work
has utilized auxiliary tasks to help handle zero-shot tables, expensive extra
manual annotation limits their practicality. In this paper, we propose a new
approach for the zero-shot text-to-SQL task which does not rely on any
additional manual annotations. Our approach consists of two parts. First, we
propose a new model that leverages the abundant information of table content to
help establish the mapping between questions and zero-shot tables. Further, we
propose a simple but efficient meta-learning strategy to train our model. The
strategy utilizes the two-step gradient update to force the model to learn a
generalization ability towards zero-shot tables. We conduct extensive
experiments on a public open-domain text-to-SQL dataset WikiSQL and a
domain-specific dataset ESQL. Compared to existing approaches using the same
pre-trained model, our approach achieves significant improvements on both
datasets. Compared to the larger pre-trained model and the tabular-specific
pre-trained model, our approach is still competitive. More importantly, on the
zero-shot subsets of both the datasets, our approach further increases the
improvements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08727">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While day-to-day questions come with a variety of answer types, the current
question-answering (QA) literature has failed to adequately address the answer
diversity of questions. To this end, we present GooAQ, a large-scale dataset
with a variety of answer types. This dataset contains over 5 million questions
and 3 million answers collected from Google. GooAQ questions are collected
semi-automatically from the Google search engine using its autocomplete
feature. This results in naturalistic questions of practical interest that are
nonetheless short and expressed using simple language. GooAQ answers are mined
from Google&#x27;s responses to our collected questions, specifically from the
answer boxes in the search results. This yields a rich space of answer types,
containing both textual answers (short and long) as well as more structured
ones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)
in line with recent work, LM&#x27;s strong performance on GooAQ&#x27;s short-answer
questions heavily benefit from annotated data; however, (b) their quality in
generating coherent and accurate responses for questions requiring long
responses (such as &#x27;how&#x27; and &#x27;why&#x27; questions) is less reliant on observing
annotated data and mainly supported by their pre-training. We release GooAQ to
facilitate further research on improving QA with diverse response types.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14583">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for
automatic speech recognition (ASR). Yet many endangered languages lack
sufficient data for pre-training such models, or are predominantly oral
vernaculars without a standardised writing system, precluding fine-tuning.
Query-by-example spoken term detection (QbE-STD) offers an alternative for
iteratively indexing untranscribed speech corpora by locating spoken query
terms. Using data from 7 Australian Aboriginal languages and a regional variety
of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can
be improved by leveraging representations developed for ASR (wav2vec 2.0: the
English monolingual model and XLSR53 multilingual model). Surprisingly, the
English model outperformed the multilingual model on 4 Australian language
datasets, raising questions around how to optimally leverage self-supervised
speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0
representations (either English or XLSR53) offer large improvements (56-86%
relative) over state-of-the-art approaches on our endangered language datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11830">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dialogue models trained on human conversations inadvertently learn to
generate toxic responses. In addition to producing explicitly offensive
utterances, these models can also implicitly insult a group or individual by
aligning themselves with an offensive statement. To better understand the
dynamics of contextually offensive language, we investigate the stance of
dialogue model responses in offensive Reddit conversations. Specifically, we
create ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model
responses labeled with offensive language and stance. Our analysis reveals that
42% of human responses agree with toxic comments, whereas only 13% agree with
safe comments. This undesirable behavior is learned by neural dialogue models,
such as DialoGPT, which we show are two times more likely to agree with
offensive comments. To enable automatic detection of offensive language, we
fine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for
offensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the
effectiveness of controllable text generation (CTG) methods to mitigate the
tendency of neural dialogue models to agree with offensive comments. Compared
to the baseline, our best CTG model achieves a 19% reduction in agreement with
offensive comments and produces 29% fewer offensive replies. Our work
highlights the need for further efforts to characterize and analyze
inappropriate behavior in dialogue models, in order to help make them safer.
Our code and corpus are available at https://github.com/abaheti95/ToxiChat .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05523">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00049">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language representations are known to carry stereotypical biases and, as a
result, lead to biased predictions in downstream tasks. While existing methods
are effective at mitigating biases by linear projection, such methods are too
aggressive: they not only remove bias, but also erase valuable information from
word embeddings. We develop new measures for evaluating specific information
retention that demonstrate the tradeoff between bias removal and information
retention. To address this challenge, we propose OSCaR (Orthogonal Subspace
Correction and Rectification), a bias-mitigating method that focuses on
disentangling biased associations between concepts instead of removing concepts
wholesale. Our experiments on gender biases show that OSCaR is a well-balanced
approach that ensures that semantic information is retained in the embeddings
and bias is also effectively mitigated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05427">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Fine-grained classification involves dealing with datasets with larger number
of classes with subtle differences between them. Guiding the model to focus on
differentiating dimensions between these commonly confusable classes is key to
improving performance on fine-grained tasks. In this work, we analyse the
contrastive fine-tuning of pre-trained language models on two fine-grained text
classification tasks, emotion classification and sentiment analysis. We
adaptively embed class relationships into a contrastive objective function to
help differently weigh the positives and negatives, and in particular,
weighting closely confusable negatives more than less similar negative
examples. We find that Label-aware Contrastive Loss outperforms previous
contrastive methods, in the presence of larger number and/or more confusable
classes, and helps models to produce output distributions that are more
differentiated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05522">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal language analysis is a burgeoning field of NLP that aims to
simultaneously model a speaker&#x27;s words, acoustical annotations, and facial
expressions. In this area, lexicon features usually outperform other modalities
because they are pre-trained on large corpora via Transformer-based models.
Despite their strong performance, training a new self-supervised learning (SSL)
Transformer on any modality is not usually attainable due to insufficient data,
which is the case in multimodal language learning. This work proposes a
Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the
mentioned constraints without training a complete Transformer model. TEASEL
model includes speech modality as a dynamic prefix besides the textual modality
compared to a conventional language model. This method exploits a conventional
pre-trained language model as a cross-modal Transformer model. We evaluated
TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.
Extensive experiments show that our model outperforms unimodal baseline
language models by 4% and outperforms the current multimodal state-of-the-art
(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%
smaller than the SoTA model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05494">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Building an automatic speech recognition (ASR) system from scratch requires a
large amount of annotated speech data, which is difficult to collect in many
languages. However, there are cases where the low-resource language shares a
common acoustic space with a high-resource language having enough annotated
data to build an ASR. In such cases, we show that the domain-independent
acoustic models learned from the high-resource language through unsupervised
domain adaptation (UDA) schemes can enhance the performance of the ASR in the
low-resource language. We use the specific example of Hindi in the source
domain and Sanskrit in the target domain. We explore two architectures: i)
domain adversarial training using gradient reversal layer (GRL) and ii) domain
separation networks (DSN). The GRL and DSN architectures give absolute
improvements of 6.71% and 7.32%, respectively, in word error rate over the
baseline deep neural network model when trained on just 5.5 hours of data in
the target domain. We also show that choosing a proper language (Telugu) in the
source domain can bring further improvement. The results suggest that UDA
schemes can be helpful in the development of ASR systems for low-resource
languages, mitigating the hassle of collecting large amounts of annotated
speech data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05244">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cross-attention is an important component of neural machine translation
(NMT), which is always realized by dot-product attention in previous methods.
However, dot-product attention only considers the pair-wise correlation between
words, resulting in dispersion when dealing with long sentences and neglect of
source neighboring relationships. Inspired by linguistics, the above issues are
caused by ignoring a type of cross-attention, called concentrated attention,
which focuses on several central words and then spreads around them. In this
work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention
in cross-attention. Experiments and analyses we conducted on three datasets
show that the proposed method outperforms the baseline and has significant
improvement on alignment quality, N-gram accuracy, and long sentence
translation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05238">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Extracting relations across large text spans has been relatively
underexplored in NLP, but it is particularly important for high-value domains
such as biomedicine, where obtaining high recall of the latest findings is
crucial for practical applications. Compared to conventional information
extraction confined to short text spans, document-level relation extraction
faces additional challenges in both inference and learning. Given longer text
spans, state-of-the-art neural architectures are less effective and
task-specific self-supervision such as distant supervision becomes very noisy.
In this paper, we propose decomposing document-level relation extraction into
relation detection and argument resolution, taking inspiration from Davidsonian
semantics. This enables us to incorporate explicit discourse modeling and
leverage modular self-supervision for each sub-problem, which is less
noise-prone and can be further refined end-to-end via variational EM. We
conduct a thorough evaluation in biomedical machine reading for precision
oncology, where cross-paragraph relation mentions are prevalent. Our method
outperforms prior state of the art, such as multi-scale learning and graph
neural networks, by over 20 absolute F1 points. The gain is particularly
pronounced among the most challenging relation instances whose arguments never
co-occur in a paragraph.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generating goal-oriented questions in Visual Dialogue tasks is a challenging
and long-standing problem. State-Of-The-Art systems are shown to generate
questions that, although grammatically correct, often lack an effective
strategy and sound unnatural to humans. Inspired by the cognitive literature on
information search and cross-situational word learning, we design Confirm-it, a
model based on a beam search re-ranking algorithm that guides an effective
goal-oriented strategy by asking questions that confirm the model&#x27;s conjecture
about the referent. We take the GuessWhat?! game as a case-study. We show that
dialogues generated by Confirm-it are more natural and effective than beam
search decoding without re-ranking.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Framing has significant but subtle effects on public opinion and policy. We
propose an NLP framework to measure entity-centric frames. We use it to
understand media coverage on police violence in the United States in a new
Police Violence Frames Corpus of 82k news articles spanning 7k police killings.
Our work uncovers more than a dozen framing devices and reveals significant
differences in the way liberal and conservative news sources frame both the
issue of police violence and the entities involved. Conservative sources
emphasize when the victim is armed or attacking an officer and are more likely
to mention the victim&#x27;s criminal record. Liberal sources focus more on the
underlying systemic injustice, highlighting the victim&#x27;s race and that they
were unarmed. We discover temporary spikes in these injustice frames near
high-profile shooting events, and finally, we show protest volume correlates
with and precedes media framing decisions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05358">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Enthymemes are defined as arguments where a premise or conclusion is left
implicit. We tackle the task of generating the implicit premise in an
enthymeme, which requires not only an understanding of the stated conclusion
and premise but also additional inferences that could depend on commonsense
knowledge. The largest available dataset for enthymemes (Habernal et al., 2018)
consists of 1.7k samples, which is not large enough to train a neural text
generation model. To address this issue, we take advantage of a similar task
and dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).
However, we show that simply using a state-of-the-art seq2seq model fine-tuned
on this data might not generate meaningful implicit premises associated with
the given enthymemes. We demonstrate that encoding discourse-aware commonsense
during fine-tuning improves the quality of the generated implicit premises and
outperforms all other baselines both in automatic and human evaluations on
three different datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05433">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Internet search affects people&#x27;s cognition of the world, so mitigating biases
in search results and learning fair models is imperative for social good. We
study a unique gender bias in image search in this work: the search images are
often gender-imbalanced for gender-neutral natural language queries. We
diagnose two typical image search models, the specialized model trained on
in-domain datasets and the generalized representation model pre-trained on
massive image and text data across the internet. Both models suffer from severe
gender bias. Therefore, we introduce two novel debiasing approaches: an
in-processing fair sampling method to address the gender imbalance issue for
training models, and a post-processing feature clipping method base on mutual
information to debias multimodal representations of pre-trained models.
Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods
significantly reduce the gender bias in image search models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05250">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain
manually annotated event-centric mentions of events and entities that form
coreference chains with identity relations. ECB+ is a state-of-the-art CDCR
dataset that focuses on the resolution of events and their descriptive
attributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that
annotates coreference chains of both events and entities with a strong variance
of word choice and more loosely-related coreference anaphora, e.g., bridging or
near-identity relations. In this paper, we qualitatively and quantitatively
compare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We
propose a phrasing diversity metric (PD) that compares lexical diversity within
coreference chains on a more detailed level than previously proposed metric,
e.g., a number of unique lemmas. We discuss the different tasks that both CDCR
datasets create, i.e., lexical disambiguation and lexical diversity challenges,
and propose a direction for further CDCR evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05256">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Can pre-trained BERT for one language and GPT for another be glued together
to translate texts? Self-supervised training using only monolingual data has
led to the success of pre-trained (masked) language models in many NLP tasks.
However, directly connecting BERT as an encoder and GPT as a decoder can be
challenging in machine translation, for GPT-like models lack a cross-attention
component that is needed in seq2seq decoders. In this paper, we propose
Graformer to graft separately pre-trained (masked) language models for machine
translation. With monolingual data for pre-training and parallel data for
grafting training, we maximally take advantage of the usage of both types of
data. Experiments on 60 directions show that our method achieves average
improvements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with
the multilingual Transformer of the same size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05406">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human conversations consist of reasonable and natural topic flows, which are
observed as the shifts of the mentioned concepts across utterances. Previous
chatbots that incorporate the external commonsense knowledge graph prove that
modeling the concept shifts can effectively alleviate the dull and
uninformative response dilemma. However, there still exists a gap between the
concept relations in the natural conversation and those in the external
commonsense knowledge graph, which is an issue to solve. Specifically, the
concept relations in the external commonsense knowledge graph are not
intuitively built from the conversational scenario but the world knowledge,
which makes them insufficient for the chatbot construction. To bridge the above
gap, we propose the method to supply more concept relations extracted from the
conversational corpora and reconstruct an enhanced concept graph for the
chatbot construction. In addition, we present a novel, powerful, and fast graph
encoding architecture named the Edge-Transformer to replace the traditional GNN
architecture. Experimental results on the Reddit conversation dataset indicate
our proposed method significantly outperforms strong baseline systems and
achieves new SOTA results. Further analysis individually proves the
effectiveness of the enhanced concept graph and the Edge-Transformer
architecture.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05339">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The Linac Coherent Light Source (LCLS) is an X- ray free electron laser
(XFEL) facility enabling the study of the structure and dynamics of single
macromolecules. A major upgrade will bring the repetition rate of the X-ray
source from 120 to 1 million pulses per second. Exascale high performance
computing (HPC) capabilities will be required to process the corresponding data
rates. We present SpiniFEL, an application used for structure determination of
proteins from single-particle imaging (SPI) experiments. An emerging technique
for imaging individual proteins and other large molecular complexes by
outrunning radiation damage, SPI breaks free from the need for crystallization
(which is difficult for some proteins) and allows for imaging molecular
dynamics at near ambient conditions. SpiniFEL is being developed to run on
supercomputers in near real-time while an experiment is taking place, so that
the feedback about the data can guide the data collection strategy. We describe
here how we reformulated the mathematical framework for parallelizable
implementation and accelerated the most compute intensive parts of the
application. We also describe the use of Pygion, a Python interface for the
Legion task-based programming model and compare to our existing MPI+GPU
implementation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What&#x27;s in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05289">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A flaw in QA evaluation is that annotations often only provide one gold
answer. Thus, model predictions semantically equivalent to the answer but
superficially different are considered incorrect. This work explores mining
alias entities from knowledge bases and using them as additional gold answers
(i.e., equivalent answers). We incorporate answers for two settings: evaluation
with additional answers and model training with equivalent answers. We analyse
three QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion
increases the exact match score on all datasets for evaluation, while
incorporating it helps model training over real-world datasets. We ensure the
additional answers are valid through a human post hoc evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">&quot;Let Your Characters Tell Their Story&quot;: A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05438">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>When reading a literary piece, readers often make inferences about various
characters&#x27; roles, personalities, relationships, intents, actions, etc. While
humans can readily draw upon their past experiences to build such a
character-centric view of the narrative, understanding characters in narratives
can be a challenging task for machines. To encourage research in this field of
character-centric narrative understanding, we present LiSCU -- a new dataset of
literary pieces and their summaries paired with descriptions of characters that
appear in them. We also introduce two new tasks on LiSCU: Character
Identification and Character Description Generation. Our experiments with
several pre-trained language models adapted for these tasks demonstrate that
there is a need for better models of narrative comprehension.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05317">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Causal inference using observational text data is becoming increasingly
popular in many research areas. This paper presents the Bayesian Topic
Regression (BTR) model that uses both text and numerical information to model
an outcome variable. It allows estimation of both discrete and continuous
treatment effects. Furthermore, it allows for the inclusion of additional
numerical confounding factors next to text data. To this end, we combine a
supervised Bayesian topic model with a Bayesian regression framework and
perform supervised representation learning for the text features jointly with
the regression parameter training, respecting the Frisch-Waugh-Lovell theorem.
Our paper makes two main contributions. First, we provide a regression
framework that allows causal inference in settings when both text and numerical
confounders are of relevance. We show with synthetic and semi-synthetic
datasets that our joint approach recovers ground truth with lower bias than any
benchmark model, when text and numerical features are correlated. Second,
experiments on two real-world datasets demonstrate that a joint and supervised
learning strategy also yields superior prediction results compared to
strategies that estimate regression weights for text and non-text features
separately, being even competitive with more complex deep neural networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05487">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Although pre-training models have achieved great success in dialogue
generation, their performance drops dramatically when the input contains an
entity that does not appear in pre-training and fine-tuning datasets (unseen
entity). To address this issue, existing methods leverage an external knowledge
base to generate appropriate responses. In real-world scenario, the entity may
not be included by the knowledge base or suffer from the precision of knowledge
retrieval. To deal with this problem, instead of introducing knowledge base as
the input, we force the model to learn a better semantic representation by
predicting the information in the knowledge base, only based on the input
context. Specifically, with the help of a knowledge base, we introduce two
auxiliary training objectives: 1) Interpret Masked Word, which conjectures the
meaning of the masked entity given the context; 2) Hypernym Generation, which
predicts the hypernym of the entity based on the context. Experiment results on
two dialogue corpus verify the effectiveness of our methods under both
knowledge available and unavailable settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05178">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task
Learning (MSNF-MTCL) to predict 5 important students&#x27; retention risks: future
dropout, next semester dropout, type of dropout, duration of dropout and cause
of dropout. First, we develop a general purpose multi-modal neural fusion
network model MSNF for learning students&#x27; academic information representation
by fusing spatial and temporal unstructured advising notes with spatiotemporal
structured data. MSNF combines a Bidirectional Encoder Representations from
Transformers (BERT)-based document embedding framework to represent each
advising note, Long-Short Term Memory (LSTM) network to model temporal advising
note embeddings, LSTM network to model students&#x27; temporal performance variables
and students&#x27; static demographics altogether. The final fused representation
from MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model
towards building MSNF-MTCL for predicting 5 student retention risks. We
evaluate MSNFMTCL on a large educational database consists of 36,445 college
students over 18 years period of time that provides promising performances
comparing with the nearest state-of-art models. Additionally, we test the
fairness of such model given the existence of biases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05168">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large pre-trained language models (PLMs) have led to great success on various
commonsense question answering (QA) tasks in an end-to-end fashion. However,
little attention has been paid to what commonsense knowledge is needed to
deeply characterize these QA tasks. In this work, we proposed to categorize the
semantics needed for these tasks using the SocialIQA as an example. Building
upon our labeled social knowledge categories dataset on top of SocialIQA, we
further train neural QA models to incorporate such social knowledge categories
and relation information from a knowledge base. Unlike previous work, we
observe our models with semantic categorizations of social knowledge can
achieve comparable performance with a relatively simple model and smaller size
compared to other complex approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05388">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In order to preserve word-order information in a non-autoregressive setting,
transformer architectures tend to include positional knowledge, by (for
instance) adding positional encodings to token embeddings. Several
modifications have been proposed over the sinusoidal positional encodings used
in the original transformer architecture; these include, for instance,
separating position encodings and token embeddings, or directly modifying
attention weights based on the distance between word pairs. We first show that
surprisingly, while these modifications tend to improve monolingual language
models, none of them result in better multilingual language models. We then
answer why that is: Sinusoidal encodings were explicitly designed to facilitate
compositionality by allowing linear projections over arbitrary time steps.
Higher variances in multilingual training distributions requires higher
compression, in which case, compositionality becomes indispensable. Learned
absolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal
embeddings in multilingual settings, but more complex positional encoding
architectures lack the inductive bias to effectively learn compositionality and
cross-lingual alignment. In other words, while sinusoidal positional encodings
were originally designed for monolingual applications, they are particularly
useful in multilingual language models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05186">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper investigates continual learning for semantic parsing. In this
setting, a neural semantic parser learns tasks sequentially without accessing
full training data from previous tasks. Direct application of the SOTA
continual learning algorithms to this problem fails to achieve comparable
performance with re-training models with all seen tasks because they have not
considered the special properties of structured outputs yielded by semantic
parsers. Therefore, we propose TotalRecall, a continual learning method
designed for neural semantic parsers from two aspects: i) a sampling method for
memory replay that diversifies logical form templates and balances
distributions of parse actions in a memory; ii) a two-stage training method
that significantly improves generalization capability of the parsers across
tasks. We conduct extensive experiments to study the research problems involved
in continual semantic parsing and demonstrate that a neural semantic parser
trained with TotalRecall achieves superior performance than the one trained
directly with the SOTA continual learning algorithms and achieve a 3-6 times
speedup compared to re-training from scratch. Code and datasets are available
at: https://github.com/zhuang-li/cl_nsp.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05327">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Numerous government initiatives (e.g. the EU with GDPR) are coming to the
conclusion that the increasing complexity of modern software systems must be
contrasted with some Rights to Explanation and metrics for the Impact
Assessment of these tools, that allow humans to understand and oversee the
output of Automated Decision Making systems. Explainable AI was born as a
pathway to allow humans to explore and understand the inner working of complex
systems. But establishing what is an explanation and objectively evaluating
explainability, are not trivial tasks. With this paper, we present a new
model-agnostic metric to measure the Degree of eXplainability of correct
information in an objective way, exploiting a specific model from Ordinary
Language Philosophy called the Achinstein&#x27;s Theory of Explanations. In order to
understand whether this metric is actually behaving as explainability is
expected to, we designed a few experiments and a user-study on two realistic
AI-based systems for healthcare and finance, involving famous AI technology
including Artificial Neural Networks and TreeSHAP. The results we obtained are
very encouraging, suggesting that our proposed metric for measuring the Degree
of eXplainability is robust on several scenarios and it can be eventually
exploited for a lawful Impact Assessment of an Automated Decision Making
system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05042">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a grounded neural dialogue model that successfully collaborates
with people in a partially-observable reference game. We focus on a setting
where two agents each observe an overlapping part of a world context and need
to identify and agree on some object they share. Therefore, the agents should
pool their information and communicate pragmatically to solve the task. Our
dialogue agent accurately grounds referents from the partner&#x27;s utterances using
a structured reference resolver, conditions on these referents using a
recurrent memory, and uses a pragmatic generation procedure to ensure the
partner can resolve the references the agent produces. We evaluate on the
OneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving
a number of dots arranged on a board with continuously varying positions,
sizes, and shades. Our agent substantially outperforms the previous state of
the art for the task, obtaining a 20% relative improvement in successful task
completion in self-play evaluations and a 50% relative improvement in success
in human evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05125">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR&#x27;21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL&#x27;s performance
matches or exceeds ALIGN&#x27;s cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL&#x27;s text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05184">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Internet memes have become powerful means to transmit political,
psychological, and socio-cultural ideas. Although memes are typically humorous,
recent days have witnessed an escalation of harmful memes used for trolling,
cyberbullying, and abusing social entities. Detecting such harmful memes is
challenging as they can be highly satirical and cryptic. Moreover, while
previous work has focused on specific aspects of memes such as hate speech and
propaganda, there has been little work on harm in general, and only one
specialized dataset for it. Here, we focus on bridging this gap. In particular,
we aim to solve two novel tasks: detecting harmful memes and identifying the
social entities they target. We further extend the recently released HarMeme
dataset to generalize on two prevalent topics - COVID-19 and US politics and
name the two datasets as Harm-C and Harm-P, respectively. We then propose
MOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a
novel multimodal (text + image) deep neural model, which uses global and local
perspectives to detect harmful memes. MOMENTA identifies the object proposals
and attributes and uses a multimodal model to perceive the comprehensive
context in which the objects and the entities are portrayed in a given meme.
MOMENTA is interpretable and generalizable, and it outperforms numerous
baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05213">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Information Extraction (IE) aims to extract structural information from
unstructured texts. In practice, long-tailed distributions caused by the
selection bias of a dataset, may lead to incorrect correlations, also known as
spurious correlations, between entities and labels in the conventional
likelihood models. This motivates us to propose counterfactual IE (CFIE), a
novel framework that aims to uncover the main causalities behind data in the
view of causal inference. Specifically, 1) we first introduce a unified
structural causal model (SCM) for various IE tasks, describing the
relationships among variables; 2) with our SCM, we then generate
counterfactuals based on an explicit language structure to better calculate the
direct causal effect during the inference stage; 3) we further propose a novel
debiasing approach to yield more robust predictions. Experiments on three IE
tasks across five public datasets show the effectiveness of our CFIE model in
mitigating the spurious correlation issues.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05357">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we study the problem of named entity recognition (NER) in a low
resource scenario, focusing on few-shot and zero-shot settings. Built upon
large-scale pre-trained language models, we propose a novel NER framework,
namely SpanNER, which learns from natural language supervision and enables the
identification of never-seen entity classes without using in-domain labeled
data. We perform extensive experiments on 5 benchmark datasets and evaluate the
proposed method in the few-shot learning, domain transfer and zero-shot
learning settings. The experimental results show that the proposed method can
bring 10%, 23% and 26% improvements in average over the best baselines in
few-shot learning, domain transfer and zero-shot learning settings
respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05182">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dialogue-based relation extraction (DiaRE) aims to detect the structural
information from unstructured utterances in dialogues. Existing relation
extraction models may be unsatisfactory under such a conversational setting,
due to the entangled logic and information sparsity issues in utterances
involving multiple speakers. To this end, we introduce SOLS, a novel model
which can explicitly induce speaker-oriented latent structures for better
DiaRE. Specifically, we learn latent structures to capture the relationships
among tokens beyond the utterance boundaries, alleviating the entangled logic
issue. During the learning process, our speaker-specific regularization method
progressively highlights speaker-related key clues and erases the irrelevant
ones, alleviating the information sparsity issue. Experiments on three public
datasets demonstrate the effectiveness of our proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose an approach to improve image captioning solutions
for images with novel objects that do not have caption labels in the training
dataset. Our approach is agnostic to model architecture, and primarily focuses
on training technique that uses existing fully paired image-caption data and
the images with only the novel object detection labels (partially paired data).
We create synthetic paired captioning data for these novel objects by
leveraging context from existing image-caption pairs. We further re-use these
partially paired images with novel objects to create pseudo-label captions that
are used to fine-tune the captioning model. Using a popular captioning model
(Up-Down) as baseline, our approach achieves state-of-the-art results on
held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for
novel object images by 75.8 and 26.6 points respectively, compared to baseline
model that does not use partially paired images during training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05322">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hate speech has grown significantly on social media, causing serious
consequences for victims of all demographics. Despite much attention being paid
to characterize and detect discriminatory speech, most work has focused on
explicit or overt hate speech, failing to address a more pervasive form based
on coded or indirect language. To fill this gap, this work introduces a
theoretically-justified taxonomy of implicit hate speech and a benchmark corpus
with fine-grained labels for each message and its implication. We present
systematic analyses of our dataset using contemporary baselines to detect and
explain implicit hate speech, and we discuss key features that challenge
existing models. This dataset will continue to serve as a useful benchmark for
understanding this multifaceted issue.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05153">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Addressing the mismatch between natural language descriptions and the
corresponding SQL queries is a key challenge for text-to-SQL translation. To
bridge this gap, we propose an SQL intermediate representation (IR) called
Natural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities
of SQL, while it simplifies the queries as follows: (1) dispensing with
operators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are
usually hard to find counterparts for in the text descriptions; (2) removing
the need for nested subqueries and set operators; and (3) making schema linking
easier by reducing the required number of schema items. On Spider, a
challenging text-to-SQL benchmark that contains complex and nested SQL queries,
we demonstrate that NatSQL outperforms other IRs, and significantly improves
the performance of several previous SOTA models. Furthermore, for existing
models that do not support executable SQL generation, NatSQL easily enables
them to generate executable SQL queries, and achieves the new state-of-the-art
execution accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05252">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Datasets and methods for cross-document coreference resolution (CDCR) focus
on events or entities with strict coreference relations. They lack, however,
annotating and resolving coreference mentions with more abstract or loose
relations that may occur when news articles report about controversial and
polarized events. Bridging and loose coreference relations trigger associations
that may lead to exposing news readers to bias by word choice and labeling. For
example, coreferential mentions of &quot;direct talks between U.S. President Donald
Trump and Kim&quot; such as &quot;an extraordinary meeting following months of heated
rhetoric&quot; or &quot;great chance to solve a world problem&quot; form a more positive
perception of this event. A step towards bringing awareness of bias by word
choice and labeling is the reliable resolution of coreferences with high
lexical diversity. We propose an unsupervised method named XCoref, which is a
CDCR method that capably resolves not only previously prevalent entities, such
as persons, e.g., &quot;Donald Trump,&quot; but also abstractly defined concepts, such as
groups of persons, &quot;caravan of immigrants,&quot; events and actions, e.g., &quot;marching
to the U.S. border.&quot; In an extensive evaluation, we compare the proposed XCoref
to a state-of-the-art CDCR method and a previous method TCA that resolves such
complex coreference relations and find that XCoref outperforms these methods.
Outperforming an established CDCR model shows that the new CDCR models need to
be evaluated on semantically complex mentions with more loose coreference
relations to indicate their applicability of models to resolve mentions in the
&quot;wild&quot; of political news articles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05349">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Attention is all we need as long as we have enough data. Even so, it is
sometimes not easy to determine how much data is enough while the models are
becoming larger and larger. In this paper, we propose HYDRA heads, lightweight
pretrained linguistic self-attention heads to inject knowledge into transformer
models without pretraining them again. Our approach is a balanced paradigm
between leaving the models to learn unsupervised and forcing them to conform to
linguistic knowledge rigidly as suggested in previous studies. Our experiment
proves that the approach is not only the boost performance of the model but
also lightweight and architecture friendly. We empirically verify our framework
on benchmark datasets to show the contribution of linguistic knowledge to a
transformer model. This is a promising result for a new approach to
transferring knowledge from linguistic resources into transformer-based models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05112">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>For over thirty years, researchers have developed and analyzed methods for
latent tree induction as an approach for unsupervised syntactic parsing.
Nonetheless, modern systems still do not perform well enough compared to their
supervised counterparts to have any practical use as structural annotation of
text. In this work, we present a technique that uses distant supervision in the
form of span constraints (i.e. phrase bracketing) to improve performance in
unsupervised constituency parsing. Using a relatively small number of span
constraints we can substantially improve the output from DIORA, an already
competitive unsupervised parsing system. Compared with full parse tree
annotation, span constraints can be acquired with minimal effort, such as with
a lexicon derived from Wikipedia, to find exact text matches. Our experiments
show span constraints based on entities improves constituency parsing on
English WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to
any domain where span constraints are easily attainable, and as a case study we
demonstrate its effectiveness by parsing biomedical text from the CRAFT
dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05149">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we present a new verification style reading comprehension
dataset named VGaokao from Chinese Language tests of Gaokao. Different from
existing efforts, the new dataset is originally designed for native speakers&#x27;
evaluation, thus requiring more advanced language understanding skills. To
address the challenges in VGaokao, we propose a novel Extract-Integrate-Compete
approach, which iteratively selects complementary evidence with a novel query
updating mechanism and adaptively distills supportive evidence, followed by a
pairwise competition to push models to learn the subtle difference among
similar text pieces. Experiments show that our methods outperform various
baselines on VGaokao with retrieved complementary evidence, while having the
merits of efficiency and explainability. Our dataset and code are released for
further research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05217">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, several high-performance conversational systems have been
proposed based on the Transformer encoder-decoder model. Although previous
studies analyzed the effects of the model parameters and the decoding method on
subjective dialogue evaluations with overall metrics, they did not analyze how
the differences of fine-tuning datasets affect on user&#x27;s detailed impression.
In addition, the Transformer-based approach has only been verified for English,
not for such languages with large inter-language distances as Japanese. In this
study, we develop large-scale Transformer-based Japanese dialogue models and
Japanese chit-chat datasets to examine the effectiveness of the
Transformer-based approach for building chit-chat dialogue systems. We
evaluated and analyzed the impressions of human dialogues in different
fine-tuning datasets, model parameters, and the use of additional information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05187">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A multi-turn dialogue always follows a specific topic thread, and topic shift
at the discourse level occurs naturally as the conversation progresses,
necessitating the model&#x27;s ability to capture different topics and generate
topic-aware responses. Previous research has either predicted the topic first
and then generated the relevant response, or simply applied the attention
mechanism to all topics, ignoring the joint distribution of the topic
prediction and response generation models and resulting in uncontrollable and
unrelated responses. In this paper, we propose a joint framework with a topic
refinement mechanism to learn these two tasks simultaneously. Specifically, we
design a three-pass iteration mechanism to generate coarse response first, then
predict corresponding topics, and finally generate refined response conditioned
on predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the
topic prediction task respectively, aiming to investigate the effects of joint
learning and the understanding ability of GPT model. Experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance at response generation task and the great potential understanding
capability of GPT model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05126">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing research studies on cross-sentence relation extraction in long-form
multi-party conversations aim to improve relation extraction without
considering the explainability of such methods. This work addresses that gap by
focusing on extracting explanations that indicate that a relation exists while
using only partially labeled data. We propose our model-agnostic framework,
D-REX, a policy-guided semi-supervised algorithm that explains and ranks
relations. We frame relation extraction as a re-ranking task and include
relation- and entity-specific explanations as an intermediate step of the
inference process. We find that about 90% of the time, human annotators prefer
D-REX&#x27;s explanations over a strong BERT-based joint relation extraction and
explanation model. Finally, our evaluations on a dialogue relation extraction
dataset show that our method is simple yet effective and achieves a
state-of-the-art F1 score on relation extraction, improving upon existing
methods by 13.5%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05361">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce COMBO - a fully neural NLP system for accurate part-of-speech
tagging, morphological analysis, lemmatisation, and (enhanced) dependency
parsing. It predicts categorical morphosyntactic features whilst also exposes
their vector representations, extracted from hidden layers. COMBO is an easy to
install Python package with automatically downloadable pre-trained models for
over 40 languages. It maintains a balance between efficiency and quality. As it
is an end-to-end system and its modules are jointly trained, its training is
competitively fast. As its models are optimised for accuracy, they achieve
often better prediction quality than SOTA. The COMBO library is available at:
https://gitlab.clarin-pl.eu/syntactic-tools/combo.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05424">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05190">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Eliciting knowledge contained in language models via prompt-based learning
has shown great potential in many natural language processing tasks, such as
text classification and generation. Whereas, the applications for more complex
tasks such as event extraction are less studied, since the design of prompt is
not straightforward due to the complicated types and arguments. In this paper,
we explore to elicit the knowledge from pre-trained language models for event
trigger detection and argument extraction. Specifically, we present various
joint trigger/argument prompt methods, which can elicit more complementary
knowledge by modeling the interactions between different triggers or arguments.
The experimental results on the benchmark dataset, namely ACE2005, show the
great advantages of our proposed approach. In particular, our approach is
superior to the recent advanced methods in the few-shot scenario where only a
few samples are used for training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05097">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A hyperbole is an intentional and creative exaggeration not to be taken
literally. Despite its ubiquity in daily life, the computational explorations
of hyperboles are scarce. In this paper, we tackle the under-explored and
challenging task: sentence-level hyperbole generation. We start with a
representative syntactic pattern for intensification and systematically study
the semantic (commonsense and counterfactual) relationships between each
component in such hyperboles. Next, we leverage the COMeT and reverse COMeT
models to do commonsense and counterfactual inference. We then generate
multiple hyperbole candidates based on our findings from the pattern, and train
neural classifiers to rank and select high-quality hyperboles. Automatic and
human evaluations show that our generation method is able to generate
hyperboles creatively with high success rate and intensity scores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05233">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>State-of-the-art Named Entity Recognition(NER) models rely heavily on large
amountsof fully annotated training data. However, ac-cessible data are often
incompletely annotatedsince the annotators usually lack comprehen-sive
knowledge in the target domain. Normallythe unannotated tokens are regarded as
non-entities by default, while we underline thatthese tokens could either be
non-entities orpart of any entity. Here, we study NER mod-eling with incomplete
annotated data whereonly a fraction of the named entities are la-beled, and the
unlabeled tokens are equiva-lently multi-labeled by every possible label.Taking
multi-labeled tokens into account, thenumerous possible paths can distract the
train-ing model from the gold path (ground truthlabel sequence), and thus
hinders the learn-ing ability. In this paper, we propose AdaK-NER, named the
adaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion
where the gold path is more likely to belocated. We demonstrate the superiority
ofour approach through extensive experimentson both English and Chinese
datasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on
two Chinese datasetscompared with the prior state-of-the-art works.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05234">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot slot tagging is an emerging research topic in the field of Natural
Language Understanding (NLU). With sufficient annotated data from source
domains, the key challenge is how to train and adapt the model to another
target domain which only has few labels. Conventional few-shot approaches use
all the data from the source domains without considering inter-domain relations
and implicitly assume each sample in the domain contributes equally. However,
our experiments show that the data distribution bias among different domains
will significantly affect the adaption performance. Moreover, transferring
knowledge from dissimilar domains will even introduce some extra noises so that
affect the performance of models. To tackle this problem, we propose an
effective similarity-based method to select data from the source domains. In
addition, we propose a Shared-Private Network (SP-Net) for the few-shot slot
tagging task. The words from the same class would have some shared features. We
extract those shared features from the limited annotated data on the target
domain and merge them together as the label embedding to help us predict other
unlabelled data on the target domain. The experiment shows that our method
outperforms the state-of-the-art approaches with fewer source data. The result
also proves that some training data from dissimilar sources are redundant and
even negative for the adaption.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05056">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dialogue Act (DA) classification is the task of classifying utterances with
respect to the function they serve in a dialogue. Existing approaches to DA
classification model utterances without incorporating the turn changes among
speakers throughout the dialogue, therefore treating it no different than
non-interactive written text. In this paper, we propose to integrate the turn
changes in conversations among speakers when modeling DAs. Specifically, we
learn conversation-invariant speaker turn embeddings to represent the speaker
turns in a conversation; the learned speaker turn embeddings are then merged
with the utterance embeddings for the downstream task of DA classification.
With this simple yet effective mechanism, our model is able to capture the
semantics from the dialogue content while accounting for different speaker
turns in a conversation. Validation on three benchmark public datasets
demonstrates superior performance of our model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05179">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generating high quality question-answer pairs is a hard but meaningful task.
Although previous works have achieved great results on answer-aware question
generation, it is difficult to apply them into practical application in the
education field. This paper for the first time addresses the question-answer
pair generation task on the real-world examination data, and proposes a new
unified framework on RACE. To capture the important information of the input
passage we first automatically generate(rather than extracting) keyphrases,
thus this task is reduced to keyphrase-question-answer triplet joint
generation. Accordingly, we propose a multi-agent communication model to
generate and optimize the question and keyphrases iteratively, and then apply
the generated question and keyphrases to guide the generation of answers. To
establish a solid benchmark, we build our model on the strong generative
pre-training model. Experimental results show that our model makes great
breakthroughs in the question-answer pair generation task. Moreover, we make a
comprehensive analysis on our model, suggesting new directions for this
challenging task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05052">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge-dependent tasks typically use two sources of knowledge: parametric,
learned at training time, and contextual, given as a passage at inference time.
To understand how models use these sources together, we formalize the problem
of knowledge conflicts, where the contextual information contradicts the
learned information. Analyzing the behaviour of popular models, we measure
their over-reliance on memorized information (the cause of hallucinations), and
uncover important factors that exacerbate this behaviour. Lastly, we propose a
simple method to mitigate over-reliance on parametric knowledge, which
minimizes hallucination, and improves out-of-distribution generalization by
4%-7%. Our findings demonstrate the importance for practitioners to evaluate
model tendency to hallucinate rather than read, and show that our mitigation
strategy encourages generalization to evolving information (i.e.,
time-dependent queries). To encourage these practices, we have released our
framework for generating knowledge conflicts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05090">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural language modelling has progressed the state-of-the-art in different
downstream Natural Language Processing (NLP) tasks. One such area is of
open-domain dialog modelling, neural dialog models based on GPT-2 such as
DialoGPT have shown promising performance in single-turn conversation. However,
such (neural) dialog models have been criticized for generating responses which
although may have relevance to the previous human response, tend to quickly
dissipate human interest and descend into trivial conversation. One reason for
such performance is the lack of explicit conversation strategy being employed
in human-machine conversation. Humans employ a range of conversation strategies
while engaging in a conversation, one such key social strategies is
Self-disclosure(SD). A phenomenon of revealing information about one-self to
others. Social penetration theory (SPT) proposes that communication between two
people moves from shallow to deeper levels as the relationship progresses
primarily through self-disclosure. Disclosure helps in creating rapport among
the participants engaged in a conversation. In this paper, Self-disclosure
enhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic
Model (SDTM) during inference stage of a neural dialog model to re-rank
response candidates to enhance self-disclosure in single-turn responses from
from the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the explosive growth of livestream broadcasting, there is an urgent need
for new summarization technology that enables us to create a preview of
streamed content and tap into this wealth of knowledge. However, the problem is
nontrivial due to the informal nature of spoken language. Further, there has
been a shortage of annotated datasets that are necessary for transcript
summarization. In this paper, we present StreamHover, a framework for
annotating and summarizing livestream transcripts. With a total of over 500
hours of videos annotated with both extractive and abstractive summaries, our
benchmark dataset is significantly larger than currently existing annotated
corpora. We explore a neural extractive summarization model that leverages
vector-quantized variational autoencoder to learn latent vector representations
of spoken utterances and identify salient utterances from the transcripts to
form summaries. We show that our model generalizes better and improves
performance over strong baselines. The results of this study provide an avenue
for future research to improve summarization solutions for efficient browsing
of livestreams.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05140">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many NLG tasks such as summarization, dialogue response, or open domain
question answering focus primarily on a source text in order to generate a
target response. This standard approach falls short, however, when a user&#x27;s
intent or context of work is not easily recoverable based solely on that source
text -- a scenario that we argue is more of the rule than the exception. In
this work, we argue that NLG systems in general should place a much higher
level of emphasis on making use of additional context, and suggest that
relevance (as used in Information Retrieval) be thought of as a crucial tool
for designing user-oriented text-generating tasks. We further discuss possible
harms and hazards around such personalization, and argue that value-sensitive
design represents a crucial path forward through these challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05105">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Can we get existing language models and refine them for zero-shot commonsense
reasoning? This paper presents an initial study exploring the feasibility of
zero-shot commonsense reasoning for the Winograd Schema Challenge by
formulating the task as self-supervised refinement of a pre-trained language
model. In contrast to previous studies that rely on fine-tuning annotated
datasets, we seek to boost conceptualization via loss landscape refinement. To
this end, we propose a novel self-supervised learning approach that refines the
language model utilizing a set of linguistic perturbations of similar concept
relationships. Empirical analysis of our conceptually simple framework
demonstrates the viability of zero-shot commonsense reasoning on multiple
benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05199">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The new era of technology has brought us to the point where it is convenient
for people to share their opinions over an abundance of platforms. These
platforms have a provision for the users to express themselves in multiple
forms of representations, including text, images, videos, and audio. This,
however, makes it difficult for users to obtain all the key information about a
topic, making the task of automatic multi-modal summarization (MMS) essential.
In this paper, we present a comprehensive survey of the existing research in
the area of MMS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05108">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Self-supervised learning has recently attracted considerable attention in the
NLP community for its ability to learn discriminative features using a
contrastive objective. This paper investigates whether contrastive learning can
be extended to Transfomer attention to tackling the Winograd Schema Challenge.
To this end, we propose a novel self-supervised framework, leveraging a
contrastive loss directly at the level of self-attention. Experimental analysis
of our attention-based models on multiple datasets demonstrates superior
commonsense reasoning capabilities. The proposed approach outperforms all
comparable unsupervised approaches while occasionally surpassing supervised
ones.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance across various NLP tasks including the
identification of offensive language and hate speech, an important problem in
social media. In this paper, we present fBERT, a BERT model retrained on SOLID,
the largest English offensive language identification corpus available with
over $1.4$ million offensive instances. We evaluate fBERT&#x27;s performance on
identifying offensive content on multiple English datasets and we test several
thresholds for selecting instances from SOLID. The fBERT model will be made
freely available to the community.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05093">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large pre-trained language models for textual data have an unconstrained
output space; at each decoding step, they can produce any of 10,000s of
sub-word tokens. When fine-tuned to target constrained formal languages like
SQL, these models often generate invalid code, rendering it unusable. We
propose PICARD (code and trained models available at
https://github.com/ElementAI/picard), a method for constraining auto-regressive
decoders of language models through incremental parsing. PICARD helps to find
valid output sequences by rejecting inadmissible tokens at each decoding step.
On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that
PICARD transforms fine-tuned T5 models with passable performance into
state-of-the-art solutions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05157">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, there has been significant progress in studying neural networks for
translating text descriptions into SQL queries under the zero-shot cross-domain
setting. Despite achieving good performance on some public benchmarks, we
observe that existing text-to-SQL models do not generalize when facing domain
knowledge that does not frequently appear in the training data, which may
render the worse prediction performance for unseen domains. In this work, we
investigate the robustness of text-to-SQL models when the questions require
rarely observed domain knowledge. In particular, we define five types of domain
knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),
a human-curated dataset based on the Spider benchmark for text-to-SQL
translation. NL questions in Spider-DK are selected from Spider, and we modify
some samples by adding domain knowledge that reflects real-world question
paraphrases. We demonstrate that the prediction accuracy dramatically drops on
samples that require such domain knowledge, even if the domain knowledge
appears in the training set, and the model provides the correct predictions for
related training samples.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FINet: Dual Branches Feature Interaction for Partial-to-Partial Point Cloud Registration. (arXiv:2106.03479v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03479">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Data association is important in the point cloud registration. In this work,
we propose to solve the partial-to-partial registration from a new perspective,
by introducing multi-level feature interactions between the source and the
reference clouds at the feature extraction stage, such that the registration
can be realized without the attentions or explicit mask estimation for the
overlapping detection as adopted previously. Specifically, we present FINet, a
feature interaction-based structure with the capability to enable and
strengthen the information associating between the inputs at multiple stages.
To achieve this, we first split the features into two components, one for
rotation and one for translation, based on the fact that they belong to
different solution spaces, yielding a dual branches structure. Second, we
insert several interaction modules at the feature extractor for the data
association. Third, we propose a transformation sensitivity loss to obtain
rotation-attentive and translation-attentive features. Experiments demonstrate
that our method performs higher precision and robustness compared to the
state-of-the-art traditional and learning-based methods. Code will be available
at https://github.com/HaoXu-Work/FINet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05205">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11161">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel method for reconstructing floor plans from noisy 3D point
clouds. Our main contribution is a principled approach that relies on the Monte
Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function
efficiently despite the complexity of the problem. Like previous work, we first
project the input point cloud to a top view to create a density map and extract
room proposals from it. Our method selects and optimizes the polygonal shapes
of these room proposals jointly to fit the density map and outputs an accurate
vectorized floor map even for large complex scenes. To do this, we adapted
MCTS, an algorithm originally designed to learn to play games, to select the
room proposals by maximizing an objective function combining the fitness with
the density map as predicted by a deep network and regularizing terms on the
room shapes. We also introduce a refinement step to MCTS that adjusts the shape
of the room proposals. For this step, we propose a novel differentiable method
for rendering the polygonal shapes of these proposals. We evaluate our method
on the recent and challenging Structured3D and Floor-SP datasets and show a
significant improvement over the state-of-the-art, without imposing any hard
constraints nor assumptions on the floor plan configurations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01361">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper investigates an open research task of text-to-image synthesis for
automatically generating or manipulating images from text descriptions.
Prevailing methods mainly use the text as conditions for GAN generation, and
train different models for the text-guided image generation and manipulation
tasks. In this paper, we propose a novel unified framework of Cycle-consistent
Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image
manipulation tasks. Specifically, we first train a GAN model without text
input, aiming to generate images with high diversity and quality. Then we learn
a GAN inversion model to convert the images back to the GAN latent space and
obtain the inverted latent codes for each image, where we introduce the
cycle-consistency training to learn more robust and consistent inverted latent
codes. We further uncover the latent space semantics of the trained GAN model,
by learning a similarity model between text representations and the latent
codes. In the text-guided optimization module, we generate images with the
desired semantic attributes by optimizing the inverted latent codes. Extensive
experiments on the Recipe1M and CUB datasets validate the efficacy of our
proposed framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MovieCuts: A New Dataset and Benchmark for Cut Type Recognition. (arXiv:2109.05569v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05569">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Understanding movies and their structural patterns is a crucial task to
decode the craft of video editing. While previous works have developed tools
for general analysis such as detecting characters or recognizing cinematography
properties at the shot level, less effort has been devoted to understanding the
most basic video edit, the Cut. This paper introduces the cut type recognition
task, which requires modeling of multi-modal information. To ignite research in
the new task, we construct a large-scale dataset called MovieCuts, which
contains more than 170K videoclips labeled among ten cut types. We benchmark a
series of audio-visual approaches, including some that deal with the problem&#x27;s
multi-modal and multi-label nature. Our best model achieves 45.7% mAP, which
suggests that the task is challenging and that attaining highly accurate cut
type recognition is an open research problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00210">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We proposes a method of extracting intrest points and descriptors using
self-supervised learning method on frame-based event data, which is called
EventPoint. Different from other feature extraction methods on event data, we
train our model on real event-form driving dataset--DSEC with the
self-supervised learning method we proposed, the training progress fully
consider the characteristics of event data.To verify the effectiveness of our
work,we conducted several complete evaluations: we emulated DART and carried
out feature matching experiments on N-caltech101 dataset, the results shows
that the effect of EventPoint is better than DART; We use Vid2e tool provided
by UZH to convert Oxford robotcar data into event-based format, and combined
with INS information provided to carry out the global pose estimation
experiment which is important in SLAM. As far as we know, this is the first
work to carry out this challenging task.Sufficient experimental data show that
EventPoint can get better results while achieve real time on CPU.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09662">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Safe and proactive planning in robotic systems generally requires accurate
predictions of the environment. Prior work on environment prediction applied
video frame prediction techniques to bird&#x27;s-eye view environment
representations, such as occupancy grids. ConvLSTM-based frameworks used
previously often result in significant blurring and vanishing of moving
objects, thus hindering their applicability for use in safety-critical
applications. In this work, we propose two extensions to the ConvLSTM to
address these issues. We present the Temporal Attention Augmented ConvLSTM
(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks
for spatiotemporal occupancy prediction, and demonstrate improved performance
over baseline architectures on the real-world KITTI and Waymo datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Statistical Representation with Joint Deep Embedded Clustering. (arXiv:2109.05232v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05232">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the most promising approaches for unsupervised learning is combining
deep representation learning and deep clustering. Some recent works propose to
simultaneously learn representation using deep neural networks and perform
clustering by defining a clustering loss on top of embedded features. However,
these approaches are sensitive to imbalanced data and out-of-distribution
samples. Hence, these methods optimize clustering by pushing data close to
randomly initialized cluster centers. This is problematic when the number of
instances varies largely in different classes or a cluster with few samples has
less chance to be assigned a good centroid. To overcome these limitations, we
introduce StatDEC, a new unsupervised framework for joint statistical
representation learning and clustering. StatDEC simultaneously trains two deep
learning models, a deep statistics network that captures the data distribution,
and a deep clustering network that learns embedded features and performs
clustering by explicitly defining a clustering loss. Specifically, the
clustering network and representation network both take advantage of our
proposed statistics pooling layer that represents mean, variance, and
cardinality to handle the out-of-distribution samples as well as a class
imbalance. Our experiments show that using these representations, one can
considerably improve results on imbalanced image clustering across a variety of
image datasets. Moreover, the learned representations generalize well when
transferred to the out-of-distribution dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Channelized Axial Attention for Semantic Segmentation -- Considering Channel Relation within Spatial Attention for Semantic Segmentation. (arXiv:2101.07434v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07434">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Spatial and channel attentions, modelling the semantic interdependencies in
spatial and channel dimensions respectively, have recently been widely used for
semantic segmentation. However, computing spatial and channel attentions
separately sometimes causes errors, especially for those difficult cases. In
this paper, we propose Channelized Axial Attention (CAA) to seamlessly
integrate channel attention and spatial attention into a single operation with
negligible computation overhead. Specifically, we break down the dot-product
operation of the spatial attention into two parts and insert channel relation
in between, allowing for independently optimized channel attention on each
spatial location. We further develop grouped vectorization, which allows our
model to run with very little memory consumption without slowing down the
running speed. Comparative experiments conducted on multiple benchmark
datasets, including Cityscapes, PASCAL Context, and COCO-Stuff, demonstrate
that our CAA outperforms many state-of-the-art segmentation models (including
dual attention) on all tested datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05070">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative Adversarial Networks (GANs) can generate near photo realistic
images in narrow domains such as human faces. Yet, modeling complex
distributions of datasets such as ImageNet and COCO-Stuff remains challenging
in unconditional settings. In this paper, we take inspiration from kernel
density estimation techniques and introduce a non-parametric approach to
modeling distributions of complex datasets. We partition the data manifold into
a mixture of overlapping neighborhoods described by a datapoint and its nearest
neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),
which learns the distribution around each datapoint. Experimental results on
ImageNet and COCO-Stuff show that IC-GAN significantly improves over
unconditional models and unsupervised data partitioning baselines. Moreover, we
show that IC-GAN can effortlessly transfer to datasets not seen during training
by simply changing the conditioning instances, and still generate realistic
images. Finally, we extend IC-GAN to the class-conditional case and show
semantically controllable generation and competitive quantitative results on
ImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our
code and trained models to reproduce the reported results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>The vanilla GAN (Goodfellow et al. 2014) suffers from mode collapse deeply,
which usually manifests as that the images generated by generators tend to have
a high similarity amongst them, even though their corresponding latent vectors
have been very different. In this paper, we introduce a pluggable diversity
penalty module (DPM) to alleviate mode collapse of GANs. It reduces the
similarity of image pairs in feature space, i.e., if two latent vectors are
different, then we enforce the generator to generate two images with different
features. The normalized Gram matrix is used to measure the similarity. We
compare the proposed method with Unrolled GAN (Metz et al. 2016), BourGAN
(Xiao, Zhong, and Zheng 2018), PacGAN (Lin et al. 2018), VEEGAN (Srivastava et
al. 2017) and ALI (Dumoulin et al. 2016) on 2D synthetic dataset, and results
show that the diversity penalty module can help GAN capture much more modes of
the data distribution. Further, in classification tasks, we apply this method
as image data augmentation on MNIST, Fashion- MNIST and CIFAR-10, and the
classification testing accuracy is improved by 0.24%, 1.34% and 0.52% compared
with WGAN GP (Gulrajani et al. 2017), respectively. In domain translation,
diversity penalty module can help StarGAN (Choi et al. 2018) generate more
accurate attention masks and accelarate the convergence process. Finally, we
quantitatively evaluate the proposed method with IS and FID on CelebA,
CIFAR-10, MNIST and Fashion-MNIST, and the results suggest GAN with diversity
penalty module gets much higher IS and lower FID compared with some SOTA GAN
architectures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02339">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image captioning models generally lack the capability to take into account
user interest, and usually default to global descriptions that try to balance
readability, informativeness, and information overload. On the other hand, VQA
models generally lack the ability to provide long descriptive answers, while
expecting the textual question to be quite precise. We present a method to
control the concepts that an image caption should focus on, using an additional
input called the guiding text that refers to either groundable or ungroundable
concepts in the image. Our model consists of a Transformer-based multimodal
encoder that uses the guiding text together with global and object-level image
features to derive early-fusion representations used to generate the guided
caption. While models trained on Visual Genome data have an in-domain advantage
of fitting well when guided with automatic object labels, we find that guided
captioning models trained on Conceptual Captions generalize better on
out-of-domain images and guiding texts. Our human-evaluation results indicate
that attempting in-the-wild guided image captioning requires access to large,
unrestricted-domain training datasets, and that increased style diversity (even
without increasing the number of unique tokens) is a key factor for improved
performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00180">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We describe a deep high-dynamic-range (HDR) image tone mapping operator that
is computationally efficient and perceptually optimized. We first decompose an
HDR image into a normalized Laplacian pyramid, and use two deep neural networks
(DNNs) to estimate the Laplacian pyramid of the desired tone-mapped image from
the normalized representation. We then end-to-end optimize the entire method
over a database of HDR images by minimizing the normalized Laplacian pyramid
distance (NLPD), a recently proposed perceptual metric. Qualitative and
quantitative experiments demonstrate that our method produces images with
better visual quality, and runs the fastest among existing local tone mapping
algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TypeNet: Deep Learning Keystroke Biometrics. (arXiv:2101.05570v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.05570">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the performance of Long Short-Term Memory networks for keystroke
biometric authentication at large scale in free-text scenarios. For this we
explore the performance of Long Short-Term Memory (LSTMs) networks trained with
a moderate number of keystrokes per identity and evaluated under different
scenarios including: i) three learning approaches depending on the loss
function (softmax, contrastive, and triplet loss); ii) different number of
training samples and lengths of keystroke sequences; iii) four databases based
on two device types (physical vs touchscreen keyboard); and iv) comparison with
existing approaches based on both traditional statistical methods and deep
learning architectures. Our approach called TypeNet achieves state-of-the-art
keystroke biometric authentication performance with an Equal Error Rate of 2.2%
and 9.2% for physical and touchscreen keyboards, respectively, significantly
outperforming previous approaches. Our experiments demonstrate a moderate
increase in error with up to 100,000 subjects, demonstrating the potential of
TypeNet to operate at an Internet scale. To the best of our knowledge, the
databases used in this work are the largest existing free-text keystroke
databases available for research with more than 136 million keystrokes from
168,000 subjects in physical keyboards, and 60,000 subjects with more than 63
million keystrokes acquired on mobile touchscreens.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physical Adversarial Attacks on an Aerial Imagery Object Detector. (arXiv:2108.11765v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11765">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks (DNNs) have become essential for processing the vast
amounts of aerial imagery collected using earth-observing satellite platforms.
However, DNNs are vulnerable towards adversarial examples, and it is expected
that this weakness also plagues DNNs for aerial imagery. In this work, we
demonstrate one of the first efforts on physical adversarial attacks on aerial
imagery, whereby adversarial patches were optimised, fabricated and installed
on or near target objects (cars) to significantly reduce the efficacy of an
object detector applied on overhead images. Physical adversarial attacks on
aerial images, particularly those captured from satellite platforms, are
challenged by atmospheric factors (lighting, weather, seasons) and the distance
between the observer and target. To investigate the effects of these
challenges, we devised novel experiments and metrics to evaluate the efficacy
of physical adversarial attacks against object detectors in aerial scenes. Our
results indicate the palpable threat posed by physical adversarial attacks
towards DNNs for processing satellite imagery.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14336">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Geoscience and seismology have utilized the most advanced technologies and
equipment to monitor seismic events globally from the past few decades. With
the enormous amount of data, modern GPU-powered deep learning presents a
promising approach to analyze data and discover patterns. In recent years,
there are plenty of successful deep learning models for picking seismic waves.
However, forecasting extreme earthquakes, which can cause disasters, is still
an underdeveloped topic in history. Relevant research in spatiotemporal
dynamics mining and forecasting has revealed some successful predictions, a
crucial topic in many scientific research fields. Most studies of them have
many successful applications of using deep neural networks. In Geology and
Earth science studies, earthquake prediction is one of the world&#x27;s most
challenging problems, about which cutting-edge deep learning technologies may
help discover some valuable patterns. In this project, we propose a deep
learning modeling approach, namely \tseqpre, to mine spatiotemporal patterns
from data to nowcast extreme earthquakes by discovering visual dynamics in
regional coarse-grained spatial grids over time. In this modeling approach, we
use synthetic deep learning neural networks with domain knowledge in geoscience
and seismology to exploit earthquake patterns for prediction using
convolutional long short-term memory neural networks. Our experiments show a
strong correlation between location prediction and magnitude prediction for
earthquakes in Southern California. Ablation studies and visualization validate
the effectiveness of the proposed modeling method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep One-Class Classification via Interpolated Gaussian Descriptor. (arXiv:2101.10043v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10043">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One-class classification (OCC) aims to learn an effective data description to
enclose all normal training samples and detect anomalies based on the deviation
from the data description. Current state-of-the-art OCC models learn a compact
normality description by hyper-sphere minimisation, but they often suffer from
overfitting the training data, especially when the training set is small or
contaminated with anomalous samples. To address this issue, we introduce the
interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a
one-class Gaussian anomaly classifier trained with adversarially interpolated
training samples. The Gaussian anomaly classifier differentiates the training
samples based on their distance to the Gaussian centre and the standard
deviation of these distances, offering the model a discriminability w.r.t. the
given samples during training. The adversarial interpolation is enforced to
consistently learn a smooth Gaussian descriptor, even when the training data is
small or contaminated with anomalous samples. This enables our model to learn
the data description based on the representative normal samples rather than
fringe or anomalous samples, resulting in significantly improved normality
description. In extensive experiments on diverse popular benchmarks, including
MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves
better detection accuracy than current state-of-the-art models. IGD also shows
better robustness in problems with small or contaminated training sets. Code is
available at https://github.com/tianyu0207/IGD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02600">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative diffusion models have emerged as leading models in speech and
image generation. However, in order to perform well with a small number of
denoising steps, a costly tuning of the set of noise parameters is needed. In
this work, we present a simple and versatile learning scheme that can
step-by-step adjust those noise parameters, for any given number of steps,
while the previous work needs to retune for each number separately.
Furthermore, without modifying the weights of the diffusion model, we are able
to significantly improve the synthesis results, for a small number of steps.
Our approach comes at a negligible computation cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PERT: A Progressively Region-based Network for Scene Text Removal. (arXiv:2106.13029v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13029">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scene text removal (STR) contains two processes: text localization and
background reconstruction. Through integrating both processes into a single
network, previous methods provide an implicit erasure guidance by modifying all
pixels in the entire image. However, there exists two problems: 1) the implicit
erasure guidance causes the excessive erasure to non-text areas; 2) the
one-stage erasure lacks the exhaustive removal of text region. In this paper,
we propose a ProgrEssively Region-based scene Text eraser (PERT), introducing
an explicit erasure guidance and performing balanced multi-stage erasure for
accurate and exhaustive text removal. Firstly, we introduce a new region-based
modification strategy (RegionMS) to explicitly guide the erasure process.
Different from previous implicitly guided methods, RegionMS performs targeted
and regional erasure on only text region, and adaptively perceives stroke-level
information to improve the integrity of non-text areas with only bounding box
level annotations. Secondly, PERT performs balanced multi-stage erasure with
several progressive erasing stages. Each erasing stage takes an equal step
toward the text-erased image to ensure the exhaustive erasure of text regions.
Compared with previous methods, PERT outperforms them by a large margin without
the need of adversarial loss, obtaining SOTA results with high speed (71 FPS)
and at least 25% lower parameter complexity. Code is available at
https://github.com/wangyuxin87/PERT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In predictive tasks, real-world datasets often present different degrees of
imbalanced (i.e., long-tailed or skewed) distributions. While the majority (the
head) classes have sufficient samples, the minority (the tail) classes can be
under-represented by a rather limited number of samples. Data pre-processing
has been shown to be very effective in dealing with such problems. On one hand,
data re-sampling is a common approach to tackling class imbalance. On the other
hand, dimension reduction, which reduces the feature space, is a conventional
technique for reducing noise and inconsistencies in a dataset. However, the
possible synergy between feature selection and data re-sampling for
high-performance imbalance classification has rarely been investigated before.
To address this issue, we carry out a comprehensive empirical study on the
joint influence of feature selection and re-sampling on two-class imbalance
classification. Specifically, we study the performance of two opposite
pipelines for imbalance classification by applying feature selection before or
after data re-sampling. We conduct a large number of experiments, with a total
of 9225 tests, on 52 publicly available datasets, using 9 feature selection
methods, 6 re-sampling approaches for class imbalance learning, and 3
well-known classification algorithms. Experimental results show that there is
no constant winner between the two pipelines; thus both of them should be
considered to derive the best performing model for imbalance classification. We
find that the performance of an imbalance classification model not only depends
on the classifier adopted and the ratio between the number of majority and
minority samples, but also depends on the ratio between the number of samples
and features. Overall, this study should provide new reference value for
researchers and practitioners in imbalance learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Differential Diagnosis of Frontotemporal Dementia and Alzheimer&#x27;s Disease using Generative Adversarial Network. (arXiv:2109.05627v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05627">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Frontotemporal dementia and Alzheimer&#x27;s disease are two common forms of
dementia and are easily misdiagnosed as each other due to their similar pattern
of clinical symptoms. Differentiating between the two dementia types is crucial
for determining disease-specific intervention and treatment. Recent development
of Deep-learning-based approaches in the field of medical image computing are
delivering some of the best performance for many binary classification tasks,
although its application in differential diagnosis, such as neuroimage-based
differentiation for multiple types of dementia, has not been explored. In this
study, a novel framework was proposed by using the Generative Adversarial
Network technique to distinguish FTD, AD and normal control subjects, using
volumetric features extracted at coarse-to-fine structural scales from Magnetic
Resonance Imaging scans. Experiments of 10-folds cross-validation on 1,954
images achieved high accuracy. With the proposed framework, we have
demonstrated that the combination of multi-scale structural features and
synthetic data augmentation based on generative adversarial network can improve
the performance of challenging tasks such as differentiating Dementia
sub-types.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds. (arXiv:2109.05566v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05566">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D scene understanding from point clouds plays a vital role for various
robotic applications. Unfortunately, current state-of-the-art methods use
separate neural networks for different tasks like object detection or room
layout estimation. Such a scheme has two limitations: 1) Storing and running
several networks for different tasks are expensive for typical robotic
platforms. 2) The intrinsic structure of separate outputs are ignored and
potentially violated. To this end, we propose the first transformer
architecture that predicts 3D objects and layouts simultaneously, using point
cloud inputs. Unlike existing methods that either estimate layout keypoints or
edges, we directly parameterize room layout as a set of quads. As such, the
proposed architecture is termed as P(oint)Q(uad)-Transformer. Along with the
novel quad representation, we propose a tailored physical constraint loss
function that discourages object-layout interference. The quantitative and
qualitative evaluations on the public benchmark ScanNet show that the proposed
PQ-Transformer succeeds to jointly parse 3D objects and layouts, running at a
quasi-real-time (8.91 FPS) rate without efficiency-oriented optimization.
Moreover, the new physical constraint loss can improve strong baselines, and
the F1-score of the room layout is significantly promoted from 37.9% to 57.9%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.00460">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle &#x3D; \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pyramid Medical Transformer for Medical Image Segmentation. (arXiv:2104.14702v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14702">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks have been a prevailing technique in the field of medical
image processing. However, the most popular convolutional neural networks
(CNNs) based methods for medical image segmentation are imperfect because they
model long-range dependencies by stacking layers or enlarging filters.
Transformers and the self-attention mechanism are recently proposed to
effectively learn long-range dependencies by modeling all pairs of word-to-word
attention regardless of their positions. The idea has also been extended to the
computer vision field by creating and treating image patches as embeddings.
Considering the computation complexity for whole image self-attention, current
transformer-based models settle for a rigid partitioning scheme that
potentially loses informative relations. Besides, current medical transformers
model global context on full resolution images, leading to unnecessary
computation costs. To address these issues, we developed a novel method to
integrate multi-scale attention and CNN feature extraction using a pyramidal
network architecture, namely Pyramid Medical Transformer (PMTrans). The PMTrans
captured multi-range relations by working on multi-resolution images. An
adaptive partitioning scheme was implemented to retain informative relations
and to access different receptive fields efficiently. Experimental results on
three medical image datasets (gland segmentation, MoNuSeg, and HECKTOR
datasets) showed that PMTrans outperformed the latest CNN-based and
transformer-based models for medical image segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anomaly Detection in Medical Imaging with Deep Perceptual Autoencoders. (arXiv:2006.13265v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Anomaly detection is the problem of recognizing abnormal inputs based on the
seen examples of normal data. Despite recent advances of deep learning in
recognizing image anomalies, these methods still prove incapable of handling
complex medical images, such as barely visible abnormalities in chest X-rays
and metastases in lymph nodes. To address this problem, we introduce a new
powerful method of image anomaly detection. It relies on the classical
autoencoder approach with a re-designed training pipeline to handle
high-resolution, complex images and a robust way of computing an image
abnormality score. We revisit the very problem statement of fully unsupervised
anomaly detection, where no abnormal examples at all are provided during the
model setup. We propose to relax this unrealistic assumption by using a very
small number of anomalies of confined variability merely to initiate the search
of hyperparameters of the model. We evaluate our solution on natural image
datasets with a known benchmark, as well as on two medical datasets containing
radiology and digital pathology images. The proposed approach suggests a new
strong baseline for image anomaly detection and outperforms state-of-the-art
approaches in complex medical image analysis tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose an approach to improve image captioning solutions
for images with novel objects that do not have caption labels in the training
dataset. Our approach is agnostic to model architecture, and primarily focuses
on training technique that uses existing fully paired image-caption data and
the images with only the novel object detection labels (partially paired data).
We create synthetic paired captioning data for these novel objects by
leveraging context from existing image-caption pairs. We further re-use these
partially paired images with novel objects to create pseudo-label captions that
are used to fine-tune the captioning model. Using a popular captioning model
(Up-Down) as baseline, our approach achieves state-of-the-art results on
held-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for
novel object images by 75.8 and 26.6 points respectively, compared to baseline
model that does not use partially paired images during training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows. (arXiv:2108.05015v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05015">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Different from visible cameras which record intensity images frame by frame,
the biologically inspired event camera produces a stream of asynchronous and
sparse events with much lower latency. In practice, the visible cameras can
better perceive texture details and slow motion, while event cameras can be
free from motion blurs and have a larger dynamic range which enables them to
work well under fast motion and low illumination. Therefore, the two sensors
can cooperate with each other to achieve more reliable object tracking. In this
work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to
the lack of a realistic and scaled dataset for this task. Our dataset consists
of 820 video pairs captured under low illumination, high speed, and background
clutter scenarios, and it is divided into a training and a testing subset, each
of which contains 500 and 320 videos, respectively. Based on VisEvent, we
transform the event flows into event images and construct more than 30 baseline
methods by extending current single-modality trackers into dual-modality
versions. More importantly, we further build a simple but effective tracking
algorithm by proposing a cross-modality transformer, to achieve more effective
feature fusion between visible and event data. Extensive experiments on the
proposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and
VOT-DVS), validated the effectiveness of our model. The dataset and source code
have been released at our project page:
\url{https://sites.google.com/view/viseventtrack/}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07491">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DSSL: Deep Surroundings-person Separation Learning for Text-based Person Retrieval. (arXiv:2109.05534v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05534">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Many previous methods on text-based person retrieval tasks are devoted to
learning a latent common space mapping, with the purpose of extracting
modality-invariant features from both visual and textual modality.
Nevertheless, due to the complexity of high-dimensional data, the unconstrained
mapping paradigms are not able to properly catch discriminative clues about the
corresponding person while drop the misaligned information. Intuitively, the
information contained in visual data can be divided into person information
(PI) and surroundings information (SI), which are mutually exclusive from each
other. To this end, we propose a novel Deep Surroundings-person Separation
Learning (DSSL) model in this paper to effectively extract and match person
information, and hence achieve a superior retrieval accuracy. A
surroundings-person separation and fusion mechanism plays the key role to
realize an accurate and effective surroundings-person separation under a
mutually exclusion constraint. In order to adequately utilize multi-modal and
multi-granular information for a higher retrieval accuracy, five diverse
alignment paradigms are adopted. Extensive experiments are carried out to
evaluate the proposed DSSL on CUHK-PEDES, which is currently the only
accessible dataset for text-base person retrieval task. DSSL achieves the
state-of-the-art performance on CUHK-PEDES. To properly evaluate our proposed
DSSL in the real scenarios, a Real Scenarios Text-based Person Reidentification
(RSTPReid) dataset is constructed to benefit future research on text-based
person retrieval, which will be publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05565">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- &quot;characteristic gradient detachment&quot;
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Split for Evaluating True Zero-Shot Action Recognition. (arXiv:2107.13029v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13029">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Zero-shot action recognition is the task of classifying action categories
that are not available in the training set. In this setting, the standard
evaluation protocol is to use existing action recognition datasets(e.g. UCF101)
and randomly split the classes into seen and unseen. However, most recent work
builds on representations pre-trained on the Kinetics dataset, where classes
largely overlap with classes in the zero-shot evaluation datasets. As a result,
classes which are supposed to be unseen, are present during supervised
pre-training, invalidating the condition of the zero-shot setting. A similar
concern was previously noted several years ago for image based zero-shot
recognition but has not been considered by the zero-shot action recognition
community. In this paper, we propose a new split for true zero-shot action
recognition with no overlap between unseen test classes and training or
pre-training classes. We benchmark several recent approaches on the proposed
True Zero-Shot(TruZe) Split for UCF101 and HMDB51, with zero-shot and
generalized zero-shot evaluation. In our extensive analysis, we find that our
TruZesplits are significantly harder than comparable random splits as nothing
is leaking from pre-training, i.e. unseen performance is consistently lower,up
to 8.9% for zero-shot action recognition. In an additional evaluation we also
find that similar issues exist in the splits used in few-shot action
recognition, here we see differences of up to 17.1%. We publish oursplits1and
hope that our benchmark analysis will change how the field is evaluating zero-
and few-shot action recognition moving forward.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A semi-supervised self-training method to develop assistive intelligence for segmenting multiclass bridge elements from inspection videos. (arXiv:2109.05078v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05078">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Bridge inspection is an important step in preserving and rehabilitating
transportation infrastructure for extending their service lives. The
advancement of mobile robotic technology allows the rapid collection of a large
amount of inspection video data. However, the data are mainly images of complex
scenes, wherein a bridge of various structural elements mix with a cluttered
background. Assisting bridge inspectors in extracting structural elements of
bridges from the big complex video data, and sorting them out by classes, will
prepare inspectors for the element-wise inspection to determine the condition
of bridges. This paper is motivated to develop an assistive intelligence model
for segmenting multiclass bridge elements from inspection videos captured by an
aerial inspection platform. With a small initial training dataset labeled by
inspectors, a Mask Region-based Convolutional Neural Network (Mask R-CNN)
pre-trained on a large public dataset was transferred to the new task of
multiclass bridge element segmentation. Besides, the temporal coherence
analysis attempts to recover false negatives and identify the weakness that the
neural network can learn to improve. Furthermore, a semi-supervised
self-training (S$^3$T) method was developed to engage experienced inspectors in
refining the network iteratively. Quantitative and qualitative results from
evaluating the developed deep neural network demonstrate that the proposed
method can utilize a small amount of time and guidance from experienced
inspectors (3.58 hours for labeling 66 images) to build the network of
excellent performance (91.8% precision, 93.6% recall, and 92.7% f1-score).
Importantly, the paper illustrates an approach to leveraging the domain
knowledge and experiences of bridge professionals into computational
intelligence models to efficiently adapt the models to varied bridges in the
National Bridge Inventory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Class imbalance and noisy labels are the norm rather than the exception in
many large-scale classification datasets. Nevertheless, most works in machine
learning typically assume balanced and clean data. There have been some recent
attempts to tackle, on one side, the problem of learning from noisy labels and,
on the other side, learning from long-tailed data. Each group of methods make
simplifying assumptions about the other. Due to this separation, the proposed
solutions often underperform when both assumptions are violated. In this work,
we present a simple two-stage approach based on recent advances in
self-supervised learning to treat both challenges simultaneously. It consists
of, first, task-agnostic self-supervised pre-training, followed by
task-specific fine-tuning using an appropriate loss. Most significantly, we
find that self-supervised learning approaches are effectively able to cope with
severe class imbalance. In addition, the resulting learned representations are
also remarkably robust to label noise, when fine-tuned with an imbalance- and
noise-resistant loss function. We validate our claims with experiments on
CIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as
the large-scale inherently noisy Clothing-1M dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Voxel Transformer for 3D Object Detection. (arXiv:2109.02497v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02497">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present Voxel Transformer (VoTr), a novel and effective voxel-based
Transformer backbone for 3D object detection from point clouds. Conventional 3D
convolutional backbones in voxel-based 3D detectors cannot efficiently capture
large context information, which is crucial for object recognition and
localization, owing to the limited receptive fields. In this paper, we resolve
the problem by introducing a Transformer-based architecture that enables
long-range relationships between voxels by self-attention. Given the fact that
non-empty voxels are naturally sparse but numerous, directly applying standard
Transformer on voxels is non-trivial. To this end, we propose the sparse voxel
module and the submanifold voxel module, which can operate on the empty and
non-empty voxel positions effectively. To further enlarge the attention range
while maintaining comparable computational overhead to the convolutional
counterparts, we propose two attention mechanisms for multi-head attention in
those two modules: Local Attention and Dilated Attention, and we further
propose Fast Voxel Query to accelerate the querying process in multi-head
attention. VoTr contains a series of sparse and submanifold voxel modules and
can be applied in most voxel-based detectors. Our proposed VoTr shows
consistent improvement over the convolutional baselines while maintaining
computational efficiency on the KITTI dataset and the Waymo Open dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Domain Adaptation for Video Semantic Segmentation. (arXiv:2107.11052v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11052">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Unsupervised Domain Adaptation for semantic segmentation has gained immense
popularity since it can transfer knowledge from simulation to real (Sim2Real)
by largely cutting out the laborious per pixel labeling efforts at real. In
this work, we present a new video extension of this task, namely Unsupervised
Domain Adaptation for Video Semantic Segmentation. As it became easy to obtain
large-scale video labels through simulation, we believe attempting to maximize
Sim2Real knowledge transferability is one of the promising directions for
resolving the fundamental data-hungry issue in the video. To tackle this new
problem, we present a novel two-phase adaptation scheme. In the first step, we
exhaustively distill source domain knowledge using supervised loss functions.
Simultaneously, video adversarial training (VAT) is employed to align the
features from source to target utilizing video context. In the second step, we
apply video self-training (VST), focusing only on the target data. To construct
robust pseudo labels, we exploit the temporal information in the video, which
has been rarely explored in the previous image-based self-training approaches.
We set strong baseline scores on &#x27;VIPER to CityscapeVPS&#x27; adaptation scenario.
We show that our proposals significantly outperform previous image-based UDA
methods both on image-level (mIoU) and video-level (VPQ) evaluation metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02337">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning to model how the world changes as time elapses has proven a
challenging problem for the computer vision community. We propose a
self-supervised solution to this problem using temporal cycle consistency
jointly in vision and language, training on narrated video. Our model learns
modality-agnostic functions to predict forward and backward in time, which must
undo each other when composed. This constraint leads to the discovery of
high-level transitions between moments in time, since such transitions are
easily inverted and shared across modalities. We justify the design of our
model with an ablation study on different configurations of the cycle
consistency problem. We then show qualitatively and quantitatively that our
approach yields a meaningful, high-level model of the future and past. We apply
the learned dynamics model without further training to various tasks, such as
predicting future action and temporally ordering sets of images. Project page:
https://dave.ml/mmcc</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10472">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VSAC: Efficient and Accurate Estimator for H and F. (arXiv:2106.10240v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10240">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present VSAC, a RANSAC-type robust estimator with a number of novelties.
It benefits from the introduction of the concept of independent inliers that
improves significantly the efficacy of the dominant plane handling and, also,
allows near error-free rejection of incorrect models, without false positives.
The local optimization process and its application is improved so that it is
run on average only once. Further technical improvements include adaptive
sequential hypothesis verification and efficient model estimation via Gaussian
elimination. Experiments on four standard datasets show that VSAC is
significantly faster than all its predecessors and runs on average in 1-2 ms,
on a CPU. It is two orders of magnitude faster and yet as precise as MAGSAC++,
the currently most accurate estimator of two-view geometry. In the repeated
runs on EVD, HPatches, PhotoTourism, and Kusvod2 datasets, it never failed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multiple Domain Experts Collaborative Learning: Multi-Source Domain Generalization For Person Re-Identification. (arXiv:2105.12355v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12355">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent years have witnessed significant progress in person re-identification
(ReID). However, current ReID approaches still suffer from considerable
performance degradation when unseen testing domains exhibit different
characteristics from the source training ones, known as the domain
generalization problem. Given multiple source training domains, previous Domain
Generalizable ReID (DG-ReID) methods usually learn all domains together using a
shared network, which can&#x27;t learn sufficient knowledge from each domain. In
this paper, we propose a novel Multiple Domain Experts Collaborative Learning
(MECL) framework for better exploiting all training domains, which benefits
from the proposed Domain-Domain Collaborative Learning (DDCL) and
Universal-Domain Collaborative Learning (UDCL). DDCL utilizes domain-specific
experts for fully exploiting each domain, and prevents experts from
over-fitting the corresponding domain using a meta-learning strategy. In UDCL,
a universal expert supervises the learning of domain experts and continuously
gathers knowledge from all domain experts. Note, only the universal expert will
be used for inference. Extensive experiments on DG-ReID benchmarks demonstrate
the effectiveness of DDCL and UDCL, and show that the whole MECL framework
significantly outperforms state-of-the-arts. Experimental results on
DG-classification benchmarks also reveal the great potential of applying MECL
to other DG tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06508">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The vision of the internet of things (IoT) is a reality now. IoT devices are
getting cheaper, smaller. They are becoming more and more computationally and
energy-efficient. The global market of IoT-based video analytics has seen
significant growth in recent years and it is expected to be a growing market
segment. For any IoT-based video analytics application, few key points
required, such as cost-effectiveness, widespread use, flexible design, accurate
scene detection, reusability of the framework. Video-based smart doorbell
system is one such application domain for video analytics where many commercial
offerings are available in the consumer market. However, such existing
offerings are costly, monolithic, and proprietary. Also, there will be a
trade-off between accuracy and portability. To address the foreseen problems,
I&#x27;m proposing a distributed framework for video analytics with a use case of a
smart doorbell system. The proposed framework uses AWS cloud services as a base
platform and to meet the price affordability constraint, the system was
implemented on affordable Raspberry Pi. The smart doorbell will be able to
recognize the known/unknown person with at most accuracy. The smart doorbell
system is also having additional detection functionalities such as harmful
weapon detection, noteworthy vehicle detection, animal/pet detection. An iOS
application is specifically developed for this implementation which can receive
the notification from the smart doorbell in real-time. Finally, the paper also
mentions the classical approaches for video analytics, their feasibility in
implementing with this use-case, and comparative analysis in terms of accuracy
and time required to detect an object in the frame is carried out. Results
conclude that AWS cloud-based approach is worthy for this smart doorbell use
case.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification. (arXiv:2107.02314v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The BraTS 2021 challenge celebrates its 10th anniversary and is jointly
organized by the Radiological Society of North America (RSNA), the American
Society of Neuroradiology (ASNR), and the Medical Image Computing and Computer
Assisted Interventions (MICCAI) society. Since its inception, BraTS has been
focusing on being a common benchmarking venue for brain glioma segmentation
algorithms, with well-curated multi-institutional multi-parametric magnetic
resonance imaging (mpMRI) data. Gliomas are the most common primary
malignancies of the central nervous system, with varying degrees of
aggressiveness and prognosis. The RSNA-ASNR-MICCAI BraTS 2021 challenge targets
the evaluation of computational algorithms assessing the same tumor
compartmentalization, as well as the underlying tumor&#x27;s molecular
characterization, in pre-operative baseline mpMRI data from 2,040 patients.
Specifically, the two tasks that BraTS 2021 focuses on are: a) the segmentation
of the histologically distinct brain tumor sub-regions, and b) the
classification of the tumor&#x27;s O[6]-methylguanine-DNA methyltransferase (MGMT)
promoter methylation status. The performance evaluation of all participating
algorithms in BraTS 2021 will be conducted through the Sage Bionetworks Synapse
platform (Task 1) and Kaggle (Task 2), concluding in distributing to the top
ranked participants monetary awards of $60,000 collectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Border-SegGCN: Improving Semantic Segmentation by Refining the Border Outline using Graph Convolutional Network. (arXiv:2109.05353v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present Border-SegGCN, a novel architecture to improve semantic
segmentation by refining the border outline using graph convolutional networks
(GCN). The semantic segmentation network such as Unet or DeepLabV3+ is used as
a base network to have pre-segmented output. This output is converted into a
graphical structure and fed into the GCN to improve the border pixel prediction
of the pre-segmented output. We explored and studied the factors such as border
thickness, number of edges for a node, and the number of features to be fed
into the GCN by performing experiments. We demonstrate the effectiveness of the
Border-SegGCN on the CamVid and Carla dataset, achieving a test set performance
of 81.96% without any post-processing on CamVid dataset. It is higher than the
reported state of the art mIoU achieved on CamVid dataset by 0.404%</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05159">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the development of deep learning, medical image classification has been
significantly improved. However, deep learning requires massive data with
labels. While labeling the samples by human experts is expensive and
time-consuming, collecting labels from crowd-sourcing suffers from the noises
which may degenerate the accuracy of classifiers. Therefore, approaches that
can effectively handle label noises are highly desired. Unfortunately, recent
progress on handling label noise in deep learning has gone largely unnoticed by
the medical image. To fill the gap, this paper proposes a noise-tolerant
medical image classification framework named Co-Correcting, which significantly
improves classification accuracy and obtains more accurate labels through
dual-network mutual learning, label probability estimation, and curriculum
label correcting. On two representative medical image datasets and the MNIST
dataset, we test six latest Learning-with-Noisy-Labels methods and conduct
comparative studies. The experiments show that Co-Correcting achieves the best
accuracy and generalization under different noise ratios in various tasks. Our
project can be found at: https://github.com/JiarunLiu/Co-Correcting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03776">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Background modeling is a promising research area in video analysis with a
variety of video surveillance applications. Recent years have witnessed the
proliferation of deep neural networks via effective learning-based approaches
in motion analysis. However, these techniques only provide a limited
description of the observed scenes&#x27; insufficient properties where a
single-valued mapping is learned to approximate the temporal conditional
averages of the target background. On the other hand, statistical learning in
imagery domains has become one of the most prevalent approaches with high
adaptation to dynamic context transformation, notably Gaussian Mixture Models,
combined with a foreground extraction step. In this work, we propose a novel,
two-stage method of change detection with two convolutional neural networks.
The first architecture is grounded on the unsupervised Gaussian mixtures
statistical learning to describe the scenes&#x27; salient features. The second one
implements a light-weight pipeline of foreground detection. Our two-stage
framework contains approximately 3.5K parameters in total but still maintains
rapid convergence to intricate motion patterns. Our experiments on publicly
available datasets show that our proposed networks are not only capable of
generalizing regions of moving objects in unseen cases with promising results
but also are competitive in performance efficiency and effectiveness regarding
foreground segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Convolutional Hough Matching Networks for Robust and Efficient Visual Correspondence. (arXiv:2109.05221v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Despite advances in feature representation, leveraging geometric relations is
crucial for establishing reliable visual correspondences under large variations
of images. In this work we introduce a Hough transform perspective on
convolutional matching and propose an effective geometric matching algorithm,
dubbed Convolutional Hough Matching (CHM). The method distributes similarities
of candidate matches over a geometric transformation space and evaluates them
in a convolutional manner. We cast it into a trainable neural layer with a
semi-isotropic high-dimensional kernel, which learns non-rigid matching with a
small number of interpretable parameters. To further improve the efficiency of
high-dimensional voting, we also propose to use an efficient kernel
decomposition with center-pivot neighbors, which significantly sparsifies the
proposed semi-isotropic kernels without performance degradation. To validate
the proposed techniques, we develop the neural network with CHM layers that
perform convolutional matching in the space of translation and scaling. Our
method sets a new state of the art on standard benchmarks for semantic visual
correspondence, proving its strong robustness to challenging intra-class
variations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Decidability-Based Loss Function. (arXiv:2109.05524v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05524">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Nowadays, deep learning is the standard approach for a wide range of
problems, including biometrics, such as face recognition and speech
recognition, etc. Biometric problems often use deep learning models to extract
features from images, also known as embeddings. Moreover, the loss function
used during training strongly influences the quality of the generated
embeddings. In this work, a loss function based on the decidability index is
proposed to improve the quality of embeddings for the verification routine. Our
proposal, the D-loss, avoids some Triplet-based loss disadvantages such as the
use of hard samples and tricky parameter tuning, which can lead to slow
convergence. The proposed approach is compared against the Softmax
(cross-entropy), Triplets Soft-Hard, and the Multi Similarity losses in four
different benchmarks: MNIST, Fashion-MNIST, CIFAR10 and CASIA-IrisV4. The
achieved results show the efficacy of the proposal when compared to other
popular metrics in the literature. The D-loss computation, besides being
simple, non-parametric and easy to implement, favors both the inter-class and
intra-class scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05023">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present Free Point Transformer (FPT) - a deep neural network architecture
for non-rigid point-set registration. Consisting of two modules, a global
feature extraction module and a point transformation module, FPT does not
assume explicit constraints based on point vicinity, thereby overcoming a
common requirement of previous learning-based point-set registration methods.
FPT is designed to accept unordered and unstructured point-sets with a variable
number of points and uses a &quot;model-free&quot; approach without heuristic
constraints. Training FPT is flexible and involves minimizing an intuitive
unsupervised loss function, but supervised, semi-supervised, and partially- or
weakly-supervised training are also supported. This flexibility makes FPT
amenable to multimodal image registration problems where the ground-truth
deformations are difficult or impossible to measure. In this paper, we
demonstrate the application of FPT to non-rigid registration of prostate
magnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound
(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete
TRUS imaging and sparsely-sampled TRUS imaging, respectively. The results
indicate superior accuracy to the alternative rigid and non-rigid registration
algorithms tested and substantially lower computation time. The rapid inference
possible with FPT makes it particularly suitable for applications where
real-time registration is beneficial.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">U-Net Convolutional Network for Recognition of Vessels and Materials in Chemistry Lab. (arXiv:2109.05585v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05585">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Convolutional networks have been widely applied for computer vision system.
Encouraged by these results, a U-Net convolutional network was applied to
recognition of vessels and materials in chemistry lab using the recent
Vector-LabPics dataset, which contains 2187 images of materials within mostly
transparent vessels in a chemistry lab and other general settings, labeled with
13 classes. By optimizing hyperparameters including learning rates and learning
rate decays, 87% accuracy in vessel recognition was achieved. In the case of
relatively small training and test sets (relatively rare materials states, the
number of training set samples less than 500 and the number of test set samples
less than 100), a comprehensive improvement over 18% in IoU and 19% in accuracy
for the best model were achieved. Further improvements may be achievable by
incorporating improved convolutional network structure into our models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Training robust supervised deep learning models for many geospatial
applications of computer vision is difficult due to dearth of class-balanced
and diverse training data. Conversely, obtaining enough training data for many
applications is financially prohibitive or may be infeasible, especially when
the application involves modeling rare or extreme events. Synthetically
generating data (and labels) using a generative model that can sample from a
target distribution and exploit the multi-scale nature of images can be an
inexpensive solution to address scarcity of labeled data. Towards this goal, we
present a deep conditional generative model, called VAE-Info-cGAN, that
combines a Variational Autoencoder (VAE) with a conditional Information
Maximizing Generative Adversarial Network (InfoGAN), for synthesizing
semantically rich images simultaneously conditioned on a pixel-level condition
(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC
can only vary in the channel dimension from the synthesized image and is meant
to be a task-specific input. The FLC is modeled as an attribute vector in the
latent space of the generated image which controls the contributions of various
characteristic attributes germane to the target distribution. Experiments on a
GPS trajectories dataset show that the proposed model can accurately generate
various forms of spatiotemporal aggregates across different geographic
locations while conditioned only on a raster representation of the road
network. The primary intended application of the VAE-Info-cGAN is synthetic
data (and label) generation for targeted data augmentation for computer
vision-based modeling of problems relevant to geospatial analysis and remote
sensing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tomographic image reconstruction with deep learning is an emerging field, but
a recent landmark study reveals that several deep reconstruction networks are
unstable for computed tomography (CT) and magnetic resonance imaging (MRI).
Specifically, three kinds of instabilities were reported: (1) strong image
artefacts from tiny perturbations, (2) small features missing in a deeply
reconstructed image, and (3) decreased imaging performance with increased input
data. On the other hand, compressed sensing (CS) inspired reconstruction
methods do not suffer from these instabilities because of their built-in kernel
awareness. For deep reconstruction to realize its full potential and become a
mainstream approach for tomographic imaging, it is thus critically important to
meet this challenge by stabilizing deep reconstruction networks. Here we
propose an Analytic Compressed Iterative Deep (ACID) framework to address this
challenge. ACID synergizes a deep reconstruction network trained on big data,
kernel awareness from CS-inspired processing, and iterative refinement to
minimize the data residual relative to real measurement. Our study demonstrates
that the deep reconstruction using ACID is accurate and stable, and sheds light
on the converging mechanism of the ACID iteration under a Bounded Relative
Error Norm (BREN) condition. In particular, the study shows that ACID-based
reconstruction is resilient against adversarial attacks, superior to classic
sparsity-regularized reconstruction alone, and eliminates the three kinds of
instabilities. We anticipate that this integrative data-driven approach will
help promote development and translation of deep tomographic image
reconstruction networks into clinical applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.03918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dynamical models estimate and predict the temporal evolution of physical
systems. State Space Models (SSMs) in particular represent the system dynamics
with many desirable properties, such as being able to model uncertainty in both
the model and measurements, and optimal (in the Bayesian sense) recursive
formulations e.g. the Kalman Filter. However, they require significant domain
knowledge to derive the parametric form and considerable hand-tuning to
correctly set all the parameters. Data driven techniques e.g. Recurrent Neural
Networks have emerged as compelling alternatives to SSMs with wide success
across a number of challenging tasks, in part due to their ability to extract
relevant features from rich inputs. They however lack interpretability and
robustness to unseen conditions. In this work, we present DynaNet, a hybrid
deep learning and time-varying state-space model which can be trained
end-to-end. Our neural Kalman dynamical model allows us to exploit the relative
merits of each approach. We demonstrate state-of-the-art estimation and
prediction on a number of physically challenging tasks, including visual
odometry, sensor fusion for visual-inertial navigation and pendulum control. In
addition we show how DynaNet can indicate failures through investigation of
properties such as the rate of innovation (Kalman Gain).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Preliminary Wildfire Detection Using State-of-the-art PTZ (Pan, Tilt, Zoom) Camera Technology and Convolutional Neural Networks. (arXiv:2109.05083v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05083">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Wildfires are uncontrolled fires in the environment that can be caused by
humans or nature. In 2020 alone, wildfires in California have burned 4.2
million acres, damaged 10,500 buildings or structures, and killed more than 31
people, exacerbated by climate change and a rise in average global
temperatures. This also means there has been an increase in the costs of
extinguishing these treacherous wildfires. The objective of the research is to
detect forest fires in their earlier stages to prevent them from spreading,
prevent them from causing damage to a variety of things, and most importantly,
reduce or eliminate the chances of someone dying from a wildfire. A fire
detection system should be efficient and accurate with respect to extinguishing
wildfires in their earlier stages to prevent the spread of them along with
their consequences. Computer Vision is potentially a more reliable, fast, and
widespread method we need. The current research in the field of preliminary
fire detection has several problems related to unrepresentative data being used
to train models and their existing varied amounts of label imbalance in the
classes of their dataset. We propose a more representative and evenly
distributed data through better settings, lighting, atmospheres, etc., and
class distribution in the entire dataset. After thoroughly examining the
results of this research, it can be inferred that they supported the datasets
strengths by being a viable resource when tested in the real world on
unfamiliar data. This is evident since as the model trains on the dataset, it
is able to generalize on it, hence confirming this is a viable Machine Learning
setting that has practical impact.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MosaicOS: A Simple and Effective Use of Object-Centric Images for Long-Tailed Object Detection. (arXiv:2102.08884v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08884">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Many objects do not appear frequently enough in complex scenes (e.g., certain
handbags in living rooms) for training an accurate object detector, but are
often found frequently by themselves (e.g., in product images). Yet, these
object-centric images are not effectively leveraged for improving object
detection in scene-centric images. In this paper, we propose Mosaic of
Object-centric images as Scene-centric images (MosaicOS), a simple and novel
framework that is surprisingly effective at tackling the challenges of
long-tailed object detection. Keys to our approach are three-fold: (i) pseudo
scene-centric image construction from object-centric images for mitigating
domain differences, (ii) high-quality bounding box imputation using the
object-centric images&#x27; class labels, and (iii) a multi-stage training
procedure. On LVIS object detection (and instance segmentation), MosaicOS leads
to a massive 60% (and 23%) relative improvement in average precision for rare
object categories. We also show that our framework can be compatibly used with
other existing approaches to achieve even further gains. Our pre-trained models
are publicly available at https://github.com/czhang0528/MosaicOS/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths. (arXiv:2103.01435v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01435">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantizing deep networks with adaptive bit-widths is a promising technique
for efficient inference across many devices and resource constraints. In
contrast to static methods that repeat the quantization process and train
different models for different constraints, adaptive quantization enables us to
flexibly adjust the bit-widths of a single deep network during inference for
instant adaptation in different scenarios. While existing research shows
encouraging results on common image classification benchmarks, this paper
investigates how to train such adaptive networks more effectively.
Specifically, we present two novel techniques for quantizing deep neural
networks with adaptive bit-widths of weights and activations. First, we propose
a collaborative strategy to choose a high-precision teacher for transferring
knowledge to the low-precision student while jointly optimizing the model with
all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic
block swapping method by randomly replacing the blocks in the lower-precision
student network with the corresponding blocks in the higher-precision teacher
network. Extensive experiments on multiple image classification datasets
including video classification benchmarks for the first time, well demonstrate
the efficacy of our approach over state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking deinterlacing for early interlaced videos. (arXiv:2011.13675v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13675">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the rapid development of image restoration techniques, high-definition
reconstruction of early videos has achieved impressive results. However, there
are few studies about the interlacing artifacts that often appear in early
videos and significantly affect visual perception. Traditional deinterlacing
approaches are mainly focused on early interlacing scanning systems and thus
cannot handle the complex and complicated artifacts in real-world early
interlaced videos. Hence, this paper proposes a specific deinterlacing network
(DIN), which is motivated by the traditional deinterlacing strategy. The
proposed DIN consists of two stages, i.e., a cooperative vertical interpolation
stage for split fields, and a merging stage that is applied to perceive
movements and remove ghost artifacts. Experimental results demonstrate that the
proposed method can effectively remove complex artifacts in early interlaced
videos.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evaluating the Progress of Deep Learning for Visual Relational Concepts. (arXiv:2001.10857v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.10857">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Convolutional Neural Networks (CNNs) have become the state of the art method
for image classification in the last ten years. Despite the fact that they
achieve superhuman classification accuracy on many popular datasets, they often
perform much worse on more abstract image classification tasks. We will show
that these difficult tasks are linked to relational concepts from cognitive
psychology and that despite progress over the last few years, such relational
reasoning tasks still remain difficult for current neural network
architectures.

We will review deep learning research that is linked to relational concept
learning, even if it was not originally presented from this angle. Reviewing
the current literature, we will argue that some form of attention will be an
important component of future systems to solve relational tasks.

In addition, we will point out the shortcomings of currently used datasets,
and we will recommend steps to make future datasets more relevant for testing
systems on relational reasoning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MSGDD-cGAN: Multi-Scale Gradients Dual Discriminator Conditional Generative Adversarial Network. (arXiv:2109.05614v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05614">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Conditional Generative Adversarial Networks (cGANs) have been used in many
image processing tasks. However, they still have serious problems maintaining
the balance between conditioning the output on the input and creating the
output with the desired distribution based on the corresponding ground truth.
The traditional cGANs, similar to most conventional GANs, suffer from vanishing
gradients, which backpropagate from the discriminator to the generator.
Moreover, the traditional cGANs are sensitive to architectural changes due to
previously mentioned gradient problems. Therefore, balancing the architecture
of the cGANs is almost impossible. Recently MSG-GAN has been proposed to
stabilize the performance of the GANs by applying multiple connections between
the generator and discriminator. In this work, we propose a method called
MSGDD-cGAN, which first stabilizes the performance of the cGANs using
multi-connections gradients flow. Secondly, the proposed network architecture
balances the correlation of the output to input and the fitness of the output
on the target distribution. This balance is generated by using the proposed
dual discrimination procedure. We tested our model by segmentation of fetal
ultrasound images. Our model shows a 3.18% increase in the F1 score comparing
to the pix2pix version of cGANs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05507">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) have been utilized in various applications
ranging from image classification and facial recognition to medical imagery
analysis and real-time object detection. As our models become more
sophisticated and complex, the computational cost of training such models
becomes a burden for small companies and individuals; for this reason,
outsourcing the training process has been the go-to option for such users.
Unfortunately, outsourcing the training process comes at the cost of
vulnerability to backdoor attacks. These attacks aim at establishing hidden
backdoors in the DNN such that the model performs well on benign samples but
outputs a particular target label when a trigger is applied to the input.
Current backdoor attacks rely on generating triggers in the image/pixel domain;
however, as we show in this paper, it is not the only domain to exploit and one
should always &quot;check the other doors&quot;. In this work, we propose a complete
pipeline for generating a dynamic, efficient, and invisible backdoor attack in
the frequency domain. We show the advantages of utilizing the frequency domain
for establishing undetectable and powerful backdoor attacks through extensive
experiments on various datasets and network architectures. The backdoored
models are shown to break various state-of-the-art defences. We also show two
possible defences that succeed against frequency-based backdoor attacks and
possible ways for the attacker to bypass them. We conclude the work with some
remarks regarding a network&#x27;s learning capacity and the capability of embedding
a backdoor attack in the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Universal Adversarial Perturbations (UAPs) are input perturbations that can
fool a neural network on large sets of data. They are a class of attacks that
represents a significant threat as they facilitate realistic, practical, and
low-cost attacks on neural networks. In this work, we derive upper bounds for
the effectiveness of UAPs based on norms of data-dependent Jacobians. We
empirically verify that Jacobian regularization greatly increases model
robustness to UAPs by up to four times whilst maintaining clean performance.
Our theoretical analysis also allows us to formulate a metric for the strength
of shared adversarial perturbations between pairs of inputs. We apply this
metric to benchmark datasets and show that it is highly correlated with the
actual observed robustness. This suggests that realistic and practical
universal attacks can be reliably mitigated without sacrificing clean accuracy,
which shows promise for the robustness of machine learning systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">More Than Just Attention: Improving Cross-Modal Attentions with Contrastive Constraints for Image-Text Matching. (arXiv:2105.09597v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09597">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Cross-modal attention mechanisms have been widely applied to the image-text
matching task and have achieved remarkable improvements thanks to its
capability of learning fine-grained relevance across different modalities.
However, the cross-modal attention models of existing methods could be
sub-optimal and inaccurate because there is no direct supervision provided
during the training process. In this work, we propose two novel training
strategies, namely Contrastive Content Re-sourcing (CCR) and Contrastive
Content Swapping (CCS) constraints, to address such limitations. These
constraints supervise the training of cross-modal attention models in a
contrastive learning manner without requiring explicit attention annotations.
They are plug-in training strategies and can be easily integrated into existing
cross-modal attention models. Additionally, we introduce three metrics
including Attention Precision, Recall, and F1-Score to quantitatively measure
the quality of learned attention models. We evaluate the proposed constraints
by incorporating them into four state-of-the-art cross-modal attention-based
image-text matching models. Experimental results on both Flickr30k and MS-COCO
datasets demonstrate that integrating these constraints improves the model
performance in terms of both retrieval performance and attention metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation in cataract surgery has a wide range of applications
contributing to surgical outcome enhancement and clinical risk reduction.
However, the varying issues in segmenting the different relevant instances make
the designation of a unique network quite challenging. This paper proposes a
semantic segmentation network termed as DeepPyram that can achieve superior
performance in segmenting relevant objects in cataract surgery videos with
varying issues. This superiority mainly originates from three modules: (i)
Pyramid View Fusion, which provides a varying-angle global view of the
surrounding region centering at each pixel position in the input convolutional
feature map; (ii) Deformable Pyramid Reception, which enables a wide deformable
receptive field that can adapt to geometric transformations in the object of
interest; and (iii) Pyramid Loss that adaptively supervises multi-scale
semantic feature maps. These modules can effectively boost semantic
segmentation performance, especially in the case of transparency,
deformability, scalability, and blunt edges in objects. The proposed approach
is evaluated using four datasets of cataract surgery for objects with different
contextual features and compared with thirteen state-of-the-art segmentation
networks. The experimental results confirm that DeepPyram outperforms the rival
approaches without imposing additional trainable parameters. Our comprehensive
ablation study further proves the effectiveness of the proposed modules.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OMNet: Learning Overlapping Mask for Partial-to-Partial Point Cloud Registration. (arXiv:2103.00937v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00937">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Point cloud registration is a key task in many computational fields. Previous
correspondence matching based methods require the inputs to have distinctive
geometric structures to fit a 3D rigid transformation according to point-wise
sparse feature matches. However, the accuracy of transformation heavily relies
on the quality of extracted features, which are prone to errors with respect to
partiality and noise. In addition, they can not utilize the geometric knowledge
of all the overlapping regions. On the other hand, previous global feature
based approaches can utilize the entire point cloud for the registration,
however they ignore the negative effect of non-overlapping points when
aggregating global features. In this paper, we present OMNet, a global feature
based iterative network for partial-to-partial point cloud registration. We
learn overlapping masks to reject non-overlapping regions, which converts the
partial-to-partial registration to the registration of the same shape.
Moreover, the previously used data is sampled only once from the CAD models for
each object, resulting in the same point clouds for the source and reference.
We propose a more practical manner of data generation where a CAD model is
sampled twice for the source and reference, avoiding the previously prevalent
over-fitting issue. Experimental results show that our method achieves
state-of-the-art performance compared to traditional and deep learning based
methods. Code is available at https://github.com/megvii-research/OMNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04795">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. Without natural split of features, single-view
co-training works at the cost of training extra classifiers, where the
algorithm should be delicately designed to prevent individual classifiers from
collapsing into each other. To remove these obstacles which deter the adoption
of single-view co-training, we present a simple and efficient algorithm
Multi-Head Co-Training. By integrating base learners into a multi-head
structure, the model is in a minimal amount of extra parameters. Every
classification head in the unified model interacts with its peers through a
&quot;Weak and Strong Augmentation&quot; strategy, in which the diversity is naturally
brought by the strong data augmentation. Therefore, the proposed method
facilitates single-view co-training by 1). promoting diversity implicitly and
2). only requiring a small extra computational overhead. The effectiveness of
Multi-Head Co-Training is demonstrated in an empirical study on standard
semi-supervised learning benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Re-parameterization Residual Attention Network For Nonhomogeneous Image Dehazing. (arXiv:2109.05479v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05479">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper proposes an end-to-end Efficient Re-parameterizationResidual
Attention Network(ERRA-Net) to directly restore the nonhomogeneous hazy image.
The contribution of this paper mainly has the following three aspects: 1) A
novel Multi-branch Attention (MA) block. The spatial attention mechanism better
reconstructs high-frequency features, and the channel attention mechanism
treats the features of different channels differently. Multi-branch structure
dramatically improves the representation ability of the model and can be
changed into a single path structure after re-parameterization to speed up the
process of inference. Local Residual Connection allows the low-frequency
information in the nonhomogeneous area to pass through the block without
processing so that the block can focus on detailed features. 2) A lightweight
network structure. We use cascaded MA blocks to extract high-frequency features
step by step, and the Multi-layer attention fusion tail combines the shallow
and deep features of the model to get the residual of the clean image finally.
3)We propose two novel loss functions to help reconstruct the hazy image
ColorAttenuation loss and Laplace Pyramid loss. ERRA-Net has an impressive
speed, processing 1200x1600 HD quality images with an average runtime of 166.11
fps. Extensive evaluations demonstrate that ERSANet performs favorably against
the SOTA approaches on the real-world hazy images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Automatic Threat Detection: A Survey of Advances of Deep Learning within X-ray Security Imaging. (arXiv:2001.01293v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.01293">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>X-ray security screening is widely used to maintain aviation/transport
security, and its significance poses a particular interest in automated
screening systems. This paper aims to review computerised X-ray security
imaging algorithms by taxonomising the field into conventional machine learning
and contemporary deep learning applications. The first part briefly discusses
the classical machine learning approaches utilised within X-ray security
imaging, while the latter part thoroughly investigates the use of modern deep
learning algorithms. The proposed taxonomy sub-categorises the use of deep
learning approaches into supervised, semi-supervised and unsupervised learning,
with a particular focus on object classification, detection, segmentation and
anomaly detection tasks. The paper further explores well-established X-ray
datasets and provides a performance benchmark. Based on the current and future
trends in deep learning, the paper finally presents a discussion and future
directions for X-ray security imagery.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05633">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Garments are ubiquitous in both real and many of the virtual worlds. They are
highly deformable objects, exhibit an immense variety of designs and shapes,
and yet, most garments are created from a set of regularly shaped flat pieces.
Exploration of garment structure presents a peculiar case for an object
structure estimation task and might prove useful for downstream tasks of neural
3D garment modeling and reconstruction by providing strong prior on garment
shapes. To facilitate research in these directions, we propose a method for
generating large synthetic datasets of 3D garment designs and their sewing
patterns. Our method consists of a flexible description structure for
specifying parametric sewing pattern templates and the automatic generation
pipeline to produce garment 3D models with little-to-none manual intervention.
To add realism, the pipeline additionally creates corrupted versions of the
final meshes that imitate artifacts of 3D scanning.

With this pipeline, we created the first large-scale synthetic dataset of 3D
garment models with their sewing patterns. The dataset contains more than 20000
garment design variations produced from 19 different base types. Seven of these
garment types are specifically designed to target evaluation of the
generalization across garment sewing pattern topologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Medulloblastoma Tumor Classification using Deep Transfer Learning with Multi-Scale EfficientNets. (arXiv:2109.05025v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05025">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Medulloblastoma (MB) is the most common malignant brain tumor in childhood.
The diagnosis is generally based on the microscopic evaluation of
histopathological tissue slides. However, visual-only assessment of
histopathological patterns is a tedious and time-consuming task and is also
affected by observer variability. Hence, automated MB tumor classification
could assist pathologists by promoting consistency and robust quantification.
Recently, convolutional neural networks (CNNs) have been proposed for this
task, while transfer learning has shown promising results. In this work, we
propose an end-to-end MB tumor classification and explore transfer learning
with various input sizes and matching network dimensions. We focus on
differentiating between the histological subtypes classic and
desmoplastic/nodular. For this purpose, we systematically evaluate recently
proposed EfficientNets, which uniformly scale all dimensions of a CNN. Using a
data set with 161 cases, we demonstrate that pre-trained EfficientNets with
larger input resolutions lead to significant performance improvements compared
to commonly used pre-trained CNN architectures. Also, we highlight the
importance of transfer learning, when using such large architectures. Overall,
our best performing method achieves an F1-Score of 80.1%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Self-Supervised Deep Framework for Reference Bony Shape Estimation in Orthognathic Surgical Planning. (arXiv:2109.05191v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05191">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Virtual orthognathic surgical planning involves simulating surgical
corrections of jaw deformities on 3D facial bony shape models. Due to the lack
of necessary guidance, the planning procedure is highly experience-dependent
and the planning results are often suboptimal. A reference facial bony shape
model representing normal anatomies can provide an objective guidance to
improve planning accuracy. Therefore, we propose a self-supervised deep
framework to automatically estimate reference facial bony shape models. Our
framework is an end-to-end trainable network, consisting of a simulator and a
corrector. In the training stage, the simulator maps jaw deformities of a
patient bone to a normal bone to generate a simulated deformed bone. The
corrector then restores the simulated deformed bone back to normal. In the
inference stage, the trained corrector is applied to generate a
patient-specific normal-looking reference bone from a real deformed bone. The
proposed framework was evaluated using a clinical dataset and compared with a
state-of-the-art method that is based on a supervised point-cloud network.
Experimental results show that the estimated shape models given by our approach
are clinically acceptable and significantly more accurate than that of the
competing method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05021">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Red-lesions, i.e., microaneurysms (MAs) and hemorrhages (HMs), are the early
signs of diabetic retinopathy (DR). The automatic detection of MAs and HMs on
retinal fundus images is a challenging task. Most of the existing methods
detect either only MAs or only HMs because of the difference in their texture,
sizes, and morphology. Though some methods detect both MAs and HMs, they suffer
from the curse of dimensionality of shape and colors features and fail to
detect all shape variations of HMs such as flame-shaped HM. Leveraging the
progress in deep learning, we proposed a two-stream red lesions detection
system dealing simultaneously with small and large red lesions. For this
system, we introduced a new ROIs candidates generation method for large red
lesions fundus images; it is based on blood vessel segmentation and
morphological operations, and reduces the computational complexity, and
enhances the detection accuracy by generating a small number of potential
candidates. For detection, we adapted the Faster RCNN framework with two
streams. We used pre-trained VGGNet as a bone model and carried out several
extensive experiments to tune it for vessels segmentation and candidates
generation, and finally learning the appropriate mapping, which yields better
detection of the red lesions comparing with the state-of-the-art methods. The
experimental results validated the effectiveness of the system in the detection
of both MAs and HMs; the method yields higher performance for per lesion
detection according to sensitivity under 4 FPIs on DiaretDB1-MA and
DiaretDB1-HM datasets, and 1 FPI on e-ophtha and ROCh datasets than the state
of the art methods w.r.t. various evaluation metrics. For DR screening, the
system outperforms other methods on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Domain Adaptive Learning via Synthetic Data for Person Re-identification. (arXiv:2109.05542v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05542">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Person re-identification (re-ID) has gained more and more attention due to
its widespread applications in intelligent video surveillance. Unfortunately,
the mainstream deep learning methods still need a large quantity of labeled
data to train models, and annotating data is an expensive work in real-world
scenarios. In addition, due to domain gaps between different datasets, the
performance is dramatically decreased when re-ID models pre-trained on
label-rich datasets (source domain) are directly applied to other unlabeled
datasets (target domain). In this paper, we attempt to remedy these problems
from two aspects, namely data and methodology. Firstly, we develop a data
collector to automatically generate synthetic re-ID samples in a computer game,
and construct a data labeler to simultaneously annotate them, which free humans
from heavy data collections and annotations. Based on them, we build two
synthetic person re-ID datasets with different scales, &quot;GSPR&quot; and &quot;mini-GSPR&quot;
datasets. Secondly, we propose a synthesis-based multi-domain collaborative
refinement (SMCR) network, which contains a synthetic pretraining module and
two collaborative-refinement modules to implement sufficient learning for the
valuable knowledge from multiple domains. Extensive experiments show that our
proposed framework obtains significant performance improvements over the
state-of-the-art methods on multiple unsupervised domain adaptation tasks of
person re-ID.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09571">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Automated segmentation can assist radiotherapy treatment planning by saving
manual contouring efforts and reducing intra-observer and inter-observer
variations. The recent development of deep learning approaches has revoluted
medical data processing, including semantic segmentation, by dramatically
improving performance. However, training effective deep learning models usually
require a large amount of high-quality labeled data, which are often costly to
collect. We developed a novel semi-supervised adversarial deep learning
approach for 3D pelvic CT image semantic segmentation. Unlike supervised deep
learning methods, the new approach can utilize both annotated and un-annotated
data for training. It generates un-annotated synthetic data by a data
augmentation scheme using generative adversarial networks (GANs). We applied
the new approach to segmenting multiple organs in male pelvic CT images, where
CT images without annotations and GAN-synthesized un-annotated images were used
in semi-supervised learning. Experimental results, evaluated by three metrics
(Dice similarity coefficient, average Hausdorff distance, and average surface
Hausdorff distance), showed that the new method achieved either comparable
performance with substantially fewer annotated images or better performance
with the same amount of annotated data, outperforming the existing
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sickle Cell Disease Severity Prediction from Percoll Gradient Images using Graph Convolutional Networks. (arXiv:2109.05372v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05372">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sickle cell disease (SCD) is a severe genetic hemoglobin disorder that
results in premature destruction of red blood cells. Assessment of the severity
of the disease is a challenging task in clinical routine since the causes of
broad variance in SCD manifestation despite the common genetic cause remain
unclear. Identification of the biomarkers that would predict the severity grade
is of importance for prognosis and assessment of responsiveness of patients to
therapy. Detection of the changes in red blood cell (RBC) density through
separation of Percoll density gradient could be such marker as it allows to
resolve intercellular differences and follow the most damaged dense cells prone
to destruction and vaso-occlusion. Quantification of the images obtained from
the distribution of RBCs in Percoll gradient and interpretation of the obtained
is an important prerequisite for establishment of this approach. Here, we
propose a novel approach combining a graph convolutional network, a
convolutional neural network, fast Fourier transform, and recursive feature
elimination to predict the severity of SCD directly from a Percoll image. Two
important but expensive laboratory blood test parameters measurements are used
for training the graph convolutional network. To make the model independent
from such tests during prediction, the two parameters are estimated by a neural
network from the Percoll image directly. On a cohort of 216 subjects, we
achieve a prediction performance that is only slightly below an approach where
the groundtruth laboratory measurements are used. Our proposed method is the
first computational approach for the difficult task of SCD severity prediction.
The two-step approach relies solely on inexpensive and simple blood analysis
tools and can have a significant impact on the patients&#x27; survival in
underdeveloped countries where access to medical instruments and doctors is
limited</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05526">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The fingerprint classification is an important and effective method to
quicken the process and improve the accuracy in the fingerprint matching
process. Conventional supervised methods need a large amount of pre-labeled
data and thus consume immense human resources. In this paper, we propose a new
and efficient unsupervised deep learning method that can extract fingerprint
features and classify fingerprint patterns automatically. In this approach, a
new model named constraint convolutional auto-encoder (CCAE) is used to extract
fingerprint features and a hybrid clustering strategy is applied to obtain the
final clusters. A set of experiments in the NIST-DB4 dataset shows that the
proposed unsupervised method exhibits the efficient performance on fingerprint
classification. For example, the CCAE achieves an accuracy of 97.3% on only
1000 unlabeled fingerprints in the NIST-DB4.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal Memory Attention for Video Semantic Segmentation. (arXiv:2102.08643v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08643">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Video semantic segmentation requires to utilize the complex temporal
relations between frames of the video sequence. Previous works usually exploit
accurate optical flow to leverage the temporal relations, which suffer much
from heavy computational cost. In this paper, we propose a Temporal Memory
Attention Network (TMANet) to adaptively integrate the long-range temporal
relations over the video sequence based on the self-attention mechanism without
exhaustive optical flow prediction. Specially, we construct a memory using
several past frames to store the temporal information of the current frame. We
then propose a temporal memory attention module to capture the relation between
the current frame and the memory to enhance the representation of the current
frame. Our method achieves new state-of-the-art performances on two challenging
video semantic segmentation datasets, particularly 80.3% mIoU on Cityscapes and
76.5% mIoU on CamVid with ResNet-50.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DEF: Deep Estimation of Sharp Geometric Features in 3D Shapes. (arXiv:2011.15081v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose Deep Estimators of Features (DEFs), a learning-based framework for
predicting sharp geometric features in sampled 3D shapes. Differently from
existing data-driven methods, which reduce this problem to feature
classification, we propose to regress a scalar field representing the distance
from point samples to the closest feature line on local patches. Our approach
is the first that scales to massive point clouds by fusing distance-to-feature
estimates obtained on individual patches. We extensively evaluate our approach
against five baselines on newly proposed synthetic and real-world 3D CAD model
benchmarks. Our approach not only outperforms the baselines (with improvements
in Recall and False Positives Rates), but generalizes to real-world scans after
training our model on synthetic data and fine-tuning it on a small dataset of
scanned data. We demonstrate a downstream application, where we reconstruct an
explicit representation of straight and curved sharp feature lines from range
scan data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CPF: Learning a Contact Potential Field to Model the Hand-Object Interaction. (arXiv:2012.00924v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Modeling the hand-object (HO) interaction not only requires estimation of the
HO pose, but also pays attention to the contact due to their interaction.
Significant progress has been made in estimating hand and object separately
with deep learning methods, simultaneous HO pose estimation and contact
modeling has not yet been fully explored. In this paper, we present an explicit
contact representation namely Contact Potential Field (CPF), and a
learning-fitting hybrid framework namely MIHO to Modeling the Interaction of
Hand and Object. In CPF, we treat each contacting HO vertex pair as a
spring-mass system. Hence the whole system forms a potential field with minimal
elastic energy at the grasp position. Extensive experiments on the two commonly
used benchmarks have demonstrated that our method can achieve state-of-the-art
in several reconstruction metrics, and allow us to produce more physically
plausible HO pose even when the ground-truth exhibits severe interpenetration
or disjointedness. Our code is available at https://github.com/lixiny/CPF.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Follow the Curve: Robotic-Ultrasound Navigation with Learning Based Localization of Spinous Processes for Scoliosis Assessment. (arXiv:2109.05196v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05196">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The scoliosis progression in adolescents requires close monitoring to timely
take treatment measures. Ultrasound imaging is a radiation-free and low-cost
alternative in scoliosis assessment to X-rays, which are typically used in
clinical practice. However, ultrasound images are prone to speckle noises,
making it challenging for sonographers to detect bony features and follow the
spine&#x27;s curvature. This paper introduces a robotic-ultrasound approach for
spinal curvature tracking and automatic navigation. A fully connected network
with deconvolutional heads is developed to locate the spinous process
efficiently with real-time ultrasound images. We use this machine
learning-based method to guide the motion of the robot-held ultrasound probe
and follow the spinal curvature while capturing ultrasound images and
correspondent position. We developed a new force-driven controller that
automatically adjusts the probe&#x27;s pose relative to the skin surface to ensure a
good acoustic coupling between the probe and skin. After the scanning, the
acquired data is used to reconstruct the coronal spinal image, where the
deformity of the scoliosis spine can be assessed and measured. To evaluate the
performance of our methodology, we conducted an experimental study with human
subjects where the deviations from the image center during the robotized
procedure are compared to that obtained from manual scanning. The angles of
spinal deformity measured on spinal reconstruction images were similar for both
methods, implying that they equally reflect human anatomy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present a joint graph convolution-image convolution neural network as our
submission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model
each brain as a graph composed of distinct image regions, which is initially
segmented by a graph neural network (GNN). Subsequently, the tumorous volume
identified by the GNN is further refined by a simple (voxel) convolutional
neural network (CNN), which produces the final segmentation. This approach
captures both global brain feature interactions via the graphical
representation and local image details through the use of convolutional
filters. We find that the GNN component by itself can effectively identify and
segment the brain tumors. The addition of the CNN further improves the median
performance of the model by 2 percent across all metrics evaluated. On the
validation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,
0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm
on the whole tumor, core tumor, and enhancing tumor, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multiresolution Deep Implicit Functions for 3D Shape Representation. (arXiv:2109.05591v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05591">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce Multiresolution Deep Implicit Functions (MDIF), a hierarchical
representation that can recover fine geometry detail, while being able to
perform global operations such as shape completion. Our model represents a
complex 3D shape with a hierarchy of latent grids, which can be decoded into
different levels of detail and also achieve better accuracy. For shape
completion, we propose latent grid dropout to simulate partial data in the
latent space and therefore defer the completing functionality to the decoder
side. This along with our multires design significantly improves the shape
completion quality under decoder-only latent optimization. To the best of our
knowledge, MDIF is the first deep implicit function model that can at the same
time (1) represent different levels of detail and allow progressive decoding;
(2) support both encoder-decoder inference and decoder-only latent
optimization, and fulfill multiple applications; (3) perform detailed
decoder-only shape completion. Experiments demonstrate its superior performance
against prior art in various 3D reconstruction tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05433">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Internet search affects people&#x27;s cognition of the world, so mitigating biases
in search results and learning fair models is imperative for social good. We
study a unique gender bias in image search in this work: the search images are
often gender-imbalanced for gender-neutral natural language queries. We
diagnose two typical image search models, the specialized model trained on
in-domain datasets and the generalized representation model pre-trained on
massive image and text data across the internet. Both models suffer from severe
gender bias. Therefore, we introduce two novel debiasing approaches: an
in-processing fair sampling method to address the gender imbalance issue for
training models, and a post-processing feature clipping method base on mutual
information to debias multimodal representations of pre-trained models.
Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods
significantly reduce the gender bias in image search models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semantic-guided Automatic Natural Image Matting with Trimap Generation Network and Light-weight Non-local Attention. (arXiv:2103.17020v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.17020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Natural image matting aims to precisely separate foreground objects from
background using alpha matte. Fully automatic natural image matting without
external annotation is challenging. Well-performed matting methods usually
require accurate labor-intensive handcrafted trimap as extra input, while the
performance of automatic trimap generation method of dilating foreground
segmentation fluctuates with segmentation quality. Therefore, we argue that how
to handle trade-off of additional information input is a major issue in
automatic matting. This paper presents a semantic-guided automatic natural
image matting pipeline with Trimap Generation Network and light-weight
non-local attention, which does not need trimap and background as input.
Specifically, guided by foreground segmentation, Trimap Generation Network
estimates accurate trimap. Then, with estimated trimap as guidance, our
light-weight Non-local Matting Network with Refinement produces final alpha
matte, whose trimap-guided global aggregation attention block is equipped with
stride downsampling convolution, reducing computation complexity and promoting
performance. Experimental results show that our matting algorithm has
competitive performance with state-of-the-art methods in both trimap-free and
trimap-needed aspects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild. (arXiv:2003.03771v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.03771">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recently, heatmap regression models have become popular due to their superior
performance in locating facial landmarks. However, three major problems still
exist among these models: (1) they are computationally expensive; (2) they
usually lack explicit constraints on global shapes; (3) domain gaps are
commonly present. To address these problems, we propose Pixel-in-Pixel Net
(PIPNet) for facial landmark detection. The proposed model is equipped with a
novel detection head based on heatmap regression, which conducts score and
offset predictions simultaneously on low-resolution feature maps. By doing so,
repeated upsampling layers are no longer necessary, enabling the inference time
to be largely reduced without sacrificing model accuracy. Besides, a simple but
effective neighbor regression module is proposed to enforce local constraints
by fusing predictions from neighboring landmarks, which enhances the robustness
of the new detection head. To further improve the cross-domain generalization
capability of PIPNet, we propose self-training with curriculum. This training
strategy is able to mine more reliable pseudo-labels from unlabeled data across
domains by starting with an easier task, then gradually increasing the
difficulty to provide more precise labels. Extensive experiments demonstrate
the superiority of PIPNet, which obtains state-of-the-art results on three out
of six popular benchmarks under the supervised setting. The results on two
cross-domain test sets are also consistently improved compared to the
baselines. Notably, our lightweight version of PIPNet runs at 35.7 FPS and 200
FPS on CPU and GPU, respectively, while still maintaining a competitive
accuracy to state-of-the-art methods. The code of PIPNet is available at
https://github.com/jhb86253817/PIPNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Monocular Visual Odometry for Flying Robots on Planetary Missions. (arXiv:2109.05509v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05509">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the future, extraterrestrial expeditions will not only be conducted by
rovers but also by flying robots. The technical demonstration drone Ingenuity,
that just landed on Mars, will mark the beginning of a new era of exploration
unhindered by terrain traversability. Robust self-localization is crucial for
that. Cameras that are lightweight, cheap and information-rich sensors are
already used to estimate the ego-motion of vehicles. However, methods proven to
work in man-made environments cannot simply be deployed on other planets. The
highly repetitive textures present in the wastelands of Mars pose a huge
challenge to descriptor matching based approaches.

In this paper, we present an advanced robust monocular odometry algorithm
that uses efficient optical flow tracking to obtain feature correspondences
between images and a refined keyframe selection criterion. In contrast to most
other approaches, our framework can also handle rotation-only motions that are
particularly challenging for monocular odometry systems. Furthermore, we
present a novel approach to estimate the current risk of scale drift based on a
principal component analysis of the relative translation information matrix.
This way we obtain an implicit measure of uncertainty. We evaluate the validity
of our approach on all sequences of a challenging real-world dataset captured
in a Mars-like environment and show that it outperforms state-of-the-art
approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Prioritized Subnet Sampling for Resource-Adaptive Supernet Training. (arXiv:2109.05432v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05432">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A resource-adaptive supernet adjusts its subnets for inference to fit the
dynamically available resources. In this paper, we propose Prioritized Subnet
Sampling to train a resource-adaptive supernet, termed PSS-Net. We maintain
multiple subnet pools, each of which stores the information of substantial
subnets with similar resource consumption. Considering a resource constraint,
subnets conditioned on this resource constraint are sampled from a pre-defined
subnet structure space and high-quality ones will be inserted into the
corresponding subnet pool. Then, the sampling will gradually be prone to
sampling subnets from the subnet pools. Moreover, the one with a better
performance metric is assigned with higher priority to train our PSS-Net, if
sampling is from a subnet pool. At the end of training, our PSS-Net retains the
best subnet in each pool to entitle a fast switch of high-quality subnets for
inference when the available resources vary. Experiments on ImageNet using
MobileNetV1/V2 show that our PSS-Net can well outperform state-of-the-art
resource-adaptive supernets. Our project is at
https://github.com/chenbong/PSS-Net.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05485">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result
in a series of cranio-facial anomalies, and behavioral and neurocognitive
problems. Current diagnosis of FAS is typically done by identifying a set of
facial characteristics, which are often obtained by manual examination.
Anatomical landmark detection, which provides rich geometric information, is
important to detect the presence of FAS associated facial anomalies. This
imaging application is characterized by large variations in data appearance and
limited availability of labeled data. Current deep learning-based heatmap
regression methods designed for facial landmark detection in natural images
assume availability of large datasets and are therefore not wellsuited for this
application. To address this restriction, we develop a new regularized transfer
learning approach that exploits the knowledge of a network learned on large
facial recognition datasets. In contrast to standard transfer learning which
focuses on adjusting the pre-trained weights, the proposed learning approach
regularizes the model behavior. It explicitly reuses the rich visual semantics
of a domain-similar source model on the target task data as an additional
supervisory signal for regularizing landmark detection optimization.
Specifically, we develop four regularization constraints for the proposed
transfer learning, including constraining the feature outputs from
classification and intermediate layers, as well as matching activation
attention maps in both spatial and channel levels. Experimental evaluation on a
collected clinical imaging dataset demonstrate that the proposed approach can
effectively improve model generalizability under limited training samples, and
is advantageous to other approaches in the literature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-based Perception. (arXiv:2109.05441v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05441">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>State-of-the-art methods for driving-scene LiDAR-based perception (including
point cloud semantic segmentation, panoptic segmentation and 3D detection,
\etc) often project the point clouds to 2D space and then process them via 2D
convolution. Although this cooperation shows the competitiveness in the point
cloud, it inevitably alters and abandons the 3D topology and geometric
relations. A natural remedy is to utilize the 3D voxelization and 3D
convolution network. However, we found that in the outdoor point cloud, the
improvement obtained in this way is quite limited. An important reason is the
property of the outdoor point cloud, namely sparsity and varying density.
Motivated by this investigation, we propose a new framework for the outdoor
LiDAR segmentation, where cylindrical partition and asymmetrical 3D convolution
networks are designed to explore the 3D geometric pattern while maintaining
these inherent properties. The proposed model acts as a backbone and the
learned features from this model can be used for downstream tasks such as point
cloud semantic and panoptic segmentation or 3D detection. In this paper, we
benchmark our model on these three tasks. For semantic segmentation, we
evaluate the proposed model on several large-scale datasets, \ie,
SemanticKITTI, nuScenes and A2D2. Our method achieves the state-of-the-art on
the leaderboard of SemanticKITTI (both single-scan and multi-scan challenge),
and significantly outperforms existing methods on nuScenes and A2D2 dataset.
Furthermore, the proposed 3D framework also shows strong performance and good
generalization on LiDAR panoptic segmentation and LiDAR 3D detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation. (arXiv:2109.05346v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05346">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scene graphs are nodes and edges consisting of objects and object-object
relationships, respectively. Scene graph generation (SGG) aims to identify the
objects and their relationships. We propose a bidirectional GRU (BiGRU)
transformer network (BGT-Net) for the scene graph generation for images. This
model implements novel object-object communication to enhance the object
information using a BiGRU layer. Thus, the information of all objects in the
image is available for the other objects, which can be leveraged later in the
object prediction step. This object information is used in a transformer
encoder to predict the object class as well as to create object-specific edge
information via the use of another transformer encoder. To handle the dataset
bias induced by the long-tailed relationship distribution, softening with a
log-softmax function and adding a bias adaptation term to regulate the bias for
every relation prediction individually showed to be an effective approach. We
conducted an elaborate study on experiments and ablations using open-source
datasets, i.e., Visual Genome, Open-Images, and Visual Relationship Detection
datasets, demonstrating the effectiveness of the proposed model over state of
the art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05493">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The utilization of prior knowledge about anomalies is an essential issue for
anomaly detections. Recently, the visual attention mechanism has become a
promising way to improve the performance of CNNs for some computer vision
tasks. In this paper, we propose a novel model called Layer-wise External
Attention Network (LEA-Net) for efficient image anomaly detection. The core
idea relies on the integration of unsupervised and supervised anomaly detectors
via the visual attention mechanism. Our strategy is as follows: (i) Prior
knowledge about anomalies is represented as the anomaly map generated by
unsupervised learning of normal instances, (ii) The anomaly map is translated
to an attention map by the external network, (iii) The attention map is then
incorporated into intermediate layers of the anomaly detection network.
Notably, this layer-wise external attention can be applied to any CNN model in
an end-to-end training manner. For a pilot study, we validate LEA-Net on color
anomaly detection tasks. Through extensive experiments on PlantVillage, MVTec
AD, and Cloud datasets, we demonstrate that the proposed layer-wise visual
attention mechanism consistently boosts anomaly detection performances of an
existing CNN model, even on imbalanced datasets. Moreover, we show that our
attention mechanism successfully boosts the performance of several CNN models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What happens in Face during a facial expression? Using data mining techniques to analyze facial expression motion vectors. (arXiv:2109.05457v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05457">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>One of the most common problems encountered in human-computer interaction is
automatic facial expression recognition. Although it is easy for human observer
to recognize facial expressions, automatic recognition remains difficult for
machines. One of the methods that machines can recognize facial expression is
analyzing the changes in face during facial expression presentation. In this
paper, optical flow algorithm was used to extract deformation or motion vectors
created in the face because of facial expressions. Then, these extracted motion
vectors are used to be analyzed. Their positions and directions were exploited
for automatic facial expression recognition using different data mining
techniques. It means that by employing motion vector features used as our data,
facial expressions were recognized. Some of the most state-of-the-art
classification algorithms such as C5.0, CRT, QUEST, CHAID, Deep Learning (DL),
SVM and Discriminant algorithms were used to classify the extracted motion
vectors. Using 10-fold cross validation, their performances were calculated. To
compare their performance more precisely, the test was repeated 50 times.
Meanwhile, the deformation of face was also analyzed in this research. For
example, what exactly happened in each part of face when a person showed fear?
Experimental results on Extended Cohen-Kanade (CK+) facial expression dataset
demonstrated that the best methods were DL, SVM and C5.0, with the accuracy of
95.3%, 92.8% and 90.2% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05539">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies have shown that convolutional neural networks (CNNs) are not
the only feasible solution for image classification. Furthermore, weight
sharing and backpropagation used in CNNs do not correspond to the mechanisms
present in the primate visual system. To propose a more biologically plausible
solution, we designed a locally connected spiking neural network (SNN) trained
using spike-timing-dependent plasticity (STDP) and its reward-modulated variant
(R-STDP) learning rules. The use of spiking neurons and local connections along
with reinforcement learning (RL) led us to the nomenclature BioLCNet for our
proposed architecture. Our network consists of a rate-coded input layer
followed by a locally connected hidden layer and a decoding output layer. A
spike population-based voting scheme is adopted for decoding in the output
layer. We used the MNIST dataset to obtain image classification accuracy and to
assess the robustness of our rewarding system to varying target responses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09780">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging
exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,
and uses less radiation. Here, we demonstrate the impact of lung segmentation
in COVID-19 identification using CXR images and evaluate which contents of the
image influenced the most. Semantic segmentation was performed using a U-Net
CNN architecture, and the classification using three CNN architectures (VGG,
ResNet, and Inception). Explainable Artificial Intelligence techniques were
employed to estimate the impact of segmentation. A three-classes database was
composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the
impact of creating a CXR image database from different sources, and the
COVID-19 generalization from one source to another. The segmentation achieved a
Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification
using segmented images achieved an F1-Score of 0.88 for the multi-class setup,
and 0.83 for COVID-19 identification. In the cross-dataset scenario, we
obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for
COVID-19 identification using segmented images. Experiments support the
conclusion that even after segmentation, there is a strong bias introduced by
underlying factors from different sources.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Challenges and Solutions in DeepFakes. (arXiv:2109.05397v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05397">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning has been successfully appertained to solve various complex
problems in the area of big data analytics to computer vision. A deep
learning-powered application recently emerged is Deep Fake. It helps to create
fake images and videos that human cannot distinguish them from the real ones
and are recent off-shelf manipulation technique that allows swapping two
identities in a single video. Technology is a controversial technology with
many wide-reaching issues impacting society. So, to counter this emerging
problem, we introduce a dataset of 140k real and fake faces which contain 70k
real faces from the Flickr dataset collected by Nvidia, as well as 70k fake
faces sampled from 1 million fake faces generated by style GAN. We will train
our model in the dataset so that our model can identify real or fake faces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse MLP for Image Recognition: Is Self-Attention Really Necessary?. (arXiv:2109.05422v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05422">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have sprung up in the field of computer vision. In this work, we
explore whether the core self-attention module in Transformer is the key to
achieving excellent performance in image recognition. To this end, we build an
attention-free network called sMLPNet based on the existing MLP-based vision
models. Specifically, we replace the MLP module in the token-mixing step with a
novel sparse MLP (sMLP) module. For 2D image tokens, sMLP applies 1D MLP along
the axial directions and the parameters are shared among rows or columns. By
sparse connection and weight sharing, sMLP module significantly reduces the
number of model parameters and computational complexity, avoiding the common
over-fitting problem that plagues the performance of MLP-like models. When only
trained on the ImageNet-1K dataset, the proposed sMLPNet achieves 81.9% top-1
accuracy with only 24M parameters, which is much better than most CNNs and
vision Transformers under the same model size constraint. When scaling up to
66M parameters, sMLPNet achieves 83.4% top-1 accuracy, which is on par with the
state-of-the-art Swin Transformer. The success of sMLPNet suggests that the
self-attention mechanism is not necessarily a silver bullet in computer vision.
Code will be made publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Complex Constrained Total Variation Image Denoising Algorithm with Application to Phase Retrieval. (arXiv:2109.05496v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05496">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper considers the constrained total variation (TV) denoising problem
for complex-valued images. We extend the definition of TV seminorms for
real-valued images to dealing with complex-valued ones. In particular, we
introduce two types of complex TV in both isotropic and anisotropic forms. To
solve the constrained denoising problem, we adopt a dual approach and derive an
accelerated gradient projection algorithm. We further generalize the proposed
denoising algorithm as a key building block of the proximal gradient scheme to
solve a vast class of complex constrained optimization problems with TV
regularizers. As an example, we apply the proposed algorithmic framework to
phase retrieval. We combine the complex TV regularizer with the conventional
projection-based method within the constraint complex TV model. Initial results
from both simulated and optical experiments demonstrate the validity of the
constrained TV model in extracting sparsity priors within complex-valued
images, while also utilizing physically tractable constraints that help speed
up convergence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-view Snapshot Compressive Imaging via Optical Flow Aided Recurrent Neural Network. (arXiv:2109.05287v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05287">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dual-view snapshot compressive imaging (SCI) aims to capture videos from two
field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot,
achieving joint FoV and temporal compressive sensing, and thus enjoying the
advantages of low-bandwidth, low-power, and low-cost. However, it is
challenging for existing model-based decoding algorithms to reconstruct each
individual scene, which usually require exhaustive parameter tuning with
extremely long running time for large scale data. In this paper, we propose an
optical flow-aided recurrent neural network for dual video SCI systems, which
provides high-quality decoding in seconds. Firstly, we develop a diversity
amplification method to enlarge the differences between scenes of two FoVs, and
design a deep convolutional neural network with dual branches to separate
different scenes from the single measurement. Secondly, we integrate the
bidirectional optical flow extracted from adjacent frames with the recurrent
neural network to jointly reconstruct each video in a sequential manner.
Extensive results on both simulation and real data demonstrate the superior
performance of our proposed model in a short inference time. The code and data
are available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05523">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing research for image text retrieval mainly relies on sentence-level
supervision to distinguish matched and mismatched sentences for a query image.
However, semantic mismatch between an image and sentences usually happens in
finer grain, i.e., phrase level. In this paper, we explore to introduce
additional phrase-level supervision for the better identification of mismatched
units in the text. In practice, multi-grained semantic labels are automatically
constructed for a query image in both sentence-level and phrase-level. We
construct text scene graphs for the matched sentences and extract entities and
triples as the phrase-level labels. In order to integrate both supervision of
sentence-level and phrase-level, we propose Semantic Structure Aware Multimodal
Transformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,
we utilize different kinds of attention mechanisms to enforce interactions of
multi-grain semantic units in both sides of vision and language. For the
training, we propose multi-scale matching losses from both global and local
perspectives, and penalize mismatched phrases. Experimental results on MS-COCO
and Flickr30K show the effectiveness of our approach compared to some
state-of-the-art models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evaluating Computer Vision Techniques for Urban Mobility on Large-Scale, Unconstrained Roads. (arXiv:2109.05226v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05226">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conventional approaches for addressing road safety rely on manual
interventions or immobile CCTV infrastructure. Such methods are expensive in
enforcing compliance to traffic rules and do not scale to large road networks.
This paper proposes a simple mobile imaging setup to address several common
problems in road safety at scale. We use recent computer vision techniques to
identify possible irregularities on roads, the absence of street lights, and
defective traffic signs using videos from a moving camera-mounted vehicle.
Beyond the inspection of static road infrastructure, we also demonstrate the
mobile imaging solution&#x27;s applicability to spot traffic violations. Before
deploying our system in the real-world, we investigate the strengths and
shortcomings of computer vision techniques on thirteen condition-based
hierarchical labels. These conditions include different timings, road type,
traffic density, and state of road damage. Our demonstrations are then carried
out on 2000 km of unconstrained road scenes, captured across an entire city.
Through this, we quantitatively measure the overall safety of roads in the city
through carefully constructed metrics. We also show an interactive dashboard
for visually inspecting and initiating action in a time, labor and
cost-efficient manner. Code, models, and datasets used in this work will be
publicly released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RobustART: Benchmarking Robustness on Architecture Design and Training Techniques. (arXiv:2109.05211v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05211">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks (DNNs) are vulnerable to adversarial noises, which
motivates the benchmark of model robustness. Existing benchmarks mainly focus
on evaluating the defenses, but there are no comprehensive studies of how
architecture design and general training techniques affect robustness.
Comprehensively benchmarking their relationships will be highly beneficial for
better understanding and developing robust DNNs. Thus, we propose RobustART,
the first comprehensive Robustness investigation benchmark on ImageNet
(including open-source toolkit, pre-trained model zoo, datasets, and analyses)
regarding ARchitecture design (44 human-designed off-the-shelf architectures
and 1200+ networks from neural architecture search) and Training techniques
(10+ general techniques, e.g., data augmentation) towards diverse noises
(adversarial, natural, and system noises). Extensive experiments revealed and
substantiated several insights for the first time, for example: (1) adversarial
training largely improves the clean accuracy and all types of robustness for
Transformers and MLP-Mixers; (2) with comparable sizes, CNNs &gt; Transformers &gt;
MLP-Mixers on robustness against natural and system noises; Transformers &gt;
MLP-Mixers &gt; CNNs on adversarial robustness; (3) for some light-weight
architectures (e.g., EfficientNet, MobileNetV2, and MobileNetV3), increasing
model sizes or using extra training data cannot improve robustness. Our
benchmark this http URL : (1) presents an open-source platform for
conducting comprehensive evaluation on diverse robustness types; (2) provides a
variety of pre-trained models with different training techniques to facilitate
robustness evaluation; (3) proposes a new view to better understand the
mechanism towards designing robust DNN architectures, backed up by the
analysis. We will continuously contribute to building this ecosystem for the
community.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Stereoscopy exposits a natural perception of distance in a scene, and its
manifestation in 3D world understanding is an intuitive phenomenon. However, an
innate rigid calibration of binocular vision sensors is crucial for accurate
depth estimation. Alternatively, a monocular camera alleviates the limitation
at the expense of accuracy in estimating depth, and the challenge exacerbates
in harsh environmental conditions. Moreover, an optical sensor often fails to
acquire vital signals in harsh environments, and radar is used instead, which
gives coarse but more accurate signals. This work explores the utility of
coarse signals from radar when fused with fine-grained data from a monocular
camera for depth estimation in harsh environmental conditions. A variant of
feature pyramid network (FPN) extensively operates on fine-grained image
features at multiple scales with a fewer number of parameters. FPN feature maps
are fused with sparse radar features extracted with a Convolutional neural
network. The concatenated hierarchical features are used to predict the depth
with ordinal regression. We performed experiments on the nuScenes dataset, and
the proposed architecture stays on top in quantitative evaluations with reduced
parameters and faster inference. The depth estimation results suggest that the
proposed techniques can be used as an alternative to stereo depth estimation in
critical applications in robotics and self-driving cars. The source code will
be available in the following: \url{https://github.com/MI-Hussain/RVMDE}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via Online Exploration and Synthesis. (arXiv:2109.05488v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05488">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Estimating the articulated 3D hand-object pose from a single RGB image is a
highly ambiguous and challenging problem requiring large-scale datasets that
contain diverse hand poses, object poses, and camera viewpoints. Most
real-world datasets lack this diversity. In contrast, synthetic datasets can
easily ensure vast diversity, but learning from them is inefficient and suffers
from heavy training consumption. To address the above issues, we propose
ArtiBoost, a lightweight online data enrichment method that boosts articulated
hand-object pose estimation from the data perspective. ArtiBoost is employed
along with a real-world source dataset. During training, ArtiBoost
alternatively performs data exploration and synthesis. ArtiBoost can cover
various hand-object poses and camera viewpoints based on a Compositional
hand-object Configuration and Viewpoint space (CCV-space) and can adaptively
enrich the current hard-discernable samples by a mining strategy. We apply
ArtiBoost on a simple learning baseline network and demonstrate the performance
boost on several hand-object benchmarks. As an illustrative example, with
ArtiBoost, even a simple baseline network can outperform the previous
start-of-the-art based on Transformer on the HO3D dataset. Our code is
available at https://github.com/MVIG-SJTU/ArtiBoost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05409">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper gives a detailed description of the pipelines used for the 2nd
edition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.
An overview of the data preprocessing steps applied is provided along with a
brief description of the pipelines used, in terms of the architecture and the
hyperparameters. Our code for this work can be found at:
https://github.com/ivadomed/ms-challenge-2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CAN3D: Fast 3D Medical Image Segmentation via Compact Context Aggregation. (arXiv:2109.05443v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05443">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Direct automatic segmentation of objects from 3D medical imaging, such as
magnetic resonance (MR) imaging, is challenging as it often involves accurately
identifying a number of individual objects with complex geometries within a
large volume under investigation. To address these challenges, most deep
learning approaches typically enhance their learning capability by
substantially increasing the complexity or the number of trainable parameters
within their models. Consequently, these models generally require long
inference time on standard workstations operating clinical MR systems and are
restricted to high-performance computing hardware due to their large memory
requirement. Further, to fit 3D dataset through these large models using
limited computer memory, trade-off techniques such as patch-wise training are
often used which sacrifice the fine-scale geometric information from input
images which could be clinically significant for diagnostic purposes. To
address these challenges, we present a compact convolutional neural network
with a shallow memory footprint to efficiently reduce the number of model
parameters required for state-of-art performance. This is critical for
practical employment as most clinical environments only have low-end hardware
with limited computing power and memory. The proposed network can maintain data
integrity by directly processing large full-size 3D input volumes with no
patches required and significantly reduces the computational time required for
both training and inference. We also propose a novel loss function with extra
shape constraint to improve the accuracy for imbalanced classes in 3D MR
images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Class-Distribution-Aware Calibration for Long-Tailed Visual Recognition. (arXiv:2109.05263v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05263">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite impressive accuracy, deep neural networks are often miscalibrated and
tend to overly confident predictions. Recent techniques like temperature
scaling (TS) and label smoothing (LS) show effectiveness in obtaining a
well-calibrated model by smoothing logits and hard labels with scalar factors,
respectively. However, the use of uniform TS or LS factor may not be optimal
for calibrating models trained on a long-tailed dataset where the model
produces overly confident probabilities for high-frequency classes. In this
study, we propose class-distribution-aware TS (CDA-TS) and LS (CDA-LS) by
incorporating class frequency information in model calibration in the context
of long-tailed distribution. In CDA-TS, the scalar temperature value is
replaced with the CDA temperature vector encoded with class frequency to
compensate for the over-confidence. Similarly, CDA-LS uses a vector smoothing
factor and flattens the hard labels according to their corresponding class
distribution. We also integrate CDA optimal temperature vector with
distillation loss, which reduces miscalibration in self-distillation (SD). We
empirically show that class-distribution-aware TS and LS can accommodate the
imbalanced data distribution yielding superior performance in both calibration
error and predictive accuracy. We also observe that SD with an extremely
imbalanced dataset is less effective in terms of calibration performance. Code
is available in https://github.com/mobarakol/Class-Distribution-Aware-TS-LS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to high
computation and storage efficiency. Most existing hashing methods can not
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes, which is less
effective to capture subtle but discriminative visual details. To improve
fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization
(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to
capture and preserve fine-grained semantic information from multi-level
features. Besides, we propose a learnable quantization module with a partial
attention mechanism, which helps to optimize the most relevant codewords and
improves the quantization. Comprehensive experiments demonstrate that PHPQ
outperforms state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bornon: Bengali Image Captioning with Transformer-based Deep learning approach. (arXiv:2109.05218v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05218">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image captioning using Encoder-Decoder based approach where CNN is used as
the Encoder and sequence generator like RNN as Decoder has proven to be very
effective. However, this method has a drawback that is sequence needs to be
processed in order. To overcome this drawback some researcher has utilized the
Transformer model to generate captions from images using English datasets.
However, none of them generated captions in Bengali using the transformer
model. As a result, we utilized three different Bengali datasets to generate
Bengali captions from images using the Transformer model. Additionally, we
compared the performance of the transformer-based model with a visual
attention-based Encoder-Decoder approach. Finally, we compared the result of
the transformer-based model with other models that employed different Bengali
image captioning datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">iMAP: Implicit Mapping and Positioning in Real-Time. (arXiv:2103.12352v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We show for the first time that a multilayer perceptron (MLP) can serve as
the only scene representation in a real-time SLAM system for a handheld RGB-D
camera. Our network is trained in live operation without prior data, building a
dense, scene-specific implicit 3D model of occupancy and colour which is also
immediately used for tracking.

Achieving real-time SLAM via continual training of a neural network against a
live image stream requires significant innovation. Our iMAP algorithm uses a
keyframe structure and multi-processing computation flow, with dynamic
information-guided pixel sampling for speed, with tracking at 10 Hz and global
map updating at 2 Hz. The advantages of an implicit MLP over standard dense
SLAM techniques include efficient geometry representation with automatic detail
control and smooth, plausible filling-in of unobserved regions such as the back
surfaces of objects.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Ad-dressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus con-firming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.12289">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present and make available MedLatinEpi and MedLatinLit, two datasets of
medieval Latin texts to be used in research on computational authorship
analysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,
respectively, labelled by author; MedLatinEpi texts are of epistolary nature,
while MedLatinLit texts consist of literary comments and treatises about
various subjects. As such, these two datasets lend themselves to supporting
research in authorship analysis tasks, such as authorship attribution,
authorship verification, or same-author verification. Along with the datasets
we provide experimental results, obtained on these datasets, for the authorship
verification task, i.e., the task of predicting whether a text of unknown
authorship was written by a candidate author or not. We also make available the
source code of the authorship verification system we have used, thus allowing
our experiments to be reproduced, and to be used as baselines, by other
researchers. We also describe the application of the above authorship
verification system, using these datasets as training data, for investigating
the authorship of two medieval epistles whose authorship has been disputed by
scholars.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dealing with Typos for BERT-based Passage Retrieval and Ranking. (arXiv:2108.12139v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12139">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Passage retrieval and ranking is a key task in open-domain question answering
and information retrieval. Current effective approaches mostly rely on
pre-trained deep language model-based retrievers and rankers. These methods
have been shown to effectively model the semantic matching between queries and
passages, also in presence of keyword mismatch, i.e. passages that are relevant
to a query but do not contain important query keywords. In this paper we
consider the Dense Retriever (DR), a passage retrieval method, and the BERT
re-ranker, a popular passage re-ranking method. In this context, we formally
investigate how these models respond and adapt to a specific type of keyword
mismatch -- that caused by keyword typos occurring in queries. Through
empirical investigation, we find that typos can lead to a significant drop in
retrieval and ranking effectiveness. We then propose a simple typos-aware
training framework for DR and BERT re-ranker to address this issue. Our
experimental results on the MS MARCO passage ranking dataset show that, with
our proposed typos-aware training, DR and BERT re-ranker can become robust to
typos in queries, resulting in significantly improved effectiveness compared to
models trained without appropriately accounting for typos.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Framework for App Store Optimization. (arXiv:1905.11668v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.11668">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper a framework for app store optimization is proposed. The
framework is based on two main areas: developer dependent elements and user
dependent elements. Developer dependent elements are similar to factors in
search engine optimization. User dependent elements are similar to activities
in social media. The proposed framework is modelled after downloading sample
data from two leading app stores: Google Play and Apple iTunes. Results show
that developer dependent elements can be better optimized. Names and
descriptions of mobile apps are not fully utilized.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Improved Hybrid Recommender System: Integrating Document Context-Based and Behavior-Based Methods. (arXiv:2109.05516v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05516">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>One of the main challenges in recommender systems is data sparsity which
leads to high variance. Several attempts have been made to improve the
bias-variance trade-off using auxiliary information. In particular, document
modeling-based methods have improved the model&#x27;s accuracy by using textual data
such as reviews, abstracts, and storylines when the user-to-item rating matrix
is sparse. However, such models are insufficient to learn optimal
representation for users and items. User-based and item-based collaborative
filtering, owing to their efficiency and interpretability, have been long used
for building recommender systems. They create a profile for each user and item
respectively as their historically interacted items and the users who
interacted with the target item.

This work combines these two approaches with document context-aware
recommender systems by considering users&#x27; opinions on these items. Another
advantage of our model is that it supports online personalization. If a user
has new interactions, it needs to refresh the user and item history
representation vectors instead of updating model parameters. The proposed
algorithm is implemented and tested on three real-world datasets that
demonstrate our model&#x27;s effectiveness over the baseline methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14541">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
https://github.com/IBM/yaso-tsa.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Memory Augmented Multi-Instance Contrastive Predictive Coding for Sequential Recommendation. (arXiv:2109.00368v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00368">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The sequential recommendation aims to recommend items, such as products,
songs and places, to users based on the sequential patterns of their historical
records. Most existing sequential recommender models consider the next item
prediction task as the training signal. Unfortunately, there are two essential
challenges for these methods: (1) the long-term preference is difficult to
capture, and (2) the supervision signal is too sparse to effectively train a
model. In this paper, we propose a novel sequential recommendation framework to
overcome these challenges based on a memory augmented multi-instance
contrastive predictive coding scheme, denoted as MMInfoRec. The basic
contrastive predictive coding (CPC) serves as encoders of sequences and items.
The memory module is designed to augment the auto-regressive prediction in CPC
to enable a flexible and general representation of the encoded preference,
which can improve the ability to capture the long-term preference. For
effective training of the MMInfoRec model, a novel multi-instance noise
contrastive estimation (MINCE) loss is proposed, using multiple positive
samples, which offers effective exploitation of samples inside a mini-batch.
The proposed MMInfoRec framework falls into the contrastive learning style,
within which, however, a further finetuning step is not required given that its
contrastive training task is well aligned with the target recommendation task.
With extensive experiments on four benchmark datasets, MMInfoRec can outperform
the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast-adapting and Privacy-preserving Federated Recommender System. (arXiv:2104.00919v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00919">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In the mobile Internet era, the recommender system has become an
irreplaceable tool to help users discover useful items, and thus alleviating
the information overload problem. Recent deep neural network (DNN)-based
recommender system research have made significant progress in improving
prediction accuracy, which is largely attributed to the access to a large
amount of users&#x27; personal data collected from users&#x27; devices and then centrally
stored in the cloud server. However, as there are rising concerns around the
globe on user privacy leakage in the online platform, the public is becoming
anxious by such abuse of user privacy. Therefore, it is urgent and beneficial
to develop a recommender system that can achieve both high prediction accuracy
and high degree of user privacy protection.

To this end, we propose a DNN-based recommendation model called PrivRec
running on the decentralized federated learning (FL) environment, which ensures
that a user&#x27;s data never leaves his/her during the course of model training. On
the other hand, to better embrace the data heterogeneity commonly existing in
FL, we innovatively introduce a first-order meta-learning method that enables
fast in-device personalization with only few data points. Furthermore, to
defense from potential malicious participant that poses serious security threat
to other users, we develop a user-level differentially private DP-PrivRec model
so that it is unable to determine whether a particular user is present or not
solely based on the trained model. Finally, we conduct extensive experiments on
two large-scale datasets in a simulated FL environment, and the results
validate the superiority of our proposed PrivRec and DP-PrivRec.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">User Fairness, Item Fairness, and Diversity for Rankings in Two-Sided Markets. (arXiv:2010.01470v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01470">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Ranking items by their probability of relevance has long been the goal of
conventional ranking systems. While this maximizes traditional criteria of
ranking performance, there is a growing understanding that it is an
oversimplification in online platforms that serve not only a diverse user
population, but also the producers of the items. In particular, ranking
algorithms are expected to be fair in how they serve all groups of users -- not
just the majority group -- and they also need to be fair in how they divide
exposure among the items. These fairness considerations can partially be met by
adding diversity to the rankings, as done in several recent works. However, we
show in this paper that user fairness, item fairness and diversity are
fundamentally different concepts. In particular, we find that algorithms that
consider only one of the three desiderata can fail to satisfy and even harm the
other two. To overcome this shortcoming, we present the first ranking algorithm
that explicitly enforces all three desiderata. The algorithm optimizes user and
item fairness as a convex optimization problem which can be solved optimally.
From its solution, a ranking policy can be derived via a novel Birkhoff-von
Neumann decomposition algorithm that optimizes diversity. Beyond the
theoretical analysis, we investigate empirically on a new benchmark dataset how
effectively the proposed ranking algorithm can control user fairness, item
fairness and diversity, as well as the trade-offs between them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Top-N Recommendation with Counterfactual User Preference Simulation. (arXiv:2109.02444v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02444">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Top-N recommendation, which aims to learn user ranking-based preference, has
long been a fundamental problem in a wide range of applications. Traditional
models usually motivate themselves by designing complex or tailored
architectures based on different assumptions. However, the training data of
recommender system can be extremely sparse and imbalanced, which poses great
challenges for boosting the recommendation performance. To alleviate this
problem, in this paper, we propose to reformulate the recommendation task
within the causal inference framework, which enables us to counterfactually
simulate user ranking-based preferences to handle the data scarce problem. The
core of our model lies in the counterfactual question: &quot;what would be the
user&#x27;s decision if the recommended items had been different?&quot;. To answer this
question, we firstly formulate the recommendation process with a series of
structural equation models (SEMs), whose parameters are optimized based on the
observed data. Then, we actively indicate many recommendation lists (called
intervention in the causal inference terminology) which are not recorded in the
dataset, and simulate user feedback according to the learned SEMs for
generating new training samples. Instead of randomly intervening on the
recommendation list, we design a learning-based method to discover more
informative training samples. Considering that the learned SEMs can be not
perfect, we, at last, theoretically analyze the relation between the number of
generated samples and the model prediction error, based on which a heuristic
method is designed to control the negative effect brought by the prediction
error. Extensive experiments are conducted based on both synthetic and
real-world datasets to demonstrate the effectiveness of our framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Recommendation with Cross-channel Matching Representation and Hierarchical Contrastive Learning. (arXiv:2109.00676v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00676">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, using different channels to model social semantic information, and
using self-supervised learning tasks to maintain the characteristics of each
channel when fusing the information, which has been proven to be a very
promising work. However, how to deeply dig out the relationship between
different channels and make full use of it while maintaining the uniqueness of
each channel is a problem that has not been well studied and resolved in this
field. Under such circumstances, this paper explores and verifies the
deficiency of directly constructing contrastive learning tasks on different
channels with practical experiments and proposes the scheme of interactive
modeling and matching representation across different channels. This is the
first attempt in the field of recommender systems, we believe the insight of
this paper is inspirational to future self-supervised learning research based
on multi-channel information. To solve this problem, we propose a cross-channel
matching representation model based on attentive interaction, which realizes
efficient modeling of the relationship between cross-channel information. Based
on this, we also proposed a hierarchical self-supervised learning model, which
realized two levels of self-supervised learning within and between channels and
improved the ability of self-supervised tasks to autonomously mine different
levels of potential information. We have conducted abundant experiments, and
many experimental metrics on multiple public data sets show that the method
proposed in this paper has a significant improvement compared with the
state-of-the-art methods, no matter in the general or cold-start scenario. And
in the experiment of model variant analysis, the benefits of the cross-channel
matching representation model and the hierarchical self-supervised model
proposed in this paper are also fully verified.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion. (arXiv:2108.08513v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08513">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>BERT-based information retrieval models are expensive, in both time (query
latency) and computational resources (energy, hardware cost), making many of
these models impractical especially under resource constraints. The reliance on
a query encoder that only performs tokenization and on the pre-processing of
passage representations at indexing, has allowed the recently proposed TILDE
method to overcome the high query latency issue typical of BERT-based models.
This however is at the expense of a lower effectiveness compared to other
BERT-based re-rankers and dense retrievers. In addition, the original TILDE
method is characterised by indexes with a very high memory footprint, as it
expands each passage into the size of the BERT vocabulary. In this paper, we
propose TILDEv2, a new model that stems from the original TILDE but that
addresses its limitations. TILDEv2 relies on contextualized exact term matching
with expanded passages. This requires to only store in the index the score of
tokens that appear in the expanded passages (rather than all the vocabulary),
thus producing indexes that are 99% smaller than those of TILDE. This matching
mechanism also improves ranking effectiveness by 24%, without adding to the
query latency. This makes TILDEv2 the state-of-the-art passage re-ranking
method for CPU-only environments, capable of maintaining query latency below
100ms on commodity hardware.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness of Exposure in Stochastic Bandits. (arXiv:2103.02735v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02735">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contextual bandit algorithms have become widely used for recommendation in
online systems (e.g. marketplaces, music streaming, news), where they now wield
substantial influence on which items get exposed to the users. This raises
questions of fairness to the items -- and to the sellers, artists, and writers
that benefit from this exposure. We argue that the conventional bandit
formulation can lead to an undesirable and unfair winner-takes-all allocation
of exposure. To remedy this problem, we propose a new bandit objective that
guarantees merit-based fairness of exposure to the items while optimizing
utility to the users. We formulate fairness regret and reward regret in this
setting, and present algorithms for both stochastic multi-armed bandits and
stochastic linear bandits. We prove that the algorithms achieve sub-linear
fairness regret and reward regret. Beyond the theoretical analysis, we also
provide empirical evidence that these algorithms can fairly allocate exposure
to different arms effectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07814">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mismatched Estimation of rank-one symmetric matrices under Gaussian noise. (arXiv:2107.08927v2 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08927">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the estimation of an n-dimensional vector s from the noisy
element-wise measurements of $\mathbf{s}\mathbf{s}^T$, a generic problem that
arises in statistics and machine learning. We study a mismatched Bayesian
inference setting, where some of the parameters are not known to the
statistician. We derive the full exact analytic expression of the asymptotic
mean squared error (MSE) in the large system size limit for the particular case
of Gaussian priors and additive noise. From our formulas, we see that
estimation is still possible in the mismatched case; and also that the minimum
MSE (MMSE) can be achieved if the statistician chooses suitable parameters. Our
technique relies on the asymptotics of the spherical integrals and can be
applied as long as the statistician chooses a rotationally invariant prior.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to high
computation and storage efficiency. Most existing hashing methods can not
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes, which is less
effective to capture subtle but discriminative visual details. To improve
fine-grained image hashing, we propose Pyramid Hybrid Pooling Quantization
(PHPQ). Specifically, we propose a Pyramid Hybrid Pooling (PHP) module to
capture and preserve fine-grained semantic information from multi-level
features. Besides, we propose a learnable quantization module with a partial
attention mechanism, which helps to optimize the most relevant codewords and
improves the quantization. Comprehensive experiments demonstrate that PHPQ
outperforms state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05125">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR&#x27;21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL&#x27;s performance
matches or exceeds ALIGN&#x27;s cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL&#x27;s text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Existence conditions for hidden feedback loops in online recommender systems. (arXiv:2109.05278v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05278">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We explore a hidden feedback loops effect in online recommender systems.
Feedback loops result in degradation of online multi-armed bandit (MAB)
recommendations to a small subset and loss of coverage and novelty. We study
how uncertainty and noise in user interests influence the existence of feedback
loops. First, we show that an unbiased additive random noise in user interests
does not prevent a feedback loop. Second, we demonstrate that a non-zero
probability of resetting user interests is sufficient to limit the feedback
loop and estimate the size of the effect. Our experiments confirm the
theoretical findings in a simulated environment for four bandit algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation. (arXiv:2109.05446v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05446">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>News recommendation is critical for personalized news access. Most existing
news recommendation methods rely on centralized storage of users&#x27; historical
news click behavior data, which may lead to privacy concerns and hazards.
Federated Learning is a privacy-preserving framework for multiple clients to
collaboratively train models without sharing their private data. However, the
computation and communication cost of directly learning many existing news
recommendation models in a federated way are unacceptable for user clients. In
this paper, we propose an efficient federated learning framework for
privacy-preserving news recommendation. Instead of training and communicating
the whole model, we decompose the news recommendation model into a large news
model maintained in the server and a light-weight user model shared on both
server and clients, where news representations and user model are communicated
between server and clients. More specifically, the clients request the user
model and news representations from the server, and send their locally computed
gradients to the server for aggregation. The server updates its global user
model with the aggregated gradients, and further updates its news model to
infer updated news representations. Since the local gradients may contain
private information, we propose a secure aggregation method to aggregate
gradients in a privacy-preserving way. Experiments on two real-world datasets
show that our method can reduce the computation and communication cost on
clients while keep promising model performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uni-FedRec: A Unified Privacy-Preserving News Recommendation Framework for Model Training and Online Serving. (arXiv:2109.05236v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05236">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>News recommendation is important for personalized online news services. Most
existing news recommendation methods rely on centrally stored user behavior
data to both train models offline and provide online recommendation services.
However, user data is usually highly privacy-sensitive, and centrally storing
them may raise privacy concerns and risks. In this paper, we propose a unified
news recommendation framework, which can utilize user data locally stored in
user clients to train models and serve users in a privacy-preserving way.
Following a widely used paradigm in real-world recommender systems, our
framework contains two stages. The first one is for candidate news generation
(i.e., recall) and the second one is for candidate news ranking (i.e.,
ranking). At the recall stage, each client locally learns multiple interest
representations from clicked news to comprehensively model user interests.
These representations are uploaded to the server to recall candidate news from
a large news pool, which are further distributed to the user client at the
ranking stage for personalized news display. In addition, we propose an
interest decomposer-aggregator method with perturbation noise to better protect
private user information encoded in user interest representations. Besides, we
collaboratively train both recall and ranking models on the data decentralized
in a large number of user clients in a privacy-preserving way. Experiments on
two real-world news datasets show that our method can outperform baseline
methods and effectively protect user privacy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation. (arXiv:2109.05261v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05261">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning user representations based on historical behaviors lies at the core
of modern recommender systems. Recent advances in sequential recommenders have
convincingly demonstrated high capability in extracting effective user
representations from the given behavior sequences. Despite significant
progress, we argue that solely modeling the observational behaviors sequences
may end up with a brittle and unstable system due to the noisy and sparse
nature of user interactions logged. In this paper, we propose to learn accurate
and robust user representations, which are required to be less sensitive to
(attack on) noisy behaviors and trust more on the indispensable ones, by
modeling counterfactual data distribution. Specifically, given an observed
behavior sequence, the proposed CauseRec framework identifies dispensable and
indispensable concepts at both the fine-grained item level and the abstract
interest level. CauseRec conditionally samples user concept sequences from the
counterfactual data distributions by replacing dispensable and indispensable
concepts within the original concept sequence. With user representations
obtained from the synthesized user sequences, CauseRec performs contrastive
user representation learning by contrasting the counterfactual with the
observational. We conduct extensive experiments on real-world public
recommendation benchmarks and justify the effectiveness of CauseRec with
multi-aspects model analysis. The results demonstrate that the proposed
CauseRec outperforms state-of-the-art sequential recommenders by learning
accurate and robust user representations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Quantization with Code Memory for Unsupervised Image Retrieval. (arXiv:2109.05205v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05205">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The high efficiency in computation and storage makes hashing (including
binary hashing and quantization) a common strategy in large-scale retrieval
systems. To alleviate the reliance on expensive annotations, unsupervised deep
hashing becomes an important research problem. This paper provides a novel
solution to unsupervised deep quantization, namely Contrastive Quantization
with Code Memory (MeCoQ). Different from existing reconstruction-based
strategies, we learn unsupervised binary descriptors by contrastive learning,
which can better capture discriminative visual semantics. Besides, we uncover
that codeword diversity regularization is critical to prevent contrastive
learning-based quantization from model degeneration. Moreover, we introduce a
novel quantization code memory module that boosts contrastive learning with
lower feature drift than conventional feature memories. Extensive experiments
on benchmark datasets show that MeCoQ outperforms state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Discovering Technology Gaps using the IntSight Knowledge Navigator. (arXiv:2109.05142v1 [cs.DB])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05142">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge analysis is an important application of knowledge graphs. In this
paper, we present a complex knowledge analysis problem that discovers the gaps
in the technology areas of interest to an organization. Our knowledge graph is
developed on a heterogeneous data management platform. The analysis combines
semantic search, graph analytics, and polystore query optimization.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Accuracy of Analog Neural Network Inference Accelerators. (arXiv:2109.01262v2 [cs.AR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01262">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Specialized accelerators have recently garnered attention as a method to
reduce the power consumption of neural network inference. A promising category
of accelerators utilizes nonvolatile memory arrays to both store weights and
perform $\textit{in situ}$ analog computation inside the array. While prior
work has explored the design space of analog accelerators to optimize
performance and energy efficiency, there is seldom a rigorous evaluation of the
accuracy of these accelerators. This work shows how architectural design
decisions, particularly in mapping neural network parameters to analog memory
cells, influence inference accuracy. When evaluated using ResNet50 on ImageNet,
the resilience of the system to analog non-idealities - cell programming
errors, analog-to-digital converter resolution, and array parasitic resistances
- all improve when analog quantities in the hardware are made proportional to
the weights in the network. Moreover, contrary to the assumptions of prior
work, nearly equivalent resilience to cell imprecision can be achieved by fully
storing weights as analog quantities, rather than spreading weight bits across
multiple devices, often referred to as bit slicing. By exploiting
proportionality, analog system designers have the freedom to match the
precision of the hardware to the needs of the algorithm, rather than attempting
to guarantee the same level of precision in the intermediate results as an
equivalent digital accelerator. This ultimately results in an analog
accelerator that is more accurate, more robust to analog errors, and more
energy-efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14541">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current TSA evaluation in a cross-domain setup is restricted to the small set
of review domains available in existing datasets. Such an evaluation is
limited, and may not reflect true performance on sites like Amazon or Yelp that
host diverse reviews from many domains. To address this gap, we present YASO -
a new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215
English sentences from dozens of review domains, annotated with target terms
and their sentiment. Our analysis verifies the reliability of these
annotations, and explores the characteristics of the collected data. Benchmark
results using five contemporary TSA systems show there is ample room for
improvement on this challenging new dataset. YASO is available at
https://github.com/IBM/yaso-tsa.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Langevin Monte Carlo: random coordinate descent and variance reduction. (arXiv:2007.14209v6 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14209">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Langevin Monte Carlo (LMC) is a popular Bayesian sampling method. For the
log-concave distribution function, the method converges exponentially fast, up
to a controllable discretization error. However, the method requires the
evaluation of a full gradient in each iteration, and for a problem on
$\mathbb{R}^d$, this amounts to $d$ times partial derivative evaluations per
iteration. The cost is high when $d\gg1$. In this paper, we investigate how to
enhance computational efficiency through the application of RCD (random
coordinate descent) on LMC. There are two sides of the theory:

1 By blindly applying RCD to LMC, one surrogates the full gradient by a
randomly selected directional derivative per iteration. Although the cost is
reduced per iteration, the total number of iteration is increased to achieve a
preset error tolerance. Ultimately there is no computational gain;

2 We then incorporate variance reduction techniques, such as SAGA (stochastic
average gradient) and SVRG (stochastic variance reduced gradient), into
RCD-LMC. It will be proved that the cost is reduced compared with the classical
LMC, and in the underdamped case, convergence is achieved with the same number
of iterations, while each iteration requires merely one-directional derivative.
This means we obtain the best possible computational cost in the
underdamped-LMC framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Utility Fairness for the Differentially Private Federated Learning. (arXiv:2109.05267v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05267">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning (FL) allows predictive model training on the sensed data
in a wireless Internet of things (IoT) network evading data collection cost in
terms of energy, time, and privacy. In this paper, for a FL setting, we model
the learning gain achieved by an IoT device against its participation cost as
its utility. The local model quality and the associated cost differs from
device to device due to the device-heterogeneity which could be time-varying.
We identify that this results in utility unfairness because the same global
model is shared among the devices. In the vanilla FL setting, the master is
unaware of devices&#x27; local model computation and transmission costs, thus it is
unable to address the utility unfairness problem. In addition, a device may
exploit this lack of knowledge at the master to intentionally reduce its
expenditure and thereby boost its utility. We propose to control the quality of
the global model shared with the devices, in each round, based on their
contribution and expenditure. This is achieved by employing differential
privacy to curtail global model divulgence based on the learning contribution.
Furthermore, we devise adaptive computation and transmission policies for each
device to control its expenditure in order to mitigate utility unfairness. Our
results show that the proposed scheme reduces the standard deviation of the
energy cost of devices by 99% in comparison to the benchmark scheme, while the
standard deviation of the training loss of devices varies around 0.103.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Protein sequence-to-structure learning: Is this the end(-to-end revolution)?. (arXiv:2105.07407v2 [q-bio.BM] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07407">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The potential of deep learning has been recognized in the protein structure
prediction community for some time, and became indisputable after CASP13. In
CASP14, deep learning has boosted the field to unanticipated levels reaching
near-experimental accuracy. This success comes from advances transferred from
other machine learning areas, as well as methods specifically designed to deal
with protein sequences and structures, and their abstractions. Novel emerging
approaches include (i) geometric learning, i.e. learning on representations
such as graphs, 3D Voronoi tessellations, and point clouds; (ii) pre-trained
protein language models leveraging attention; (iii) equivariant architectures
preserving the symmetry of 3D space; (iv) use of large meta-genome databases;
(v) combinations of protein representations; (vi) and finally truly end-to-end
architectures, i.e. differentiable models starting from a sequence and
returning a 3D structure. Here, we provide an overview and our opinion of the
novel deep learning approaches developed in the last two years and widely used
in CASP14.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Benchmarking Processor Performance by Multi-Threaded Machine Learning Algorithms. (arXiv:2109.05276v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05276">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine learning algorithms have enabled computers to predict things by
learning from previous data. The data storage and processing power are
increasing rapidly, thus increasing machine learning and Artificial
intelligence applications. Much of the work is done to improve the accuracy of
the models built in the past, with little research done to determine the
computational costs of machine learning acquisitions. In this paper, I will
proceed with this later research work and will make a performance comparison of
multi-threaded machine learning clustering algorithms. I will be working on
Linear Regression, Random Forest, and K-Nearest Neighbors to determine the
performance characteristics of the algorithms as well as the computation costs
to the obtained results. I will be benchmarking system hardware performance by
running these multi-threaded algorithms to train and test the models on a
dataset to note the differences in performance matrices of the algorithms. In
the end, I will state the best performing algorithms concerning the performance
efficiency of these algorithms on my system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Doubly Adaptive Scaled Algorithm for Machine Learning Using Second-Order Information. (arXiv:2109.05198v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05198">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a novel adaptive optimization algorithm for large-scale machine
learning problems. Equipped with a low-cost estimate of local curvature and
Lipschitz smoothness, our method dynamically adapts the search direction and
step-size. The search direction contains gradient information preconditioned by
a well-scaled diagonal preconditioning matrix that captures the local curvature
information. Our methodology does not require the tedious task of learning rate
tuning, as the learning rate is updated automatically without adding an extra
hyperparameter. We provide convergence guarantees on a comprehensive collection
of optimization problems, including convex, strongly convex, and nonconvex
problems, in both deterministic and stochastic regimes. We also conduct an
extensive empirical evaluation on standard machine learning problems,
justifying our algorithm&#x27;s versatility and demonstrating its strong performance
compared to other start-of-the-art first-order and second-order methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning From Long-Tailed Data With Noisy Labels. (arXiv:2108.11096v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Class imbalance and noisy labels are the norm rather than the exception in
many large-scale classification datasets. Nevertheless, most works in machine
learning typically assume balanced and clean data. There have been some recent
attempts to tackle, on one side, the problem of learning from noisy labels and,
on the other side, learning from long-tailed data. Each group of methods make
simplifying assumptions about the other. Due to this separation, the proposed
solutions often underperform when both assumptions are violated. In this work,
we present a simple two-stage approach based on recent advances in
self-supervised learning to treat both challenges simultaneously. It consists
of, first, task-agnostic self-supervised pre-training, followed by
task-specific fine-tuning using an appropriate loss. Most significantly, we
find that self-supervised learning approaches are effectively able to cope with
severe class imbalance. In addition, the resulting learned representations are
also remarkably robust to label noise, when fine-tuned with an imbalance- and
noise-resistant loss function. We validate our claims with experiments on
CIFAR-10 and CIFAR-100 augmented with synthetic imbalance and noise, as well as
the large-scale inherently noisy Clothing-1M dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TS2Vec: Towards Universal Representation of Time Series. (arXiv:2106.10466v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10466">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents TS2Vec, a universal framework for learning
representations of time series in an arbitrary semantic level. Unlike existing
methods, TS2Vec performs contrastive learning in a hierarchical way over
augmented context views, which enables a robust contextual representation for
each timestamp. Furthermore, to obtain the representation of an arbitrary
sub-sequence in the time series, we can apply a simple aggregation over the
representations of corresponding timestamps. We conduct extensive experiments
on time series classification tasks to evaluate the quality of time series
representations. As a result, TS2Vec achieves significant improvement over
existing SOTAs of unsupervised time series representation on 125 UCR datasets
and 29 UEA datasets. The learned timestamp-level representations also achieve
superior results in time series forecasting and anomaly detection tasks. A
linear regression trained on top of the learned representations outperforms
previous SOTAs of time series forecasting. Furthermore, we present a simple way
to apply the learned representations for unsupervised anomaly detection, which
establishes SOTA results in the literature. The source code is publicly
available at https://github.com/yuezhihan/ts2vec.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Communication Efficient Adaptive Gradient Method. (arXiv:2109.05109v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05109">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, distributed optimization is proven to be an effective
approach to accelerate training of large scale machine learning models such as
deep neural networks. With the increasing computation power of GPUs, the
bottleneck of training speed in distributed training is gradually shifting from
computation to communication. Meanwhile, in the hope of training machine
learning models on mobile devices, a new distributed training paradigm called
&#x60;&#x60;federated learning&#x27;&#x27; has become popular. The communication time in federated
learning is especially important due to the low bandwidth of mobile devices.
While various approaches to improve the communication efficiency have been
proposed for federated learning, most of them are designed with SGD as the
prototype training algorithm. While adaptive gradient methods have been proven
effective for training neural nets, the study of adaptive gradient methods in
federated learning is scarce. In this paper, we propose an adaptive gradient
method that can guarantee both the convergence and the communication efficiency
for federated learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unified Framework for Feature Extraction based on Contrastive Learning. (arXiv:2101.11703v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.11703">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Feature extraction is an efficient approach for alleviating the issue of
dimensionality in high-dimensional data. As a popular self-supervised learning
method, contrastive learning has recently garnered considerable attention. In
this study, we proposed a unified framework based on a new perspective of
contrastive learning (CL) that is suitable for both unsupervised and supervised
feature extraction. The proposed framework first constructed two CL graph for
uniquely defining the positive and negative pairs. Subsequently, the projection
matrix was determined by minimizing the contrastive loss function. In addition,
the proposed framework considered both similar and dissimilar samples to unify
unsupervised and supervised feature extraction. Moreover, we propose the three
specific methods: unsupervised contrastive learning method, supervised
contrastive learning method 1 ,and supervised contrastive learning method 2.
Finally, the numerical experiments on five real datasets demonstrated the
superior performance of the proposed framework in comparison to the existing
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beneficial Perturbations Network for Defending Adversarial Examples. (arXiv:2009.12724v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.12724">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks can be fooled by adversarial attacks: adding carefully
computed small adversarial perturbations to clean inputs can cause
misclassification on state-of-the-art machine learning models. The reason is
that neural networks fail to accommodate the distribution drift of the input
data caused by adversarial perturbations. Here, we present a new solution -
Beneficial Perturbation Network (BPN) - to defend against adversarial attacks
by fixing the distribution drift. During training, BPN generates and leverages
beneficial perturbations (somewhat opposite to well-known adversarial
perturbations) by adding new, out-of-network biasing units. Biasing units
influence the parameter space of the network, to preempt and neutralize future
adversarial perturbations on input data samples. To achieve this, BPN creates
reverse adversarial attacks during training, with very little cost, by
recycling the training gradients already computed. Reverse attacks are captured
by the biasing units, and the biases can in turn effectively defend against
future adversarial examples. Reverse attacks are a shortcut, i.e., they affect
the network&#x27;s parameters without requiring instantiation of adversarial
examples that could assist training. We provide comprehensive empirical
evidence showing that 1) BPN is robust to adversarial examples and is much more
running memory and computationally efficient compared to classical adversarial
training. 2) BPN can defend against adversarial examples with negligible
additional computation and parameter costs compared to training only on clean
examples; 3) BPN hurts the accuracy on clean examples much less than classic
adversarial training; 4) BPN can improve the generalization of the network 5)
BPN trained only with Fast Gradient Sign Attack can generalize to defend PGD
attacks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quaternion Graph Neural Networks. (arXiv:2008.05089v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.05089">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, graph neural networks (GNNs) have become an important and active
research direction in deep learning. It is worth noting that most of the
existing GNN-based methods learn graph representations within the Euclidean
vector space. Beyond the Euclidean space, learning representation and
embeddings in hyper-complex space have also shown to be a promising and
effective approach. To this end, we propose Quaternion Graph Neural Networks
(QGNN) and Gated Quaternion Graph Neural Networks (GQGNN) to learn graph
representations within the Quaternion space. As demonstrated, the Quaternion
space, a hyper-complex vector space, provides highly meaningful computations
and analogical calculus through Hamilton product compared to the Euclidean and
complex vector spaces. Our QGNN obtains state-of-the-art results on a range of
benchmark datasets for graph classification and node classification. Besides,
regarding knowledge graphs, our QGNN-based knowledge graph embedding method
gets state-of-the-art results on three new and challenging benchmark datasets
for knowledge graph completion. Furthermore, regarding text graphs, our
GQGNN-based text classification method works better than state-of-the-art
methods on benchmark datasets for inductive text classification. Our code is
available at: \url{https://github.com/daiquocnguyen/QGNN}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Causal Learner: A Toolbox for Causal Structure and Markov Blanket Learning. (arXiv:2103.06544v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06544">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Causal Learner is a toolbox for learning causal structure and Markov blanket
(MB) from data. It integrates functions for generating simulated Bayesian
network data, a set of state-of-the-art global causal structure learning
algorithms, a set of state-of-the-art local causal structure learning
algorithms, a set of state-of-the-art MB learning algorithms, and functions for
evaluating algorithms. The data generation part of Causal Learner is written in
R, and the rest of Causal Learner is written in MATLAB. Causal Learner aims to
provide researchers and practitioners with an open-source platform for causal
learning from data and for the development and evaluation of new causal
learning algorithms. The Causal Learner project is available at
this http URL</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Graphs. (arXiv:2003.02320v6 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.02320">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we provide a comprehensive introduction to knowledge graphs,
which have recently garnered significant attention from both industry and
academia in scenarios that require exploiting diverse, dynamic, large-scale
collections of data. After some opening remarks, we motivate and contrast
various graph-based data models and query languages that are used for knowledge
graphs. We discuss the roles of schema, identity, and context in knowledge
graphs. We explain how knowledge can be represented and extracted using a
combination of deductive and inductive techniques. We summarise methods for the
creation, enrichment, quality assessment, refinement, and publication of
knowledge graphs. We provide an overview of prominent open knowledge graphs and
enterprise knowledge graphs, their applications, and how they use the
aforementioned techniques. We conclude with high-level future research
directions for knowledge graphs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine Learning for Naval Architecture, Ocean and Marine Engineering. (arXiv:2109.05574v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05574">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine Learning (ML) based algorithms have found significant impact in many
fields of engineering and sciences, where datasets are available from
experiments and high fidelity numerical simulations. Those datasets are
generally utilized in a machine learning model to extract information about the
underlying physics and derive functional relationships mapping input variables
to target quantities of interest. Commonplace machine learning algorithms
utilized in Scientific Machine Learning (SciML) include neural networks,
regression trees, random forests, support vector machines, etc. The focus of
this article is to review the applications of ML in naval architecture, ocean,
and marine engineering problems; and identify priority directions of research.
We discuss the applications of machine learning algorithms for different
problems such as wave height prediction, calculation of wind loads on ships,
damage detection of offshore platforms, calculation of ship added resistance,
and various other applications in coastal and marine environments. The details
of the data sets including the source of data-sets utilized in the ML model
development are included. The features used as the inputs to the ML models are
presented in detail and finally, the methods employed in optimization of the ML
models were also discussed. Based on this comprehensive analysis we point out
future directions of research that may be fruitful for the application of ML to
the ocean and marine engineering problems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Representations from large pretrained models such as BERT encode a range of
features into monolithic vectors, affording strong predictive accuracy across a
multitude of downstream tasks. In this paper we explore whether it is possible
to learn disentangled representations by identifying existing subnetworks
within pretrained models that encode distinct, complementary aspect
representations. Concretely, we learn binary masks over transformer weights or
hidden units to uncover subsets of features that correlate with a specific
factor of variation; this eliminates the need to train a disentangled model
from scratch for a particular task. We evaluate this method with respect to its
ability to disentangle representations of sentiment from genre in movie
reviews, &quot;toxicity&quot; from dialect in Tweets, and syntax from semantics.

By combining masking with magnitude pruning we find that we can identify
sparse subnetworks within BERT that strongly encode particular aspects (e.g.,
toxicity) while only weakly encoding others (e.g., race). Moreover, despite
only learning masks, we find that disentanglement-via-masking performs as well
as -- and often better than -- previously proposed methods based on variational
autoencoders and adversarial training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Noise Estimation for Generative Diffusion Models. (arXiv:2104.02600v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02600">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generative diffusion models have emerged as leading models in speech and
image generation. However, in order to perform well with a small number of
denoising steps, a costly tuning of the set of noise parameters is needed. In
this work, we present a simple and versatile learning scheme that can
step-by-step adjust those noise parameters, for any given number of steps,
while the previous work needs to retune for each number separately.
Furthermore, without modifying the weights of the diffusion model, we are able
to significantly improve the synthesis results, for a small number of steps.
Our approach comes at a negligible computation cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixing between the Cross Entropy and the Expectation Loss Terms. (arXiv:2109.05635v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05635">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The cross entropy loss is widely used due to its effectiveness and solid
theoretical grounding. However, as training progresses, the loss tends to focus
on hard to classify samples, which may prevent the network from obtaining gains
in performance. While most work in the field suggest ways to classify hard
negatives, we suggest to strategically leave hard negatives behind, in order to
focus on misclassified samples with higher probabilities. We show that adding
to the optimization goal the expectation loss, which is a better approximation
of the zero-one loss, helps the network to achieve better accuracy. We,
therefore, propose to shift between the two losses during training, focusing
more on the expectation loss gradually during the later stages of training. Our
experiments show that the new training protocol improves performance across a
diverse set of classification domains, including computer vision, natural
language processing, tabular data, and sequences. Our code and scripts are
available at supplementary.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Analytics for Smart cities: Challenges and Promises. (arXiv:2109.05581v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05581">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The explosion of advancements in artificial intelligence, sensor
technologies, and wireless communication activates ubiquitous sensing through
distributed sensors. These sensors are various domains of networks that lead us
to smart systems in healthcare, transportation, environment, and other relevant
branches/networks. Having collaborative interaction among the smart systems
connects end-user devices to each other which enables achieving a new
integrated entity called Smart Cities. The goal of this study is to provide a
comprehensive survey of data analytics in smart cities. In this paper, we aim
to focus on one of the smart cities important branches, namely Smart Mobility,
and its positive ample impact on the smart cities decision-making process.
Intelligent decision-making systems in smart mobility offer many advantages
such as saving energy, relaying city traffic, and more importantly, reducing
air pollution by offering real-time useful information and imperative
knowledge. Making a decision in smart cities in time is challenging due to
various and high dimensional factors and parameters, which are not frequently
collected. In this paper, we first address current challenges in smart cities
and provide an overview of potential solutions to these challenges. Then, we
offer a framework of these solutions, called universal smart cities decision
making, with three main sections of data capturing, data analysis, and decision
making to optimize the smart mobility within smart cities. With this framework,
we elaborate on fundamental concepts of big data, machine learning, and deep
leaning algorithms that have been applied to smart cities and discuss the role
of these algorithms in decision making for smart mobility in smart cities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Critical Learning Periods in Federated Learning. (arXiv:2109.05613v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05613">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning (FL) is a popular technique to train machine learning (ML)
models with decentralized data. Extensive works have studied the performance of
the global model; however, it is still unclear how the training process affects
the final test accuracy. Exacerbating this problem is the fact that FL
executions differ significantly from traditional ML with heterogeneous data
characteristics across clients, involving more hyperparameters. In this work,
we show that the final test accuracy of FL is dramatically affected by the
early phase of the training process, i.e., FL exhibits critical learning
periods, in which small gradient errors can have irrecoverable impact on the
final test accuracy. To further explain this phenomenon, we generalize the
trace of the Fisher Information Matrix (FIM) to FL and define a new notion
called FedFIM, a quantity reflecting the local curvature of each clients from
the beginning of the training in FL. Our findings suggest that the {\em initial
learning phase} plays a critical role in understanding the FL performance. This
is in contrast to many existing works which generally do not connect the final
accuracy of FL to the early phase training. Finally, seizing critical learning
periods in FL is of independent interest and could be useful for other problems
such as the choices of hyperparameters such as the number of client selected
per round, batch size, and more, so as to improve the performance of FL
training and testing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Geometry of the Loss Landscape in Overparameterized Neural Networks: Symmetries and Invariances. (arXiv:2105.12221v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study how permutation symmetries in overparameterized multi-layer neural
networks generate &#x60;symmetry-induced&#x27; critical points. Assuming a network with $
L $ layers of minimal widths $ r_1^*, \ldots, r_{L-1}^* $ reaches a zero-loss
minimum at $ r_1^*! \cdots r_{L-1}^*! $ isolated points that are permutations
of one another, we show that adding one extra neuron to each layer is
sufficient to connect all these previously discrete minima into a single
manifold. For a two-layer overparameterized network of width $ r^*+ h &#x3D;: m $ we
explicitly describe the manifold of global minima: it consists of $ T(r^*, m) $
affine subspaces of dimension at least $ h $ that are connected to one another.
For a network of width $m$, we identify the number $G(r,m)$ of affine subspaces
containing only symmetry-induced critical points that are related to the
critical points of a smaller network of width $r&lt;r^*$. Via a combinatorial
analysis, we derive closed-form formulas for $ T $ and $ G $ and show that the
number of symmetry-induced critical subspaces dominates the number of affine
subspaces forming the global minima manifold in the mildly overparameterized
regime (small $ h $) and vice versa in the vastly overparameterized regime ($h
\gg r^*$). Our results provide new insights into the minimization of the
non-convex loss function of overparameterized neural networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders. (arXiv:2012.00560v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.00560">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Major complications arise from the recent increase in the amount of
high-dimensional data, including high computational costs and memory
requirements. Feature selection, which identifies the most relevant and
informative attributes of a dataset, has been introduced as a solution to this
problem. Most of the existing feature selection methods are computationally
inefficient; inefficient algorithms lead to high energy consumption, which is
not desirable for devices with limited computational and energy resources. In
this paper, a novel and flexible method for unsupervised feature selection is
proposed. This method, named QuickSelection, introduces the strength of the
neuron in sparse neural networks as a criterion to measure the feature
importance. This criterion, blended with sparsely connected denoising
autoencoders trained with the sparse evolutionary training procedure, derives
the importance of all input features simultaneously. We implement
QuickSelection in a purely sparse manner as opposed to the typical approach of
using a binary mask over connections to simulate sparsity. It results in a
considerable speed increase and memory reduction. When tested on several
benchmark datasets, including five low-dimensional and three high-dimensional
datasets, the proposed method is able to achieve the best trade-off of
classification and clustering accuracy, running time, and maximum memory usage,
among widely used approaches for feature selection. Besides, our proposed
method requires the least amount of energy among the state-of-the-art
autoencoder-based feature selection methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Link Scheduling using Graph Neural Networks. (arXiv:2109.05536v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05536">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Efficient scheduling of transmissions is a key problem in wireless networks.
The main challenge stems from the fact that optimal link scheduling involves
solving a maximum weighted independent set (MWIS) problem, which is known to be
NP-hard. For practical link scheduling schemes, centralized and distributed
greedy heuristics are commonly used to approximate the solution to the MWIS
problem. However, these greedy schemes mostly ignore important topological
information of the wireless network. To overcome this limitation, we propose
fast heuristics based on graph convolutional networks (GCNs) that can be
implemented in centralized and distributed manners. Our centralized MWIS solver
is based on tree search guided by a trainable GCN module and 1-step rollout. In
our distributed MWIS solver, a trainable GCN module learns topology-aware node
embeddings that are combined with the network weights before calling a
distributed greedy solver. Test results on medium-sized wireless networks show
that a GCN-based centralized MWIS solver can reach a near-optimal solution
quickly. Moreover, we demonstrate that a shallow GCN-based distributed MWIS
scheduler can reduce by nearly half the suboptimality gap of the distributed
greedy solver with minimal increase in complexity. The proposed scheduling
solutions also exhibit good generalizability across graph and weight
distributions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Team NeuroPoly: Description of the Pipelines for the MICCAI 2021 MS New Lesions Segmentation Challenge. (arXiv:2109.05409v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05409">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper gives a detailed description of the pipelines used for the 2nd
edition of the MICCAI 2021 Challenge on Multiple Sclerosis Lesion Segmentation.
An overview of the data preprocessing steps applied is provided along with a
brief description of the pipelines used, in terms of the architecture and the
hyperparameters. Our code for this work can be found at:
https://github.com/ivadomed/ms-challenge-2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Split Computing and Early Exiting for Deep Learning Applications: Survey and Research Challenges. (arXiv:2103.04505v2 [eess.SP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04505">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Mobile devices such as smartphones and autonomous vehicles increasingly rely
on deep neural networks (DNNs) to execute complex inference tasks such as image
classification and speech recognition, among others. However, continuously
executing the entire DNN on the mobile device can quickly deplete its battery.
Although task offloading to edge servers may decrease the mobile device&#x27;s
computational burden, erratic patterns in channel quality, network and edge
server load can lead to a significant delay in task execution.
Recently,approaches based on split computing (SC) have been proposed, where the
DNN is split into a head and a tail model, executed respectively on the mobile
device and on the edge server. Ultimately, this may reduce bandwidth usage as
well as energy consumption. Another approach, called early exiting (EE), trains
models to present multiple &quot;exits&quot; earlier in the architecture, each providing
increasingly higher target accuracy. Therefore, the trade-off between accuracy
and delay can be tuned according to the current conditions or application
demands. In this paper, we provide a comprehensive survey of the state of the
art in SC and EE strategies, by presenting a comparison of the most relevant
approaches. We conclude the paper by providing a set of compelling research
challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks. (arXiv:2109.05539v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05539">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies have shown that convolutional neural networks (CNNs) are not
the only feasible solution for image classification. Furthermore, weight
sharing and backpropagation used in CNNs do not correspond to the mechanisms
present in the primate visual system. To propose a more biologically plausible
solution, we designed a locally connected spiking neural network (SNN) trained
using spike-timing-dependent plasticity (STDP) and its reward-modulated variant
(R-STDP) learning rules. The use of spiking neurons and local connections along
with reinforcement learning (RL) led us to the nomenclature BioLCNet for our
proposed architecture. Our network consists of a rate-coded input layer
followed by a locally connected hidden layer and a decoding output layer. A
spike population-based voting scheme is adopted for decoding in the output
layer. We used the MNIST dataset to obtain image classification accuracy and to
assess the robustness of our rewarding system to varying target responses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-time multimodal image registration with partial intraoperative point-set data. (arXiv:2109.05023v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05023">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present Free Point Transformer (FPT) - a deep neural network architecture
for non-rigid point-set registration. Consisting of two modules, a global
feature extraction module and a point transformation module, FPT does not
assume explicit constraints based on point vicinity, thereby overcoming a
common requirement of previous learning-based point-set registration methods.
FPT is designed to accept unordered and unstructured point-sets with a variable
number of points and uses a &quot;model-free&quot; approach without heuristic
constraints. Training FPT is flexible and involves minimizing an intuitive
unsupervised loss function, but supervised, semi-supervised, and partially- or
weakly-supervised training are also supported. This flexibility makes FPT
amenable to multimodal image registration problems where the ground-truth
deformations are difficult or impossible to measure. In this paper, we
demonstrate the application of FPT to non-rigid registration of prostate
magnetic resonance (MR) imaging and sparsely-sampled transrectal ultrasound
(TRUS) images. The registration errors were 4.71 mm and 4.81 mm for complete
TRUS imaging and sparsely-sampled TRUS imaging, respectively. The results
indicate superior accuracy to the alternative rigid and non-rigid registration
algorithms tested and substantially lower computation time. The rapid inference
possible with FPT makes it particularly suitable for applications where
real-time registration is beneficial.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05075">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite the growing availability of high-capacity computational platforms,
implementation complexity still has been a great concern for the real-world
deployment of neural networks. This concern is not exclusively due to the huge
costs of state-of-the-art network architectures, but also due to the recent
push towards edge intelligence and the use of neural networks in embedded
applications. In this context, network compression techniques have been gaining
interest due to their ability for reducing deployment costs while keeping
inference accuracy at satisfactory levels. The present paper is dedicated to
the development of a novel compression scheme for neural networks. To this end,
a new $\ell_0$-norm-based regularization approach is firstly developed, which
is capable of inducing strong sparseness in the network during training. Then,
targeting the smaller weights of the trained network with pruning techniques,
smaller yet highly effective networks can be obtained. The proposed compression
scheme also involves the use of $\ell_2$-norm regularization to avoid
overfitting as well as fine tuning to improve the performance of the pruned
network. Experimental results are presented aiming to show the effectiveness of
the proposed scheme as well as to make comparisons with competing approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Black-Box Optimization via Generative Adversarial Nets. (arXiv:2102.03888v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03888">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Black-box optimization (BBO) algorithms are concerned with finding the best
solutions for problems with missing analytical details. Most classical methods
for such problems are based on strong and fixed a priori assumptions, such as
Gaussianity. However, the complex real-world problems, especially when the
global optimum is desired, could be very far from the a priori assumptions
because of their diversities, bringing some unexpected obstacles to these
methods. In this paper, we present a generative adversarial nets-based
optimizer (OPT-GAN) to adapt to diverse black-box problems via estimating the
distribution of optima. The method learns the extensive distribution of the
optimal region dominated by selective and randomly moving candidates, balancing
the exploration and exploitation. Experiments conducted on Black-box
Optimization Benchmarking (BBOB) problems and several other benchmarks with
diversified distributions exhibit that, the OPT-GAN outperforms many
traditional and neural net-based BBO algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pathfinder: Parallel quasi-Newton variational inference. (arXiv:2108.03782v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03782">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce Pathfinder, a variational method for approximately sampling from
differentiable log densities. Starting from a random initialization, Pathfinder
locates normal approximations to the target density along a quasi-Newton
optimization path, with local covariance estimated using the inverse Hessian
estimates produced by the optimizer. Pathfinder returns draws from the
approximation with the lowest estimated Kullback-Leibler (KL) divergence to the
true posterior. We evaluate Pathfinder on a wide range of posterior
distributions, demonstrating that its approximate draws are better than those
from automatic differentiation variational inference (ADVI) and comparable to
those produced by short chains of dynamic Hamiltonian Monte Carlo (HMC), as
measured by 1-Wasserstein distance. Compared to ADVI and short dynamic HMC
runs, Pathfinder requires one to two orders of magnitude fewer log density and
gradient evaluations, with greater reductions for more challenging posteriors.
Importance resampling over multiple runs of Pathfinder improves the diversity
of approximate draws, reducing 1-Wasserstein distance further and providing a
measure of robustness to optimization failures on plateaus, saddle points, or
in minor modes. The Monte Carlo KL-divergence estimates are embarrassingly
parallelizable in the core Pathfinder algorithm, as are multiple runs in the
resampling version, further increasing Pathfinder&#x27;s speed advantage with
multiple cores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tight FPT Approximation for Socially Fair Clustering. (arXiv:2106.06755v2 [cs.DS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06755">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we study the socially fair $k$-median/$k$-means problem. We are
given a set of points $P$ in a metric space $\mathcal{X}$ with a distance
function $d(.,.)$. There are $\ell$ groups: $P_1,\dotsc,P_{\ell} \subseteq P$.
We are also given a set $F$ of feasible centers in $\mathcal{X}$. The goal in
the socially fair $k$-median problem is to find a set $C \subseteq F$ of $k$
centers that minimizes the maximum average cost over all the groups. That is,
find $C$ that minimizes the objective function $\Phi(C,P) \equiv \max_{j}
\Big\{ \sum_{x \in P_j} d(C,x)/|P_j| \Big\}$, where $d(C,x)$ is the distance of
$x$ to the closest center in $C$. The socially fair $k$-means problem is
defined similarly by using squared distances, i.e., $d^{2}(.,.)$ instead of
$d(.,.)$. The current best approximation guarantee for both the problems is
$O\left( \frac{\log \ell}{\log \log \ell} \right)$ due to Makarychev and
Vakilian [COLT 2021]. In this work, we study the fixed parameter tractability
of the problems with respect to parameter $k$. We design $(3+\varepsilon)$ and
$(9 + \varepsilon)$ approximation algorithms for the socially fair $k$-median
and $k$-means problems, respectively, in FPT (fixed parameter tractable) time
$f(k,\varepsilon) \cdot n^{O(1)}$, where $f(k,\varepsilon) &#x3D;
(k/\varepsilon)^{{O}(k)}$ and $n &#x3D; |P \cup F|$. Furthermore, we show that if
Gap-ETH holds, then better approximation guarantees are not possible in FPT
time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Empirical Study on the Joint Impact of Feature Selection and Data Re-sampling on Imbalance Classification. (arXiv:2109.00201v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>In predictive tasks, real-world datasets often present different degrees of
imbalanced (i.e., long-tailed or skewed) distributions. While the majority (the
head) classes have sufficient samples, the minority (the tail) classes can be
under-represented by a rather limited number of samples. Data pre-processing
has been shown to be very effective in dealing with such problems. On one hand,
data re-sampling is a common approach to tackling class imbalance. On the other
hand, dimension reduction, which reduces the feature space, is a conventional
technique for reducing noise and inconsistencies in a dataset. However, the
possible synergy between feature selection and data re-sampling for
high-performance imbalance classification has rarely been investigated before.
To address this issue, we carry out a comprehensive empirical study on the
joint influence of feature selection and re-sampling on two-class imbalance
classification. Specifically, we study the performance of two opposite
pipelines for imbalance classification by applying feature selection before or
after data re-sampling. We conduct a large number of experiments, with a total
of 9225 tests, on 52 publicly available datasets, using 9 feature selection
methods, 6 re-sampling approaches for class imbalance learning, and 3
well-known classification algorithms. Experimental results show that there is
no constant winner between the two pipelines; thus both of them should be
considered to derive the best performing model for imbalance classification. We
find that the performance of an imbalance classification model not only depends
on the classifier adopted and the ratio between the number of majority and
minority samples, but also depends on the ratio between the number of samples
and features. Overall, this study should provide new reference value for
researchers and practitioners in imbalance learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based
on propositional logic, which has demonstrated competitive performance in many
Natural Language Processing (NLP) tasks, including sentiment analysis, text
classification, and Word Sense Disambiguation. To obtain human-level
interpretability, legacy TM employs Boolean input features such as bag-of-words
(BOW). However, the BOW representation makes it difficult to use any
pre-trained information, for instance, word2vec and GloVe word representations.
This restriction has constrained the performance of TM compared to deep neural
networks (DNNs) in NLP. To reduce the performance gap, in this paper, we
propose a novel way of using pre-trained word representations for TM. The
approach significantly enhances the performance and interpretability of TM. We
achieve this by extracting semantically related words from pre-trained word
representations as input features to the TM. Our experiments show that the
accuracy of the proposed approach is significantly higher than the previous
BOW-based TM, reaching the level of DNN-based models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Existence conditions for hidden feedback loops in online recommender systems. (arXiv:2109.05278v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05278">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We explore a hidden feedback loops effect in online recommender systems.
Feedback loops result in degradation of online multi-armed bandit (MAB)
recommendations to a small subset and loss of coverage and novelty. We study
how uncertainty and noise in user interests influence the existence of feedback
loops. First, we show that an unbiased additive random noise in user interests
does not prevent a feedback loop. Second, we demonstrate that a non-zero
probability of resetting user interests is sufficient to limit the feedback
loop and estimate the size of the effect. Our experiments confirm the
theoretical findings in a simulated environment for four bandit algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Novel Intrinsic Measure of Data Separability. (arXiv:2109.05180v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05180">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In machine learning, the performance of a classifier depends on both the
classifier model and the separability/complexity of datasets. To quantitatively
measure the separability of datasets, we create an intrinsic measure -- the
Distance-based Separability Index (DSI), which is independent of the classifier
model. We consider the situation in which different classes of data are mixed
in the same distribution to be the most difficult for classifiers to separate.
We then formally show that the DSI can indicate whether the distributions of
datasets are identical for any dimensionality. And we verify the DSI to be an
effective separability measure by comparing to several state-of-the-art
separability/complexity measures using synthetic and real datasets. Having
demonstrated the DSI&#x27;s ability to compare distributions of samples, we also
discuss some of its other promising applications, such as measuring the
performance of generative adversarial networks (GANs) and evaluating the
results of clustering methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Remaining Useful Life Estimation of Hard Disk Drives using Bidirectional LSTM Networks. (arXiv:2109.05351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Physical and cloud storage services are well-served by functioning and
reliable high-volume storage systems. Recent observations point to hard disk
reliability as one of the most pressing reliability issues in data centers
containing massive volumes of storage devices such as HDDs. In this regard,
early detection of impending failure at the disk level aids in reducing system
downtime and reduces operational loss making proactive health monitoring a
priority for AIOps in such settings. In this work, we introduce methods of
extracting meaningful attributes associated with operational failure and of
pre-processing the highly imbalanced health statistics data for subsequent
prediction tasks using data-driven approaches. We use a Bidirectional LSTM with
a multi-day look back period to learn the temporal progression of health
indicators and baseline them against vanilla LSTM and Random Forest models to
come up with several key metrics that establish the usefulness of and
superiority of our model under some tightly defined operational constraints.
For example, using a 15 day look back period, our approach can predict the
occurrence of disk failure with an accuracy of 96.4% considering test data 60
days before failure. This helps to alert operations maintenance well in-advance
about potential mitigation needs. In addition, our model reports a mean
absolute error of 0.12 for predicting failure up to 60 days in advance, placing
it among the state-of-the-art in recent literature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cost-Effective Federated Learning in Mobile Edge Networks. (arXiv:2109.05411v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05411">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning (FL) is a distributed learning paradigm that enables a
large number of mobile devices to collaboratively learn a model under the
coordination of a central server without sharing their raw data. Despite its
practical efficiency and effectiveness, the iterative on-device learning
process (e.g., local computations and global communications with the server)
incurs a considerable cost in terms of learning time and energy consumption,
which depends crucially on the number of selected clients and the number of
local iterations in each training round. In this paper, we analyze how to
design adaptive FL in mobile edge networks that optimally chooses these
essential control variables to minimize the total cost while ensuring
convergence. We establish the analytical relationship between the total cost
and the control variables with the convergence upper bound. To efficiently
solve the cost minimization problem, we develop a low-cost sampling-based
algorithm to learn the convergence related unknown parameters. We derive
important solution properties that effectively identify the design principles
for different optimization metrics. Practically, we evaluate our theoretical
results both in a simulated environment and on a hardware prototype.
Experimental evidence verifies our derived properties and demonstrates that our
proposed solution achieves near-optimal performance for different optimization
metrics for various datasets and heterogeneous system and statistical settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SphereFace Revived: Unifying Hyperspherical Face Recognition. (arXiv:2109.05565v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05565">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper addresses the deep face recognition problem under an open-set
protocol, where ideal face features are expected to have smaller maximal
intra-class distance than minimal inter-class distance under a suitably chosen
metric space. To this end, hyperspherical face recognition, as a promising line
of research, has attracted increasing attention and gradually become a major
focus in face recognition research. As one of the earliest works in
hyperspherical face recognition, SphereFace explicitly proposed to learn face
embeddings with large inter-class angular margin. However, SphereFace still
suffers from severe training instability which limits its application in
practice. In order to address this problem, we introduce a unified framework to
understand large angular margin in hyperspherical face recognition. Under this
framework, we extend the study of SphereFace and propose an improved variant
with substantially better training stability -- SphereFace-R. Specifically, we
propose two novel ways to implement the multiplicative margin, and study
SphereFace-R under three different feature normalization schemes (no feature
normalization, hard feature normalization and soft feature normalization). We
also propose an implementation strategy -- &quot;characteristic gradient detachment&quot;
-- to stabilize training. Extensive experiments on SphereFace-R show that it is
consistently better than or competitive with state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Efficiency of Subclass Knowledge Distillation in Classification Tasks. (arXiv:2109.05587v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05587">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work introduces a novel knowledge distillation framework for
classification tasks where information on existing subclasses is available and
taken into consideration. In classification tasks with a small number of
classes or binary detection (two classes) the amount of information transferred
from the teacher to the student network is restricted, thus limiting the
utility of knowledge distillation. Performance can be improved by leveraging
information about possible subclasses within the available classes in the
classification task. To that end, we propose the so-called Subclass Knowledge
Distillation (SKD) framework, which is the process of transferring the
subclasses&#x27; prediction knowledge from a large teacher model into a smaller
student one. Through SKD, additional meaningful information which is not in the
teacher&#x27;s class logits but exists in subclasses (e.g., similarities inside
classes) will be conveyed to the student and boost its performance.
Mathematically, we measure how many extra information bits the teacher can
provide for the student via SKD framework. The framework developed is evaluated
in clinical application, namely colorectal polyp binary classification. In this
application, clinician-provided annotations are used to define subclasses based
on the annotation label&#x27;s variability in a curriculum style of learning. A
lightweight, low complexity student trained with the proposed framework
achieves an F1-score of 85.05%, an improvement of 2.14% and 1.49% gain over the
student that trains without and with conventional knowledge distillation,
respectively. These results show that the extra subclasses&#x27; knowledge (i.e.,
0.4656 label bits per training sample in our experiment) can provide more
information about the teacher generalization, and therefore SKD can benefit
from using more information to increase the student performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic Adversarial Koopman Model for Dynamical Systems. (arXiv:2109.05095v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05095">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dynamical systems are ubiquitous and are often modeled using a non-linear
system of governing equations. Numerical solution procedures for many dynamical
systems have existed for several decades, but can be slow due to
high-dimensional state space of the dynamical system. Thus, deep learning-based
reduced order models (ROMs) are of interest and one such family of algorithms
along these lines are based on the Koopman theory. This paper extends a
recently developed adversarial Koopman model (Balakrishnan \&amp; Upadhyay,
arXiv:2006.05547) to stochastic space, where the Koopman operator applies on
the probability distribution of the latent encoding of an encoder.
Specifically, the latent encoding of the system is modeled as a Gaussian, and
is advanced in time by using an auxiliary neural network that outputs two
Koopman matrices $K_{\mu}$ and $K_{\sigma}$. Adversarial and gradient losses
are used and this is found to lower the prediction errors. A reduced Koopman
formulation is also undertaken where the Koopman matrices are assumed to have a
tridiagonal structure, and this yields predictions comparable to the baseline
model with full Koopman matrices. The efficacy of the stochastic Koopman model
is demonstrated on different test problems in chaos, fluid dynamics,
combustion, and reaction-diffusion models. The proposed model is also applied
in a setting where the Koopman matrices are conditioned on other input
parameters for generalization and this is applied to simulate the state of a
Lithium-ion battery in time. The Koopman models discussed in this study are
very promising for the wide range of problems considered.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Jacobian Regularization for Mitigating Universal Adversarial Perturbations. (arXiv:2104.10459v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Universal Adversarial Perturbations (UAPs) are input perturbations that can
fool a neural network on large sets of data. They are a class of attacks that
represents a significant threat as they facilitate realistic, practical, and
low-cost attacks on neural networks. In this work, we derive upper bounds for
the effectiveness of UAPs based on norms of data-dependent Jacobians. We
empirically verify that Jacobian regularization greatly increases model
robustness to UAPs by up to four times whilst maintaining clean performance.
Our theoretical analysis also allows us to formulate a metric for the strength
of shared adversarial perturbations between pairs of inputs. We apply this
metric to benchmark datasets and show that it is highly correlated with the
actual observed robustness. This suggests that realistic and practical
universal attacks can be reliably mitigated without sacrificing clean accuracy,
which shows promise for the robustness of machine learning systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Low-Latency Federated Learning over Wireless Channels with Differential Privacy. (arXiv:2106.13039v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In federated learning (FL), model training is distributed over clients and
local models are aggregated by a central server. The performance of uploaded
models in such situations can vary widely due to imbalanced data distributions,
potential demands on privacy protections, and quality of transmissions. In this
paper, we aim to minimize FL training delay over wireless channels, constrained
by overall training performance as well as each client&#x27;s differential privacy
(DP) requirement. We solve this problem in the framework of multi-agent
multi-armed bandit (MAMAB) to deal with the situation where there are multiple
clients confornting different unknown transmission environments, e.g., channel
fading and interferences. Specifically, we first transform the long-term
constraints on both training performance and each client&#x27;s DP into a virtual
queue based on the Lyapunov drift technique. Then, we convert the MAMAB to a
max-min bipartite matching problem at each communication round, by estimating
rewards with the upper confidence bound (UCB) approach. More importantly, we
propose two efficient solutions to this matching problem, i.e., modified
Hungarian algorithm and greedy matching with a better alternative (GMBA), in
which the first one can achieve the optimal solution with a high complexity
while the second one approaches a better trade-off by enabling a verified
low-complexity with little performance loss. In addition, we develop an upper
bound on the expected regret of this MAMAB based FL framework, which shows a
linear growth over the logarithm of communication rounds, justifying its
theoretical feasibility. Extensive experimental results are conducted to
validate the effectiveness of our proposed algorithms, and the impacts of
various parameters on the FL performance over wireless edge networks are also
discussed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Realtime Robust Malicious Traffic Detection via Frequency Domain Analysis. (arXiv:2106.14707v2 [cs.CR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14707">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Machine learning (ML) based malicious traffic detection is an emerging
security paradigm, particularly for zero-day attack detection, which is
complementary to existing rule based detection. However, the existing ML based
detection has low detection accuracy and low throughput incurred by inefficient
traffic features extraction. Thus, they cannot detect attacks in realtime
especially in high throughput networks. Particularly, these detection systems
similar to the existing rule based detection can be easily evaded by
sophisticated attacks. To this end, we propose Whisper, a realtime ML based
malicious traffic detection system that achieves both high accuracy and high
throughput by utilizing frequency domain features. It utilizes sequential
features represented by the frequency domain features to achieve bounded
information loss, which ensures high detection accuracy, and meanwhile
constrains the scale of features to achieve high detection throughput.
Particularly, attackers cannot easily interfere with the frequency domain
features and thus Whisper is robust against various evasion attacks. Our
experiments with 42 types of attacks demonstrate that, compared with the
state-of-theart systems, Whisper can accurately detect various sophisticated
and stealthy attacks, achieving at most 18.36% improvement, while achieving two
orders of magnitude throughput. Even under various evasion attacks, Whisper is
still able to maintain around 90% detection accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural network based order parameter for phase transitions and its applications in high-entropy alloys. (arXiv:2109.05598v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05598">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Phase transition is one of the most important phenomena in nature and plays a
central role in materials design. All phase transitions are characterized by
suitable order parameters, including the order-disorder phase transition.
However, finding a representative order parameter for complex systems is
nontrivial, such as for high-entropy alloys. Given variational autoencoder&#x27;s
(VAE) strength of reducing high dimensional data into few principal components,
here we coin a new concept of &quot;VAE order parameter&quot;. We propose that the
Manhattan distance in the VAE latent space can serve as a generic order
parameter for order-disorder phase transitions. The physical properties of the
order parameter are quantitatively interpreted and demonstrated by multiple
refractory high-entropy alloys. Assisted by it, a generally applicable alloy
design concept is proposed by mimicking the nature mixing of elements. Our
physically interpretable &quot;VAE order parameter&quot; lays the foundation for the
understanding of and alloy design by chemical ordering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Upper Bounds on the Generalization Error of Private Algorithms for Discrete Data. (arXiv:2005.05889v3 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.05889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we study the generalization capability of algorithms from an
information-theoretic perspective. It has been shown that the expected
generalization error of an algorithm is bounded from above by a function of the
relative entropy between the conditional probability distribution of the
algorithm&#x27;s output hypothesis, given the dataset with which it was trained, and
its marginal probability distribution. We build upon this fact and introduce a
mathematical formulation to obtain upper bounds on this relative entropy.
Assuming that the data is discrete, we then develop a strategy using this
formulation, based on the method of types and typicality, to find explicit
upper bounds on the generalization error of stable algorithms, i.e., algorithms
that produce similar output hypotheses given similar input datasets. In
particular, we show the bounds obtained with this strategy for the case of
$\epsilon$-DP and $\mu$-GDP algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation. (arXiv:2109.05490v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05490">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Discrete-continuous hybrid action space is a natural setting in many
practical problems, such as robot control and game AI. However, most previous
Reinforcement Learning (RL) works only demonstrate the success in controlling
with either discrete or continuous action space, while seldom take into account
the hybrid action space. One naive way to address hybrid action RL is to
convert the hybrid action space into a unified homogeneous action space by
discretization or continualization, so that conventional RL algorithms can be
applied. However, this ignores the underlying structure of hybrid action space
and also induces the scalability issue and additional approximation
difficulties, thus leading to degenerated results. In this paper, we propose
Hybrid Action Representation (HyAR) to learn a compact and decodable latent
representation space for the original hybrid action space. HyAR constructs the
latent space and embeds the dependence between discrete action and continuous
parameter via an embedding table and conditional Variantional Auto-Encoder
(VAE). To further improve the effectiveness, the action representation is
trained to be semantically smooth through unsupervised environmental dynamics
prediction. Finally, the agent then learns its policy with conventional DRL
algorithms in the learned representation space and interacts with the
environment by decoding the hybrid action embeddings to the original action
space. We evaluate HyAR in a variety of environments with discrete-continuous
action space. The results demonstrate the superiority of HyAR when compared
with previous baselines, especially for high-dimensional action spaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CDN-MEDAL: Two-stage Density and Difference Approximation Framework for Motion Analysis. (arXiv:2106.03776v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03776">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Background modeling is a promising research area in video analysis with a
variety of video surveillance applications. Recent years have witnessed the
proliferation of deep neural networks via effective learning-based approaches
in motion analysis. However, these techniques only provide a limited
description of the observed scenes&#x27; insufficient properties where a
single-valued mapping is learned to approximate the temporal conditional
averages of the target background. On the other hand, statistical learning in
imagery domains has become one of the most prevalent approaches with high
adaptation to dynamic context transformation, notably Gaussian Mixture Models,
combined with a foreground extraction step. In this work, we propose a novel,
two-stage method of change detection with two convolutional neural networks.
The first architecture is grounded on the unsupervised Gaussian mixtures
statistical learning to describe the scenes&#x27; salient features. The second one
implements a light-weight pipeline of foreground detection. Our two-stage
framework contains approximately 3.5K parameters in total but still maintains
rapid convergence to intricate motion patterns. Our experiments on publicly
available datasets show that our proposed networks are not only capable of
generalizing regions of moving objects in unseen cases with promising results
but also are competitive in performance efficiency and effectiveness regarding
foreground segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Componentwise Boosting: An Interpretable AutoML System. (arXiv:2109.05583v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05583">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In practice, machine learning (ML) workflows require various different steps,
from data preprocessing, missing value imputation, model selection, to model
tuning as well as model evaluation. Many of these steps rely on human ML
experts. AutoML - the field of automating these ML pipelines - tries to help
practitioners to apply ML off-the-shelf without any expert knowledge. Most
modern AutoML systems like auto-sklearn, H20-AutoML or TPOT aim for high
predictive performance, thereby generating ensembles that consist almost
exclusively of black-box models. This, in turn, makes the interpretation for
the layperson more intricate and adds another layer of opacity for users. We
propose an AutoML system that constructs an interpretable additive model that
can be fitted using a highly scalable componentwise boosting algorithm. Our
system provides tools for easy model interpretation such as visualizing partial
effects and pairwise interactions, allows for a straightforward calculation of
feature importance, and gives insights into the required model complexity to
fit the given task. We introduce the general framework and outline its
implementation autocompboost. To demonstrate the frameworks efficacy, we
compare autocompboost to other existing systems based on the OpenML
AutoML-Benchmark. Despite its restriction to an interpretable model space, our
system is competitive in terms of predictive performance on most data sets
while being more user-friendly and transparent.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Locally Valid and Discriminative Predictive Intervals for Deep Learning Models. (arXiv:2106.00225v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00225">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Crucial for building trust in deep learning models for critical real-world
applications is efficient and theoretically sound uncertainty quantification, a
task that continues to be challenging. Useful uncertainty information is
expected to have two key properties: It should be valid (guaranteeing coverage)
and discriminative (more uncertain when the expected risk is high). Moreover,
when combined with deep learning (DL) methods, it should be scalable and affect
the DL model performance minimally. Most existing Bayesian methods lack
frequentist coverage guarantees and usually affect model performance. The few
available frequentist methods are rarely discriminative and/or violate coverage
guarantees due to unrealistic assumptions. Moreover, many methods are expensive
or require substantial modifications to the base neural network. Building upon
recent advances in conformal prediction [13, 31] and leveraging the classical
idea of kernel regression, we propose Locally Valid and Discriminative
predictive intervals (LVD), a simple, efficient, and lightweight method to
construct discriminative predictive intervals (PIs) for almost any DL model.
With no assumptions on the data distribution, such PIs also offer finite-sample
local coverage guarantees (contrasted to the simpler marginal coverage). We
empirically verify, using diverse datasets, that besides being the only locally
valid method, LVD also exceeds or matches the performance (including coverage
rate and prediction accuracy) of existing uncertainty quantification methods,
while offering additional benefits in scalability and flexibility.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Edge-augmented Graph Transformers: Global Self-attention is Enough for Graphs. (arXiv:2108.03348v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03348">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer neural networks have achieved state-of-the-art results for
unstructured data such as text and images but their adoption for
graph-structured data has been limited. This is partly due to the difficulty of
incorporating complex structural information in the basic transformer
framework. We propose a simple yet powerful extension to the transformer -
residual edge channels. The resultant framework, which we call Edge-augmented
Graph Transformer (EGT), can directly accept, process and output structural
information as well as node information. It allows us to use global
self-attention, the key element of transformers, directly for graphs and comes
with the benefit of long-range interaction among nodes. Moreover, the edge
channels allow the structural information to evolve from layer to layer, and
prediction tasks on edges/links can be performed directly from the output
embeddings of these channels. In addition, we introduce a generalized
positional encoding scheme for graphs based on Singular Value Decomposition
which can improve the performance of EGT. Our framework, which relies on global
node feature aggregation, achieves better performance compared to
Convolutional/Message-Passing Graph Neural Networks, which rely on local
feature aggregation within a neighborhood. We verify the performance of EGT in
a supervised learning setting on a wide range of experiments on benchmark
datasets. Our findings indicate that convolutional aggregation is not an
essential inductive bias for graphs and global self-attention can serve as a
flexible and adaptive alternative.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05125">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Both image-caption pairs and translation pairs provide the means to learn
deep representations of and connections between languages. We use both types of
pairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual
encoder that solves two tasks: 1) image-text matching and 2) translation pair
matching. By incorporating billions of translation pairs, MURAL extends ALIGN
(Jia et al. PMLR&#x27;21)--a state-of-the-art dual encoder learned from 1.8 billion
noisy image-text pairs. When using the same encoders, MURAL&#x27;s performance
matches or exceeds ALIGN&#x27;s cross-modal retrieval performance on well-resourced
languages across several datasets. More importantly, it considerably improves
performance on under-resourced languages, showing that text-text learning can
overcome a paucity of image-caption examples for these languages. On the
Wikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean
recall by 8.1% on average for eight under-resourced languages and by 6.8% on
average when fine-tuning. We additionally show that MURAL&#x27;s text
representations cluster not only with respect to genealogical connections but
also based on areal linguistics, such as the Balkan Sprachbund.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning. (arXiv:2007.08844v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.08844">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While semi-supervised learning (SSL) has proven to be a promising way for
leveraging unlabeled data when labeled data is scarce, the existing SSL
algorithms typically assume that training class distributions are balanced.
However, these SSL algorithms trained under imbalanced class distributions can
severely suffer when generalizing to a balanced testing criterion, since they
utilize biased pseudo-labels of unlabeled data toward majority classes. To
alleviate this issue, we formulate a convex optimization problem to softly
refine the pseudo-labels generated from the biased model, and develop a simple
algorithm, named Distribution Aligning Refinery of Pseudo-label (DARP) that
solves it provably and efficiently. Under various class-imbalanced
semi-supervised scenarios, we demonstrate the effectiveness of DARP and its
compatibility with state-of-the-art SSL schemes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DynSTGAT: Dynamic Spatial-Temporal Graph Attention Network for Traffic Signal Control. (arXiv:2109.05491v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05491">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adaptive traffic signal control plays a significant role in the construction
of smart cities. This task is challenging because of many essential factors,
such as cooperation among neighboring intersections and dynamic traffic
scenarios. First, to facilitate cooperation of traffic signals, existing work
adopts graph neural networks to incorporate the temporal and spatial influences
of the surrounding intersections into the target intersection, where
spatial-temporal information is used separately. However, one drawback of these
methods is that the spatial-temporal correlations are not adequately exploited
to obtain a better control scheme. Second, in a dynamic traffic environment,
the historical state of the intersection is also critical for predicting future
signal switching. Previous work mainly solves this problem using the current
intersection&#x27;s state, neglecting the fact that traffic flow is continuously
changing both spatially and temporally and does not handle the historical
state.

In this paper, we propose a novel neural network framework named DynSTGAT,
which integrates dynamic historical state into a new spatial-temporal graph
attention network to address the above two problems. More specifically, our
DynSTGAT model employs a novel multi-head graph attention mechanism, which aims
to adequately exploit the joint relations of spatial-temporal information.
Then, to efficiently utilize the historical state information of the
intersection, we design a sequence model with the temporal convolutional
network (TCN) to capture the historical information and further merge it with
the spatial information to improve its performance. Extensive experiments
conducted in the multi-intersection scenario on synthetic data and real-world
data confirm that our method can achieve superior performance in travel time
and throughput against the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Particle reconstruction of volumetric particle image velocimetry with strategy of machine learning. (arXiv:1909.07815v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.07815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Three-dimensional particle reconstruction with limited two-dimensional
projections is an under-determined inverse problem that the exact solution is
often difficult to be obtained. In general, approximate solutions can be
obtained by iterative optimization methods. In the current work, a practical
particle reconstruction method based on a convolutional neural network (CNN)
with geometry-informed features is proposed. The proposed technique can refine
the particle reconstruction from a very coarse initial guess of particle
distribution generated by any traditional algebraic reconstruction technique
(ART) based methods. Compared with available ART-based algorithms, the novel
technique makes significant improvements in terms of reconstruction quality,
{robustness to noises}, and at least an order of magnitude faster in the
offline stage.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Check Your Other Door! Establishing Backdoor Attacks in the Frequency Domain. (arXiv:2109.05507v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05507">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) have been utilized in various applications
ranging from image classification and facial recognition to medical imagery
analysis and real-time object detection. As our models become more
sophisticated and complex, the computational cost of training such models
becomes a burden for small companies and individuals; for this reason,
outsourcing the training process has been the go-to option for such users.
Unfortunately, outsourcing the training process comes at the cost of
vulnerability to backdoor attacks. These attacks aim at establishing hidden
backdoors in the DNN such that the model performs well on benign samples but
outputs a particular target label when a trigger is applied to the input.
Current backdoor attacks rely on generating triggers in the image/pixel domain;
however, as we show in this paper, it is not the only domain to exploit and one
should always &quot;check the other doors&quot;. In this work, we propose a complete
pipeline for generating a dynamic, efficient, and invisible backdoor attack in
the frequency domain. We show the advantages of utilizing the frequency domain
for establishing undetectable and powerful backdoor attacks through extensive
experiments on various datasets and network architectures. The backdoored
models are shown to break various state-of-the-art defences. We also show two
possible defences that succeed against frequency-based backdoor attacks and
possible ways for the attacker to bypass them. We conclude the work with some
remarks regarding a network&#x27;s learning capacity and the capability of embedding
a backdoor attack in the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Developers of text generation models rely on automated evaluation metrics as
a stand-in for slow and expensive manual evaluations. However, image captioning
metrics have struggled to give accurate learned estimates of the semantic and
pragmatic success of output text. We address this weakness by introducing the
first discourse-aware learned generation metric for evaluating image
descriptions. Our approach is inspired by computational theories of discourse
for capturing information goals using coherence. We present a dataset of
image$\unicode{x2013}$description pairs annotated with coherence relations. We
then train a coherence-aware metric on a subset of the Conceptual Captions
dataset and measure its effectiveness$\unicode{x2014}$its ability to predict
human ratings of output captions$\unicode{x2014}$on a test set composed of
out-of-domain images. We demonstrate a higher Kendall Correlation Coefficient
for our proposed metric with the human judgments for the results of a number of
state-of-the-art coherence-aware caption generation models when compared to
several other metrics including recently proposed learned metrics such as
BLEURT and BERTScore.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimizing a domestic battery and solar photovoltaic system with deep reinforcement learning. (arXiv:2109.05024v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05024">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A lowering in the cost of batteries and solar PV systems has led to a high
uptake of solar battery home systems. In this work, we use the deep
deterministic policy gradient algorithm to optimise the charging and
discharging behaviour of a battery within such a system. Our approach outputs a
continuous action space when it charges and discharges the battery, and can
function well in a stochastic environment. We show good performance of this
algorithm by lowering the expenditure of a single household on electricity to
almost \$1AUD for large batteries across selected weeks within a year.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improved Algorithms for Misspecified Linear Markov Decision Processes. (arXiv:2109.05546v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05546">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For the misspecified linear Markov decision process (MLMDP) model of Jin et
al. [2020], we propose an algorithm with three desirable properties. (P1) Its
regret after $K$ episodes scales as $K \max \{ \varepsilon_{\text{mis}},
\varepsilon_{\text{tol}} \}$, where $\varepsilon_{\text{mis}}$ is the degree of
misspecification and $\varepsilon_{\text{tol}}$ is a user-specified error
tolerance. (P2) Its space and per-episode time complexities remain bounded as
$K \rightarrow \infty$. (P3) It does not require $\varepsilon_{\text{mis}}$ as
input. To our knowledge, this is the first algorithm satisfying all three
properties. For concrete choices of $\varepsilon_{\text{tol}}$, we also improve
existing regret bounds (up to log factors) while achieving either (P2) or (P3)
(existing algorithms satisfy neither). At a high level, our algorithm
generalizes (to MLMDPs) and refines the Sup-Lin-UCB algorithm, which Takemura
et al. [2021] recently showed satisfies (P3) in the contextual bandit setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Learning with Multi-Head Co-Training. (arXiv:2107.04795v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.04795">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Co-training, extended from self-training, is one of the frameworks for
semi-supervised learning. Without natural split of features, single-view
co-training works at the cost of training extra classifiers, where the
algorithm should be delicately designed to prevent individual classifiers from
collapsing into each other. To remove these obstacles which deter the adoption
of single-view co-training, we present a simple and efficient algorithm
Multi-Head Co-Training. By integrating base learners into a multi-head
structure, the model is in a minimal amount of extra parameters. Every
classification head in the unified model interacts with its peers through a
&quot;Weak and Strong Augmentation&quot; strategy, in which the diversity is naturally
brought by the strong data augmentation. Therefore, the proposed method
facilitates single-view co-training by 1). promoting diversity implicitly and
2). only requiring a small extra computational overhead. The effectiveness of
Multi-Head Co-Training is demonstrated in an empirical study on standard
semi-supervised learning benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards a Rigorous Evaluation of Time-series Anomaly Detection. (arXiv:2109.05257v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05257">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, proposed studies on time-series anomaly detection (TAD)
report high F1 scores on benchmark TAD datasets, giving the impression of clear
improvements. However, most studies apply a peculiar evaluation protocol called
point adjustment (PA) before scoring. In this paper, we theoretically and
experimentally reveal that the PA protocol has a great possibility of
overestimating the detection performance; that is, even a random anomaly score
can easily turn into a state-of-the-art TAD method. Therefore, the comparison
of TAD methods with F1 scores after the PA protocol can lead to misguided
rankings. Furthermore, we question the potential of existing TAD methods by
showing that an untrained model obtains comparable detection performance to the
existing methods even without PA. Based on our findings, we propose a new
baseline and an evaluation protocol. We expect that our study will help a
rigorous evaluation of TAD and lead to further improvement in future
researches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Performing fact verification based on structured data is important for many
real-life applications and is a challenging research problem, particularly when
it involves both symbolic operations and informal inference based on language
understanding. In this paper, we present a Program-enhanced Verbalization and
Graph Attention Network (ProgVGAT) to integrate programs and execution into
textual inference models. Specifically, a verbalization with program execution
model is proposed to accumulate evidences that are embedded in operations over
the tables. Built on that, we construct the graph attention verification
networks, which are designed to fuse different sources of evidences from
verbalized program execution, program structures, and the original statements
and tables, to make the final verification decision. To support the above
framework, we propose a program selection module optimized with a new training
strategy based on margin loss, to produce more accurate programs, which is
shown to be effective in enhancing the final verification results. Experimental
results show that the proposed framework achieves the new state-of-the-art
performance, a 74.4% accuracy, on the benchmark dataset TABFACT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedTriNet: A Pseudo Labeling Method with Three Players for Federated Semi-supervised Learning. (arXiv:2109.05612v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05612">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated Learning has shown great potentials for the distributed data
utilization and privacy protection. Most existing federated learning approaches
focus on the supervised setting, which means all the data stored in each client
has labels. However, in real-world applications, the client data are impossible
to be fully labeled. Thus, how to exploit the unlabeled data should be a new
challenge for federated learning. Although a few studies are attempting to
overcome this challenge, they may suffer from information leakage or misleading
information usage problems. To tackle these issues, in this paper, we propose a
novel federated semi-supervised learning method named FedTriNet, which consists
of two learning phases. In the first phase, we pre-train FedTriNet using
labeled data with FedAvg. In the second phase, we aim to make most of the
unlabeled data to help model learning. In particular, we propose to use three
networks and a dynamic quality control mechanism to generate high-quality
pseudo labels for unlabeled data, which are added to the training set. Finally,
FedTriNet uses the new training set to retrain the model. Experimental results
on three publicly available datasets show that the proposed FedTriNet
outperforms state-of-the-art baselines under both IID and Non-IID settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyperspectral and Multispectral Classification for Coastal Wetland Using Depthwise Feature Interaction Network. (arXiv:2106.06896v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06896">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The monitoring of coastal wetlands is of great importance to the protection
of marine and terrestrial ecosystems. However, due to the complex environment,
severe vegetation mixture, and difficulty of access, it is impossible to
accurately classify coastal wetlands and identify their species with
traditional classifiers. Despite the integration of multisource remote sensing
data for performance enhancement, there are still challenges with acquiring and
exploiting the complementary merits from multisource data. In this paper, the
Deepwise Feature Interaction Network (DFINet) is proposed for wetland
classification. A depthwise cross attention module is designed to extract
self-correlation and cross-correlation from multisource feature pairs. In this
way, meaningful complementary information is emphasized for classification.
DFINet is optimized by coordinating consistency loss, discrimination loss, and
classification loss. Accordingly, DFINet reaches the standard solution-space
under the regularity of loss functions, while the spatial consistency and
feature discrimination are preserved. Comprehensive experimental results on two
hyperspectral and multispectral wetland datasets demonstrate that the proposed
DFINet outperforms other competitive methods in terms of overall accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anomaly Detection in Video via Self-Supervised and Multi-Task Learning. (arXiv:2011.07491v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07491">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Anomaly detection in video is a challenging computer vision problem. Due to
the lack of anomalous events at training time, anomaly detection requires the
design of learning methods without full supervision. In this paper, we approach
anomalous event detection in video through self-supervised and multi-task
learning at the object level. We first utilize a pre-trained detector to detect
objects. Then, we train a 3D convolutional neural network to produce
discriminative anomaly-specific information by jointly learning multiple proxy
tasks: three self-supervised and one based on knowledge distillation. The
self-supervised tasks are: (i) discrimination of forward/backward moving
objects (arrow of time), (ii) discrimination of objects in
consecutive/intermittent frames (motion irregularity) and (iii) reconstruction
of object-specific appearance information. The knowledge distillation task
takes into account both classification and detection information, generating
large prediction discrepancies between teacher and student models when
anomalies occur. To the best of our knowledge, we are the first to approach
anomalous event detection in video as a multi-task learning problem,
integrating multiple self-supervised and knowledge distillation proxy tasks in
a single architecture. Our lightweight architecture outperforms the
state-of-the-art methods on three benchmarks: Avenue, ShanghaiTech and UCSD
Ped2. Additionally, we perform an ablation study demonstrating the importance
of integrating self-supervised learning and normality-specific distillation in
a multi-task learning setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cooperative Multi-Agent Fairness and Equivariant Policies. (arXiv:2106.05727v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05727">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study fairness through the lens of cooperative multi-agent learning. Our
work is motivated by empirical evidence that naive maximization of team reward
yields unfair outcomes for individual team members. To address fairness in
multi-agent contexts, we introduce team fairness, a group-based fairness
measure for multi-agent learning. We then prove that it is possible to enforce
team fairness during policy optimization by transforming the team&#x27;s joint
policy into an equivariant map. We refer to our multi-agent learning strategy
as Fairness through Equivariance (Fair-E) and demonstrate its effectiveness
empirically. We then introduce Fairness through Equivariance Regularization
(Fair-ER) as a soft-constraint version of Fair-E and show that it reaches
higher levels of utility than Fair-E and fairer outcomes than non-equivariant
policies. Finally, we present novel findings regarding the fairness-utility
trade-off in multi-agent settings; showing that the magnitude of the trade-off
is dependent on agent skill level.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OmniLytics: A Blockchain-based Secure Data Market for Decentralized Machine Learning. (arXiv:2107.05252v2 [cs.CR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05252">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose OmniLytics, a blockchain-based secure data trading marketplace for
machine learning applications. Utilizing OmniLytics, many distributed data
owners can contribute their private data to collectively train an ML model
requested by some model owners, and receive compensation for data contribution.
OmniLytics enables such model training while simultaneously providing 1) model
security against curious data owners; 2) data security against the curious
model and data owners; 3) resilience to malicious data owners who provide
faulty results to poison model training; and 4) resilience to malicious model
owners who intend to evade payment. OmniLytics is implemented as a blockchain
smart contract to guarantee the atomicity of payment. In OmniLytics, a model
owner splits its model into the private and public parts and publishes the
public part on the contract. Through the execution of the contract, the
participating data owners securely aggregate their locally trained models to
update the model owner&#x27;s public model and receive reimbursement through the
contract. We implement a working prototype of OmniLytics on Ethereum blockchain
and perform extensive experiments to measure its gas cost, execution time, and
model quality under various parameter combinations. For training a CNN on the
MNIST dataset, the MO is able to boost its model accuracy from 62% to 83%
within 500ms in blockchain processing time.This demonstrates the effectiveness
of OmniLytics for practical deployment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Meta-Interpretive Learning as Metarule Specialisation. (arXiv:2106.07464v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07464">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In Meta-Interpretive Learning (MIL) the metarules, second-order datalog
clauses acting as inductive bias, are manually defined by the user. In this
work we show that second-order metarules for MIL can be learned by MIL. We
define a generality ordering of metarules by $\theta$-subsumption and show that
user-defined sort metarules are derivable by specialisation of the most-general
matrix metarules in a language class; and that these matrix metarules are in
turn derivable by specialisation of third-order punch metarules with variables
that range over the set of second-order literals and for which only an upper
bound on their number of literals need be user-defined. We show that the
cardinality of a metarule language is polynomial in the number of literals in
punch metarules. We re-frame MIL as metarule specialisation by resolution. We
modify the MIL metarule specialisation operator to return new metarules rather
than first-order clauses and prove the correctness of the new operator. We
implement the new operator as TOIL, a sub-system of the MIL system Louise. Our
experiments show that as user-defined sort metarules are progressively replaced
by sort metarules learned by TOIL, Louise&#x27;s predictive accuracy is maintained
at the cost of a small increase in training times. We conclude that
automatically derived metarules can replace user-defined metarules.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding and Resolving Performance Degradation in Graph Convolutional Networks. (arXiv:2006.07107v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A Graph Convolutional Network (GCN) stacks several layers and in each layer
performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN)
for learning node representations over graph-structured data. Though powerful,
GCNs tend to suffer performance drop when the model gets deep. Previous works
focus on PROPs to study and mitigate this issue, but the role of TRANs is
barely investigated. In this work, we study performance degradation of GCNs by
experimentally examining how stacking only TRANs or PROPs works. We find that
TRANs contribute significantly, or even more than PROPs, to declining
performance, and moreover that they tend to amplify node-wise feature variance
in GCNs, causing variance inflammation that we identify as a key factor for
causing performance drop. Motivated by such observations, we propose a
variance-controlling technique termed Node Normalization (NodeNorm), which
scales each node&#x27;s features using its own standard deviation. Experimental
results validate the effectiveness of NodeNorm on addressing performance
degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow
ones in cases where deep models are needed, and to achieve comparable results
with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and
can well generalize to other GNN architectures. Code is publicly available at
https://github.com/miafei/NodeNorm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00544">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training, a method for learning robust deep neural networks,
constructs adversarial examples during training. However, recent methods for
generating NLP adversarial examples involve combinatorial search and expensive
sentence encoders for constraining the generated instances. As a result, it
remains challenging to use vanilla adversarial training to improve NLP models&#x27;
performance, and the benefits are mainly uninvestigated. This paper proposes a
simple and improved vanilla adversarial training process for NLP models, which
we name Attacking to Training (A2T). The core part of A2T is a new and cheaper
word substitution attack optimized for vanilla adversarial training. We use A2T
to train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI
datasets. Our results empirically show that it is possible to train robust NLP
models using a much cheaper adversary. We demonstrate that vanilla adversarial
training with A2T can improve an NLP model&#x27;s robustness to the attack it was
originally trained with and also defend the model against other types of word
substitution attacks. Furthermore, we show that A2T can improve NLP models&#x27;
standard accuracy, cross-domain generalization, and interpretability. Code is
available at https://github.com/QData/Textattack-A2T .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constrained Non-Affine Alignment of Embeddings. (arXiv:1910.05862v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.05862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Embeddings are one of the fundamental building blocks for data analysis
tasks. Embeddings are already essential tools for large language models and
image analysis, and their use is being extended to many other research domains.
The generation of these distributed representations is often a data- and
computation-expensive process; yet the holistic analysis and adjustment of them
after they have been created is still a developing area. In this paper, we
first propose a very general quantitatively measure for the presence of
features in the embedding data based on if it can be learned. We then devise a
method to remove or alleviate undesired features in the embedding while
retaining the essential structure of the data. We use a Domain Adversarial
Network (DAN) to generate a non-affine transformation, but we add constraints
to ensure the essential structure of the embedding is preserved. Our empirical
results demonstrate that the proposed algorithm significantly outperforms the
state-of-art unsupervised algorithm on several data sets, including novel
applications from the industry.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-Driven Reachability Analysis Using Matrix Zonotopes. (arXiv:2011.08472v3 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08472">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a data-driven reachability analysis approach for
unknown system dynamics. Reachability analysis is an essential tool for
guaranteeing safety properties. However, most current reachability analysis
heavily relies on the existence of a suitable system model, which is often not
directly available in practice. We instead propose a data-driven reachability
analysis approach from noisy data. More specifically, we first provide an
algorithm for over-approximating the reachable set of a linear time-invariant
system using matrix zonotopes. Then we introduce an extension for Lipschitz
nonlinear systems. We provide theoretical guarantees in both cases. Numerical
examples show the potential and applicability of the introduced methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning To Describe Player Form in The MLB. (arXiv:2109.05280v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05280">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Major League Baseball (MLB) has a storied history of using statistics to
better understand and discuss the game of baseball, with an entire discipline
of statistics dedicated to the craft, known as sabermetrics. At their core, all
sabermetrics seek to quantify some aspect of the game, often a specific aspect
of a player&#x27;s skill set - such as a batter&#x27;s ability to drive in runs (RBI) or
a pitcher&#x27;s ability to keep batters from reaching base (WHIP). While useful,
such statistics are fundamentally limited by the fact that they are derived
from an account of what happened on the field, not how it happened. As a first
step towards alleviating this shortcoming, we present a novel, contrastive
learning-based framework for describing player form in the MLB. We use form to
refer to the way in which a player has impacted the course of play in their
recent appearances. Concretely, a player&#x27;s form is described by a
72-dimensional vector. By comparing clusters of players resulting from our form
representations and those resulting from traditional abermetrics, we
demonstrate that our form representations contain information about how players
impact the course of play, not present in traditional, publicly available
statistics. We believe these embeddings could be utilized to predict both
in-game and game-level events, such as the result of an at-bat or the winner of
a game.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05424">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many recent successes in sentence representation learning have been achieved
by simply fine-tuning on the Natural Language Inference (NLI) datasets with
triplet loss or siamese loss. Nevertheless, they share a common weakness:
sentences in a contradiction pair are not necessarily from different semantic
categories. Therefore, optimizing the semantic entailment and contradiction
reasoning objective alone is inadequate to capture the high-level semantic
structure. The drawback is compounded by the fact that the vanilla siamese or
triplet losses only learn from individual sentence pairs or triplets, which
often suffer from bad local optima. In this paper, we propose PairSupCon, an
instance discrimination based approach aiming to bridge semantic entailment and
contradiction understanding with high-level categorical concept encoding. We
evaluate PairSupCon on various downstream tasks that involve understanding
sentence semantics at different granularities. We outperform the previous
state-of-the-art method with $10\%$--$13\%$ averaged improvement on eight
clustering tasks, and $5\%$--$6\%$ averaged improvement on seven semantic
textual similarity (STS) tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On The Radon-Nikodym Spectral Approach With Optimal Clustering. (arXiv:1906.00460v17 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.00460">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Problems of interpolation, classification, and clustering are considered. In
the tenets of Radon--Nikodym approach $\langle f(\mathbf{x})\psi^2 \rangle /
\langle\psi^2\rangle$, where the $\psi(\mathbf{x})$ is a linear function on
input attributes, all the answers are obtained from a generalized eigenproblem
$|f|\psi^{[i]}\rangle &#x3D; \lambda^{[i]} |\psi^{[i]}\rangle$. The solution to the
interpolation problem is a regular Radon-Nikodym derivative. The solution to
the classification problem requires prior and posterior probabilities that are
obtained using the Lebesgue quadrature[1] technique. Whereas in a Bayesian
approach new observations change only outcome probabilities, in the
Radon-Nikodym approach not only outcome probabilities but also the probability
space $|\psi^{[i]}\rangle$ change with new observations. This is a remarkable
feature of the approach: both the probabilities and the probability space are
constructed from the data. The Lebesgue quadrature technique can be also
applied to the optimal clustering problem. The problem is solved by
constructing a Gaussian quadrature on the Lebesgue measure. A distinguishing
feature of the Radon-Nikodym approach is the knowledge of the invariant group:
all the answers are invariant relatively any non-degenerated linear transform
of input vector $\mathbf{x}$ components. A software product implementing the
algorithms of interpolation, classification, and optimal clustering is
available from the authors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07814">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Growing polarization of the news media has been blamed for fanning
disagreement, controversy and even violence. Early identification of polarized
topics is thus an urgent matter that can help mitigate conflict. However,
accurate measurement of topic-wise polarization is still an open research
challenge. To address this gap, we propose Partisanship-aware Contextualized
Topic Embeddings (PaCTE), a method to automatically detect polarized topics
from partisan news sources. Specifically, utilizing a language model that has
been finetuned on recognizing partisanship of the news articles, we represent
the ideology of a news corpus on a topic by corpus-contextualized topic
embedding and measure the polarization using cosine distance. We apply our
method to a dataset of news articles about the COVID-19 pandemic. Extensive
experiments on different news sources and topics demonstrate the efficacy of
our method to capture topical polarization, as indicated by its effectiveness
of retrieving the most polarized topics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor Plans. (arXiv:2103.11161v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11161">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel method for reconstructing floor plans from noisy 3D point
clouds. Our main contribution is a principled approach that relies on the Monte
Carlo Tree Search (MCTS) algorithm to maximize a suitable objective function
efficiently despite the complexity of the problem. Like previous work, we first
project the input point cloud to a top view to create a density map and extract
room proposals from it. Our method selects and optimizes the polygonal shapes
of these room proposals jointly to fit the density map and outputs an accurate
vectorized floor map even for large complex scenes. To do this, we adapted
MCTS, an algorithm originally designed to learn to play games, to select the
room proposals by maximizing an objective function combining the fitness with
the density map as predicted by a deep network and regularizing terms on the
room shapes. We also introduce a refinement step to MCTS that adjusts the shape
of the room proposals. For this step, we propose a novel differentiable method
for rendering the polygonal shapes of these proposals. We evaluate our method
on the recent and challenging Structured3D and Floor-SP datasets and show a
significant improvement over the state-of-the-art, without imposing any hard
constraints nor assumptions on the floor plan configurations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine-Learned Phase Diagrams of Generalized Kitaev Honeycomb Magnets. (arXiv:2102.01103v2 [cond-mat.str-el] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01103">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We use a recently developed interpretable and unsupervised machine-learning
method, the tensorial kernel support vector machine (TK-SVM), to investigate
the low-temperature classical phase diagram of a generalized
Heisenberg-Kitaev-$\Gamma$ ($J$-$K$-$\Gamma$) model on a honeycomb lattice.
Aside from reproducing phases reported by previous quantum and classical
studies, our machine finds a hitherto missed nested zigzag-stripy order and
establishes the robustness of a recently identified modulated $S_3 \times Z_3$
phase, which emerges through the competition between the Kitaev and $\Gamma$
spin liquids, against Heisenberg interactions. The results imply that, in the
restricted parameter space spanned by the three primary exchange interactions
-- $J$, $K$, and $\Gamma$, the representative Kitaev material $\alpha$-${\rm
RuCl}_3$ lies close to the boundaries of several phases, including a simple
ferromagnet, the unconventional $S_3 \times Z_3$ and nested zigzag-stripy
magnets. A zigzag order is stabilized by a finite $\Gamma^{\prime}$ and/or
$J_3$ term, whereas the four magnetic orders may compete in particular if
$\Gamma^{\prime}$ is anti-ferromagnetic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physics-based machine learning for modeling stochastic IP3-dependent calcium dynamics. (arXiv:2109.05053v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05053">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a machine learning method for model reduction which incorporates
domain-specific physics through candidate functions. Our method estimates an
effective probability distribution and differential equation model from
stochastic simulations of a reaction network. The close connection between
reduced and fine scale descriptions allows approximations derived from the
master equation to be introduced into the learning problem. This representation
is shown to improve generalization and allows a large reduction in network size
for a classic model of inositol trisphosphate (IP3) dependent calcium
oscillations in non-excitable cells.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning. (arXiv:2006.08767v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.08767">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work introduces a neuro-symbolic agent that combines deep reinforcement
learning (DRL) with temporal logic (TL) to achieve systematic zero-shot, i.e.,
never-seen-before, generalisation of formally specified instructions. In
particular, we present a neuro-symbolic framework where a symbolic module
transforms TL specifications into a form that helps the training of a DRL agent
targeting generalisation, while a neural module learns systematically to solve
the given tasks. We study the emergence of systematic learning in different
settings and find that the architecture of the convolutional layers is key when
generalising to new instructions. We also provide evidence that systematic
learning can emerge with abstract operators such as negation when learning from
a few training examples, which previous research have struggled with.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nested Counterfactual Identification from Arbitrary Surrogate Experiments. (arXiv:2107.03190v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03190">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Ladder of Causation describes three qualitatively different types of
activities an agent may be interested in engaging in, namely, seeing
(observational), doing (interventional), and imagining (counterfactual) (Pearl
and Mackenzie, 2018). The inferential challenge imposed by the causal hierarchy
is that data is collected by an agent observing or intervening in a system
(layers 1 and 2), while its goal may be to understand what would have happened
had it taken a different course of action, contrary to what factually ended up
happening (layer 3). While there exists a solid understanding of the conditions
under which cross-layer inferences are allowed from observations to
interventions, the results are somewhat scarcer when targeting counterfactual
quantities. In this paper, we study the identification of nested
counterfactuals from an arbitrary combination of observations and experiments.
Specifically, building on a more explicit definition of nested counterfactuals,
we prove the counterfactual unnesting theorem (CUT), which allows one to map
arbitrary nested counterfactuals to unnested ones. For instance, applications
in mediation and fairness analysis usually evoke notions of direct, indirect,
and spurious effects, which naturally require nesting. Second, we introduce a
sufficient and necessary graphical condition for counterfactual identification
from an arbitrary combination of observational and experimental distributions.
Lastly, we develop an efficient and complete algorithm for identifying nested
counterfactuals; failure of the algorithm returning an expression for a query
implies it is not identifiable.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Local-Global Knowledge Distillation in Heterogeneous Federated Learning with Non-IID Data. (arXiv:2107.00051v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.00051">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning enables multiple clients to collaboratively learn a global
model by periodically aggregating the clients&#x27; models without transferring the
local data. However, due to the heterogeneity of the system and data, many
approaches suffer from the &quot;client-drift&quot; issue that could significantly slow
down the convergence of the global model training. As clients perform local
updates on heterogeneous data through heterogeneous systems, their local models
drift apart. To tackle this issue, one intuitive idea is to guide the local
model training by the global teachers, i.e., past global models, where each
client learns the global knowledge from past global models via adaptive
knowledge distillation techniques. Coming from these insights, we propose a
novel approach for heterogeneous federated learning, namely FedGKD, which fuses
the knowledge from historical global models for local training to alleviate the
&quot;client-drift&quot; issue. In this paper, we evaluate FedGKD with extensive
experiments on various CV/NLP datasets (i.e., CIFAR-10/100, Tiny-ImageNet, AG
News, SST5) and different heterogeneous settings. The proposed method is
guaranteed to converge under common assumptions, and achieves superior
empirical accuracy in fewer communication runs than five state-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Fundamental Limits of Matrix Completion: Leveraging Hierarchical Similarity Graphs. (arXiv:2109.05408v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05408">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the matrix completion problem that leverages hierarchical similarity
graphs as side information in the context of recommender systems. Under a
hierarchical stochastic block model that well respects practically-relevant
social graphs and a low-rank rating matrix model, we characterize the exact
information-theoretic limit on the number of observed matrix entries (i.e.,
optimal sample complexity) by proving sharp upper and lower bounds on the
sample complexity. In the achievability proof, we demonstrate that probability
of error of the maximum likelihood estimator vanishes for sufficiently large
number of users and items, if all sufficient conditions are satisfied. On the
other hand, the converse (impossibility) proof is based on the genie-aided
maximum likelihood estimator. Under each necessary condition, we present
examples of a genie-aided estimator to prove that the probability of error does
not vanish for sufficiently large number of users and items. One important
consequence of this result is that exploiting the hierarchical structure of
social graphs yields a substantial gain in sample complexity relative to the
one that simply identifies different groups without resorting to the relational
structure across them. More specifically, we analyze the optimal sample
complexity and identify different regimes whose characteristics rely on quality
metrics of side information of the hierarchical similarity graph. Finally, we
present simulation results to corroborate our theoretical findings and show
that the characterized information-theoretic limit can be asymptotically
achieved.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training. (arXiv:2007.12826v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.12826">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Modern neural networks are often operated in a strongly overparametrized
regime: they comprise so many parameters that they can interpolate the training
set, even if actual labels are replaced by purely random ones. Despite this,
they achieve good prediction error on unseen data: interpolating the training
set does not lead to a large generalization error. Further, overparametrization
appears to be beneficial in that it simplifies the optimization landscape. Here
we study these phenomena in the context of two-layers neural networks in the
neural tangent (NT) regime. We consider a simple data model, with isotropic
covariates vectors in $d$ dimensions, and $N$ hidden neurons. We assume that
both the sample size $n$ and the dimension $d$ are large, and they are
polynomially related. Our first main result is a characterization of the
eigenstructure of the empirical NT kernel in the overparametrized regime $Nd\gg
n$. This characterization implies as a corollary that the minimum eigenvalue of
the empirical NT kernel is bounded away from zero as soon as $Nd\gg n$, and
therefore the network can exactly interpolate arbitrary labels in the same
regime.

Our second main result is a characterization of the generalization error of
NT ridge regression including, as a special case, min-$\ell_2$ norm
interpolation. We prove that, as soon as $Nd\gg n$, the test error is well
approximated by the one of kernel ridge regression with respect to the
infinite-width kernel. The latter is in turn well approximated by the error of
polynomial ridge regression, whereby the regularization parameter is increased
by a &#x60;self-induced&#x27; term related to the high-degree components of the
activation function. The polynomial degree depends on the sample size and the
dimension (in particular on $\log n/\log d$).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.13567">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>A conventional subject-dependent (SD) brain-computer interface (BCI) requires
a complete data-gathering, training, and calibration phase for each user before
it can be used. In recent years, a number of subject-independent (SI) BCIs have
been developed. However, there are many problems preventing them from being
used in real-world BCI applications. A weaker performance compared to the
subject-dependent (SD) approach, and a relatively large model requiring high
computational power are the most important ones. Therefore, a potential
real-world BCI would greatly benefit from a compact low-power
subject-independent BCI framework, ready to be used immediately after the user
puts it on. To move towards this goal, we propose a novel subject-independent
BCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)
trained on the motor imagery (MI) paradigm of a large-scale
electroencephalography (EEG) signals database consisting of 21600 trials for 54
subjects performing two-class hand-movement MI tasks. The proposed framework
applies a wavelet kernel convolutional neural network (WKCNN) and a temporal
convolutional neural network (TCNN) in order to represent and extract the
diverse spectral features of EEG signals. The outputs of the convolutional
layers go through a common spatial pattern (CSP) algorithm for spatial feature
extraction. The number of CSP features is reduced by a dense neural network,
and the final class label is determined by a linear discriminative analysis
(LDA) classifier. The CCSPNet framework evaluation results show that it is
possible to have a low-power compact BCI that achieves both SD and SI
performance comparable to complex and computationally expensive models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GraphITE: Estimating Individual Effects of Graph-structured Treatments. (arXiv:2009.14061v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14061">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Outcome estimation of treatments for target individuals is an important
foundation for decision making based on causal relations. Most existing outcome
estimation methods deal with binary or multiple-choice treatments; however, in
some applications, the number of treatments can be significantly large, while
the treatments themselves have rich information. In this study, we considered
one important instance of such cases: the outcome estimation problem of
graph-structured treatments such as drugs. Owing to the large number of
possible treatments, the counterfactual nature of observational data that
appears in conventional treatment effect estimation becomes more of a concern
for this problem. Our proposed method, GraphITE (pronounced &quot;graphite&quot;) learns
the representations of graph-structured treatments using graph neural networks
while mitigating observation biases using Hilbert-Schmidt Independence
Criterion regularization, which increases the independence of the
representations of the targets and treatments. Experiments on two real-world
datasets show that GraphITE outperforms baselines, especially in cases with a
large number of treatments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05522">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal language analysis is a burgeoning field of NLP that aims to
simultaneously model a speaker&#x27;s words, acoustical annotations, and facial
expressions. In this area, lexicon features usually outperform other modalities
because they are pre-trained on large corpora via Transformer-based models.
Despite their strong performance, training a new self-supervised learning (SSL)
Transformer on any modality is not usually attainable due to insufficient data,
which is the case in multimodal language learning. This work proposes a
Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the
mentioned constraints without training a complete Transformer model. TEASEL
model includes speech modality as a dynamic prefix besides the textual modality
compared to a conventional language model. This method exploits a conventional
pre-trained language model as a cross-modal Transformer model. We evaluated
TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.
Extensive experiments show that our model outperforms unimodal baseline
language models by 4% and outperforms the current multimodal state-of-the-art
(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%
smaller than the SoTA model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Joint Graph and Image Convolution Network for Automatic Brain Tumor Segmentation. (arXiv:2109.05580v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a joint graph convolution-image convolution neural network as our
submission to the Brain Tumor Segmentation (BraTS) 2021 challenge. We model
each brain as a graph composed of distinct image regions, which is initially
segmented by a graph neural network (GNN). Subsequently, the tumorous volume
identified by the GNN is further refined by a simple (voxel) convolutional
neural network (CNN), which produces the final segmentation. This approach
captures both global brain feature interactions via the graphical
representation and local image details through the use of convolutional
filters. We find that the GNN component by itself can effectively identify and
segment the brain tumors. The addition of the CNN further improves the median
performance of the model by 2 percent across all metrics evaluated. On the
validation set, our joint GNN-CNN model achieves mean Dice scores of 0.89,
0.81, 0.73 and mean Hausdorff distances (95th percentile) of 6.8, 12.6, 28.2mm
on the whole tumor, core tumor, and enhancing tumor, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tensor Estimation with Nearly Linear Samples Given Weak Side Information. (arXiv:2007.00736v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00736">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tensor completion exhibits an interesting computational-statistical gap in
terms of the number of samples needed to perform tensor estimation. While there
are only $\Theta(tn)$ degrees of freedom in a $t$-order tensor with $n^t$
entries, the best known polynomial time algorithm requires $O(n^{t/2})$ samples
in order to guarantee consistent estimation. In this paper, we show that weak
side information is sufficient to reduce the sample complexity to $O(n)$. The
side information consists of a weight vector for each of the modes which is not
orthogonal to any of the latent factors along that mode; this is significantly
weaker than assuming noisy knowledge of the subspaces. We provide an algorithm
that utilizes this side information to produce a consistent estimator with
$O(n^{1+\kappa})$ samples for any small constant $\kappa &gt; 0$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient and Interpretable Robot Manipulation with Graph Neural Networks. (arXiv:2102.13177v3 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13177">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Manipulation tasks like loading a dishwasher can be seen as a sequence of
spatial constraints and relationships between different objects. For example, a
plate can be placed in a tray only if the tray is open. We aim to discover such
task-specific rules from demonstrations. We pose manipulation as a
classification problem over a graph, whose nodes represent task relevant
entities like objects and goals, transform the environment scene into a graph
and learn a graph neural network (GNN) policy using imitation learning. In our
experiments, a single learned GNN policy, trained using 20 expert
demonstrations, can solve multiple blockstacking and rearrangement tasks in
both simulation and on hardware, without any task description. The policy
successfully generalizes over the number of objects in the environment, their
positions, and goal configurations (trained on single stacks, generalizes to
pyramids and multiple stacks). We also apply our approach to a complex
simulated dishwasher environment, where a robot learns to load a dishwasher
from only 5 high-level human demonstrations. These experiments show that
imitation learning on a graphical state and policy is a simple, yet powerful
tool for solving complex long-horizon manipulation problems, without requiring
detailed task descriptions. Videos can be found at:
https://youtu.be/x9hcKBh6K0A.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Differentially Private Variable Selection via the Knockoff Filter. (arXiv:2109.05402v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05402">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The knockoff filter, recently developed by Barber and Candes, is an effective
procedure to perform variable selection with a controlled false discovery rate
(FDR). We propose a private version of the knockoff filter by incorporating
Gaussian and Laplace mechanisms, and show that variable selection with
controlled FDR can be achieved. Simulations demonstrate that our setting has
reasonable statistical power.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A review of possible effects of cognitive biases on the interpretation of rule-based machine learning models. (arXiv:1804.02969v7 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1804.02969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While the interpretability of machine learning models is often equated with
their mere syntactic comprehensibility, we think that interpretability goes
beyond that, and that human interpretability should also be investigated from
the point of view of cognitive science. The goal of this paper is to discuss to
what extent cognitive biases may affect human understanding of interpretable
machine learning models, in particular of logical rules discovered from data.
Twenty cognitive biases are covered, as are possible debiasing techniques that
can be adopted by designers of machine learning algorithms and software. Our
review transfers results obtained in cognitive psychology to the domain of
machine learning, aiming to bridge the current gap between these two areas. It
needs to be followed by empirical studies specifically focused on the machine
learning domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatiotemporal Pattern Mining for Nowcasting Extreme Earthquakes in Southern California. (arXiv:2012.14336v3 [physics.geo-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14336">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Geoscience and seismology have utilized the most advanced technologies and
equipment to monitor seismic events globally from the past few decades. With
the enormous amount of data, modern GPU-powered deep learning presents a
promising approach to analyze data and discover patterns. In recent years,
there are plenty of successful deep learning models for picking seismic waves.
However, forecasting extreme earthquakes, which can cause disasters, is still
an underdeveloped topic in history. Relevant research in spatiotemporal
dynamics mining and forecasting has revealed some successful predictions, a
crucial topic in many scientific research fields. Most studies of them have
many successful applications of using deep neural networks. In Geology and
Earth science studies, earthquake prediction is one of the world&#x27;s most
challenging problems, about which cutting-edge deep learning technologies may
help discover some valuable patterns. In this project, we propose a deep
learning modeling approach, namely \tseqpre, to mine spatiotemporal patterns
from data to nowcast extreme earthquakes by discovering visual dynamics in
regional coarse-grained spatial grids over time. In this modeling approach, we
use synthetic deep learning neural networks with domain knowledge in geoscience
and seismology to exploit earthquake patterns for prediction using
convolutional long short-term memory neural networks. Our experiments show a
strong correlation between location prediction and magnitude prediction for
earthquakes in Southern California. Ablation studies and visualization validate
the effectiveness of the proposed modeling method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-time End-to-End Federated Learning: An Automotive Case Study. (arXiv:2103.11879v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11879">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the development and the increasing interests in ML/DL fields, companies
are eager to apply Machine Learning/Deep Learning approaches to increase
service quality and customer experience. Federated Learning was implemented as
an effective model training method for distributing and accelerating
time-consuming model training while protecting user data privacy. However,
common Federated Learning approaches, on the other hand, use a synchronous
protocol to conduct model aggregation, which is inflexible and unable to adapt
to rapidly changing environments and heterogeneous hardware settings in
real-world scenarios. In this paper, we present an approach to real-time
end-to-end Federated Learning combined with a novel asynchronous model
aggregation protocol. Our method is validated in an industrial use case in the
automotive domain, focusing on steering wheel angle prediction for autonomous
driving. Our findings show that asynchronous Federated Learning can
significantly improve the prediction performance of local edge models while
maintaining the same level of accuracy as centralized machine learning.
Furthermore, by using a sliding training window, the approach can minimize
communication overhead, accelerate model training speed and consume real-time
streaming data, proving high efficiency when deploying ML/DL components to
heterogeneous real-world embedded systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07467">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Stance detection concerns the classification of a writer&#x27;s viewpoint towards
a target. There are different task variants, e.g., stance of a tweet vs. a full
article, or stance with respect to a claim vs. an (implicit) topic. Moreover,
task definitions vary, which includes the label inventory, the data collection,
and the annotation protocol. All these aspects hinder cross-domain studies, as
they require changes to standard domain adaptation approaches. In this paper,
we perform an in-depth analysis of 16 stance detection datasets, and we explore
the possibility for cross-domain learning from them. Moreover, we propose an
end-to-end unsupervised framework for out-of-domain prediction of unseen,
user-defined labels. In particular, we combine domain adaptation techniques
such as mixture of experts and domain-adversarial training with label
embeddings, and we demonstrate sizable performance gains over strong baselines,
both (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for
unseen targets. Finally, we perform an exhaustive analysis of the cross-domain
results, and we highlight the important factors influencing the model
performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-source Heterogeneous Domain Adaptation with Conditional Weighting Adversarial Network. (arXiv:2008.02714v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.02714">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Heterogeneous domain adaptation (HDA) tackles the learning of cross-domain
samples with both different probability distributions and feature
representations. Most of the existing HDA studies focus on the single-source
scenario. In reality, however, it is not uncommon to obtain samples from
multiple heterogeneous domains. In this article, we study the multisource HDA
problem and propose a conditional weighting adversarial network (CWAN) to
address it. The proposed CWAN adversarially learns a feature transformer, a
label classifier, and a domain discriminator. To quantify the importance of
different source domains, CWAN introduces a sophisticated conditional weighting
scheme to calculate the weights of the source domains according to the
conditional distribution divergence between the source and target domains.
Different from existing weighting schemes, the proposed conditional weighting
scheme not only weights the source domains but also implicitly aligns the
conditional distributions during the optimization process. Experimental results
clearly demonstrate that the proposed CWAN performs much better than several
state-of-the-art methods on four real-world datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05097">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A hyperbole is an intentional and creative exaggeration not to be taken
literally. Despite its ubiquity in daily life, the computational explorations
of hyperboles are scarce. In this paper, we tackle the under-explored and
challenging task: sentence-level hyperbole generation. We start with a
representative syntactic pattern for intensification and systematically study
the semantic (commonsense and counterfactual) relationships between each
component in such hyperboles. Next, we leverage the COMeT and reverse COMeT
models to do commonsense and counterfactual inference. We then generate
multiple hyperbole candidates based on our findings from the pattern, and train
neural classifiers to rank and select high-quality hyperboles. Automatic and
human evaluations show that our generation method is able to generate
hyperboles creatively with high success rate and intensity scores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Initial Behavior Monitoring Issues in Federated Learning. (arXiv:2109.05385v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05385">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In Federated Learning (FL), a group of workers participate to build a global
model under the coordination of one node, the chief. Regarding the
cybersecurity of FL, some attacks aim at injecting the fabricated local model
updates into the system. Some defenses are based on malicious worker detection
and behavioral pattern analysis. In this context, without timely and dynamic
monitoring methods, the chief cannot detect and remove the malicious or
unreliable workers from the system. Our work emphasize the urgency to prepare
the federated learning process for monitoring and eventually behavioral pattern
analysis. We study the information inside the learning process in the early
stages of training, propose a monitoring process and evaluate the monitoring
period required. The aim is to analyse at what time is it appropriate to start
the detection algorithm in order to remove the malicious or unreliable workers
from the system and optimise the defense mechanism deployment. We tested our
strategy on a behavioral pattern analysis defense applied to the FL process of
different benchmark systems for text and image classification. Our results show
that the monitoring process lowers false positives and false negatives and
consequently increases system efficiency by enabling the distributed learning
system to achieve better performance in the early stage of training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Context-aware surrogate modeling for balancing approximation and sampling costs in multi-fidelity importance sampling and Bayesian inverse problems. (arXiv:2010.11708v2 [math.NA] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11708">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-fidelity methods leverage low-cost surrogate models to speed up
computations and make occasional recourse to expensive high-fidelity models to
establish accuracy guarantees. Because surrogate and high-fidelity models are
used together, poor predictions by surrogate models can be compensated with
frequent recourse to high-fidelity models. Thus, there is a trade-off between
investing computational resources to improve the accuracy of surrogate models
versus simply making more frequent recourse to expensive high-fidelity models;
however, this trade-off is ignored by traditional modeling methods that
construct surrogate models that are meant to replace high-fidelity models
rather than being used together with high-fidelity models. This work considers
multi-fidelity importance sampling and theoretically and computationally trades
off increasing the fidelity of surrogate models for constructing more accurate
biasing densities and the numbers of samples that are required from the
high-fidelity models to compensate poor biasing densities. Numerical examples
demonstrate that such context-aware surrogate models for multi-fidelity
importance sampling have lower fidelity than what typically is set as tolerance
in traditional model reduction, leading to runtime speedups of up to one order
of magnitude in the presented examples.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Structural Vulnerability in Graph Convolutional Networks. (arXiv:2108.06280v2 [cs.LG] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06280">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies have shown that Graph Convolutional Networks (GCNs) are
vulnerable to adversarial attacks on the graph structure. Although multiple
works have been proposed to improve their robustness against such structural
adversarial attacks, the reasons for the success of the attacks remain unclear.
In this work, we theoretically and empirically demonstrate that structural
adversarial examples can be attributed to the non-robust aggregation scheme
(i.e., the weighted mean) of GCNs. Specifically, our analysis takes advantage
of the breakdown point which can quantitatively measure the robustness of
aggregation schemes. The key insight is that weighted mean, as the basic design
of GCNs, has a low breakdown point and its output can be dramatically changed
by injecting a single edge. We show that adopting the aggregation scheme with a
high breakdown point (e.g., median or trimmed mean) could significantly enhance
the robustness of GCNs against structural attacks. Extensive experiments on
four real-world datasets demonstrate that such a simple but effective method
achieves the best robustness performance compared to state-of-the-art models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic Iterative Graph Matching. (arXiv:2106.02206v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent works leveraging Graph Neural Networks to approach graph matching
tasks have shown promising results. Recent progress in learning discrete
distributions poses new opportunities for learning graph matching models. In
this work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),
to address the graph matching problem. Our model defines a distribution of
matchings for a graph pair so the model can explore a wide range of possible
matchings. We further introduce a novel multi-step matching procedure, which
learns how to refine a graph pair&#x27;s matching results incrementally. The model
also includes dummy nodes so that the model does not have to find matchings for
nodes without correspondence. We fit this model to data via scalable stochastic
optimization. We conduct extensive experiments across synthetic graph datasets
as well as biochemistry and computer vision applications. Across all tasks, our
results show that SIGMA can produce significantly improved graph matching
results compared to state-of-the-art models. Ablation studies verify that each
of our components (stochastic training, iterative matching, and dummy nodes)
offers noticeable improvement.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accelerating Federated Learning with a Global Biased Optimiser. (arXiv:2108.09134v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09134">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated Learning (FL) is a recent development in the field of machine
learning that collaboratively trains models without the training data leaving
client devices, to preserve data privacy. In realistic FL settings, the
training set is distributed over clients in a highly non-Independent and
Identically Distributed (non-IID) fashion, which has been shown extensively to
harm FL convergence speed and final model performance. To address this
challenge, we propose a novel, generalised approach for incorporating adaptive
optimisation techniques into FL with the Federated Global Biased Optimiser
(FedGBO) algorithm. FedGBO accelerates FL by employing a set of global biased
optimiser values during the client-training phase, which helps to reduce
&#x60;client-drift&#x27; from non-IID data, whilst also benefiting from adaptive
optimisation. We show that the FedGBO update with a generic optimiser can be
reformulated as centralised training using biased gradients and optimiser
updates, and apply this theoretical framework to prove the convergence of
FedGBO using momentum-Stochastic Gradient Descent (SGDm). We also conduct
extensive experiments using 4 realistic FL benchmark datasets (CIFAR100,
Sent140, FEMNIST, Shakespeare) and 3 popular adaptive optimisers (RMSProp,
SGDm, Adam) to compare the performance of state-of-the-art adaptive-FL
algorithms. The results demonstrate that FedGBO has highly competitive
performance whilst achieving lower communication and computation costs, and
provide practical insights into the trade-offs associated with the different
adaptive-FL algorithms and optimisers for real-world FL deployments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MO-PaDGAN: Reparameterizing Engineering Designs for Augmented Multi-objective Optimization. (arXiv:2009.07110v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.07110">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-objective optimization is key to solving many Engineering Design
problems, where design parameters are optimized for several performance
indicators. However, optimization results are highly dependent on how the
designs are parameterized. Researchers have shown that deep generative models
can learn compact design representations, providing a new way of parameterizing
designs to achieve faster convergence and improved optimization performance.
Despite their success in capturing complex distributions, existing generative
models face three challenges when used for design problems: 1) generated
designs have limited design space coverage, 2) the generator ignores design
performance, and 3)~the new parameterization is unable to represent designs
beyond training data. To address these challenges, we propose MO-PaDGAN, which
adds a Determinantal Point Processes based loss function to the generative
adversarial network to simultaneously model diversity and (multi-variate)
performance. MO-PaDGAN can thus improve the performances and coverage of
generated designs, and even generate designs with performances exceeding those
from training data. When using MO-PaDGAN as a new parameterization in
multi-objective optimization, we can discover much better Pareto fronts even
though the training data do not cover those Pareto fronts. In a real-world
multi-objective airfoil design example, we demonstrate that MO-PaDGAN achieves,
on average, an over 180\% improvement in the hypervolume indicator when
compared to the vanilla GAN or other state-of-the-art parameterization methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">No Size Fits All: Automated Radio Configuration for LPWANs. (arXiv:2109.05103v1 [cs.NI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05103">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Low power long-range networks like LoRa have become increasingly mainstream
for Internet of Things deployments. Given the versatility of applications that
these protocols enable, they support many data rates and bandwidths. Yet, for a
given network that supports hundreds of devices over multiple miles, the
network operator typically needs to specify the same configuration or among a
small subset of configurations for all the client devices to communicate with
the gateway. This one-size-fits-all approach is highly inefficient in large
networks. We propose an alternative approach -- we allow network devices to
transmit at any data rate they choose. The gateway uses the first few symbols
in the preamble to classify the correct data rate, switches its configuration,
and then decodes the data. Our design leverages the inherent asymmetry in
outdoor IoT deployments where the clients are power-starved and
resource-constrained, but the gateway is not. Our gateway design, Proteus, runs
a neural network architecture and is backward compatible with existing LoRa
protocols. Our experiments reveal that Proteus can identify the correct
configuration with over 97% accuracy in both indoor and outdoor deployments.
Our network architecture leads to a 3.8 to 11 times increase in throughput for
our LoRa testbed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physics-based Deep Learning. (arXiv:2109.05237v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05237">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This digital book contains a practical and comprehensive introduction of
everything related to deep learning in the context of physical simulations. As
much as possible, all topics come with hands-on code examples in the form of
Jupyter notebooks to quickly get started. Beyond standard supervised learning
from data, we&#x27;ll look at physical loss constraints, more tightly coupled
learning algorithms with differentiable simulations, as well as reinforcement
learning and uncertainty modeling. We live in exciting times: these methods
have a huge potential to fundamentally change what computer simulations can
achieve.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MLReal: Bridging the gap between training on synthetic data and real data applications in machine learning. (arXiv:2109.05294v1 [physics.geo-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05294">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Among the biggest challenges we face in utilizing neural networks trained on
waveform data (i.e., seismic, electromagnetic, or ultrasound) is its
application to real data. The requirement for accurate labels forces us to
develop solutions using synthetic data, where labels are readily available.
However, synthetic data often do not capture the reality of the field/real
experiment, and we end up with poor performance of the trained neural network
(NN) at the inference stage. We describe a novel approach to enhance supervised
training on synthetic data with real data features (domain adaptation).
Specifically, for tasks in which the absolute values of the vertical axis (time
or depth) of the input data are not crucial, like classification, or can be
corrected afterward, like velocity model building using a well-log, we suggest
a series of linear operations on the input so the training and application data
have similar distributions. This is accomplished by applying two operations on
the input data to the NN model: 1) The crosscorrelation of the input data
(i.e., shot gather, seismic image, etc.) with a fixed reference trace from the
same dataset. 2) The convolution of the resulting data with the mean (or a
random sample) of the autocorrelated data from another domain. In the training
stage, the input data are from the synthetic domain and the auto-correlated
data are from the real domain, and random samples from real data are drawn at
every training epoch. In the inference/application stage, the input data are
from the real subset domain and the mean of the autocorrelated sections are
from the synthetic data subset domain. Example applications on passive seismic
data for microseismic event source location determination and active seismic
data for predicting low frequencies are used to demonstrate the power of this
approach in improving the applicability of trained models to real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Near Instance Optimal Model Selection for Pure Exploration Linear Bandits. (arXiv:2109.05131v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05131">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The model selection problem in the pure exploration linear bandit setting is
introduced and studied in both the fixed confidence and fixed budget settings.
The model selection problem considers a nested sequence of hypothesis classes
of increasing complexities. Our goal is to automatically adapt to the
instance-dependent complexity measure of the smallest hypothesis class
containing the true model, rather than suffering from the complexity measure
related to the largest hypothesis class. We provide evidence showing that a
standard doubling trick over dimension fails to achieve the optimal
instance-dependent sample complexity. Our algorithms define a new optimization
problem based on experimental design that leverages the geometry of the action
set to efficiently identify a near-optimal hypothesis class. Our fixed budget
algorithm uses a novel application of a selection-validation trick in bandits.
This provides a new method for the understudied fixed budget setting in linear
bandits (even without the added challenge of model selection). We further
generalize the model selection problem to the misspecified regime, adapting our
algorithms in both fixed confidence and fixed budget settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Explainable Deep Learning: A Field Guide for the Uninitiated. (arXiv:2004.14545v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14545">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks (DNNs) have become a proven and indispensable machine
learning tool. As a black-box model, it remains difficult to diagnose what
aspects of the model&#x27;s input drive the decisions of a DNN. In countless
real-world domains, from legislation and law enforcement to healthcare, such
diagnosis is essential to ensure that DNN decisions are driven by aspects
appropriate in the context of its use. The development of methods and studies
enabling the explanation of a DNN&#x27;s decisions has thus blossomed into an
active, broad area of research. A practitioner wanting to study explainable
deep learning may be intimidated by the plethora of orthogonal directions the
field has taken. This complexity is further exacerbated by competing
definitions of what it means &#x60;&#x60;to explain&#x27;&#x27; the actions of a DNN and to
evaluate an approach&#x27;s &#x60;&#x60;ability to explain&#x27;&#x27;. This article offers a field
guide to explore the space of explainable deep learning aimed at those
uninitiated in the field. The field guide: i) Introduces three simple
dimensions defining the space of foundational methods that contribute to
explainable deep learning, ii) discusses the evaluations for model
explanations, iii) places explainability in the context of other related deep
learning research areas, and iv) finally elaborates on user-oriented
explanation designing and potential future directions on explainable deep
learning. We hope the guide is used as an easy-to-digest starting point for
those just embarking on research in this field.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visualization of Labeled Mixed-featured Datasets. (arXiv:1904.06366v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.06366">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We develop methodology for visualization of labeled mixed-featured datasets.
We first investigate datasets with continuous features where our Max-Ratio
Projection (MRP) method utilizes the group information in high dimensions to
provide distinctive lower-dimensional projections that are then displayed using
Radviz3D. Our methodology is extended to datasets with discrete and continuous
features where a Gaussianized distributional transform is used in conjunction
with copula models before applying MRP and visualizing the result using
RadViz3D. A R package $radviz3d$ implementing our complete methodology is
available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improved Analysis of the Tsallis-INF Algorithm in Stochastically Constrained Adversarial Bandits and Stochastic Bandits with Adversarial Corruptions. (arXiv:2103.12487v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12487">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We derive improved regret bounds for the Tsallis-INF algorithm of Zimmert and
Seldin (2021). We show that in adversarial regimes with a $(\Delta,C,T)$
self-bounding constraint the algorithm achieves
$\mathcal{O}\left(\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{\left(\sum_{i\neq i^*}
\frac{1}{\Delta_i}\right)^2}\right)+\sqrt{C\left(\sum_{i\neq
i^*}\frac{1}{\Delta_i}\right)\log_+\left(\frac{(K-1)T}{C\sum_{i\neq
i^*}\frac{1}{\Delta_i}}\right)}\right)$ regret bound, where $T$ is the time
horizon, $K$ is the number of arms, $\Delta_i$ are the suboptimality gaps,
$i^*$ is the best arm, $C$ is the corruption magnitude, and $\log_+(x) &#x3D;
\max\left(1,\log x\right)$. The regime includes stochastic bandits,
stochastically constrained adversarial bandits, and stochastic bandits with
adversarial corruptions as special cases. Additionally, we provide a general
analysis, which allows to achieve the same kind of improvement for
generalizations of Tsallis-INF to other settings beyond multiarmed bandits.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Ensemble Model-based Reinforcement Learning. (arXiv:2109.05549v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05549">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative training among geographically distributed and
heterogeneous users without gathering their data. Extending FL beyond the
conventional supervised learning paradigm, federated Reinforcement Learning
(RL) was proposed to handle sequential decision-making problems for various
privacy-sensitive applications such as autonomous driving. However, the
existing federated RL algorithms directly combine model-free RL with FL, and
thus generally have high sample complexity and lack theoretical guarantees. To
address the above challenges, we propose a new federated RL algorithm that
incorporates model-based RL and ensemble knowledge distillation into FL.
Specifically, we utilise FL and knowledge distillation to create an ensemble of
dynamics models from clients, and then train the policy by solely using the
ensemble model without interacting with the real environment. Furthermore, we
theoretically prove that the monotonic improvement of the proposed algorithm is
guaranteed. Extensive experimental results demonstrate that our algorithm
obtains significantly higher sample efficiency compared to federated model-free
RL algorithms in the challenging continuous control benchmark environments. The
results also show the impact of non-IID client data and local update steps on
the performance of federated RL, validating the insights obtained from our
theoretical analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03416">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We combine recent advancements in end-to-end speech recognition to
non-autoregressive automatic speech recognition. We push the limits of
non-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,
Fisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC
on giant Conformer neural network architectures with SpecAugment and wav2vec2
pre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,
5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without
a language model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Temporal Dynamics from Cycles in Narrated Video. (arXiv:2101.02337v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02337">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning to model how the world changes as time elapses has proven a
challenging problem for the computer vision community. We propose a
self-supervised solution to this problem using temporal cycle consistency
jointly in vision and language, training on narrated video. Our model learns
modality-agnostic functions to predict forward and backward in time, which must
undo each other when composed. This constraint leads to the discovery of
high-level transitions between moments in time, since such transitions are
easily inverted and shared across modalities. We justify the design of our
model with an ablation study on different configurations of the cycle
consistency problem. We then show qualitatively and quantitatively that our
approach yields a meaningful, high-level model of the future and past. We apply
the learned dynamics model without further training to various tasks, such as
predicting future action and temporally ordering sets of images. Project page:
https://dave.ml/mmcc</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v4 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09780">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging
exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread,
and uses less radiation. Here, we demonstrate the impact of lung segmentation
in COVID-19 identification using CXR images and evaluate which contents of the
image influenced the most. Semantic segmentation was performed using a U-Net
CNN architecture, and the classification using three CNN architectures (VGG,
ResNet, and Inception). Explainable Artificial Intelligence techniques were
employed to estimate the impact of segmentation. A three-classes database was
composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the
impact of creating a CXR image database from different sources, and the
COVID-19 generalization from one source to another. The segmentation achieved a
Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification
using segmented images achieved an F1-Score of 0.88 for the multi-class setup,
and 0.83 for COVID-19 identification. In the cross-dataset scenario, we
obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for
COVID-19 identification using segmented images. Experiments support the
conclusion that even after segmentation, there is a strong bias introduced by
underlying factors from different sources.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lenient Regret for Multi-Armed Bandits. (arXiv:2008.03959v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.03959">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the Multi-Armed Bandit (MAB) problem, where an agent sequentially
chooses actions and observes rewards for the actions it took. While the
majority of algorithms try to minimize the regret, i.e., the cumulative
difference between the reward of the best action and the agent&#x27;s action, this
criterion might lead to undesirable results. For example, in large problems, or
when the interaction with the environment is brief, finding an optimal arm is
infeasible, and regret-minimizing algorithms tend to over-explore. To overcome
this issue, algorithms for such settings should instead focus on playing
near-optimal arms. To this end, we suggest a new, more lenient, regret
criterion that ignores suboptimality gaps smaller than some $\epsilon$. We then
present a variant of the Thompson Sampling (TS) algorithm, called
$\epsilon$-TS, and prove its asymptotic optimality in terms of the lenient
regret. Importantly, we show that when the mean of the optimal arm is high
enough, the lenient regret of $\epsilon$-TS is bounded by a constant. Finally,
we show that $\epsilon$-TS can be applied to improve the performance when the
agent knows a lower bound of the suboptimality gaps.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AstronomicAL: An interactive dashboard for visualisation, integration and classification of data using Active Learning. (arXiv:2109.05207v1 [astro-ph.IM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05207">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>AstronomicAL is a human-in-the-loop interactive labelling and training
dashboard that allows users to create reliable datasets and robust classifiers
using active learning. This technique prioritises data that offer high
information gain, leading to improved performance using substantially less
data. The system allows users to visualise and integrate data from different
sources and deal with incorrect or missing labels and imbalanced class sizes.
AstronomicAL enables experts to visualise domain-specific plots and key
information relating both to broader context and details of a point of interest
drawn from a variety of data sources, ensuring reliable labels. In addition,
AstronomicAL provides functionality to explore all aspects of the training
process, including custom models and query strategies. This makes the software
a tool for experimenting with both domain-specific classifications and more
general-purpose machine learning strategies. We illustrate using the system
with an astronomical dataset due to the field&#x27;s immediate need; however,
AstronomicAL has been designed for datasets from any discipline. Finally, by
exporting a simple configuration file, entire layouts, models, and assigned
labels can be shared with the community. This allows for complete transparency
and ensures that the process of reproducing results is effortless</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction. (arXiv:1908.03918v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.03918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dynamical models estimate and predict the temporal evolution of physical
systems. State Space Models (SSMs) in particular represent the system dynamics
with many desirable properties, such as being able to model uncertainty in both
the model and measurements, and optimal (in the Bayesian sense) recursive
formulations e.g. the Kalman Filter. However, they require significant domain
knowledge to derive the parametric form and considerable hand-tuning to
correctly set all the parameters. Data driven techniques e.g. Recurrent Neural
Networks have emerged as compelling alternatives to SSMs with wide success
across a number of challenging tasks, in part due to their ability to extract
relevant features from rich inputs. They however lack interpretability and
robustness to unseen conditions. In this work, we present DynaNet, a hybrid
deep learning and time-varying state-space model which can be trained
end-to-end. Our neural Kalman dynamical model allows us to exploit the relative
merits of each approach. We demonstrate state-of-the-art estimation and
prediction on a number of physically challenging tasks, including visual
odometry, sensor fusion for visual-inertial navigation and pendulum control. In
addition we show how DynaNet can indicate failures through investigation of
properties such as the rate of innovation (Kalman Gain).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Facial Anatomical Landmark Detection using Regularized Transfer Learning with Application to Fetal Alcohol Syndrome Recognition. (arXiv:2109.05485v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05485">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Fetal alcohol syndrome (FAS) caused by prenatal alcohol exposure can result
in a series of cranio-facial anomalies, and behavioral and neurocognitive
problems. Current diagnosis of FAS is typically done by identifying a set of
facial characteristics, which are often obtained by manual examination.
Anatomical landmark detection, which provides rich geometric information, is
important to detect the presence of FAS associated facial anomalies. This
imaging application is characterized by large variations in data appearance and
limited availability of labeled data. Current deep learning-based heatmap
regression methods designed for facial landmark detection in natural images
assume availability of large datasets and are therefore not wellsuited for this
application. To address this restriction, we develop a new regularized transfer
learning approach that exploits the knowledge of a network learned on large
facial recognition datasets. In contrast to standard transfer learning which
focuses on adjusting the pre-trained weights, the proposed learning approach
regularizes the model behavior. It explicitly reuses the rich visual semantics
of a domain-similar source model on the target task data as an additional
supervisory signal for regularizing landmark detection optimization.
Specifically, we develop four regularization constraints for the proposed
transfer learning, including constraining the feature outputs from
classification and intermediate layers, as well as matching activation
attention maps in both spatial and channel levels. Experimental evaluation on a
collected clinical imaging dataset demonstrate that the proposed approach can
effectively improve model generalizability under limited training samples, and
is advantageous to other approaches in the literature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generating Datasets of 3D Garments with Sewing Patterns. (arXiv:2109.05633v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05633">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Garments are ubiquitous in both real and many of the virtual worlds. They are
highly deformable objects, exhibit an immense variety of designs and shapes,
and yet, most garments are created from a set of regularly shaped flat pieces.
Exploration of garment structure presents a peculiar case for an object
structure estimation task and might prove useful for downstream tasks of neural
3D garment modeling and reconstruction by providing strong prior on garment
shapes. To facilitate research in these directions, we propose a method for
generating large synthetic datasets of 3D garment designs and their sewing
patterns. Our method consists of a flexible description structure for
specifying parametric sewing pattern templates and the automatic generation
pipeline to produce garment 3D models with little-to-none manual intervention.
To add realism, the pipeline additionally creates corrupted versions of the
final meshes that imitate artifacts of 3D scanning.

With this pipeline, we created the first large-scale synthetic dataset of 3D
garment models with their sewing patterns. The dataset contains more than 20000
garment design variations produced from 19 different base types. Seven of these
garment types are specifically designed to target evaluation of the
generalization across garment sewing pattern topologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LEA-Net: Layer-wise External Attention Network for Efficient Color Anomaly Detection. (arXiv:2109.05493v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05493">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The utilization of prior knowledge about anomalies is an essential issue for
anomaly detections. Recently, the visual attention mechanism has become a
promising way to improve the performance of CNNs for some computer vision
tasks. In this paper, we propose a novel model called Layer-wise External
Attention Network (LEA-Net) for efficient image anomaly detection. The core
idea relies on the integration of unsupervised and supervised anomaly detectors
via the visual attention mechanism. Our strategy is as follows: (i) Prior
knowledge about anomalies is represented as the anomaly map generated by
unsupervised learning of normal instances, (ii) The anomaly map is translated
to an attention map by the external network, (iii) The attention map is then
incorporated into intermediate layers of the anomaly detection network.
Notably, this layer-wise external attention can be applied to any CNN model in
an end-to-end training manner. For a pilot study, we validate LEA-Net on color
anomaly detection tasks. Through extensive experiments on PlantVillage, MVTec
AD, and Cloud datasets, we demonstrate that the proposed layer-wise visual
attention mechanism consistently boosts anomaly detection performances of an
existing CNN model, even on imbalanced datasets. Moreover, we show that our
attention mechanism successfully boosts the performance of several CNN models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regularized Orthogonal Machine Learning for Nonlinear Semiparametric Models. (arXiv:1806.04823v8 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1806.04823">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper proposes a Lasso-type estimator for a high-dimensional sparse
parameter identified by a single index conditional moment restriction (CMR). In
addition to this parameter, the moment function can also depend on a nuisance
function, such as the propensity score or the conditional choice probability,
which we estimate by modern machine learning tools. We first adjust the moment
function so that the gradient of the future loss function is insensitive
(formally, Neyman-orthogonal) with respect to the first-stage regularization
bias, preserving the single index property. We then take the loss function to
be an indefinite integral of the adjusted moment function with respect to the
single index. The proposed Lasso estimator converges at the oracle rate, where
the oracle knows the nuisance function and solves only the parametric problem.
We demonstrate our method by estimating the short-term heterogeneous impact of
Connecticut&#x27;s Jobs First welfare reform experiment on women&#x27;s welfare
participation decision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constrained Ensemble Langevin Monte Carlo. (arXiv:2102.04279v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04279">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The classical Langevin Monte Carlo method looks for samples from a target
distribution by descending the samples along the gradient of the target
distribution. The method enjoys a fast convergence rate. However, the numerical
cost is sometimes high because each iteration requires the computation of a
gradient. One approach to eliminate the gradient computation is to employ the
concept of &quot;ensemble&quot;. A large number of particles are evolved together so the
neighboring particles provide gradient information to each other. In this
article, we discuss two algorithms that integrate the ensemble feature into LMC
and the associated properties.

In particular, we find that if one directly surrogates the gradient using the
ensemble approximation, the algorithm, termed Ensemble Langevin Monte Carlo, is
unstable due to a high variance term. If the gradients are replaced by the
ensemble approximations only in a constrained manner, to protect from the
unstable points, the algorithm, termed Constrained Ensemble Langevin Monte
Carlo, resembles the classical LMC up to an ensemble error but removes most of
the gradient computation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Modified AUC for Training Convolutional Neural Networks: Taking Confidence into Account. (arXiv:2006.04836v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.04836">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Receiver operating characteristic (ROC) curve is an informative tool in
binary classification and Area Under ROC Curve (AUC) is a popular metric for
reporting performance of binary classifiers. In this paper, first we present a
comprehensive review of ROC curve and AUC metric. Next, we propose a modified
version of AUC that takes confidence of the model into account and at the same
time, incorporates AUC into Binary Cross Entropy (BCE) loss used for training a
Convolutional neural Network for classification tasks. We demonstrate this on
three datasets: MNIST, prostate MRI, and brain MRI. Furthermore, we have
published GenuineAI, a new python library, which provides the functions for
conventional AUC and the proposed modified AUC along with metrics including
sensitivity, specificity, recall, precision, and F1 for each point of the ROC
curve.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RVMDE: Radar Validated Monocular Depth Estimation for Robotics. (arXiv:2109.05265v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Stereoscopy exposits a natural perception of distance in a scene, and its
manifestation in 3D world understanding is an intuitive phenomenon. However, an
innate rigid calibration of binocular vision sensors is crucial for accurate
depth estimation. Alternatively, a monocular camera alleviates the limitation
at the expense of accuracy in estimating depth, and the challenge exacerbates
in harsh environmental conditions. Moreover, an optical sensor often fails to
acquire vital signals in harsh environments, and radar is used instead, which
gives coarse but more accurate signals. This work explores the utility of
coarse signals from radar when fused with fine-grained data from a monocular
camera for depth estimation in harsh environmental conditions. A variant of
feature pyramid network (FPN) extensively operates on fine-grained image
features at multiple scales with a fewer number of parameters. FPN feature maps
are fused with sparse radar features extracted with a Convolutional neural
network. The concatenated hierarchical features are used to predict the depth
with ordinal regression. We performed experiments on the nuScenes dataset, and
the proposed architecture stays on top in quantitative evaluations with reduced
parameters and faster inference. The depth estimation results suggest that the
proposed techniques can be used as an alternative to stereo depth estimation in
critical applications in robotics and self-driving cars. The source code will
be available in the following: \url{https://github.com/MI-Hussain/RVMDE}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attention Augmented ConvLSTM for Environment Prediction. (arXiv:2010.09662v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09662">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Safe and proactive planning in robotic systems generally requires accurate
predictions of the environment. Prior work on environment prediction applied
video frame prediction techniques to bird&#x27;s-eye view environment
representations, such as occupancy grids. ConvLSTM-based frameworks used
previously often result in significant blurring and vanishing of moving
objects, thus hindering their applicability for use in safety-critical
applications. In this work, we propose two extensions to the ConvLSTM to
address these issues. We present the Temporal Attention Augmented ConvLSTM
(TAAConvLSTM) and Self-Attention Augmented ConvLSTM (SAAConvLSTM) frameworks
for spatiotemporal occupancy prediction, and demonstrate improved performance
over baseline architectures on the real-world KITTI and Waymo datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single-Read Reconstruction for DNA Data Storage Using Transformers. (arXiv:2109.05478v1 [cs.ET])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05478">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As the global need for large-scale data storage is rising exponentially,
existing storage technologies are approaching their theoretical and functional
limits in terms of density and energy consumption, making DNA based storage a
potential solution for the future of data storage. Several studies introduced
DNA based storage systems with high information density (petabytes/gram).
However, DNA synthesis and sequencing technologies yield erroneous outputs.
Algorithmic approaches for correcting these errors depend on reading multiple
copies of each sequence and result in excessive reading costs. The
unprecedented success of Transformers as a deep learning architecture for
language modeling has led to its repurposing for solving a variety of tasks
across various domains. In this work, we propose a novel approach for
single-read reconstruction using an encoder-decoder Transformer architecture
for DNA based data storage. We address the error correction process as a
self-supervised sequence-to-sequence task and use synthetic noise injection to
train the model using only the decoded reads. Our approach exploits the
inherent redundancy of each decoded file to learn its underlying structure. To
demonstrate our proposed approach, we encode text, image and code-script files
to DNA, produce errors with high-fidelity error simulator, and reconstruct the
original files from the noisy reads. Our model achieves lower error rates when
reconstructing the original data from a single read of each DNA strand compared
to state-of-the-art algorithms using 2-3 copies. This is the first
demonstration of using deep learning models for single-read reconstruction in
DNA based storage which allows for the reduction of the overall cost of the
process. We show that this approach is applicable for various domains and can
be generalized to new domains as well.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generalized Second Order Value Iteration in Markov Decision Processes. (arXiv:1905.03927v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.03927">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Value iteration is a fixed point iteration technique utilized to obtain the
optimal value function and policy in a discounted reward Markov Decision
Process (MDP). Here, a contraction operator is constructed and applied
repeatedly to arrive at the optimal solution. Value iteration is a first order
method and therefore it may take a large number of iterations to converge to
the optimal solution. Successive relaxation is a popular technique that can be
applied to solve a fixed point equation. It has been shown in the literature
that, under a special structure of the MDP, successive over-relaxation
technique computes the optimal value function faster than standard value
iteration. In this work, we propose a second order value iteration procedure
that is obtained by applying the Newton-Raphson method to the successive
relaxation value iteration scheme. We prove the global convergence of our
algorithm to the optimal solution asymptotically and show the second order
convergence. Through experiments, we demonstrate the effectiveness of our
proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00049">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language representations are known to carry stereotypical biases and, as a
result, lead to biased predictions in downstream tasks. While existing methods
are effective at mitigating biases by linear projection, such methods are too
aggressive: they not only remove bias, but also erase valuable information from
word embeddings. We develop new measures for evaluating specific information
retention that demonstrate the tradeoff between bias removal and information
retention. To address this challenge, we propose OSCaR (Orthogonal Subspace
Correction and Rectification), a bias-mitigating method that focuses on
disentangling biased associations between concepts instead of removing concepts
wholesale. Our experiments on gender biases show that OSCaR is a well-balanced
approach that ensures that semantic information is retained in the embeddings
and bias is also effectively mitigated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Generation Method for Learning a Low-dimensional Safe Region in Safe Reinforcement Learning. (arXiv:2109.05077v1 [eess.SY])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05077">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Safe reinforcement learning aims to learn a control policy while ensuring
that neither the system nor the environment gets damaged during the learning
process. For implementing safe reinforcement learning on highly nonlinear and
high-dimensional dynamical systems, one possible approach is to find a
low-dimensional safe region via data-driven feature extraction methods, which
provides safety estimates to the learning algorithm. As the reliability of the
learned safety estimates is data-dependent, we investigate in this work how
different training data will affect the safe reinforcement learning approach.
By balancing between the learning performance and the risk of being unsafe, a
data generation method that combines two sampling methods is proposed to
generate representative training data. The performance of the method is
demonstrated with a three-link inverted pendulum example.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Neural Network Subspaces. (arXiv:2102.10472v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10472">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent observations have advanced our understanding of the neural network
optimization landscape, revealing the existence of (1) paths of high accuracy
containing diverse solutions and (2) wider minima offering improved
performance. Previous methods observing diverse paths require multiple training
runs. In contrast we aim to leverage both property (1) and (2) with a single
method and in a single training run. With a similar computational cost as
training one model, we learn lines, curves, and simplexes of high-accuracy
neural networks. These neural network subspaces contain diverse solutions that
can be ensembled, approaching the ensemble performance of independently trained
networks without the training cost. Moreover, using the subspace midpoint
boosts accuracy, calibration, and robustness to label noise, outperforming
Stochastic Weight Averaging.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards a variational Jordan-Lee-Preskill quantum algorithm. (arXiv:2109.05547v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05547">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Rapid developments of quantum information technology show promising
opportunities for simulating quantum field theory in near-term quantum devices.
In this work, we formulate the theory of (time-dependent) variational quantum
simulation, explicitly designed for quantum simulation of quantum field theory.
We develop hybrid quantum-classical algorithms for crucial ingredients in
particle scattering experiments, including encoding, state preparation, and
time evolution, with several numerical simulations to demonstrate our
algorithms in the 1+1 dimensional $\lambda \phi^4$ quantum field theory. These
algorithms could be understood as near-term analogs of the Jordan-Lee-Preskill
algorithm, the basic algorithm for simulating quantum field theory using
universal quantum devices. Our contribution also includes a bosonic version of
the Unitary Coupled Cluster ansatz with physical interpretation in quantum
field theory, a discussion about the subspace fidelity, a comparison among
different bases in the 1+1 dimensional $\lambda \phi^4$ theory, and the
&quot;spectral crowding&quot; in the quantum field theory simulation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimation of Local Average Treatment Effect by Data Combination. (arXiv:2109.05175v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05175">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is important to estimate the local average treatment effect (LATE) when
compliance with a treatment assignment is incomplete. The previously proposed
methods for LATE estimation required all relevant variables to be jointly
observed in a single dataset; however, it is sometimes difficult or even
impossible to collect such data in many real-world problems for technical or
privacy reasons. We consider a novel problem setting in which LATE, as a
function of covariates, is nonparametrically identified from the combination of
separately observed datasets. For estimation, we show that the direct least
squares method, which was originally developed for estimating the average
treatment effect under complete compliance, is applicable to our setting.
However, model selection and hyperparameter tuning for the direct least squares
estimator can be unstable in practice since it is defined as a solution to the
minimax problem. We then propose a weighted least squares estimator that
enables simpler model selection by avoiding the minimax objective formulation.
Unlike the inverse probability weighted (IPW) estimator, the proposed estimator
directly uses the pre-estimated weight without inversion, avoiding the problems
caused by the IPW methods. We demonstrate the effectiveness of our method
through experiments using synthetic and real-world datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Handwritten Mathematical Terms with Sensor Based Data. (arXiv:2109.05594v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05594">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work we propose a solution to the UbiComp 2021 Challenge by Stabilo
in which handwritten mathematical terms are supposed to be automatically
classified based on time series sensor data captured on the DigiPen. The input
data set contains data of different writers, with label strings constructed
from a total of 15 different possible characters. The label should first be
split into separate characters to classify them one by one. This issue is
solved by applying a data-dependant and rule-based information extraction
algorithm to the labeled data. Using the resulting data, two classifiers are
constructed. The first is a binary classifier that is able to predict, for
unknown data, if a sample is part of a writing activity, and consists of a Deep
Neural Network feature extractor in concatenation with a Random Forest that is
trained to classify the extracted features at an F1 score of &gt;90%. The second
classifier is a Deep Neural Network that combines convolution layers with
recurrent layers to predict windows with a single label, out of the 15 possible
classes, at an F1 score of &gt;60%. A simulation of the challenge evaluation
procedure reports a Levensthein Distance of 8 and shows that the chosen
approach still lacks in overall accuracy and real-time applicability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bilevel Programs Meet Deep Learning: A Unifying View on Inference Learning Methods. (arXiv:2105.07231v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07231">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work we unify a number of inference learning methods, that are
proposed in the literature as alternative training algorithms to the ones based
on regular error back-propagation. These inference learning methods were
developed with very diverse motivations, mainly aiming to enhance the
biological plausibility of deep neural networks and to improve the intrinsic
parallelism of training methods. We show that these superficially very
different methods can all be obtained by successively applying a particular
reformulation of bilevel optimization programs. As a by-product it becomes also
evident that all considered inference learning methods include back-propagation
as a special case, and therefore at least approximate error back-propagation in
typical settings. Finally, we propose Fenchel back-propagation, that replaces
the propagation of infinitesimal corrections performed in standard
back-propagation with finite targets as the learning signal. Fenchel
back-propagation can therefore be seen as an instance of learning via explicit
target propagation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DRo: A data-scarce mechanism to revolutionize the performance of Deep Learning based Security Systems. (arXiv:2109.05470v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05470">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Supervised Deep Learning requires plenty of labeled data to converge, and
hence perform optimally for task-specific learning. Therefore, we propose a
novel mechanism named DRo (for Deep Routing) for data-scarce domains like
security. The DRo approach builds upon some of the recent developments in
Deep-Clustering. In particular, it exploits the self-augmented training
mechanism using synthetically generated local perturbations. DRo not only
allays the challenges with sparse-labeled data but also offers many unique
advantages. We also developed a system named DRoID that uses the DRo mechanism
for enhancing the performance of an existing Malware Detection System that uses
(low information features like the) Android implicit Intent(s) as the only
features. We conduct experiments on DRoID using a popular and standardized
Android malware dataset and found that the DRo mechanism could successfully
reduce the false-alarms generated by the downstream classifier by 67.9%, and
also simultaneously boosts its accuracy by 11.3%. This is significant not only
because the gains achieved are unparalleled but also because the features used
were never considered rich enough to train a classifier on; and hence no decent
performance could ever be reported by any malware classification system
till-date using these features in isolation. Owing to the results achieved, the
DRo mechanism claims a dominant position amongst all known systems that aims to
enhance the classification performance of deep learning models with
sparse-labeled data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">2-in-1 Accelerator: Enabling Random Precision Switch for Winning Both Adversarial Robustness and Efficiency. (arXiv:2109.05223v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05223">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The recent breakthroughs of deep neural networks (DNNs) and the advent of
billions of Internet of Things (IoT) devices have excited an explosive demand
for intelligent IoT devices equipped with domain-specific DNN accelerators.
However, the deployment of DNN accelerator enabled intelligent functionality
into real-world IoT devices still remains particularly challenging. First,
powerful DNNs often come at prohibitive complexities, whereas IoT devices often
suffer from stringent resource constraints. Second, while DNNs are vulnerable
to adversarial attacks especially on IoT devices exposed to complex real-world
environments, many IoT applications require strict security. Existing DNN
accelerators mostly tackle only one of the two aforementioned challenges (i.e.,
efficiency or adversarial robustness) while neglecting or even sacrificing the
other. To this end, we propose a 2-in-1 Accelerator, an integrated
algorithm-accelerator co-design framework aiming at winning both the
adversarial robustness and efficiency of DNN accelerators. Specifically, we
first propose a Random Precision Switch (RPS) algorithm that can effectively
defend DNNs against adversarial attacks by enabling random DNN quantization as
an in-situ model switch. Furthermore, we propose a new precision-scalable
accelerator featuring (1) a new precision-scalable MAC unit architecture which
spatially tiles the temporal MAC units to boost both the achievable efficiency
and flexibility and (2) a systematically optimized dataflow that is searched by
our generic accelerator optimizer. Extensive experiments and ablation studies
validate that our 2-in-1 Accelerator can not only aggressively boost both the
adversarial robustness and efficiency of DNN accelerators under various
attacks, but also naturally support instantaneous robustness-efficiency
trade-offs adapting to varied resources without the necessity of DNN
retraining.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Feature Importance in Gradient Boosting Trees with Cross-Validation Feature Selection. (arXiv:2109.05468v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Gradient Boosting Machines (GBM) are among the go-to algorithms on tabular
data, which produce state of the art results in many prediction tasks. Despite
its popularity, the GBM framework suffers from a fundamental flaw in its base
learners. Specifically, most implementations utilize decision trees that are
typically biased towards categorical variables with large cardinalities. The
effect of this bias was extensively studied over the years, mostly in terms of
predictive performance. In this work, we extend the scope and study the effect
of biased base learners on GBM feature importance (FI) measures. We show that
although these implementation demonstrate highly competitive predictive
performance, they still, surprisingly, suffer from bias in FI. By utilizing
cross-validated (CV) unbiased base learners, we fix this flaw at a relatively
low computational cost. We demonstrate the suggested framework in a variety of
synthetic and real-world setups, showing a significant improvement in all GBM
FI measures while maintaining relatively the same level of prediction accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fundamental limits of over-the-air optimization: Are analog schemes optimal?. (arXiv:2109.05222v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05222">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider over-the-air convex optimization on a d dimensional space where
coded gradients are sent over an additive Gaussian noise channel with variance
\sigma^2. The codewords satisfy an average power constraint P, resulting in the
signal-to-noise ratio (SNR) of P/\sigma^2. We derive bounds for the convergence
rates for over-the-air optimization. Our first result is a lower bound for the
convergence rate showing that any code must slowdown the convergence rate by a
factor of roughly \sqrt{d/log(1 + SNR)}. Next, we consider a popular class of
schemes called analog coding, where a linear function of the gradient is sent.
We show that a simple scaled transmission analog coding scheme results in a
slowdown in convergence rate by a factor of \sqrt{d(1 + 1/SNR)}. This matches
the previous lower bound up to constant factors for low SNR, making the scaled
transmission scheme optimal at low SNR. However, we show that this slowdown is
necessary for any analog coding scheme. In particular, a slowdown in
convergence by a factor of \sqrt{d} for analog coding remains even when SNR
tends to infinity. Remarkably, we present a simple quantize-and-modulate scheme
that uses Amplitude Shift Keying and almost attains the optimal convergence
rate at all SNRs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EMVLight: A Decentralized Reinforcement Learning Framework for EfficientPassage of Emergency Vehicles. (arXiv:2109.05429v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05429">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Emergency vehicles (EMVs) play a crucial role in responding to time-critical
events such as medical emergencies and fire outbreaks in an urban area. The
less time EMVs spend traveling through the traffic, the more likely it would
help save people&#x27;s lives and reduce property loss. To reduce the travel time of
EMVs, prior work has used route optimization based on historical traffic-flow
data and traffic signal pre-emption based on the optimal route. However,
traffic signal pre-emption dynamically changes the traffic flow which, in turn,
modifies the optimal route of an EMV. In addition, traffic signal pre-emption
practices usually lead to significant disturbances in traffic flow and
subsequently increase the travel time for non-EMVs. In this paper, we propose
EMVLight, a decentralized reinforcement learning (RL) framework for
simultaneous dynamic routing and traffic signal control. EMVLight extends
Dijkstra&#x27;s algorithm to efficiently update the optimal route for the EMVs in
real time as it travels through the traffic network. The decentralized RL
agents learn network-level cooperative traffic signal phase strategies that not
only reduce EMV travel time but also reduce the average travel time of non-EMVs
in the network. This benefit has been demonstrated through comprehensive
experiments with synthetic and real-world maps. These experiments show that
EMVLight outperforms benchmark transportation engineering techniques and
existing RL-based signal control methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Internet of Things (IoT) Based Video Analytics: a use case of Smart Doorbell. (arXiv:2105.06508v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06508">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The vision of the internet of things (IoT) is a reality now. IoT devices are
getting cheaper, smaller. They are becoming more and more computationally and
energy-efficient. The global market of IoT-based video analytics has seen
significant growth in recent years and it is expected to be a growing market
segment. For any IoT-based video analytics application, few key points
required, such as cost-effectiveness, widespread use, flexible design, accurate
scene detection, reusability of the framework. Video-based smart doorbell
system is one such application domain for video analytics where many commercial
offerings are available in the consumer market. However, such existing
offerings are costly, monolithic, and proprietary. Also, there will be a
trade-off between accuracy and portability. To address the foreseen problems,
I&#x27;m proposing a distributed framework for video analytics with a use case of a
smart doorbell system. The proposed framework uses AWS cloud services as a base
platform and to meet the price affordability constraint, the system was
implemented on affordable Raspberry Pi. The smart doorbell will be able to
recognize the known/unknown person with at most accuracy. The smart doorbell
system is also having additional detection functionalities such as harmful
weapon detection, noteworthy vehicle detection, animal/pet detection. An iOS
application is specifically developed for this implementation which can receive
the notification from the smart doorbell in real-time. Finally, the paper also
mentions the classical approaches for video analytics, their feasibility in
implementing with this use-case, and comparative analysis in terms of accuracy
and time required to detect an object in the frame is carried out. Results
conclude that AWS cloud-based approach is worthy for this smart doorbell use
case.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Space Meets Time: Local Spacetime Neural Network For Traffic Flow Forecasting. (arXiv:2109.05225v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05225">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Traffic flow forecasting is a crucial task in urban computing. The challenge
arises as traffic flows often exhibit intrinsic and latent spatio-temporal
correlations that cannot be identified by extracting the spatial and temporal
patterns of traffic data separately. We argue that such correlations are
universal and play a pivotal role in traffic flow. We put forward spacetime
interval learning as a paradigm to explicitly capture these correlations
through a unified analysis of both spatial and temporal features. Unlike the
state-of-the-art methods, which are restricted to a particular road network, we
model the universal spatio-temporal correlations that are transferable from
cities to cities. To this end, we propose a new spacetime interval learning
framework that constructs a local-spacetime context of a traffic sensor
comprising the data from its neighbors within close time points. Based on this
idea, we introduce spacetime neural network (STNN), which employs novel
spacetime convolution and attention mechanism to learn the universal
spatio-temporal correlations. The proposed STNN captures local traffic
patterns, which does not depend on a specific network structure. As a result, a
trained STNN model can be applied on any unseen traffic networks. We evaluate
the proposed STNN on two public real-world traffic datasets and a simulated
dataset on dynamic networks. The experiment results show that STNN not only
improves prediction accuracy by 15% over state-of-the-art methods, but is also
effective in handling the case when the traffic network undergoes dynamic
changes as well as the superior generalization capability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Conditional Generation of Synthetic Geospatial Images from Pixel-level and Feature-level Inputs. (arXiv:2109.05201v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Training robust supervised deep learning models for many geospatial
applications of computer vision is difficult due to dearth of class-balanced
and diverse training data. Conversely, obtaining enough training data for many
applications is financially prohibitive or may be infeasible, especially when
the application involves modeling rare or extreme events. Synthetically
generating data (and labels) using a generative model that can sample from a
target distribution and exploit the multi-scale nature of images can be an
inexpensive solution to address scarcity of labeled data. Towards this goal, we
present a deep conditional generative model, called VAE-Info-cGAN, that
combines a Variational Autoencoder (VAE) with a conditional Information
Maximizing Generative Adversarial Network (InfoGAN), for synthesizing
semantically rich images simultaneously conditioned on a pixel-level condition
(PLC) and a macroscopic feature-level condition (FLC). Dimensionally, the PLC
can only vary in the channel dimension from the synthesized image and is meant
to be a task-specific input. The FLC is modeled as an attribute vector in the
latent space of the generated image which controls the contributions of various
characteristic attributes germane to the target distribution. Experiments on a
GPS trajectories dataset show that the proposed model can accurately generate
various forms of spatiotemporal aggregates across different geographic
locations while conditioned only on a raster representation of the road
network. The primary intended application of the VAE-Info-cGAN is synthetic
data (and label) generation for targeted data augmentation for computer
vision-based modeling of problems relevant to geospatial analysis and remote
sensing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CupNet -- Pruning a network for geometric data. (arXiv:2005.05276v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.05276">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Using data from a simulated cup drawing process, we demonstrate how the
inherent geometrical structure of cup meshes can be used to effectively prune
an artificial neural network in a straightforward way.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets. (arXiv:1911.02728v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.02728">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>There has been huge interest in studying human brain connectomes inferred
from different imaging modalities and exploring their relationship with human
traits, such as cognition. Brain connectomes are usually represented as
networks, with nodes corresponding to different regions of interest (ROIs) and
edges to connection strengths between ROIs. Due to the high-dimensionality and
non-Euclidean nature of networks, it is challenging to depict their population
distribution and relate them to human traits. Current approaches focus on
summarizing the network using either pre-specified topological features or
principal components analysis (PCA). In this paper, building on recent advances
in deep learning, we develop a nonlinear latent factor model to characterize
the population distribution of brain graphs and infer the relationships between
brain structural connectomes and human traits. We refer to our method as Graph
AuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging
datasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human
Connectome Project (HCP) for adults, to understand the structural brain
connectome and its relationship with cognition. Numerical results demonstrate
huge advantages of GATE over competitors in terms of prediction accuracy,
statistical inference and computing efficiency. We found that structural
connectomes have a stronger association with a wide range of human cognitive
traits than was apparent using previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Max Entrywise Error Bounds for Sparse Tensor Estimation via Similarity Based Collaborative Filtering. (arXiv:1908.01241v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.01241">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Consider the task of estimating a 3-order $n \times n \times n$ tensor from
noisy observations of randomly chosen entries in the sparse regime. We
introduce a similarity based collaborative filtering algorithm for sparse
tensor estimation and argue that it achieves sample complexity that nearly
matches the conjectured computationally efficient lower bound on the sample
complexity for the setting of low-rank tensors. Our algorithm uses the matrix
obtained from the flattened tensor to compute similarity, and estimates the
tensor entries using a nearest neighbor estimator. We prove that the algorithm
recovers a low rank tensor with maximum entry-wise error (MEE) and
mean-squared-error (MSE) decaying to $0$ as long as each entry is observed
independently with probability $p &#x3D; \Omega(n^{-3/2 + \kappa})$ for any
arbitrarily small $\kappa &gt; 0$. % as long as tensor has finite rank $r &#x3D;
\Theta(1)$. More generally, we establish robustness of the estimator, showing
that when arbitrary noise bounded by $\epsilon \geq 0$ is added to each
observation, the estimation error with respect to MEE and MSE degrades by ${\sf
poly}(\epsilon)$. Consequently, even if the tensor may not have finite rank but
can be approximated within $\epsilon \geq 0$ by a finite rank tensor, then the
estimation error converges to ${\sf poly}(\epsilon)$. Our analysis sheds
insight into the conjectured sample complexity lower bound, showing that it
matches the connectivity threshold of the graph used by our algorithm for
estimating similarity between coordinates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Debiased Machine Learning of Set-Identified Linear Models. (arXiv:1712.10024v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1712.10024">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper provides estimation and inference methods for an identified set&#x27;s
boundary (i.e., support function) where the selection among a very large number
of covariates is based on modern regularized tools. I characterize the boundary
using a semiparametric moment equation. Combining Neyman-orthogonality and
sample splitting ideas, I construct a root-N consistent, uniformly
asymptotically Gaussian estimator of the boundary and propose a multiplier
bootstrap procedure to conduct inference. I apply this result to the partially
linear model and the partially linear IV model with an interval-valued outcome.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets. (arXiv:2109.05554v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05554">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Out-of-distribution detection is an important component of reliable ML
systems. Prior literature has proposed various methods (e.g., MSP (Hendrycks &amp;
Gimpel, 2017), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018)),
claiming they are state-of-the-art by showing they outperform previous methods
on a selected set of in-distribution (ID) and out-of-distribution (OOD)
datasets. In this work, we show that none of these methods are inherently
better at OOD detection than others on a standardized set of 16 (ID, OOD)
pairs. We give possible explanations for these inconsistencies with simple toy
datasets where whether one method outperforms another depends on the structure
of the ID and OOD datasets in question. Finally, we show that a method
outperforming another on a certain (ID, OOD) pair may not do so in a low-data
regime. In the low-data regime, we propose a distance-based method, Pairwise
OOD detection (POD), which is based on Siamese networks and improves over
Mahalanobis by sidestepping the expensive covariance estimation step. Our
results suggest that the OOD detection problem may be too broad, and we should
consider more specific structures for leverage.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive network reliability analysis: Methodology and applications to power grid. (arXiv:2109.05360v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05360">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Flow network models can capture the underlying physics and operational
constraints of many networked systems including the power grid and
transportation and water networks. However, analyzing reliability of systems
using computationally expensive flow-based models faces substantial challenges,
especially for rare events. Existing actively trained meta-models, which
present a new promising direction in reliability analysis, are not applicable
to networks due to the inability of these methods to handle high-dimensional
problems as well as discrete or mixed variable inputs. This study presents the
first adaptive surrogate-based Network Reliability Analysis using Bayesian
Additive Regression Trees (ANR-BART). This approach integrates BART and Monte
Carlo simulation (MCS) via an active learning method that identifies the most
valuable training samples based on the credible intervals derived by BART over
the space of predictor variables as well as the proximity of the points to the
estimated limit state. Benchmark power grids including IEEE 30, 57, 118, and
300-bus systems and their power flow models for cascading failure analysis are
considered to investigate ANR-BART, MCS, subset simulation, and
passively-trained optimal deep neural networks and BART. Results indicate that
ANR-BART is robust and yields accurate estimates of network failure
probability, while significantly reducing the computational cost of reliability
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Structure-preserving Sparse Identification of Nonlinear Dynamics for Data-driven Modeling. (arXiv:2109.05364v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05364">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Discovery of dynamical systems from data forms the foundation for data-driven
modeling and recently, structure-preserving geometric perspectives have been
shown to provide improved forecasting, stability, and physical realizability
guarantees. We present here a unification of the Sparse Identification of
Nonlinear Dynamics (SINDy) formalism with neural ordinary differential
equations. The resulting framework allows learning of both &quot;black-box&quot; dynamics
and learning of structure preserving bracket formalisms for both reversible and
irreversible dynamics. We present a suite of benchmarks demonstrating
effectiveness and structure preservation, including for chaotic systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stabilizing Deep Tomographic Reconstruction. (arXiv:2008.01846v5 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tomographic image reconstruction with deep learning is an emerging field, but
a recent landmark study reveals that several deep reconstruction networks are
unstable for computed tomography (CT) and magnetic resonance imaging (MRI).
Specifically, three kinds of instabilities were reported: (1) strong image
artefacts from tiny perturbations, (2) small features missing in a deeply
reconstructed image, and (3) decreased imaging performance with increased input
data. On the other hand, compressed sensing (CS) inspired reconstruction
methods do not suffer from these instabilities because of their built-in kernel
awareness. For deep reconstruction to realize its full potential and become a
mainstream approach for tomographic imaging, it is thus critically important to
meet this challenge by stabilizing deep reconstruction networks. Here we
propose an Analytic Compressed Iterative Deep (ACID) framework to address this
challenge. ACID synergizes a deep reconstruction network trained on big data,
kernel awareness from CS-inspired processing, and iterative refinement to
minimize the data residual relative to real measurement. Our study demonstrates
that the deep reconstruction using ACID is accurate and stable, and sheds light
on the converging mechanism of the ACID iteration under a Bounded Relative
Error Norm (BREN) condition. In particular, the study shows that ACID-based
reconstruction is resilient against adversarial attacks, superior to classic
sparsity-regularized reconstruction alone, and eliminates the three kinds of
instabilities. We anticipate that this integrative data-driven approach will
help promote development and translation of deep tomographic image
reconstruction networks into clinical applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Making Table Understanding Work in Practice. (arXiv:2109.05173v1 [cs.DB])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Understanding the semantics of tables at scale is crucial for tasks like data
integration, preparation, and search. Table understanding methods aim at
detecting a table&#x27;s topic, semantic column types, column relations, or
entities. With the rise of deep learning, powerful models have been developed
for these tasks with excellent accuracy on benchmarks. However, we observe that
there exists a gap between the performance of these models on these benchmarks
and their applicability in practice. In this paper, we address the question:
what do we need for these models to work in practice?

We discuss three challenges of deploying table understanding models and
propose a framework to address them. These challenges include 1) difficulty in
customizing models to specific domains, 2) lack of training data for typical
database tables often found in enterprises, and 3) lack of confidence in the
inferences made by models. We present SigmaTyper which implements this
framework for the semantic column type detection task. SigmaTyper encapsulates
a hybrid model trained on GitTables and integrates a lightweight
human-in-the-loop approach to customize the model. Lastly, we highlight avenues
for future research that further close the gap towards making table
understanding effective in practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based models such as BERT, XLNET, and XLM-R have achieved
state-of-the-art performance across various NLP tasks including the
identification of offensive language and hate speech, an important problem in
social media. In this paper, we present fBERT, a BERT model retrained on SOLID,
the largest English offensive language identification corpus available with
over $1.4$ million offensive instances. We evaluate fBERT&#x27;s performance on
identifying offensive content on multiple English datasets and we test several
thresholds for selecting instances from SOLID. The fBERT model will be made
freely available to the community.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05463">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Post-hoc interpretation aims to explain a trained model and reveal how the
model arrives at a decision. Though research on post-hoc interpretations has
developed rapidly, one growing pain in this field is the difficulty in
evaluating interpretations. There are some crucial logic traps behind existing
evaluation methods, which are ignored by most works. In this opinion piece, we
summarize four kinds evaluation methods and point out the corresponding logic
traps behind them. We argue that we should be clear about these traps rather
than ignore them and draw conclusions assertively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint Sampling Method. (arXiv:2011.03176v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.03176">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The randomized midpoint method, proposed by [SL19], has emerged as an optimal
discretization procedure for simulating the continuous time Langevin
diffusions. Focusing on the case of strong-convex and smooth potentials, in
this paper, we analyze several probabilistic properties of the randomized
midpoint discretization method for both overdamped and underdamped Langevin
diffusions. We first characterize the stationary distribution of the discrete
chain obtained with constant step-size discretization and show that it is
biased away from the target distribution. Notably, the step-size needs to go to
zero to obtain asymptotic unbiasedness. Next, we establish the asymptotic
normality for numerical integration using the randomized midpoint method and
highlight the relative advantages and disadvantages over other discretizations.
Our results collectively provide several insights into the behavior of the
randomized midpoint discretization method, including obtaining confidence
intervals for numerical integrations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Memory-based Deep Reinforcement Learning for POMDPs. (arXiv:2102.12344v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12344">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A promising characteristic of Deep Reinforcement Learning (DRL) is its
capability to learn optimal policy in an end-to-end manner without relying on
feature engineering. However, most approaches assume a fully observable state
space, i.e. fully observable Markov Decision Processes (MDPs). In real-world
robotics, this assumption is unpractical, because of issues such as sensor
sensitivity limitations and sensor noise, and the lack of knowledge about
whether the observation design is complete or not. These scenarios lead to
Partially Observable MDPs (POMDPs). In this paper, we propose
Long-Short-Term-Memory-based Twin Delayed Deep Deterministic Policy Gradient
(LSTM-TD3) by introducing a memory component to TD3, and compare its
performance with other DRL algorithms in both MDPs and POMDPs. Our results
demonstrate the significant advantages of the memory component in addressing
POMDPs, including the ability to handle missing and noisy observation data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness of Exposure in Stochastic Bandits. (arXiv:2103.02735v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02735">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Contextual bandit algorithms have become widely used for recommendation in
online systems (e.g. marketplaces, music streaming, news), where they now wield
substantial influence on which items get exposed to the users. This raises
questions of fairness to the items -- and to the sellers, artists, and writers
that benefit from this exposure. We argue that the conventional bandit
formulation can lead to an undesirable and unfair winner-takes-all allocation
of exposure. To remedy this problem, we propose a new bandit objective that
guarantees merit-based fairness of exposure to the items while optimizing
utility to the users. We formulate fairness regret and reward regret in this
setting, and present algorithms for both stochastic multi-armed bandits and
stochastic linear bandits. We prove that the algorithms achieve sub-linear
fairness regret and reward regret. Beyond the theoretical analysis, we also
provide empirical evidence that these algorithms can fairly allocate exposure
to different arms effectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The research community has proposed copious modifications to the Transformer
architecture since it was introduced over three years ago, relatively few of
which have seen widespread adoption. In this paper, we comprehensively evaluate
many of these modifications in a shared experimental setting that covers most
of the common uses of the Transformer in natural language processing.
Surprisingly, we find that most modifications do not meaningfully improve
performance. Furthermore, most of the Transformer variants we found beneficial
were either developed in the same codebase that we used or are relatively minor
changes. We conjecture that performance improvements may strongly depend on
implementation details and correspondingly make some recommendations for
improving the generality of experimental results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepPyram: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos. (arXiv:2109.05352v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation in cataract surgery has a wide range of applications
contributing to surgical outcome enhancement and clinical risk reduction.
However, the varying issues in segmenting the different relevant instances make
the designation of a unique network quite challenging. This paper proposes a
semantic segmentation network termed as DeepPyram that can achieve superior
performance in segmenting relevant objects in cataract surgery videos with
varying issues. This superiority mainly originates from three modules: (i)
Pyramid View Fusion, which provides a varying-angle global view of the
surrounding region centering at each pixel position in the input convolutional
feature map; (ii) Deformable Pyramid Reception, which enables a wide deformable
receptive field that can adapt to geometric transformations in the object of
interest; and (iii) Pyramid Loss that adaptively supervises multi-scale
semantic feature maps. These modules can effectively boost semantic
segmentation performance, especially in the case of transparency,
deformability, scalability, and blunt edges in objects. The proposed approach
is evaluated using four datasets of cataract surgery for objects with different
contextual features and compared with thirteen state-of-the-art segmentation
networks. The experimental results confirm that DeepPyram outperforms the rival
approaches without imposing additional trainable parameters. Our comprehensive
ablation study further proves the effectiveness of the proposed modules.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine learning reveals how personalized climate communication can both succeed and backfire. (arXiv:2109.05104v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05104">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Different advertising messages work for different people. Machine learning
can be an effective way to personalise climate communications. In this paper we
use machine learning to reanalyse findings from a recent study, showing that
online advertisements increased some people&#x27;s belief in climate change while
resulting in decreased belief in others. In particular, we show that the effect
of the advertisements could change depending on people&#x27;s age and ethnicity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate Prediction Using Triangular Type-2 Fuzzy Linear Regression. (arXiv:2109.05461v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05461">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Many works have been done to handle the uncertainties in the data using type
1 fuzzy regression. Few type 2 fuzzy regression works used interval type 2 for
indeterminate modeling using type 1 fuzzy membership. The current survey
proposes a triangular type-2 fuzzy regression (TT2FR) model to ameliorate the
efficiency of the model by handling the uncertainty in the data. The triangular
secondary membership function is used instead of widely used interval type
models. In the proposed model, vagueness in primary and secondary fuzzy sets is
minimized and also, a specified x-plane of observed value is included in the
same {\alpha}- plane of the predicted value. Complex calculations of the type-2
fuzzy (T2F) model are simplified by reducing three dimensional type-2 fuzzy set
(3DT2FS) into two dimensional interval type-2 fuzzy (2DIT2F) models. The
current survey presents a new regression model of T2F by considering the more
general form of T2F membership functions and thus avoids high complexity. The
performance of the developed model is evaluated using the TAIEX and COVID-19
forecasting datasets. Our developed model reached the highest performance as
compared to the other state-of-art techniques. Our developed method is ready to
be tested with more uncertain data and has the potential to use to predict the
weather and stock prediction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adversarial Representation Learning With Closed-Form Solvers. (arXiv:2109.05535v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05535">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial representation learning aims to learn data representations for a
target task while removing unwanted sensitive information at the same time.
Existing methods learn model parameters iteratively through stochastic gradient
descent-ascent, which is often unstable and unreliable in practice. To overcome
this challenge, we adopt closed-form solvers for the adversary and target task.
We model them as kernel ridge regressors and analytically determine an
upper-bound on the optimal dimensionality of representation. Our solution,
dubbed OptNet-ARL, reduces to a stable one one-shot optimization problem that
can be solved reliably and efficiently. OptNet-ARL can be easily generalized to
the case of multiple target tasks and sensitive attributes. Numerical
experiments, on both small and large scale datasets, show that, from an
optimization perspective, OptNet-ARL is stable and exhibits three to five times
faster convergence. Performance wise, when the target and sensitive attributes
are dependent, OptNet-ARL learns representations that offer a better trade-off
front between (a) utility and bias for fair classification and (b) utility and
privacy by mitigating leakage of private information than existing solutions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global and Local Interpretation of black-box Machine Learning models to determine prognostic factors from early COVID-19 data. (arXiv:2109.05087v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05087">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 corona virus has claimed 4.1 million lives, as of July 24, 2021.
A variety of machine learning models have been applied to related data to
predict important factors such as the severity of the disease, infection rate
and discover important prognostic factors. Often the usefulness of the findings
from the use of these techniques is reduced due to lack of method
interpretability. Some recent progress made on the interpretability of machine
learning models has the potential to unravel more insights while using
conventional machine learning models. In this work, we analyze COVID-19 blood
work data with some of the popular machine learning models; then we employ
state-of-the-art post-hoc local interpretability techniques(e.g.- SHAP, LIME),
and global interpretability techniques(e.g. - symbolic metamodeling) to the
trained black-box models to draw interpretable conclusions. In the gamut of
machine learning algorithms, regressions remain one of the simplest and most
explainable models with clear mathematical formulation. We explore one of the
most recent techniques called symbolic metamodeling to find the mathematical
expression of the machine learning models for COVID-19. We identify Acute
Kidney Injury (AKI), initial Albumin level (ALBI), Aspartate aminotransferase
(ASTI), Total Bilirubin initial(TBILI) and D-Dimer initial (DIMER) as major
prognostic factors of the disease severity. Our contributions are- (i) uncover
the underlying mathematical expression for the black-box models on COVID-19
severity prediction task (ii) we are the first to apply symbolic metamodeling
to this task, and (iii) discover important features and feature interactions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Concave Utility Reinforcement Learning with Zero-Constraint Violations. (arXiv:2109.05439v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05439">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the problem of tabular infinite horizon concave utility
reinforcement learning (CURL) with convex constraints. Various learning
applications with constraints, such as robotics, do not allow for policies that
can violate constraints. To this end, we propose a model-based learning
algorithm that achieves zero constraint violations. To obtain this result, we
assume that the concave objective and the convex constraints have a solution
interior to the set of feasible occupation measures. We then solve a tighter
optimization problem to ensure that the constraints are never violated despite
the imprecise model knowledge and model stochasticity. We also propose a novel
Bellman error based analysis for tabular infinite-horizon setups which allows
to analyse stochastic policies. Combining the Bellman error based analysis and
tighter optimization equation, for $T$ interactions with the environment, we
obtain a regret guarantee for objective which grows as $\Tilde{O}(1/\sqrt{T})$,
excluding other factors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Empirical Comparison of Off-policy Prediction Learning Algorithms in the Four Rooms Environment. (arXiv:2109.05110v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05110">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Many off-policy prediction learning algorithms have been proposed in the past
decade, but it remains unclear which algorithms learn faster than others. We
empirically compare 11 off-policy prediction learning algorithms with linear
function approximation on two small tasks: the Rooms task, and the High
Variance Rooms task. The tasks are designed such that learning fast in them is
challenging. In the Rooms task, the product of importance sampling ratios can
be as large as $2^{14}$ and can sometimes be two. To control the high variance
caused by the product of the importance sampling ratios, step size should be
set small, which in turn slows down learning. The High Variance Rooms task is
more extreme in that the product of the ratios can become as large as
$2^{14}\times 25$. This paper builds upon the empirical study of off-policy
prediction learning algorithms by Ghiassian and Sutton (2021). We consider the
same set of algorithms as theirs and employ the same experimental methodology.
The algorithms considered are: Off-policy TD($\lambda$), five Gradient-TD
algorithms, two Emphatic-TD algorithms, Tree Backup($\lambda$),
Vtrace($\lambda$), and ABTD($\zeta$). We found that the algorithms&#x27; performance
is highly affected by the variance induced by the importance sampling ratios.
The data shows that Tree Backup($\lambda$), Vtrace($\lambda$), and
ABTD($\zeta$) are not affected by the high variance as much as other algorithms
but they restrict the effective bootstrapping parameter in a way that is too
limiting for tasks where high variance is not present. We observed that
Emphatic TD($\lambda$) tends to have lower asymptotic error than other
algorithms, but might learn more slowly in some cases. We suggest algorithms
for practitioners based on their problem of interest, and suggest approaches
that can be applied to specific algorithms that might result in substantially
improved algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HyP-ABC: A Novel Automated Hyper-Parameter Tuning Algorithm Using Evolutionary Optimization. (arXiv:2109.05319v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05319">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Machine learning techniques lend themselves as promising decision-making and
analytic tools in a wide range of applications. Different ML algorithms have
various hyper-parameters. In order to tailor an ML model towards a specific
application, a large number of hyper-parameters should be tuned. Tuning the
hyper-parameters directly affects the performance (accuracy and run-time).
However, for large-scale search spaces, efficiently exploring the ample number
of combinations of hyper-parameters is computationally challenging. Existing
automated hyper-parameter tuning techniques suffer from high time complexity.
In this paper, we propose HyP-ABC, an automatic innovative hybrid
hyper-parameter optimization algorithm using the modified artificial bee colony
approach, to measure the classification accuracy of three ML algorithms, namely
random forest, extreme gradient boosting, and support vector machine. Compared
to the state-of-the-art techniques, HyP-ABC is more efficient and has a limited
number of parameters to be tuned, making it worthwhile for real-world
hyper-parameter optimization problems. We further compare our proposed HyP-ABC
algorithm with state-of-the-art techniques. In order to ensure the robustness
of the proposed method, the algorithm takes a wide range of feasible
hyper-parameter values, and is tested using a real-world educational dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11715">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>End-to-end automatic speech recognition (ASR) systems are increasingly
popular due to their relative architectural simplicity and competitive
performance. However, even though the average accuracy of these systems may be
high, the performance on rare content words often lags behind hybrid ASR
systems. To address this problem, second-pass rescoring is often applied
leveraging upon language modeling. In this paper, we propose a second-pass
system with multi-task learning, utilizing semantic targets (such as intent and
slot prediction) to improve speech recognition performance. We show that our
rescoring model trained with these additional tasks outperforms the baseline
rescoring model, trained with only the language modeling task, by 1.4% on a
general test and by 2.6% on a rare word test set in terms of word-error-rate
relative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR
deduction compared with RNN Transducer only ASR baseline for rare words
recognition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic Cluster Embedding. (arXiv:2108.08003v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08003">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neighbor Embedding (NE) aims to preserve pairwise similarities between data
items and has been shown to yield an effective principle for data
visualization. However, even the best existing NE methods such as Stochastic
Neighbor Embedding (SNE) may leave large-scale patterns hidden, for example,
clusters, despite strong signals being present in the data. To address this, we
propose a new cluster visualization method based on the Neighbor Embedding
principle. We first present a family of Neighbor Embedding methods which
generalizes SNE by using non-normalized Kullback-Leibler divergence with a
scale parameter. In this family, much better cluster visualizations often
appear with a parameter value different from the one corresponding to SNE. We
also develop an efficient software which employs asynchronous stochastic block
coordinate descent to optimize the new family of objective functions. Our
experimental results demonstrate that the method consistently and substantially
improves visualization of data clusters compared with the state-of-the-art NE
approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CoG: a Two-View Co-training Framework for Defending Adversarial Attacks on Graph. (arXiv:2109.05558v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05558">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph neural networks exhibit remarkable performance in graph data analysis.
However, the robustness of GNN models remains a challenge. As a result, they
are not reliable enough to be deployed in critical applications. Recent studies
demonstrate that GNNs could be easily fooled with adversarial perturbations,
especially structural perturbations. Such vulnerability is attributed to the
excessive dependence on the structure information to make predictions. To
achieve better robustness, it is desirable to build the prediction of GNNs with
more comprehensive features. Graph data, in most cases, has two views of
information, namely structure information and feature information. In this
paper, we propose CoG, a simple yet effective co-training framework to combine
these two views for the purpose of robustness. CoG trains sub-models from the
feature view and the structure view independently and allows them to distill
knowledge from each other by adding their most confident unlabeled data into
the training set. The orthogonality of these two views diversifies the
sub-models, thus enhancing the robustness of their ensemble. We evaluate our
framework on three popular datasets, and results show that CoG significantly
improves the robustness of graph models against adversarial attacks without
sacrificing their performance on clean data. We also show that CoG still
achieves good robustness when both node features and graph structures are
perturbed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Ordering-Based Causal Discovery with Reinforcement Learning. (arXiv:2105.06631v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06631">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is a long-standing question to discover causal relations among a set of
variables in many empirical sciences. Recently, Reinforcement Learning (RL) has
achieved promising results in causal discovery from observational data.
However, searching the space of directed graphs and enforcing acyclicity by
implicit penalties tend to be inefficient and restrict the existing RL-based
method to small scale problems. In this work, we propose a novel RL-based
approach for causal discovery, by incorporating RL into the ordering-based
paradigm. Specifically, we formulate the ordering search problem as a
multi-step Markov decision process, implement the ordering generating process
with an encoder-decoder architecture, and finally use RL to optimize the
proposed model based on the reward mechanisms designed for~each ordering. A
generated ordering would then be processed using variable selection to obtain
the final causal graph. We analyze the consistency and computational complexity
of the proposed method, and empirically show that a pretrained model can be
exploited to accelerate training. Experimental results on both synthetic and
real data sets shows that the proposed method achieves a much improved
performance over existing RL-based method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05317">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Causal inference using observational text data is becoming increasingly
popular in many research areas. This paper presents the Bayesian Topic
Regression (BTR) model that uses both text and numerical information to model
an outcome variable. It allows estimation of both discrete and continuous
treatment effects. Furthermore, it allows for the inclusion of additional
numerical confounding factors next to text data. To this end, we combine a
supervised Bayesian topic model with a Bayesian regression framework and
perform supervised representation learning for the text features jointly with
the regression parameter training, respecting the Frisch-Waugh-Lovell theorem.
Our paper makes two main contributions. First, we provide a regression
framework that allows causal inference in settings when both text and numerical
confounders are of relevance. We show with synthetic and semi-synthetic
datasets that our joint approach recovers ground truth with lower bias than any
benchmark model, when text and numerical features are correlated. Second,
experiments on two real-world datasets demonstrate that a joint and supervised
learning strategy also yields superior prediction results compared to
strategies that estimate regression weights for text and non-text features
separately, being even competitive with more complex deep neural networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Is Heterophily A Real Nightmare For Graph Neural Networks To Do Node Classification?. (arXiv:2109.05641v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05641">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph Neural Networks (GNNs) extend basic Neural Networks (NNs) by using the
graph structures based on the relational inductive bias (homophily assumption).
Though GNNs are believed to outperform NNs in real-world tasks, performance
advantages of GNNs over graph-agnostic NNs seem not generally satisfactory.
Heterophily has been considered as a main cause and numerous works have been
put forward to address it. In this paper, we first show that not all cases of
heterophily are harmful for GNNs with aggregation operation. Then, we propose
new metrics based on a similarity matrix which considers the influence of both
graph structure and input features on GNNs. The metrics demonstrate advantages
over the commonly used homophily metrics by tests on synthetic graphs. From the
metrics and the observations, we find some cases of harmful heterophily can be
addressed by diversification operation. With this fact and knowledge of
filterbanks, we propose the Adaptive Channel Mixing (ACM) framework to
adaptively exploit aggregation, diversification and identity channels in each
GNN layer to address harmful heterophily. We validate the ACM-augmented
baselines with 10 real-world node classification tasks. They consistently
achieve significant performance gain and exceed the state-of-the-art GNNs on
most of the tasks without incurring significant computational burden.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Potential-based Reward Shaping in Sokoban. (arXiv:2109.05022v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning to solve sparse-reward reinforcement learning problems is difficult,
due to the lack of guidance towards the goal. But in some problems, prior
knowledge can be used to augment the learning process. Reward shaping is a way
to incorporate prior knowledge into the original reward function in order to
speed up the learning. While previous work has investigated the use of expert
knowledge to generate potential functions, in this work, we study whether we
can use a search algorithm(A*) to automatically generate a potential function
for reward shaping in Sokoban, a well-known planning task. The results showed
that learning with shaped reward function is faster than learning from scratch.
Our results indicate that distance functions could be a suitable function for
Sokoban. This work demonstrates the possibility of solving multiple instances
with the help of reward shaping. The result can be compressed into a single
policy, which can be seen as the first phrase towards training a general policy
that is able to solve unseen instances.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Omnipredictors. (arXiv:2109.05389v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05389">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Loss minimization is a dominant paradigm in machine learning, where a
predictor is trained to minimize some loss function that depends on an
uncertain event (e.g., &quot;will it rain tomorrow?&#x27;&#x27;). Different loss functions
imply different learning algorithms and, at times, very different predictors.
While widespread and appealing, a clear drawback of this approach is that the
loss function may not be known at the time of learning, requiring the algorithm
to use a best-guess loss function. We suggest a rigorous new paradigm for loss
minimization in machine learning where the loss function can be ignored at the
time of learning and only be taken into account when deciding an action.

We introduce the notion of an (${\mathcal{L}},\mathcal{C}$)-omnipredictor,
which could be used to optimize any loss in a family ${\mathcal{L}}$. Once the
loss function is set, the outputs of the predictor can be post-processed (a
simple univariate data-independent transformation of individual predictions) to
do well compared with any hypothesis from the class $\mathcal{C}$. The post
processing is essentially what one would perform if the outputs of the
predictor were true probabilities of the uncertain events. In a sense,
omnipredictors extract all the predictive power from the class $\mathcal{C}$,
irrespective of the loss function in $\mathcal{L}$.

We show that such &quot;loss-oblivious&#x27;&#x27; learning is feasible through a connection
to multicalibration, a notion introduced in the context of algorithmic
fairness. In addition, we show how multicalibration can be viewed as a solution
concept for agnostic boosting, shedding new light on past results. Finally, we
transfer our insights back to the context of algorithmic fairness by providing
omnipredictors for multi-group loss minimization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05472">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The progress of some AI paradigms such as deep learning is said to be linked
to an exponential growth in the number of parameters. There are many studies
corroborating these trends, but does this translate into an exponential
increase in energy consumption? In order to answer this question we focus on
inference costs rather than training costs, as the former account for most of
the computing effort, solely because of the multiplicative factors. Also, apart
from algorithmic innovations, we account for more specific and powerful
hardware (leading to higher FLOPS) that is usually accompanied with important
energy efficiency optimisations. We also move the focus from the first
implementation of a breakthrough paper towards the consolidated version of the
techniques one or two year later. Under this distinctive and comprehensive
perspective, we study relevant models in the areas of computer vision and
natural language processing: for a sustained increase in performance we see a
much softer growth in energy consumption than previously anticipated. The only
caveat is, yet again, the multiplicative factor, as future AI increases
penetration and becomes more pervasive.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05052">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Knowledge-dependent tasks typically use two sources of knowledge: parametric,
learned at training time, and contextual, given as a passage at inference time.
To understand how models use these sources together, we formalize the problem
of knowledge conflicts, where the contextual information contradicts the
learned information. Analyzing the behaviour of popular models, we measure
their over-reliance on memorized information (the cause of hallucinations), and
uncover important factors that exacerbate this behaviour. Lastly, we propose a
simple method to mitigate over-reliance on parametric knowledge, which
minimizes hallucination, and improves out-of-distribution generalization by
4%-7%. Our findings demonstrate the importance for practitioners to evaluate
model tendency to hallucinate rather than read, and show that our mitigation
strategy encourages generalization to evolving information (i.e.,
time-dependent queries). To encourage these practices, we have released our
framework for generating knowledge conflicts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Kernel PCA with the Nystr\&quot;om method. (arXiv:2109.05578v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05578">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Kernel methods are powerful but computationally demanding techniques for
non-linear learning. A popular remedy, the Nystr\&quot;om method has been shown to
be able to scale up kernel methods to very large datasets with little loss in
accuracy. However, kernel PCA with the Nystr\&quot;om method has not been widely
studied. In this paper we derive kernel PCA with the Nystr\&quot;om method and study
its accuracy, providing a finite-sample confidence bound on the difference
between the Nystr\&quot;om and standard empirical reconstruction errors. The
behaviours of the method and bound are illustrated through extensive computer
experiments on real-world data. As an application of the method we present
kernel principal component regression with the Nystr\&quot;om method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Unsupervised Deep-Learning Method for Fingerprint Classification: the CCAE Network and the Hybrid Clustering Strategy. (arXiv:2109.05526v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05526">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The fingerprint classification is an important and effective method to
quicken the process and improve the accuracy in the fingerprint matching
process. Conventional supervised methods need a large amount of pre-labeled
data and thus consume immense human resources. In this paper, we propose a new
and efficient unsupervised deep learning method that can extract fingerprint
features and classify fingerprint patterns automatically. In this approach, a
new model named constraint convolutional auto-encoder (CCAE) is used to extract
fingerprint features and a hybrid clustering strategy is applied to obtain the
final clusters. A set of experiments in the NIST-DB4 dataset shows that the
proposed unsupervised method exhibits the efficient performance on fingerprint
classification. For example, the CCAE achieves an accuracy of 97.3% on only
1000 unlabeled fingerprints in the NIST-DB4.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Network Based Single-Pixel Compressive Direction of Arrival Estimation. (arXiv:2109.05466v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05466">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we present a single-pixel compressive direction of arrival
(DoA) estimation technique leveraging a graph attention network (GAT) based
deep-learning framework. The physical layer compression is achieved using a
coded-aperture technique, probing the spectrum of far-field sources incident on
the aperture using a set of spatio-temporally incoherent modes. This
information is then encoded and compressed into the channel of the
coded-aperture. The coded-aperture based receiver exhibits a single-channel,
replacing the conventional multichannel raster scan based solutions for DoA
estimation. The GAT network enables the compressive DoA estimation framework to
learn the DoA information directly from the measurements acquired using the
coded-aperture. This step eliminates the need for an additional reconstruction
step and significantly simplifies the processing layer to obtain the DoA
estimate. We show that the presented GAT integrated single-pixel radar
framework can retrieve high fidelity DoA information even under relatively low
signal-to-noise ratio (SNR) levels.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Gradients and Subgradients of Buffered Failure Probability. (arXiv:2109.05391v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05391">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Gradients and subgradients are central to optimization and sensitivity
analysis of buffered failure probabilities. We furnish a characterization of
subgradients based on subdifferential calculus in the case of finite
probability distributions and, under additional assumptions, also a gradient
expression for general distributions. Several examples illustrate the
application of the results, especially in the context of optimality conditions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Back to the Basics: A Quantitative Analysis of Statistical and Graph-Based Term Weighting Schemes for Keyword Extraction. (arXiv:2104.08028v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08028">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Term weighting schemes are widely used in Natural Language Processing and
Information Retrieval. In particular, term weighting is the basis for keyword
extraction. However, there are relatively few evaluation studies that shed
light about the strengths and shortcomings of each weighting scheme. In fact,
in most cases researchers and practitioners resort to the well-known tf-idf as
default, despite the existence of other suitable alternatives, including
graph-based models. In this paper, we perform an exhaustive and large-scale
empirical comparison of both statistical and graph-based term weighting methods
in the context of keyword extraction. Our analysis reveals some interesting
findings such as the advantages of the less-known lexical specificity with
respect to tf-idf, or the qualitative differences between statistical and
graph-based methods. Finally, based on our findings we discuss and devise some
suggestions for practitioners. Source code to reproduce our experimental
results, including a keyword extraction library, are available in the following
repository: https://github.com/asahi417/kex</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-Conditioned GAN. (arXiv:2109.05070v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05070">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative Adversarial Networks (GANs) can generate near photo realistic
images in narrow domains such as human faces. Yet, modeling complex
distributions of datasets such as ImageNet and COCO-Stuff remains challenging
in unconditional settings. In this paper, we take inspiration from kernel
density estimation techniques and introduce a non-parametric approach to
modeling distributions of complex datasets. We partition the data manifold into
a mixture of overlapping neighborhoods described by a datapoint and its nearest
neighbors, and introduce a model, called instance-conditioned GAN (IC-GAN),
which learns the distribution around each datapoint. Experimental results on
ImageNet and COCO-Stuff show that IC-GAN significantly improves over
unconditional models and unsupervised data partitioning baselines. Moreover, we
show that IC-GAN can effortlessly transfer to datasets not seen during training
by simply changing the conditioning instances, and still generate realistic
images. Finally, we extend IC-GAN to the class-conditional case and show
semantically controllable generation and competitive quantitative results on
ImageNet; while improving over BigGAN on ImageNet-LT. We will opensource our
code and trained models to reproduce the reported results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Co-Correcting: Noise-tolerant Medical Image Classification via mutual Label Correction. (arXiv:2109.05159v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05159">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the development of deep learning, medical image classification has been
significantly improved. However, deep learning requires massive data with
labels. While labeling the samples by human experts is expensive and
time-consuming, collecting labels from crowd-sourcing suffers from the noises
which may degenerate the accuracy of classifiers. Therefore, approaches that
can effectively handle label noises are highly desired. Unfortunately, recent
progress on handling label noise in deep learning has gone largely unnoticed by
the medical image. To fill the gap, this paper proposes a noise-tolerant
medical image classification framework named Co-Correcting, which significantly
improves classification accuracy and obtains more accurate labels through
dual-network mutual learning, label probability estimation, and curriculum
label correcting. On two representative medical image datasets and the MNIST
dataset, we test six latest Learning-with-Noisy-Labels methods and conduct
comparative studies. The experiments show that Co-Correcting achieves the best
accuracy and generalization under different noise ratios in various tasks. Our
project can be found at: https://github.com/JiarunLiu/Co-Correcting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lipschitz Normalization for Self-Attention Layers with Application to Graph Neural Networks. (arXiv:2103.04886v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.04886">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Attention based neural networks are state of the art in a large range of
applications. However, their performance tends to degrade when the number of
layers increases. In this work, we show that enforcing Lipschitz continuity by
normalizing the attention scores can significantly improve the performance of
deep attention models. First, we show that, for deep graph attention networks
(GAT), gradient explosion appears during training, leading to poor performance
of gradient-based training algorithms. To address this issue, we derive a
theoretical analysis of the Lipschitz continuity of attention modules and
introduce LipschitzNorm, a simple and parameter-free normalization for
self-attention mechanisms that enforces the model to be Lipschitz continuous.
We then apply LipschitzNorm to GAT and Graph Transformers and show that their
performance is substantially improved in the deep setting (10 to 30 layers).
More specifically, we show that a deep GAT model with LipschitzNorm achieves
state of the art results for node label prediction tasks that exhibit
long-range dependencies, while showing consistent improvements over their
unnormalized counterparts in benchmark node classification tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RealFormer: Transformer Likes Residual Attention. (arXiv:2012.11747v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer is the backbone of modern NLP models. In this paper, we propose
RealFormer, a simple and generic technique to create Residual Attention Layer
Transformer networks that significantly outperform the canonical Transformer
and its variants (BERT, ETC, etc.) on a wide spectrum of tasks including Masked
Language Modeling, GLUE, SQuAD, Neural Machine Translation, WikiHop, HotpotQA,
Natural Questions, and OpenKP. We also observe empirically that RealFormer
stabilizes training and leads to models with sparser attention. Source code and
pre-trained checkpoints for RealFormer can be found at
https://github.com/google-research/google-research/tree/master/realformer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Controlled Gaussian Process Dynamical Models with Application to Robotic Cloth Manipulation. (arXiv:2103.06615v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06615">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Over the last years, robotic cloth manipulation has gained relevance within
the research community. While significant advances have been made in robotic
manipulation of rigid objects, the manipulation of non-rigid objects such as
cloth garments is still a challenging problem. The uncertainty on how cloth
behaves often requires the use of model-based approaches. However, cloth models
have a very high dimensionality. Therefore, it is difficult to find a middle
point between providing a manipulator with a dynamics model of cloth and
working with a state space of tractable dimensionality. For this reason, most
cloth manipulation approaches in literature perform static or quasi-static
manipulation. In this paper, we propose a variation of Gaussian Process
Dynamical Models (GPDMs) to model cloth dynamics in a low-dimensional manifold.
GPDMs project a high-dimensional state space into a smaller dimension latent
space which is capable of keeping the dynamic properties. Using such approach,
we add control variables to the original formulation. In this way, it is
possible to take into account the robot commands exerted on the cloth dynamics.
We call this new version Controlled Gaussian Process Dynamical Model (CGPDM).
Moreover, we propose an alternative parametric structure for the model, that is
richer than the one employed in previous GPDM realizations. The modeling
capacity of our proposal has been tested in both a simulated and a real
scenario, where CGPDM proved to be capable of generalizing over a wide range of
movements and correctly predicting the cloth motions obtained by previously
unseen sequences of control actions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Direct Random Search for Fine Tuning of Deep Reinforcement Learning Policies. (arXiv:2109.05604v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05604">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Researchers have demonstrated that Deep Reinforcement Learning (DRL) is a
powerful tool for finding policies that perform well on complex robotic
systems. However, these policies are often unpredictable and can induce highly
variable behavior when evaluated with only slightly different initial
conditions. Training considerations constrain DRL algorithm designs in that
most algorithms must use stochastic policies during training. The resulting
policy used during deployment, however, can and frequently is a deterministic
one that uses the Maximum Likelihood Action (MLA) at each step. In this work,
we show that a direct random search is very effective at fine-tuning DRL
policies by directly optimizing them using deterministic rollouts. We
illustrate this across a large collection of reinforcement learning
environments, using a wide variety of policies obtained from different
algorithms. Our results show that this method yields more consistent and higher
performing agents on the environments we tested. Furthermore, we demonstrate
how this method can be used to extend our previous work on shrinking the
dimensionality of the reachable state space of closed-loop systems run under
Deep Neural Network (DNN) policies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine Learning Interpretability Meets TLS Fingerprinting. (arXiv:2011.06304v2 [cs.NI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06304">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Protecting users&#x27; privacy over the Internet is of great importance; however,
it becomes harder and harder to maintain due to the increasing complexity of
network protocols and components. Therefore, investigating and understanding
how data is leaked from the information transmission platforms and protocols
can lead us to a more secure environment.

In this paper, we propose a framework to systematically find the most
vulnerable information fields in a network protocol. To this end, focusing on
the transport layer security (TLS) protocol, we perform different
machine-learning-based fingerprinting attacks on the collected data from more
than 70 domains (websites) to understand how and where this information leakage
occurs in the TLS protocol. Then, by employing the interpretation techniques
developed in the machine learning community and applying our framework, we find
the most vulnerable information fields in the TLS protocol. Our findings
demonstrate that the TLS handshake (which is mainly unencrypted), the TLS
record length appearing in the TLS application data header, and the
initialization vector (IV) field are among the most critical leaker parts in
this protocol, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A decreasing scaling transition scheme from Adam to SGD. (arXiv:2106.06749v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06749">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Adaptive gradient algorithm (AdaGrad) and its variants, such as RMSProp,
Adam, AMSGrad, etc, have been widely used in deep learning. Although these
algorithms are faster in the early phase of training, their generalization
performance is often not as good as stochastic gradient descent (SGD). Hence, a
trade-off method of transforming Adam to SGD after a certain iteration to gain
the merits of both algorithms is theoretically and practically significant. To
that end, we propose a decreasing scaling transition scheme to achieve a smooth
and stable transition from Adam to SGD, which is called DSTAdam. The
convergence of the proposed DSTAdam is also proved in an online convex setting.
Finally, the effectiveness of the DSTAdam is verified on the CIFAR-10/100
datasets. Our implementation is available at:
https://github.com/kunzeng/DSTAdam.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Graph Normalized Auto-Encoders. (arXiv:2108.08046v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08046">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Link prediction is one of the key problems for graph-structured data. With
the advancement of graph neural networks, graph autoencoders (GAEs) and
variational graph autoencoders (VGAEs) have been proposed to learn graph
embeddings in an unsupervised way. It has been shown that these methods are
effective for link prediction tasks. However, they do not work well in link
predictions when a node whose degree is zero (i.g., isolated node) is involved.
We have found that GAEs/VGAEs make embeddings of isolated nodes close to zero
regardless of their content features. In this paper, we propose a novel
Variational Graph Normalized AutoEncoder (VGNAE) that utilize L2-normalization
to derive better embeddings for isolated nodes. We show that our VGNAEs
outperform the existing state-of-the-art models for link prediction tasks. The
code is available at https://github.com/SeongJinAhn/VGNAE.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A physics-informed variational DeepONet for predicting the crack path in brittle materials. (arXiv:2108.06905v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06905">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Failure trajectories, identifying the probable failure zones, and damage
statistics are some of the key quantities of relevance in brittle fracture
applications. High-fidelity numerical solvers that reliably estimate these
relevant quantities exist but they are computationally demanding requiring a
high resolution of the crack. Moreover, independent intensive simulations need
to be carried out even for a small change in domain parameters and/or material
properties. Therefore, fast and generalizable surrogate models are needed to
alleviate the computational burden but the discontinuous nature of fracture
mechanics presents a major challenge to developing such models. We propose a
physics-informed variational formulation of DeepONet (V-DeepONet) for brittle
fracture analysis. V-DeepONet is trained to map the initial configuration of
the defect to the relevant fields of interests (e.g., damage and displacement
fields). Once the network is trained, the entire global solution can be rapidly
obtained for any initial crack configuration and loading steps on that domain.
While the original DeepONet is solely data-driven, we take a different path to
train the V-DeepONet by imposing the governing equations in variational form
and we also use some labelled data. We demonstrate the effectiveness of
V-DeepOnet through two benchmarks of brittle fracture, and we verify its
accuracy using results from high-fidelity solvers. Encoding the physical laws
and also some data to train the network renders the surrogate model capable of
accurately performing both interpolation and extrapolation tasks, considering
that fracture modeling is very sensitive to fluctuations. The proposed hybrid
training of V-DeepONet is superior to state-of-the-art methods and can be
applied to a wide array of dynamical systems with complex responses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation. (arXiv:2109.05629v1 [cs.HC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05629">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Rapid improvements in the performance of machine learning models have pushed
them to the forefront of data-driven decision-making. Meanwhile, the increased
integration of these models into various application domains has further
highlighted the need for greater interpretability and transparency. To identify
problems such as bias, overfitting, and incorrect correlations, data scientists
require tools that explain the mechanisms with which these model decisions are
made. In this paper we introduce AdViCE, a visual analytics tool that aims to
guide users in black-box model debugging and validation. The solution rests on
two main visual user interface innovations: (1) an interactive visualization
design that enables the comparison of decisions on user-defined data subsets;
(2) an algorithm and visual design to compute and visualize counterfactual
explanations - explanations that depict model outcomes when data features are
perturbed from their original values. We provide a demonstration of the tool
through a use case that showcases the capabilities and potential limitations of
the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-13">2021-09-13</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08799">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a
classical task for capturing the central idea from a given document. Based on
Seq2Seq models, the previous reinforcement learning framework on KG tasks
utilizes the evaluation metrics to further improve the well-trained neural
models. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are
only aware of the exact correctness of predictions on phrase-level and ignore
the semantic similarities between similar predictions and targets, which
inhibits the model from learning deep linguistic patterns. In response to this
problem, we propose a new fine-grained evaluation metric to improve the RL
framework, which considers different granularities: token-level $F_1$ score,
edit distance, duplication, and prediction quantities. On the whole, the new
framework includes two reward functions: the fine-grained evaluation score and
the vanilla $F_1$ score. This framework helps the model identifying some
partial match phrases which can be further optimized as the exact match ones.
Experiments on KG benchmarks show that our proposed training framework
outperforms the previous RL training frameworks among all evaluation scores. In
addition, our method can effectively ease the synonym problem and generate a
higher quality prediction. The source code is available at
\url{https://github.com/xuyige/FGRL4KG}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We find that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we propose a supervised
approach, which incorporates annotated pairs from natural language inference
datasets into our contrastive learning framework, by using &quot;entailment&quot; pairs
as positives and &quot;contradiction&quot; pairs as hard negatives. We evaluate SimCSE on
standard semantic textual similarity (STS) tasks, and our unsupervised and
supervised models using BERT base achieve an average of 76.3% and 81.6%
Spearman&#x27;s correlation respectively, a 4.2% and 2.2% improvement compared to
previous best results. We also show -- both theoretically and empirically --
that contrastive learning objective regularizes pre-trained embeddings&#x27;
anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08685">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Are pairs of words that tend to occur together also likely to stand in a
linguistic dependency? This empirical question is motivated by a long history
of literature in cognitive science, psycholinguistics, and NLP. In this work we
contribute an extensive analysis of the relationship between linguistic
dependencies and statistical dependence between words. Improving on previous
work, we introduce the use of large pretrained language models to compute
contextualized estimates of the pointwise mutual information between words
(CPMI). For multiple models and languages, we extract dependency trees which
maximize CPMI, and compare to gold standard linguistic dependencies. Overall,
we find that CPMI dependencies achieve an unlabelled undirected attachment
score of at most $\approx 0.5$. While far above chance, and consistently above
a non-contextualized PMI baseline, this score is generally comparable to a
simple baseline formed by connecting adjacent words. We analyze which kinds of
linguistic dependencies are best captured in CPMI dependencies, and also find
marked differences between the estimates of the large pretrained language
models, illustrating how their different training schemes affect the type of
dependencies they capture.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A health outcome is a measurement or an observation used to capture and
assess the effect of a treatment. Automatic detection of health outcomes from
text would undoubtedly speed up access to evidence necessary in healthcare
decision making. Prior work on outcome detection has modelled this task as
either (a) a sequence labelling task, where the goal is to detect which text
spans describe health outcomes, or (b) a classification task, where the goal is
to classify a text into a pre-defined set of categories depending on an outcome
that is mentioned somewhere in that text. However, this decoupling of span
detection and classification is problematic from a modelling perspective and
ignores global structural correspondences between sentence-level and word-level
information present in a given text. To address this, we propose a method that
uses both word-level and sentence-level information to simultaneously perform
outcome span detection and outcome type classification. In addition to
injecting contextual information to hidden vectors, we use label attention to
appropriately weight both word and sentence level information. Experimental
results on several benchmark datasets for health outcome detection show that
our proposed method consistently outperforms decoupled methods, reporting
competitive results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A possible explanation for the impressive performance of masked language
model (MLM) pre-training is that such models have learned to represent the
syntactic structures prevalent in classical NLP pipelines. In this paper, we
propose a different explanation: MLMs succeed on downstream tasks almost
entirely due to their ability to model higher-order word co-occurrence
statistics. To demonstrate this, we pre-train MLMs on sentences with randomly
shuffled word order, and show that these models still achieve high accuracy
after fine-tuning on many downstream tasks -- including on tasks specifically
designed to be challenging for models that ignore word order. Our models
perform surprisingly well according to some parametric syntactic probes,
indicating possible deficiencies in how we test representations for syntactic
information. Overall, our results show that purely distributional information
largely explains the success of pre-training, and underscore the importance of
curating challenging evaluation datasets that require deeper linguistic
knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04870">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Readability assessment is the task of determining how difficult or easy a
text is or which level/grade it has. Traditionally, language dependent
readability formula have been used, but these formulae take few text
characteristics into account. However, Natural Language Processing (NLP) tools
that assess the complexity of texts are able to measure more different features
and can be adapted to different languages. In this paper, we present the
MultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over
125 measures of cohesion,language, and readability for English, Spanish and
Basque, but whose architecture is designed to easily adapt other languages;
(ii) readability assessment classifiers that improve the performance of
Coh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a
web tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into
three reading levels (elementary, intermediate, and advanced) in English and
95.50 % in Basque and 90 % in Spanish when classifying into two reading levels
(simple and complex) using a SMO classifier. Using cross-lingual features,
MultiAzterTest also obtains competitive results above all in a complex vs
simple distinction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14388">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a novel training method, Conditional Masked Language
Modeling (CMLM), to effectively learn sentence representations on large scale
unlabeled corpora. CMLM integrates sentence representation learning into MLM
training by conditioning on the encoded vectors of adjacent sentences. Our
English CMLM model achieves state-of-the-art performance on SentEval, even
outperforming models learned using supervised signals. As a fully unsupervised
learning method, CMLM can be conveniently extended to a broad range of
languages and domains. We find that a multilingual CMLM model co-trained with
bitext retrieval (BR) and natural language inference (NLI) tasks outperforms
the previous state-of-the-art multilingual models by a large margin, e.g. 10%
improvement upon baseline models on cross-lingual semantic search. We explore
the same language bias of the learned representations, and propose a simple,
post-training and model agnostic approach to remove the language identifying
information from the representation while still retaining sentence semantics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08597">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cross-lingual named-entity lexica are an important resource to multilingual
NLP tasks such as machine translation and cross-lingual wikification. While
knowledge bases contain a large number of entities in high-resource languages
such as English and French, corresponding entities for lower-resource languages
are often missing. To address this, we propose Lexical-Semantic-Phonetic Align
(LSP-Align), a technique to automatically mine cross-lingual entity lexica from
mined web data. We demonstrate LSP-Align outperforms baselines at extracting
cross-lingual entity pairs and mine 164 million entity pairs from 120 different
languages aligned with English. We release these cross-lingual entity pairs
along with the massively multilingual tagged named entity corpus as a resource
to the NLP community.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04726">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural models for low-resource named entity recognition (NER) have shown
impressive results by leveraging distant super-vision or other meta-level
information (e.g. explanation). However, the costs of acquiring such additional
information are generally prohibitive, especially in domains where existing
resources (e.g. databases to be used for distant supervision) may not exist. In
this paper, we present a novel two-stage framework (AutoTriggER) to improve NER
performance by automatically generating and leveraging &quot;entity triggers&quot; which
are essentially human-readable clues in the text that can help guide the model
to make better decisions. Thus, the framework is able to both create and
leverage auxiliary supervision by itself. Through experiments on three
well-studied NER datasets, we show that our automatically extracted triggers
are well-matched to human triggers, and AutoTriggER improves performance over a
RoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a
low resource setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12738">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent advances in summarization provide models that can generate summaries
of higher quality. Such models now exist for a number of summarization tasks,
including query-based summarization, dialogue summarization, and multi-document
summarization. While such models and tasks are rapidly growing in the research
field, it has also become challenging for non-experts to keep track of them. To
make summarization methods more accessible to a wider audience, we develop
SummerTime by rethinking the summarization task from the perspective of an NLP
non-expert. SummerTime is a complete toolkit for text summarization, including
various models, datasets and evaluation metrics, for a full spectrum of
summarization-related tasks. SummerTime integrates with libraries designed for
NLP researchers, and enables users with easy-to-use APIs. With SummerTime,
users can locate pipeline solutions and search for the best model with their
own data, and visualize the differences, all with a few lines of code. We also
provide explanations for models and evaluation metrics to help users understand
the model behaviors and select models that best suit their needs. Our library,
along with a notebook demo, is available at
https://github.com/Yale-LILY/SummerTime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model&#x27;s decision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04620">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current approaches to incorporating terminology constraints in machine
translation (MT) typically assume that the constraint terms are provided in
their correct morphological forms. This limits their application to real-world
scenarios where constraint terms are provided as lemmas. In this paper, we
introduce a modular framework for incorporating lemma constraints in neural MT
(NMT) in which linguistic knowledge and diverse types of NMT models can be
flexibly applied. It is based on a novel cross-lingual inflection module that
inflects the target lemma constraints based on the source context. We explore
linguistically motivated rule-based and data-driven neural-based inflection
modules and design English-German health and English-Lithuanian news test
suites to evaluate them in domain adaptation and low-resource MT settings.
Results show that our rule-based inflection module helps NMT models incorporate
lemma constraints more accurately than a neural module and outperforms the
existing end-to-end approach with lower training costs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Labeling data can be an expensive task as it is usually performed manually by
domain experts. This is cumbersome for deep learning, as it is dependent on
large labeled datasets. Active learning (AL) is a paradigm that aims to reduce
labeling effort by only using the data which the used model deems most
informative. Little research has been done on AL in a text classification
setting and next to none has involved the more recent, state-of-the-art NLP
models. Here, we present an empirical study that compares different
uncertainty-based algorithms with BERT$_{base}$ as the used classifier. We
evaluate the algorithms on two NLP classification datasets: Stanford Sentiment
Treebank and KvK-Frontpages. Additionally, we explore heuristics that aim to
solve presupposed problems of uncertainty-based AL; namely, that it is
unscalable and that it is prone to selecting outliers. Furthermore, we explore
the influence of the query-pool size on the performance of AL. Whereas it was
found that the proposed heuristics for AL did not improve performance of AL;
our results show that using uncertainty-based AL with BERT$_{base}$ outperforms
random sampling of data. This difference in performance can decrease as the
query-pool size gets larger.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05669">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Isolated silos of scientific research and the growing challenge of
information overload limit awareness across the literature and hinder
innovation. Algorithmic curation and recommendation, which often prioritize
relevance, can further reinforce these informational &quot;filter bubbles.&quot; In
response, we describe Bridger, a system for facilitating discovery of scholars
and their work, to explore design tradeoffs between relevant and novel
recommendations. We construct a faceted representation of authors with
information gleaned from their papers and inferred author personas, and use it
to develop an approach that locates commonalities (&quot;bridges&quot;) and contrasts
between scientists -- retrieving partially similar authors rather than aiming
for strict similarity. In studies with computer science researchers, this
approach helps users discover authors considered useful for generating novel
research directions, outperforming a state-of-art neural model. In addition to
recommending new content, we also demonstrate an approach for displaying it in
a manner that boosts researchers&#x27; ability to understand the work of authors
with whom they are unfamiliar. Finally, our analysis reveals that Bridger
connects authors who have different citation profiles, publish in different
venues, and are more distant in social co-authorship networks, raising the
prospect of bridging diverse communities and facilitating discovery.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04655">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Zero-shot transfer learning for dialogue state tracking (DST) enables us to
handle a variety of task-oriented dialogue domains without the expense of
collecting in-domain data. In this work, we propose to transfer the
\textit{cross-task} knowledge from general question answering (QA) corpora for
the zero-shot DST task. Specifically, we propose TransferQA, a transferable
generative QA model that seamlessly combines extractive QA and multi-choice QA
via a text-to-text transformer framework, and tracks both categorical slots and
non-categorical slots in DST. In addition, we introduce two effective ways to
construct unanswerable questions, namely, negative question sampling and
context truncation, which enable our model to handle &quot;none&quot; value slots in the
zero-shot DST setting. The extensive experiments show that our approaches
substantially improve the existing zero-shot and few-shot results on MultiWoz.
Moreover, compared to the fully trained baseline on the Schema-Guided Dialogue
dataset, our approach shows better generalization ability in unseen domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04703">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The encoder-decoder framework achieves state-of-the-art results in keyphrase
generation (KG) tasks by predicting both present keyphrases that appear in the
source document and absent keyphrases that do not. However, relying solely on
the source document can result in generating uncontrollable and inaccurate
absent keyphrases. To address these problems, we propose a novel graph-based
method that can capture explicit knowledge from related references. Our model
first retrieves some document-keyphrases pairs similar to the source document
from a pre-defined index as references. Then a heterogeneous graph is
constructed to capture relationships of different granularities between the
source document and its references. To guide the decoding process, a
hierarchical attention and copy mechanism is introduced, which directly copies
appropriate words from both the source document and its references based on
their relevance and significance. The experimental results on multiple KG
benchmarks show that the proposed model achieves significant improvements
against other baseline models, especially with regard to the absent keyphrase
prediction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12800">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.
Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,
Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from
55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query
Bank and Relevance Set, where the former contains 1,236 human-paraphrased
queries while the latter contains ~32 human-annotated FAQ items for each query.
We analyze COUGH by testing different FAQ retrieval models built on top of BM25
and BERT, among which the best model achieves 48.8 under P@5, indicating a
great challenge presented by COUGH and encouraging future research for further
improvement. Our COUGH dataset is available at
https://github.com/sunlab-osu/covid-faq.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Word embeddings are powerful representations that form the foundation of many
natural language processing architectures, both in English and in other
languages. To gain further insight into word embeddings, we explore their
stability (e.g., overlap between the nearest neighbors of a word in different
embedding spaces) in diverse languages. We discuss linguistic properties that
are related to stability, drawing out insights about correlations with
affixing, language gender systems, and other features. This has implications
for embedding use, particularly in research that uses them to study language
trends.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent work has demonstrated that pre-training in-domain language models can
boost performance when adapting to a new domain. However, the costs associated
with pre-training raise an important question: given a fixed budget, what steps
should an NLP practitioner take to maximize performance? In this paper, we
study domain adaptation under budget constraints, and approach it as a customer
choice problem between data annotation and pre-training. Specifically, we
measure the annotation cost of three procedural text datasets and the
pre-training cost of three in-domain language models. Then we evaluate the
utility of different combinations of pre-training and data annotation under
varying budget constraints to assess which combination strategy works best. We
find that, for small budgets, spending all funds on annotation leads to the
best performance; once the budget becomes large enough, a combination of data
annotation and in-domain pre-training works more optimally. We therefore
suggest that task-specific data annotation should be part of an economical
strategy when adapting an NLP model to a new domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04810">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Infusing factual knowledge into pre-trained models is fundamental for many
knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions
(MoP), an infusion approach that can handle a very large knowledge graph (KG)
by partitioning it into smaller sub-graphs and infusing their specific
knowledge into various BERT models using lightweight adapters. To leverage the
overall factual knowledge for a target task, these sub-graph adapters are
further fine-tuned along with the underlying BERT through a mixture layer. We
evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on
six downstream tasks (inc. NLI, QA, Classification), and the results show that
our MoP consistently enhances the underlying BERTs in task performance, and
achieves new SOTA performances on five evaluated datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09574">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans use commonsense reasoning (CSR) implicitly to produce natural and
coherent responses in conversations. Aiming to close the gap between current
response generation (RG) models and human communication abilities, we want to
understand why RG models respond as they do by probing RG model&#x27;s understanding
of commonsense reasoning that elicits proper responses. We formalize the
problem by framing commonsense as a latent variable in the RG task and using
explanations for responses as textual form of commonsense. We collect 6k
annotated explanations justifying responses from four dialogue datasets and ask
humans to verify them and propose two probing settings to evaluate RG models&#x27;
CSR capabilities. Probing results show that models fail to capture the logical
relations between commonsense explanations and responses and fine-tuning on
in-domain data and increasing model sizes do not lead to understanding of CSR
for RG. We hope our study motivates more research in making RG models emulate
the human reasoning process in pursuit of smooth human-AI communication.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07425">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Masked language models (MLMs) have contributed to drastic performance
improvements with regard to zero anaphora resolution (ZAR). To further improve
this approach, in this study, we made two proposals. The first is a new
pretraining task that trains MLMs on anaphoric relations with explicit
supervision, and the second proposal is a new finetuning method that remedies a
notorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese
ZAR demonstrated that our two proposals boost the state-of-the-art performance,
and our detailed analysis provides new insights on the remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04867">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As neural language models approach human performance on NLP benchmark tasks,
their advances are widely seen as evidence of an increasingly complex
understanding of syntax. This view rests upon a hypothesis that has not yet
been empirically tested: that word order encodes meaning essential to
performing these tasks. We refute this hypothesis in many cases: in the GLUE
suite and in various genres of English text, the words in a sentence or phrase
can rarely be permuted to form a phrase carrying substantially different
information. Our surprising result relies on inference by iterative shuffling
(IBIS), a novel, efficient procedure that finds the ordering of a bag of words
having the highest likelihood under a fixed language model. IBIS can use any
black-box model without additional training and is superior to existing word
ordering algorithms. Coalescing our findings, we discuss how shuffling
inference procedures such as IBIS can benefit language modeling and constrained
generation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04727">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Language agnostic and semantic-language information isolation is an emerging
research direction for multilingual representations models. We explore this
problem from a novel angle of geometric algebra and semantic space. A simple
but highly effective method &quot;Language Information Removal (LIR)&quot; factors out
language identity information from semantic related components in multilingual
representations pre-trained on multi-monolingual data. A post-training and
model-agnostic method, LIR only uses simple linear operations, e.g. matrix
factorization and orthogonal projection. LIR reveals that for weak-alignment
multilingual systems, the principal components of semantic spaces primarily
encodes language identity information. We first evaluate the LIR on a
cross-lingual question answer retrieval task (LAReQA), which requires the
strong alignment for the multilingual embedding space. Experiment shows that
LIR is highly effectively on this task, yielding almost 100% relative
improvement in MAP for weak-alignment models. We then evaluate the LIR on
Amazon Reviews and XEVAL dataset, with the observation that removing language
information is able to improve the cross-lingual transfer performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04650">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>GPT-3 shows remarkable in-context learning ability of large-scale language
models (LMs) trained on hundreds of billion scale data. Here we address some
remaining issues less reported by the GPT-3 paper, such as a non-English LM,
the performances of different sized models, and the effect of recently
introduced prompt optimization on in-context learning. To achieve this, we
introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric
corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA
with our training configuration shows state-of-the-art in-context zero-shot and
few-shot learning performances on various downstream tasks in Korean. Also, we
show the performance benefits of prompt-based learning and demonstrate how it
can be integrated into the prompt engineering pipeline. Then we discuss the
possibility of materializing the No Code AI paradigm by providing AI
prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,
an interactive prompt engineering interface. Lastly, we demonstrate the
potential of our methods with three successful in-house applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04705">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Zero-shot translation, directly translating between language pairs unseen in
training, is a promising capability of multilingual neural machine translation
(NMT). However, it usually suffers from capturing spurious correlations between
the output language and language invariant semantics due to the maximum
likelihood training objective, leading to poor transfer performance on
zero-shot translation. In this paper, we introduce a denoising autoencoder
objective based on pivot language into traditional training objective to
improve the translation accuracy on zero-shot directions. The theoretical
analysis from the perspective of latent variables shows that our approach
actually implicitly maximizes the probability distributions for zero-shot
directions. On two benchmark machine translation datasets, we demonstrate that
the proposed method is able to effectively eliminate the spurious correlations
and significantly outperforms state-of-the-art methods with a remarkable
performance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04652">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Natural language relies on a finite lexicon to express an unbounded set of
emerging ideas. One result of this tension is the formation of new
compositions, such that existing linguistic units can be combined with emerging
items into novel expressions. We develop a framework that exploits the
cognitive mechanisms of chaining and multimodal knowledge to predict emergent
compositional expressions through time. We present the syntactic frame
extension model (SFEM) that draws on the theory of chaining and knowledge from
&quot;percept&quot;, &quot;concept&quot;, and &quot;language&quot; to infer how verbs extend their frames to
form new compositions with existing and novel nouns. We evaluate SFEM
rigorously on the 1) modalities of knowledge and 2) categorization models of
chaining, in a syntactically parsed English corpus over the past 150 years. We
show that multimodal SFEM predicts newly emerged verb syntax and arguments
substantially better than competing models using purely linguistic or unimodal
knowledge. We find support for an exemplar view of chaining as opposed to a
prototype view and reveal how the joint approach of multimodal chaining may be
fundamental to the creation of literal and figurative language uses including
metaphor and metonymy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04500">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for
semantic parsing in task-oriented dialog. Our model consists of an encoder
network that incrementally builds the semantic parse tree by predicting the
non-terminal label and its positions in the linearized tree. At the generation
time, the model constructs the semantic parse tree by recursively inserting the
predicted non-terminal labels at the predicted positions until termination.
RINE achieves state-of-the-art exact match accuracy on low- and high-resource
versions of the conversational semantic parsing benchmark TOP (Gupta et al.,
2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and
transition-based parsers. We also show that our model design is applicable to
nested named entity recognition task, where it performs on par with
state-of-the-art approach designed for that task. Finally, we demonstrate that
our approach is 2-3.5 times faster than the sequence-to-sequence model at
inference time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05094">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive Learning has emerged as a powerful representation learning method
and facilitates various downstream tasks especially when supervised data is
limited. How to construct efficient contrastive samples through data
augmentation is key to its success. Unlike vision tasks, the data augmentation
method for contrastive learning has not been investigated sufficiently in
language tasks. In this paper, we propose a novel approach to construct
contrastive samples for language tasks using text summarization. We use these
samples for supervised contrastive learning to gain better text representations
which greatly benefit text classification tasks with limited annotations. To
further improve the method, we mix up samples from different classes and add an
extra regularization, named Mixsum, in addition to the cross-entropy-loss.
Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG
News, and IMDb) demonstrate the effectiveness of the proposed contrastive
learning framework with summarization-based data augmentation and Mixsum
regularization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12251">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Natural Language Understanding (NLU) is an established component within a
conversational AI or digital assistant system, and it is responsible for
producing semantic understanding of a user request. We propose a scalable and
automatic approach for improving NLU in a large-scale conversational AI system
by leveraging implicit user feedback, with an insight that user interaction
data and dialog context have rich information embedded from which user
satisfaction and intention can be inferred. In particular, we propose a general
domain-agnostic framework for curating new supervision data for improving NLU
from live production traffic. With an extensive set of experiments, we show the
results of applying the framework and improving NLU for a large-scale
production system and show its impact across 10 domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04853">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with
hierarchical label spaces, such as automatic assignment of ICD-9 codes to
discharge summaries. Performance of models in prior art is evaluated with
standard precision, recall, and F1 measures without regard for the rich
hierarchical structure. In this work we argue for hierarchical evaluation of
the predictions of neural LMTC models. With the example of the ICD-9 ontology
we describe a structural issue in the representation of the structured label
space in prior art, and propose an alternative representation based on the
depth of the ontology. We propose a set of metrics for hierarchical evaluation
using the depth-based representation. We compare the evaluation scores from the
proposed metrics with previously used metrics on prior art LMTC models for
ICD-9 coding in MIMIC-III. We also propose further avenues of research
involving the proposed ontological representation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04562">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human conversations naturally evolve around different topics and fluently
move between them. In research on dialog systems, the ability to actively and
smoothly transition to new topics is often ignored. In this paper we introduce
TIAGE, a new topic-shift aware dialog benchmark constructed utilizing human
annotations on topic shifts. Based on TIAGE, we introduce three tasks to
investigate different scenarios of topic-shift modeling in dialog settings:
topic-shift detection, topic-shift triggered response generation and
topic-aware dialog generation. Experiments on these tasks show that the
topic-shift signals in TIAGE are useful for topic-shift response generation. On
the other hand, dialog systems still struggle to decide when to change topic.
This indicates further research is needed in topic-shift aware dialog modeling.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15710">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers (Vaswani et al., 2017) have brought a remarkable improvement in
the performance of neural machine translation (NMT) systems but they could be
surprisingly vulnerable to noise. In this work, we try to investigate how noise
breaks Transformers and if there exist solutions to deal with such issues.
There is a large body of work in the NMT literature on analyzing the behavior
of conventional models for the problem of noise but Transformers are relatively
understudied in this context. Motivated by this, we introduce a novel
data-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate
noise during training. This idea is comparable to the well-known fine-tuning
strategy. Moreover, we propose two other novel extensions to the original
Transformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that
modify the neural architecture as well as the training process to handle noise.
One important characteristic of our techniques is that they only impact the
training phase and do not impose any overhead at inference time. We evaluated
our techniques to translate the English--German pair in both directions and
observed that our models have a higher tolerance to noise. More specifically,
they perform with no deterioration where up to 10% of entire test words are
infected by noise.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04834">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-turn response selection models have recently shown comparable
performance to humans in several benchmark datasets. However, in the real
environment, these models often have weaknesses, such as making incorrect
predictions based heavily on superficial patterns without a comprehensive
understanding of the context. For example, these models often give a high score
to the wrong response candidate containing several keywords related to the
context but using the inconsistent tense. In this study, we analyze the
weaknesses of the open-domain Korean Multi-turn response selection models and
publish an adversarial dataset to evaluate these weaknesses. We also suggest a
strategy to build a robust model in this adversarial environment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we examine different meaning representations that are commonly
used in different natural language applications today and discuss their limits,
both in terms of the aspects of the natural language meaning they are modelling
and in terms of the aspects of the application for which they are used.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text classification systems based on contextual embeddings are not viable
options for many of the low resource languages. On the other hand, recently
introduced capsule networks have shown performance in par with these text
classification models. Thus, they could be considered as a viable alternative
for text classification for languages that do not have pre-trained contextual
embedding models. However, current capsule networks depend upon spatial
patterns without considering the sequential features of the text. They are also
sub-optimal in capturing the context-level information in longer sequences.
This paper presents a novel Dual-State Capsule (DS-Caps) network-based
technique for text classification, which is optimized to mitigate these issues.
Two varieties of states, namely sentence-level and word-level, are integrated
with capsule layers to capture deeper context-level information for language
modeling. The dynamic routing process among capsules was also optimized using
the context-level information obtained through sentence-level states. The
DS-Caps networks outperform the existing capsule network architectures for
multiple datasets, particularly for tasks with longer sequences of text. We
also demonstrate the superiority of DS-Caps in text classification for a low
resource language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04732">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Various measures have been proposed to quantify human-like social biases in
word embeddings. However, bias scores based on these measures can suffer from
measurement error. One indication of measurement quality is reliability,
concerning the extent to which a measure produces consistent results. In this
paper, we assess three types of reliability of word embedding gender bias
measures, namely test-retest reliability, inter-rater consistency and internal
consistency. Specifically, we investigate the consistency of bias scores across
different choices of random seeds, scoring rules and words. Furthermore, we
analyse the effects of various factors on these measures&#x27; reliability scores.
Our findings inform better design of word embedding gender bias measures.
Moreover, we urge researchers to be more critical about the application of such
measures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04552">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Selective rationalization aims to produce decisions along with rationales
(e.g., text highlights or word alignments between two sentences). Commonly,
rationales are modeled as stochastic binary masks, requiring sampling-based
gradient estimators, which complicates training and requires careful
hyperparameter tuning. Sparse attention mechanisms are a deterministic
alternative, but they lack a way to regularize the rationale extraction (e.g.,
to control the sparsity of a text highlight or the number of alignments). In
this paper, we present a unified framework for deterministic extraction of
structured explanations via constrained inference on a factor graph, forming a
differentiable layer. Our approach greatly eases training and rationale
regularization, generally outperforming previous work on what comes to
performance and plausibility of the extracted rationales. We further provide a
comparative study of stochastic and deterministic methods for rationale
extraction for classification and natural language inference tasks, jointly
assessing their predictive power, quality of the explanations, and model
variability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08247">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Intermediate task fine-tuning has been shown to culminate in large transfer
gains across many NLP tasks. With an abundance of candidate datasets as well as
pre-trained language models, it has become infeasible to run the cross-product
of all combinations to find the best transfer setting. In this work we first
establish that similar sequential fine-tuning gains can be achieved in adapter
settings, and subsequently consolidate previously proposed methods that
efficiently identify beneficial tasks for intermediate transfer learning. We
experiment with a diverse set of 42 intermediate and 11 target English
classification, multiple choice, question answering, and sequence tagging
tasks. Our results show that efficient embedding based methods that rely solely
on the respective datasets outperform computational expensive few-shot
fine-tuning approaches. Our best methods achieve an average Regret@3 of less
than 1% across all target tasks, demonstrating that we are able to efficiently
identify the best datasets for intermediate training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-trained multilingual language encoders, such as multilingual BERT and
XLM-R, show great potential for zero-shot cross-lingual transfer. However,
these multilingual encoders do not precisely align words and phrases across
languages. Especially, learning alignments in the multilingual embedding space
usually requires sentence-level or word-level parallel corpora, which are
expensive to be obtained for low-resource languages. An alternative is to make
the multilingual encoders more robust; when fine-tuning the encoder using
downstream task, we train the encoder to tolerate noise in the contextual
embedding spaces such that even if the representations of different languages
are not aligned well, the model can still achieve good performance on zero-shot
cross-lingual transfer. In this work, we propose a learning strategy for
training robust models by drawing connections between adversarial examples and
the failure cases of zero-shot cross-lingual transfer. We adopt two widely used
robust training methods, adversarial training and randomized smoothing, to
train the desired robust model. The experimental results demonstrate that
robust training improves zero-shot cross-lingual transfer on text
classification tasks. The improvement is more significant in the generalized
cross-lingual transfer setting, where the pair of input sentences belong to two
different languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07637">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Natural languages display a trade-off among different strategies to convey
syntactic structure, such as word order or inflection. This trade-off, however,
has not appeared in recent simulations of iterated language learning with
neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in
light of three factors that play an important role in comparable experiments
from the Language Evolution field: (i) speaker bias towards efficient
messaging, (ii) non systematic input languages, and (iii) learning bottleneck.
Our simulations show that neural agents mainly strive to maintain the utterance
type distribution observed during learning, instead of developing a more
efficient or systematic language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15562">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Massively multilingual language models such as multilingual BERT offer
state-of-the-art cross-lingual transfer performance on a range of NLP tasks.
However, due to limited capacity and large differences in pretraining data
sizes, there is a profound performance gap between resource-rich and
resource-poor target languages. The ultimate challenge is dealing with
under-resourced languages not covered at all by the models and written in
scripts unseen during pretraining. In this work, we propose a series of novel
data-efficient methods that enable quick and effective adaptation of pretrained
multilingual models to such low-resource languages and unseen scripts. Relying
on matrix factorization, our methods capitalize on the existing latent
knowledge about multiple languages already available in the pretrained model&#x27;s
embedding matrix. Furthermore, we show that learning of the new dedicated
embedding matrix in the target language can be improved by leveraging a small
number of vocabulary items (i.e., the so-called lexically overlapping tokens)
shared between mBERT&#x27;s and target language vocabulary. Our adaptation
techniques offer substantial performance gains for languages with unseen
scripts. We also demonstrate that they can yield improvements for low-resource
languages written in scripts covered by the pretrained model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06979">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning sentence embeddings often requires a large amount of labeled data.
However, for most tasks and domains, labeled data is seldom available and
creating it is expensive. In this work, we present a new state-of-the-art
unsupervised method based on pre-trained Transformers and Sequential Denoising
Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.
It can achieve up to 93.1% of the performance of in-domain supervised
approaches. Further, we show that TSDAE is a strong domain adaptation and
pre-training method for sentence embeddings, significantly outperforming other
approaches like Masked Language Model.

A crucial shortcoming of previous studies is the narrow evaluation: Most work
mainly evaluates on the single task of Semantic Textual Similarity (STS), which
does not require any domain knowledge. It is unclear if these proposed methods
generalize to other domains and tasks. We fill this gap and evaluate TSDAE and
other recent approaches on four different datasets from heterogeneous domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such &quot;saturated&quot; networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04611">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer-based rankers have shown state-of-the-art performance. However,
their self-attention operation is mostly unable to process long sequences. One
of the common approaches to train these rankers is to heuristically select some
segments of each document, such as the first segment, as training data.
However, these segments may not contain the query-related parts of documents.
To address this problem, we propose query-driven segment selection from long
documents to build training data. The segment selector provides relevant
samples with more accurate labels and non-relevant samples which are harder to
be predicted. The experimental results show that the basic BERT-based ranker
trained with the proposed segment selector significantly outperforms that
trained by the heuristically selected segments, and performs equally to the
state-of-the-art model with localized self-attention that can process longer
input sequences. Our findings open up new direction to design efficient
transformer-based rankers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural Machine Translation models are sensitive to noise in the input texts,
such as misspelled words and ungrammatical constructions. Existing robustness
techniques generally fail when faced with unseen types of noise and their
performance degrades on clean texts. In this paper, we focus on three types of
realistic noise that are commonly generated by humans and introduce the idea of
visual context to improve translation robustness for noisy texts. In addition,
we describe a novel error correction training regime that can be used as an
auxiliary task to further improve translation robustness. Experiments on
English-French and English-German translation show that both multimodal and
error correction components improve model robustness to noisy texts, while
still retaining translation quality on clean texts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03432">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Concept-to-text Natural Language Generation is the task of expressing an
input meaning representation in natural language. Previous approaches in this
task have been able to generalise to rare or unseen instances by relying on a
delexicalisation of the input. However, this often requires that the input
appears verbatim in the output text. This poses challenges in multilingual
settings, where the task expands to generate the output text in multiple
languages given the same input. In this paper, we explore the application of
multilingual models in concept-to-text and propose Language Agnostic
Delexicalisation, a novel delexicalisation method that uses multilingual
pretrained embeddings, and employs a character-level post-editing model to
inflect words in their correct form during relexicalisation. Our experiments
across five datasets and five languages show that multilingual models
outperform monolingual models in concept-to-text and that our framework
outperforms previous approaches, especially for low resource languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a parameter sharing method for Transformers (Vaswani et al.,
2017). The proposed approach relaxes a widely used technique, which shares
parameters for one layer with all layers such as Universal Transformers
(Dehghani et al., 2019), to increase the efficiency in the computational time.
We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign
parameters to each layer. Experimental results show that the proposed
strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the
configuration where we use many training data such as the recent WMT
competition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11601">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Software developers write a lot of source code and documentation during
software development. Intrinsically, developers often recall parts of source
code or code summaries that they had written in the past while implementing
software or documenting them. To mimic developers&#x27; code or summary generation
behavior, we propose a retrieval augmented framework, REDCODER, that retrieves
relevant code or summaries from a retrieval database and provides them as a
supplement to code generation or summarization models. REDCODER has a couple of
uniqueness. First, it extends the state-of-the-art dense retrieval technique to
search for relevant code or summaries. Second, it can work with retrieval
databases that include unimodal (only code or natural language description) or
bimodal instances (code-description pairs). We conduct experiments and
extensive analysis on two benchmark datasets of code generation and
summarization in Java and Python, and the promising results endorse the
effectiveness of our proposed retrieval augmented framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">&quot;Wikily&quot; Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08384">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a simple but effective approach for leveraging Wikipedia for
neural machine translation as well as cross-lingual tasks of image captioning
and dependency parsing without using any direct supervision from external
parallel data or supervised models in the target language. We show that first
sentences and titles of linked Wikipedia pages, as well as cross-lingual image
captions, are strong signals for a seed parallel data to extract bilingual
dictionaries and cross-lingual word embeddings for mining parallel text from
Wikipedia. Our final model achieves high BLEU scores that are close to or
sometimes higher than strong supervised baselines in low-resource languages;
e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.
Moreover, we tailor our wikily supervised translation models to unsupervised
image captioning, and cross-lingual dependency parser transfer. In image
captioning, we train a multi-tasking machine translation and image captioning
pipeline for Arabic and English from which the Arabic training data is a
translated version of the English captioning data, using our wikily-supervised
translation models. Our captioning results on Arabic are slightly better than
that of its supervised model. In dependency parsing, we translate a large
amount of monolingual text, and use it as artificial training data in an
annotation projection framework. We show that our model outperforms recent work
on cross-lingual transfer of dependency parsers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07971">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12971">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A smart contract is a program executed on a blockchain, based on which many
cryptocurrencies are implemented, and is being used for automating
transactions. Due to the large amount of money that smart contracts deal with,
there is a surging demand for a method that can statically and formally verify
them.

This article describes our type-based static verification tool HELMHOLTZ for
Michelson, which is a statically typed stack-based language for writing smart
contracts that are executed on the blockchain platform Tezos. HELMHOLTZ is
designed on top of our extension of Michelson&#x27;s type system with refinement
types. HELMHOLTZ takes a Michelson program annotated with a user-defined
specification written in the form of a refinement type as input; it then
typechecks the program against the specification based on the refinement type
system, discharging the generated verification conditions with the SMT solver
Z3. We briefly introduce our refinement type system for the core calculus
Mini-Michelson of Michelson, which incorporates the characteristic features
such as compound datatypes (e.g., lists and pairs), higher-order functions, and
invocation of another contract. \HELMHOLTZ{} successfully verifies several
practical Michelson programs, including one that transfers money to an account
and that checks a digital signature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07497">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Codifying commonsense knowledge in machines is a longstanding goal of
artificial intelligence. Recently, much progress toward this goal has been made
with automatic knowledge base (KB) construction techniques. However, such
techniques focus primarily on the acquisition of positive (true) KB statements,
even though negative (false) statements are often also important for
discriminative reasoning over commonsense KBs. As a first step toward the
latter, this paper proposes NegatER, a framework that ranks potential negatives
in commonsense KBs using a contextual language model (LM). Importantly, as most
KBs do not contain negatives, NegatER relies only on the positive knowledge in
the LM and does not require ground-truth negative examples. Experiments
demonstrate that, compared to multiple contrastive data augmentation
approaches, NegatER yields negatives that are more grammatical, coherent, and
informative -- leading to statistically significant accuracy improvements in a
challenging KB completion task and confirming that the positive knowledge in
LMs can be &quot;re-purposed&quot; to generate negative knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Keeping track of scientific challenges, advances and emerging directions is a
fundamental part of research. However, researchers face a flood of papers that
hinders discovery of important knowledge. In biomedicine, this directly impacts
human lives. To address this problem, we present a novel task of extraction and
search of scientific challenges and directions, to facilitate rapid knowledge
discovery. We construct and release an expert-annotated corpus of texts sampled
from full-length papers, labeled with novel semantic categories that generalize
across many types of challenges and directions. We focus on a large corpus of
interdisciplinary work relating to the COVID-19 pandemic, ranging from
biomedicine to areas such as AI and economics. We apply a model trained on our
data to identify challenges and directions across the corpus and build a
dedicated search engine. In experiments with 19 researchers and clinicians
using our system, we outperform a popular scientific search engine in assisting
knowledge discovery. Finally, we show that models trained on our resource
generalize to the wider biomedical domain and to AI papers, highlighting its
broad utility. We make our data, model and search engine publicly available.
https://challenges.apps.allenai.org/</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04919">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The ability to recognise emotions lends a conversational artificial
intelligence a human touch. While emotions in chit-chat dialogues have received
substantial attention, emotions in task-oriented dialogues have been largely
overlooked despite having an equally important role, such as to signal failure
or success. Existing emotion-annotated task-oriented corpora are limited in
size, label richness, and public availability, creating a bottleneck for
downstream tasks. To lay a foundation for studies on emotions in task-oriented
dialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus
of task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain
task-oriented dialogue dataset. It contains more than 11K dialogues with more
than 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz
dialogues from MultiWOZ, we collect human-machine dialogues within the same set
of domains to sufficiently cover the space of various emotions that can happen
during the lifetime of a data-driven dialogue system. To the best of our
knowledge, this is the first large-scale open-source corpus of its kind. We
propose a novel emotion labelling scheme, which is tailored to task-oriented
dialogues. We report a set of experimental results to show the usability of
this corpus for emotion recognition and state tracking in task-oriented
dialogues.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Relational knowledge bases (KBs) are commonly used to represent world
knowledge in machines. However, while advantageous for their high degree of
precision and interpretability, KBs are usually organized according to
manually-defined schemas, which limit their expressiveness and require
significant human efforts to engineer and maintain. In this review, we take a
natural language processing perspective to these limitations, examining how
they may be addressed in part by training deep contextual language models (LMs)
to internalize and express relational knowledge in more flexible forms. We
propose to organize knowledge representation strategies in LMs by the level of
KB supervision provided, from no KB supervision at all to entity- and
relation-level supervision. Our contributions are threefold: (1) We provide a
high-level, extensible taxonomy for knowledge representation in LMs; (2) Within
our taxonomy, we highlight notable models, evaluation tasks, and findings, in
order to provide an up-to-date review of current knowledge representation
capabilities in LMs; and (3) We suggest future research directions that build
upon the complementary aspects of LMs and KBs as knowledge representations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01192">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce Generative Spoken Language Modeling, the task of learning the
acoustic and linguistic characteristics of a language from raw audio (no text,
no labels), and a set of metrics to automatically evaluate the learned
representations at acoustic and linguistic levels for both encoding and
generation. We set up baseline systems consisting of a discrete speech encoder
(returning pseudo-text units), a generative language model (trained on
pseudo-text), and a speech decoder (generating a waveform from pseudo-text) all
trained without supervision and validate the proposed metrics with human
evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that
the number of discrete units (50, 100, or 200) matters in a task-dependent and
encoder-dependent way, and that some combinations approach text-based systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04997">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A major factor contributing to the success of modern representation learning
is the ease of performing various vector operations. Recently, objects with
geometric structures (eg. distributions, complex or hyperbolic vectors, or
regions such as cones, disks, or boxes) have been explored for their
alternative inductive biases and additional representational capacities. In
this work, we introduce Box Embeddings, a Python library that enables
researchers to easily apply and extend probabilistic box embeddings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04953">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretraining techniques leveraging enormous datasets have driven recent
advances in text summarization. While folk explanations suggest that knowledge
transfer accounts for pretraining&#x27;s benefits, little is known about why it
works or what makes a pretraining task or dataset suitable. In this paper, we
challenge the knowledge transfer story, showing that pretraining on documents
consisting of character n-grams selected at random, we can nearly match the
performance of models pretrained on real corpora. This work holds the promise
of eliminating upstream corpora, which may alleviate some concerns over
offensive language, bias, and copyright issues. To see whether the small
residual benefit of using real data could be accounted for by the structure of
the pretraining task, we design several tasks motivated by a qualitative study
of summarization corpora. However, these tasks confer no appreciable benefit,
leaving open the possibility of a small role for knowledge transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15781">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Influence functions approximate the &quot;influences&quot; of training data-points for
test predictions and have a wide variety of applications. Despite the
popularity, their computational cost does not scale well with model and
training data size. We present FastIF, a set of simple modifications to
influence functions that significantly improves their run-time. We use
k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good
candidate data points, identify the configurations that best balance the
speed-quality trade-off in estimating the inverse Hessian-vector product, and
introduce a fast parallel variant. Our proposed method achieves about 80X
speedup while being highly correlated with the original influence values. With
the availability of the fast influence functions, we demonstrate their
usefulness in four applications. First, we examine whether influential
data-points can &quot;explain&quot; test time behavior using the framework of
simulatability. Second, we visualize the influence interactions between
training and test data-points. Third, we show that we can correct model errors
by additional fine-tuning on certain influential data-points, improving the
accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we
experiment with a similar setup but fine-tuning on datapoints not seen during
training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI
datasets respectively. Overall, our fast influence functions can be efficiently
applied to large models and datasets, and our experiments demonstrate the
potential of influence functions in model interpretation and correcting model
errors. Code is available at
https://github.com/salesforce/fast-influence-functions</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.14658">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The Transformer translation model is based on the multi-head attention
mechanism, which can be parallelized easily. The multi-head attention network
performs the scaled dot-product attention function in parallel, empowering the
model by jointly attending to information from different representation
subspaces at different positions. In this paper, we present an approach to
learning a hard retrieval attention where an attention head only attends to one
token in the sentence rather than all tokens. The matrix multiplication between
attention probabilities and the value sequence in the standard scaled
dot-product attention can thus be replaced by a simple and efficient retrieval
operation. We show that our hard retrieval attention mechanism is 1.43 times
faster in decoding, while preserving translation quality on a wide range of
machine translation tasks when used in the decoder self- and cross-attention
networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04707">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Meta-learning has achieved great success in leveraging the historical learned
knowledge to facilitate the learning process of the new task. However, merely
learning the knowledge from the historical tasks, adopted by current
meta-learning algorithms, may not generalize well to testing tasks when they
are not well-supported by training tasks. This paper studies a low-resource
text classification problem and bridges the gap between meta-training and
meta-testing tasks by leveraging the external knowledge bases. Specifically, we
propose KGML to introduce additional representation for each sentence learned
from the extracted sentence-specific knowledge graph. The extensive experiments
on three datasets demonstrate the effectiveness of KGML under both supervised
adaptation and unsupervised adaptation settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04574">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based models have gained increasing popularity achieving
state-of-the-art performance in many research fields including speech
translation. However, Transformer&#x27;s quadratic complexity with respect to the
input sequence length prevents its adoption as is with audio signals, which are
typically represented by long sequences. Current solutions resort to an initial
sub-optimal compression based on a fixed sampling of raw audio features.
Therefore, potentially useful linguistic information is not accessible to
higher-level layers in the architecture. To solve this issue, we propose
Speechformer, an architecture that, thanks to reduced memory usage in the
attention layers, avoids the initial lossy compression and aggregates
information only at a higher level according to more informed linguistic
criteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy
of our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and
of up to 4.0 BLEU in a low resource scenario.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We show that Transformer encoder architectures can be sped up, with limited
accuracy costs, by replacing the self-attention sublayers with simple linear
transformations that &quot;mix&quot; input tokens. These linear mixers, along with
standard nonlinearities in feed-forward layers, prove competent at modeling
semantic relationships in several text classification tasks. Most surprisingly,
we find that replacing the self-attention sublayer in a Transformer encoder
with a standard, unparameterized Fourier Transform achieves 92-97% of the
accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input
lengths, our FNet model is significantly faster: when compared to the
&quot;efficient&quot; Transformers on the Long Range Arena benchmark, FNet matches the
accuracy of the most accurate models, while outpacing the fastest models across
all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).
Finally, FNet has a light memory footprint and is particularly efficient at
smaller model sizes; for a fixed speed and accuracy budget, small FNet models
outperform Transformer counterparts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04947">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Large-scale, pre-trained language models (LMs) have achieved human-level
performance on a breadth of language understanding tasks. However, evaluations
only based on end task performance shed little light on machines&#x27; true ability
in language understanding and reasoning. In this paper, we highlight the
importance of evaluating the underlying reasoning process in addition to end
performance. Toward this goal, we introduce Tiered Reasoning for Intuitive
Physics (TRIP), a novel commonsense reasoning dataset with dense annotations
that enable multi-tiered evaluation of machines&#x27; reasoning process. Our
empirical results show that while large LMs can achieve high end performance,
they struggle to support their predictions with valid supporting evidence. The
TRIP dataset and our baseline results will motivate verifiable evaluation of
commonsense reasoning and facilitate future research toward developing better
language understanding and reasoning models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04607">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present IndoBERTweet, the first large-scale pretrained model for
Indonesian Twitter that is trained by extending a monolingually-trained
Indonesian BERT model with additive domain-specific vocabulary. We focus in
particular on efficient model adaptation under vocabulary mismatch, and
benchmark different ways of initializing the BERT embedding layer for new word
types. We find that initializing with the average BERT subword embedding makes
pretraining five times faster, and is more effective than proposed methods for
vocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04740">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is widely accepted that fine-tuning pre-trained language models usually
brings about performance improvements in downstream tasks. However, there are
limited studies on the reasons behind this effectiveness, particularly from the
viewpoint of structural changes in the embedding space. Trying to fill this
gap, in this paper, we analyze the extent to which the isotropy of the
embedding space changes after fine-tuning. We demonstrate that, even though
isotropy is a desirable geometrical property, fine-tuning does not necessarily
result in isotropy enhancements. Moreover, local structures in pre-trained
contextual word representations (CWRs), such as those encoding token types or
frequency, undergo a massive change during fine-tuning. Our experiments show
dramatic growth in the number of elongated directions in the embedding space,
which, in contrast to pre-trained CWRs, carry the essential linguistic
knowledge in the fine-tuned embedding space, making existing isotropy
enhancement methods ineffective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Exploratory Study on Long Dialogue Summarization: What Works and What&#x27;s Next. (arXiv:2109.04609v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04609">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dialogue summarization helps readers capture salient information from long
conversations in meetings, interviews, and TV series. However, real-world
dialogues pose a great challenge to current summarization models, as the
dialogue length typically exceeds the input limits imposed by recent
transformer-based pre-trained models, and the interactive nature of dialogues
makes relevant information more context-dependent and sparsely distributed than
news articles. In this work, we perform a comprehensive study on long dialogue
summarization by investigating three strategies to deal with the lengthy input
problem and locate relevant information: (1) extended transformer models such
as Longformer, (2) retrieve-then-summarize pipeline models with several
dialogue utterance retrieval methods, and (3) hierarchical dialogue encoding
models such as HMNet. Our experimental results on three long dialogue datasets
(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline
models yield the best performance. We also demonstrate that the summary quality
can be further improved with a stronger retrieval model and pretraining on
proper external summarization datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where &quot;too much information&quot; clouds a reader&#x27;s understanding of what a system
description means. Disclosive transparency&#x27;s subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04715">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Reproducible benchmarks are crucial in driving progress of machine
translation research. However, existing machine translation benchmarks have
been mostly limited to high-resource or well-represented languages. Despite an
increasing interest in low-resource machine translation, there are no
standardized reproducible benchmarks for many African languages, many of which
are used by millions of speakers but have less digitized textual data. To
tackle these challenges, we propose AfroMT, a standardized, clean, and
reproducible machine translation benchmark for eight widely spoken African
languages. We also develop a suite of analysis tools for system diagnosis
taking into account the unique properties of these languages. Furthermore, we
explore the newly considered case of low-resource focused pretraining and
develop two novel data augmentation-based strategies, leveraging word-level
alignment information and pseudo-monolingual data for pretraining multilingual
sequence-to-sequence models. We demonstrate significant improvements when
pretraining on 11 languages, with gains of up to 2 BLEU points over strong
baselines. We also show gains of up to 12 BLEU points over cross-lingual
transfer baselines in data-constrained scenarios. All code and pretrained
models will be released as further steps towards larger reproducible benchmarks
for African languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04602">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Current language models are usually trained using a self-supervised scheme,
where the main focus is learning representations at the word or sentence level.
However, there has been limited progress in generating useful discourse-level
representations. In this work, we propose to use ideas from predictive coding
theory to augment BERT-style language models with a mechanism that allows them
to learn suitable discourse-level representations. As a result, our proposed
approach is able to predict future sentences using explicit top-down
connections that operate at the intermediate layers of the network. By
experimenting with benchmarks designed to evaluate discourse-related knowledge
using pre-trained sentence representations, we demonstrate that our approach
improves performance in 6 out of 11 tasks by excelling in discourse
relationship detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05003">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study the problem of training named entity recognition (NER) models using
only distantly-labeled data, which can be automatically obtained by matching
entity mentions in the raw text with entity types in a knowledge base. The
biggest challenge of distantly-supervised NER is that the distant supervision
may induce incomplete and noisy labels, rendering the straightforward
application of supervised learning ineffective. In this paper, we propose (1) a
noise-robust learning scheme comprised of a new loss function and a noisy label
removal step, for training NER models on distantly-labeled data, and (2) a
self-training method that uses contextualized augmentations created by
pre-trained language models to improve the generalization ability of the NER
model. On three benchmark datasets, our method achieves superior performance,
outperforming existing distantly-supervised NER models by significant margins.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04832">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Asking questions about a situation is an inherent step towards understanding
it. To this end, we introduce the task of role question generation, which,
given a predicate mention and a passage, requires producing a set of questions
asking about all possible semantic roles of the predicate. We develop a
two-stage model for this task, which first produces a context-independent
question prototype for each role and then revises it to be contextually
appropriate for the passage. Unlike most existing approaches to question
generation, our approach does not require conditioning on existing answers in
the text. Instead, we condition on the type of information to inquire about,
regardless of whether the answer appears explicitly in the text, could be
inferred from it, or should be sought elsewhere. Our evaluation demonstrates
that we generate diverse and well-formed questions for a large, broad-coverage
ontology of predicates and roles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04666">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>It is a well-known approach for fringe groups and organizations to use
euphemisms -- ordinary-sounding and innocent-looking words with a secret
meaning -- to conceal what they are discussing. For instance, drug dealers
often use &quot;pot&quot; for marijuana and &quot;avocado&quot; for heroin. From a social media
content moderation perspective, though recent advances in NLP have enabled the
automatic detection of such single-word euphemisms, no existing work is capable
of automatically detecting multi-word euphemisms, such as &quot;blue dream&quot;
(marijuana) and &quot;black tar&quot; (heroin). Our paper tackles the problem of
euphemistic phrase detection without human effort for the first time, as far as
we are aware. We first perform phrase mining on a raw text corpus (e.g., social
media posts) to extract quality phrases. Then, we utilize word embedding
similarities to select a set of euphemistic phrase candidates. Finally, we rank
those candidates by a masked language model -- SpanBERT. Compared to strong
baselines, we report 20-50% higher detection accuracies using our algorithm for
detecting euphemistic phrases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In interpretable NLP, we require faithful rationales that reflect the model&#x27;s
decision-making process for an explained instance. While prior work focuses on
extractive rationales (a subset of the input words), we investigate their
less-studied counterpart: free-text natural language rationales. We demonstrate
that pipelines, existing models for faithful extractive rationalization on
information-extraction style tasks, do not extend as reliably to &quot;reasoning&quot;
tasks requiring free-text rationales. We turn to models that jointly predict
and rationalize, a class of widely used high-performance models for free-text
rationalization whose faithfulness is not yet established. We define
label-rationale association as a necessary property for faithfulness: the
internal mechanisms of the model producing the label and the rationale must be
meaningfully correlated. We propose two measurements to test this property:
robustness equivalence and feature importance agreement. We find that
state-of-the-art T5-based joint models exhibit both properties for
rationalizing commonsense question-answering and natural language inference,
indicating their potential for producing faithful free-text rationales.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04957">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Framing a news article means to portray the reported event from a specific
perspective, e.g., from an economic or a health perspective. Reframing means to
change this perspective. Depending on the audience or the submessage, reframing
can become necessary to achieve the desired effect on the readers. Reframing is
related to adapting style and sentiment, which can be tackled with neural text
generation techniques. However, it is more challenging since changing a frame
requires rewriting entire sentences rather than single phrases. In this paper,
we study how to computationally reframe sentences in news articles while
maintaining their coherence to the context. We treat reframing as a
sentence-level fill-in-the-blank task for which we train neural models on an
existing media frame corpus. To guide the training, we propose three
strategies: framed-language pretraining, named-entity preservation, and
adversarial learning. We evaluate respective models automatically and manually
for topic consistency, coherence, and successful reframing. Our results
indicate that generating properly-framed text works well but with tradeoffs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16590">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text generation systems are ubiquitous in natural language processing
applications. However, evaluation of these systems remains a challenge,
especially in multilingual settings. In this paper, we propose L&#x27;AMBRE -- a
metric to evaluate the morphosyntactic well-formedness of text using its
dependency parse and morphosyntactic rules of the language. We present a way to
automatically extract various rules governing morphosyntax directly from
dependency treebanks. To tackle the noisy outputs from text generation systems,
we propose a simple methodology to train robust parsers. We show the
effectiveness of our metric on the task of machine translation through a
diachronic study of systems translating into morphologically-rich languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The problem of representing the atomic elements of language in modern neural
learning systems is one of the central challenges of the field of natural
language processing. I present a survey of the distributional, compositional,
and relational approaches to addressing this task, and discuss various means of
integrating them into systems, with special emphasis on the word level and the
out-of-vocabulary phenomenon.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00782">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-trained language models (PTLMs) have achieved impressive performance on
commonsense inference benchmarks, but their ability to employ commonsense to
make robust inferences, which is crucial for effective communications with
humans, is debated. In the pursuit of advancing fluid human-AI communication,
we propose a new challenge, RICA: Robust Inference capability based on
Commonsense Axioms, that evaluates robust commonsense inference despite textual
perturbations. To generate data for this challenge, we develop a systematic and
scalable procedure using commonsense knowledge bases and probe PTLMs across two
different evaluation settings. Extensive experiments on our generated probe
sets with more than 10k statements show that PTLMs perform no better than
random guessing on the zero-shot setting, are heavily impacted by statistical
biases, and are not robust to perturbation attacks. We also find that
fine-tuning on similar statements offer limited gains, as PTLMs still fail to
generalize to unseen inferences. Our new large-scale benchmark exposes a
significant gap between PTLMs and human-level language understanding and offers
a new challenge for PTLMs to demonstrate commonsense.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04921">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>State-of-the-art contextual embeddings are obtained from large language
models available only for a few languages. For others, we need to learn
representations using a multilingual model. There is an ongoing debate on
whether multilingual embeddings can be aligned in a space shared across many
languages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\v{c}ek,
2021) allows us to answer this question for specific linguistic features and
learn a projection based only on mono-lingual annotated datasets. We evaluate
syntactic (UD) and lexical (WordNet) structural information encoded inmBERT&#x27;s
contextual representations for nine diverse languages. We observe that for
languages closely related to English, no transformation is needed. The
evaluated information is encoded in a shared cross-lingual embedding space. For
other languages, it is beneficial to apply orthogonal transformation learned
separately for each language. We successfully apply our findings to zero-shot
and few-shot cross-lingual parsing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09761">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large repositories of products, patents and scientific papers offer an
opportunity for building systems that scour millions of ideas and help users
discover inspirations. However, idea descriptions are typically in the form of
unstructured text, lacking key structure that is required for supporting
creative innovation interactions. Prior work has explored idea representations
that were limited in expressivity, required significant manual effort from
users, or dependent on curated knowledge bases with poor coverage. We explore a
novel representation that automatically breaks up products into fine-grained
functional facets capturing the purposes and mechanisms of ideas, and use it to
support important creative innovation interactions: functional search for
ideas, and exploration of the design space around a focal problem by viewing
related problem perspectives pooled from across many products. In user studies,
our approach boosts the quality of creative search and inspirations,
outperforming strong baselines by 50-60%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, with the expansion of the Internet and attractive social
media infrastructures, people prefer to follow the news through these media.
Despite the many advantages of these media in the news field, the lack of any
control and verification mechanism has led to the spread of fake news, as one
of the most important threats to democracy, economy, journalism and freedom of
expression. Designing and using automatic methods to detect fake news on social
media has become a significant challenge. In this paper, we examine the
publishers&#x27; role in detecting fake news on social media. We also suggest a high
accurate multi-modal framework, namely FR-Detect, using user-related and
content-related features with early detection capability. For this purpose, two
new user-related features, namely Activity Credibility and Influence, have been
introduced for publishers. Furthermore, a sentence-level convolutional neural
network is provided to combine these features with latent textual content
features properly. Experimental results have shown that the publishers&#x27;
features can improve the performance of content-based models by up to 13% and
29% in accuracy and F1-score, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05006">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An important task in NLP applications such as sentence simplification is the
ability to take a long, complex sentence and split it into shorter sentences,
rephrasing as necessary. We introduce a novel dataset and a new model for this
&#x60;split and rephrase&#x27; task. Our BiSECT training data consists of 1 million long
English sentences paired with shorter, meaning-equivalent English sentences. We
obtain these by extracting 1-2 sentence alignments in bilingual parallel
corpora and then using machine translation to convert both sides of the corpus
into the same language. BiSECT contains higher quality training examples than
previous Split and Rephrase corpora, with sentence splits that require more
significant modifications. We categorize examples in our corpus, and use these
categories in a novel model that allows us to target specific regions of the
input sentence to be split and edited. Moreover, we show that models trained on
BiSECT can perform a wider variety of split operations and improve upon
previous state-of-the-art approaches in automatic and human evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16410">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To alleviate human efforts from obtaining large-scale annotations,
Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in
addition to learning from limited samples. Existing self-training methods
suffer from the gradual drift problem, where noisy pseudo labels on unlabeled
data are incorporated during training. To alleviate the noise in pseudo labels,
we propose a method called MetaSRE, where a Relation Label Generation Network
generates quality assessment on pseudo labels by (meta) learning from the
successful and failed attempts on Relation Classification Network as an
additional meta-objective. To reduce the influence of noisy pseudo labels,
MetaSRE adopts a pseudo label selection and exploitation scheme which assesses
pseudo label quality on unlabeled samples and only exploits high-quality pseudo
labels in a self-training fashion to incrementally augment labeled samples for
both robustness and accuracy. Experimental results on two public datasets
demonstrate the effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04994">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Unlike well-structured text, such as news reports and encyclopedia articles,
dialogue content often comes from two or more interlocutors, exchanging
information with each other. In such a scenario, the topic of a conversation
can vary upon progression and the key information for a certain topic is often
scattered across multiple utterances of different speakers, which poses
challenges to abstractly summarize dialogues. To capture the various topic
information of a conversation and outline salient facts for the captured
topics, this work proposes two topic-aware contrastive learning objectives,
namely coherence detection and sub-summary generation objectives, which are
expected to implicitly model the topic change and handle information scattering
challenges for the dialogue summarization task. The proposed contrastive
objectives are framed as auxiliary tasks for the primary dialogue summarization
task, united via an alternative parameter updating strategy. Extensive
experiments on benchmark datasets demonstrate that the proposed simple method
significantly outperforms strong baselines and achieves new state-of-the-art
performance. The code and trained models are publicly available via
\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04922">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As large-scale, pre-trained language models achieve human-level and
superhuman accuracy on existing language understanding tasks, statistical bias
in benchmark data and probing studies have recently called into question their
true capabilities. For a more informative evaluation than accuracy on text
classification tasks can offer, we propose evaluating systems through a novel
measure of prediction coherence. We apply our framework to two existing
language understanding benchmarks with different properties to demonstrate its
versatility. Our experimental results show that this evaluation framework,
although simple in ideas and implementation, is a quick, effective, and
versatile measure to provide insight into the coherence of machines&#x27;
predictions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11320">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Warning: this paper contains content that may be offensive or upsetting.

Numerous natural language processing models have tried injecting commonsense
by using the ConceptNet knowledge base to improve performance on different
tasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect
human biases such as &quot;lawyers are dishonest.&quot; It is important that these biases
are not conflated with the notion of commonsense. We study this missing yet
important problem by first defining and quantifying biases in ConceptNet as two
types of representational harms: overgeneralization of polarized perceptions
and representation disparity. We find that ConceptNet contains severe biases
and disparities across four demographic categories. In addition, we analyze two
downstream models that use ConceptNet as a source for commonsense knowledge and
find the existence of biases in those models as well. We further propose a
filtered-based bias-mitigation approach and examine its effectiveness. We show
that our mitigation approach can reduce the issues in both resource and models
but leads to a performance drop, leaving room for future work to build fairer
and stronger commonsense models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04600">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work, we present
EVOQUER, a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video, the temporal grounding model predicts the target
interval, and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets,
Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04778">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multilingual Neural Machine Translation (NMT) enables one model to serve all
translation directions, including ones that are unseen during training, i.e.
zero-shot translation. Despite being theoretically attractive, current models
often produce low quality translations -- commonly failing to even produce
outputs in the right target language. In this work, we observe that off-target
translation is dominant even in strong multilingual systems, trained on massive
multilingual corpora. To address this issue, we propose a joint approach to
regularize NMT models at both representation-level and gradient-level. At the
representation level, we leverage an auxiliary target language prediction task
to regularize decoder outputs to retain information about the target language.
At the gradient level, we leverage a small amount of direct data (in thousands
of sentence pairs) to regularize model gradients. Our results demonstrate that
our approach is highly effective in both reducing off-target translation
occurrences and improving zero-shot translation performance by +5.59 and +10.38
BLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our
method also works well when the small amount of direct data is not available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04775">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing black box search methods have achieved high success rate in
generating adversarial attacks against NLP models. However, such search methods
are inefficient as they do not consider the amount of queries required to
generate adversarial attacks. Also, prior attacks do not maintain a consistent
search space while comparing different search methods. In this paper, we
propose a query efficient attack strategy to generate plausible adversarial
examples on text classification and entailment tasks. Our attack jointly
leverages attention mechanism and locality sensitive hashing (LSH) to reduce
the query count. We demonstrate the efficacy of our approach by comparing our
attack with four baselines across three different search spaces. Further, we
benchmark our results across the same search space used in prior attacks. In
comparison to attacks proposed, on an average, we are able to reduce the query
count by 75% across all datasets and target models. We also demonstrate that
our attack achieves a higher success rate when compared to prior attacks in a
limited query setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04673">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Identifying relevant knowledge to be used in conversational systems that are
grounded in long documents is critical to effective response generation. We
introduce a knowledge identification model that leverages the document
structure to provide dialogue-contextualized passage encodings and better
locate knowledge relevant to the conversation. An auxiliary loss captures the
history of dialogue-document connections. We demonstrate the effectiveness of
our model on two document-grounded conversational datasets and provide analyses
showing generalization to unseen documents and long dialogue contexts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06127">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers that are pre-trained on multilingual corpora, such as, mBERT and
XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In
the zero-shot transfer setting, only English training data is used, and the
fine-tuned model is evaluated on another target language. While this works
surprisingly well, substantial variance has been observed in target language
performance between different fine-tuning runs, and in the zero-shot setup, no
target-language development data is available to select among multiple
fine-tuned models. Prior work has relied on English dev data to select among
models that are fine-tuned with different learning rates, number of steps and
other hyperparameters, often resulting in suboptimal choices. In this paper, we
show that it is possible to select consistently better models when small
amounts of annotated data are available in auxiliary pivot languages. We
propose a machine learning approach to model selection that uses the fine-tuned
model&#x27;s own internal representations to predict its cross-lingual capabilities.
In extensive experiments we find that this method consistently selects better
models than English validation data across twenty five languages (including
eight low-resource languages), and often achieves results that are comparable
to model selection using target language development data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Style is an integral part of natural language. However, evaluation methods
for style measures are rare, often task-specific and usually do not control for
content. We propose the modular, fine-grained and content-controlled
similarity-based STyle EvaLuation framework (STEL) to test the performance of
any model that can compare two sentences on style. We illustrate STEL with two
general dimensions of style (formal/informal and simple/complex) as well as two
specific characteristics of style (contrac&#x27;tion and numb3r substitution). We
find that BERT-based methods outperform simple versions of commonly used style
measures like 3-grams, punctuation frequency and LIWC-based approaches. We
invite the addition of further tasks and task instances to STEL and hope to
facilitate the improvement of style-sensitive measures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05016">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We test the natural expectation that using MT in professional translation
saves human processing time. The last such study was carried out by
Sanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the
translation quality. In contrast, we focus on neural MT (NMT) of high quality,
which has become the state-of-the-art approach since then and also got adopted
by most translation companies.

Through an experimental study involving over 30 professional translators for
English -&gt; Czech translation, we examine the relationship between NMT
performance and post-editing time and quality. Across all models, we found that
better MT systems indeed lead to fewer changes in the sentences in this
industry setting. The relation between system quality and post-editing time is
however not straightforward and, contrary to the results on phrase-based MT,
BLEU is definitely not a stable predictor of the time or final output quality.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04588">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The success of bidirectional encoders using masked language models, such as
BERT, on numerous natural language processing tasks has prompted researchers to
attempt to incorporate these pre-trained models into neural machine translation
(NMT) systems. However, proposed methods for incorporating pre-trained models
are non-trivial and mainly focus on BERT, which lacks a comparison of the
impact that other pre-trained models may have on translation performance. In
this paper, we demonstrate that simply using the output (contextualized
embeddings) of a tailored and suitable bilingual pre-trained language model
(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art
translation performance. Moreover, we also propose a stochastic layer selection
approach and a concept of dual-directional translation model to ensure the
sufficient utilization of contextualized embeddings. In the case of without
using back translation, our best models achieve BLEU scores of 30.45 for En-&gt;De
and 38.61 for De-&gt;En on the IWSLT&#x27;14 dataset, and 31.26 for En-&gt;De and 34.94
for De-&gt;En on the WMT&#x27;14 dataset, which exceeds all published numbers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent work has shown that monolingual masked language models learn to
represent data-driven notions of language variation which can be used for
domain-targeted training data selection. Dataset genre labels are already
frequently available, yet remain largely unexplored in cross-lingual setups. We
harness this genre metadata as a weak supervision signal for targeted data
selection in zero-shot dependency parsing. Specifically, we project
treebank-level genre information to the finer-grained sentence level, with the
goal to amplify information implicitly stored in unsupervised contextualized
representations. We demonstrate that genre is recoverable from multilingual
contextual embeddings and that it provides an effective signal for training
data selection in cross-lingual, zero-shot scenarios. For 12 low-resource
language treebanks, six of which are test-only, our genre-specific methods
significantly outperform competitive baselines as well as recent
embedding-based methods for data selection. Moreover, genre-based data
selection provides new state-of-the-art results for three of these target
languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04712">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-label text classification is a challenging task because it requires
capturing label dependencies. It becomes even more challenging when class
distribution is long-tailed. Resampling and re-weighting are common approaches
used for addressing the class imbalance problem, however, they are not
effective when there is label dependency besides class imbalance because they
result in oversampling of common labels. Here, we introduce the application of
balancing loss functions for multi-label text classification. We perform
experiments on a general domain dataset with 90 labels (Reuters-21578) and a
domain-specific dataset from PubMed with 18211 labels. We find that a
distribution-balanced loss function, which inherently addresses both the class
imbalance and label linkage problems, outperforms commonly used loss functions.
Distribution balancing methods have been successfully used in the image
recognition field. Here, we show their effectiveness in natural language
processing. Source code is available at
https://github.com/blessu/BalancedLossNLP.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04708">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The majority of language domains require prudent use of terminology to ensure
clarity and adequacy of information conveyed. While the correct use of
terminology for some languages and domains can be achieved by adapting
general-purpose MT systems on large volumes of in-domain parallel data, such
quantities of domain-specific data are seldom available for less-resourced
languages and niche domains. Furthermore, as exemplified by COVID-19 recently,
no domain-specific parallel data is readily available for emerging domains.
However, the gravity of this recent calamity created a high demand for reliable
translation of critical information regarding pandemic and infection
prevention. This work is part of WMT2021 Shared Task: Machine Translation using
Terminologies, where we describe Tilde MT systems that are capable of dynamic
terminology integration at the time of translation. Our systems achieve up to
94% COVID-19 term use accuracy on the test set of the EN-FR language pair
without having access to any form of in-domain information during system
training. We conclude our work with a broader discussion considering the Shared
Task itself and terminology translation in MT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04543">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Style transfer aims to rewrite a source text in a different target style
while preserving its content. We propose a novel approach to this task that
leverages generic resources, and without using any task-specific parallel
(source-target) data outperforms existing unsupervised approaches on the two
most popular style transfer tasks: formality transfer and polarity swap. In
practice, we adopt a multi-step procedure which builds on a generic pre-trained
sequence-to-sequence model (BART). First, we strengthen the model&#x27;s ability to
rewrite by further pre-training BART on both an existing collection of generic
paraphrases, as well as on synthetic pairs created using a general-purpose
lexical resource. Second, through an iterative back-translation approach, we
train two models, each in a transfer direction, so that they can provide each
other with synthetically generated pairs, dynamically in the training process.
Lastly, we let our best reresulting model generate static synthetic pairs to be
used in a supervised training regime. Besides methodology and state-of-the-art
results, a core contribution of this work is a reflection on the nature of the
two tasks we address, and how their differences are highlighted by their
response to our approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04877">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Adapters are light-weight modules that allow parameter-efficient fine-tuning
of pretrained models. Specialized language and task adapters have recently been
proposed to facilitate cross-lingual transfer of multilingual pretrained models
(Pfeiffer et al., 2020b). However, this approach requires training a separate
language adapter for every language one wishes to support, which can be
impractical for languages with limited data. An intuitive solution is to use a
related language adapter for the new language variety, but we observe that this
solution can lead to sub-optimal performance. In this paper, we aim to improve
the robustness of language adapters to uncovered languages without training new
adapters. We find that ensembling multiple existing language adapters makes the
fine-tuned model significantly more robust to other language varieties not
included in these adapters. Building upon this observation, we propose Entropy
Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble
weights of the pretrained language adapters for each test sentence by
minimizing the entropy of its predictions. Experiments on three diverse groups
of language varieties show that our method leads to significant improvements on
both named entity recognition and part-of-speech tagging across all languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04604">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The general goal of text simplification (TS) is to reduce text complexity for
human consumption. This paper investigates another potential use of neural TS:
assisting machines performing natural language processing (NLP) tasks. We
evaluate the use of neural TS in two ways: simplifying input texts at
prediction time and augmenting data to provide machines with additional
information during training. We demonstrate that the latter scenario provides
positive effects on machine performance on two separate datasets. In
particular, the latter use of TS improves the performances of LSTM (1.82-1.98%)
and SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,
real-world relation extraction task. Further, the same setting yields
improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT
text classifier on MNLI, a practical natural language inference dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04672">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The transformer-based pre-trained language models have been tremendously
successful in most of the conventional NLP tasks. But they often struggle in
those tasks where numerical understanding is required. Some possible reasons
can be the tokenizers and pre-training objectives which are not specifically
designed to learn and preserve numeracy. Here we investigate the ability of
text-to-text transfer learning model (T5), which has outperformed its
predecessors in the conventional NLP tasks, to learn numeracy. We consider four
numeracy tasks: numeration, magnitude order prediction, finding minimum and
maximum in a series, and sorting. We find that, although T5 models perform
reasonably well in the interpolation setting, they struggle considerably in the
extrapolation setting across all four tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04546">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the problem of generating arithmetic math word problems (MWPs) given
a math equation that specifies the mathematical computation and a context that
specifies the problem scenario. Existing approaches are prone to generating
MWPs that are either mathematically invalid or have unsatisfactory language
quality. They also either ignore the context or require manual specification of
a problem template, which compromises the diversity of the generated MWPs. In
this paper, we develop a novel MWP generation approach that leverages i)
pre-trained language models and a context keyword selection model to improve
the language quality of the generated MWPs and ii) an equation consistency
constraint for math equations to improve the mathematical validity of the
generated MWPs. Extensive quantitative and qualitative experiments on three
real-world MWP datasets demonstrate the superior performance of our approach
compared to various baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04587">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The dominant paradigm for semantic parsing in recent years is to formulate
parsing as a sequence-to-sequence task, generating predictions with
auto-regressive sequence decoders. In this work, we explore an alternative
paradigm. We formulate semantic parsing as a dependency parsing task, applying
graph-based decoding techniques developed for syntactic parsing. We compare
various decoding techniques given the same pre-trained Transformer encoder on
the TOP dataset, including settings where training data is limited or contains
only partially-annotated examples. We find that our graph-based approach is
competitive with sequence decoders on the standard setting, and offers
significant improvements in data efficiency and settings where
partially-annotated data is available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema(definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5)is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04556">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>State-of-the-art multilingual systems rely on shared vocabularies that
sufficiently cover all considered languages. To this end, a simple and
frequently used approach makes use of subword vocabularies constructed jointly
over several languages. We hypothesize that such vocabularies are suboptimal
due to false positives (identical subwords with different meanings across
languages) and false negatives (different subwords with similar meanings). To
address these issues, we propose Subword Mapping and Anchoring across Languages
(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts
subword alignments using an unsupervised state-of-the-art mapping technique and
uses them to create cross-lingual anchors based on subword similarities. We
demonstrate the benefits of SMALA for cross-lingual natural language inference
(XNLI), where it improves zero-shot transfer to an unseen language without
task-specific data, but only by sharing subword embeddings. Moreover, in neural
machine translation, we show that joint subword vocabularies obtained with
SMALA lead to higher BLEU scores on sentences that contain many false positives
and false negatives.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04912">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present ReasonBert, a pre-training method that augments language models
with the ability to reason over long-range relations and multiple, possibly
hybrid contexts. Unlike existing pre-training methods that only harvest
learning signals from local contexts of naturally occurring texts, we propose a
generalized notion of distant supervision to automatically connect multiple
pieces of text and tables to create pre-training examples that require
long-range reasoning. Different types of reasoning are simulated, including
intersecting multiple pieces of evidence, bridging from one piece of evidence
to another, and detecting unanswerable cases. We conduct a comprehensive
evaluation on a variety of extractive question answering datasets ranging from
single-hop to multi-hop and from text-only to table-only to hybrid that require
various reasoning capabilities and show that ReasonBert achieves remarkable
improvement over an array of strong baselines. Few-shot experiments further
demonstrate that our pre-training method substantially improves sample
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04513">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present models which complete missing text given transliterations of
ancient Mesopotamian documents, originally written on cuneiform clay tablets
(2500 BCE - 100 CE). Due to the tablets&#x27; deterioration, scholars often rely on
contextual cues to manually fill in missing parts in the text in a subjective
and time-consuming process. We identify that this challenge can be formulated
as a masked language modelling task, used mostly as a pretraining objective for
contextualized language models. Following, we develop several architectures
focusing on the Akkadian language, the lingua franca of the time. We find that
despite data scarcity (1M tokens) we can achieve state of the art performance
on missing tokens prediction (89% hit@5) using a greedy decoding scheme and
pretraining on data from other languages and different time periods. Finally,
we conduct human evaluations showing the applicability of our models in
assisting experts to transcribe texts in extinct languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Document-level entity-based extraction (EE), aiming at extracting
entity-centric information such as entity roles and entity relations, is key to
automatic knowledge acquisition from text corpora for various domains. Most
document-level EE systems build extractive models, which struggle to model
long-term dependencies among entities at the document level. To address this
issue, we propose a generative framework for two document-level EE tasks:
role-filler entity extraction (REE) and relation extraction (RE). We first
formulate them as a template generation problem, allowing models to efficiently
capture cross-entity dependencies, exploit label semantics, and avoid the
exponential computation complexity of identifying N-ary relations. A novel
cross-attention guided copy mechanism, TopK Copy, is incorporated into a
pre-trained sequence-to-sequence model to enhance the capabilities of
identifying key information in the input document. Experiments done on the
MUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),
binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04593">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent advances in neural machine translation (NMT) have pushed the quality
of machine translation systems to the point where they are becoming widely
adopted to build competitive systems. However, there is still a large number of
languages that are yet to reap the benefits of NMT. In this paper, we provide
the first large-scale case study of the practical application of MT in the
Turkic language family in order to realize the gains of NMT for Turkic
languages under high-resource to extremely low-resource scenarios. In addition
to presenting an extensive analysis that identifies the bottlenecks towards
building competitive systems to ameliorate data scarcity, our study has several
key contributions, including, i) a large parallel corpus covering 22 Turkic
languages consisting of common public datasets in combination with new datasets
of approximately 2 million parallel sentences, ii) bilingual baselines for 26
language pairs, iii) novel high-quality test sets in three different
translation domains and iv) human evaluation scores. All models, scripts, and
data will be released to the public.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04780">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer-based pre-trained models, such as BERT, have achieved remarkable
results on machine reading comprehension. However, due to the constraint of
encoding length (e.g., 512 WordPiece tokens), a long document is usually split
into multiple chunks that are independently read. It results in the reading
field being limited to individual chunks without information collaboration for
long document machine reading comprehension. To address this problem, we
propose RoR, a read-over-read method, which expands the reading field from
chunk to document. Specifically, RoR includes a chunk reader and a document
reader. The former first predicts a set of regional answers for each chunk,
which are then compacted into a highly-condensed version of the original
document, guaranteeing to be encoded once. The latter further predicts the
global answers from this condensed document. Eventually, a voting strategy is
utilized to aggregate and rerank the regional and global answers for final
prediction. Extensive experiments on two benchmarks QuAC and TriviaQA
demonstrate the effectiveness of RoR for long document reading. Notably, RoR
ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of
submission (May 17th, 2021).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04653">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-trained language-vision models have shown remarkable performance on the
visual question answering (VQA) task. However, most pre-trained models are
trained by only considering monolingual learning, especially the resource-rich
language like English. Training such models for multilingual setups demand high
computing resources and multilingual language-vision dataset which hinders
their application in practice. To alleviate these challenges, we propose a
knowledge distillation approach to extend an English language-vision model
(teacher) into an equally effective multilingual and code-mixed model
(student). Unlike the existing knowledge distillation methods, which only use
the output from the last layer of the teacher network for distillation, our
student model learns and imitates the teacher from multiple intermediate layers
(language and vision encoders) with appropriately designed distillation
objectives for incremental knowledge extraction. We also create the large-scale
multilingual and code-mixed VQA dataset in eleven different language setups
considering the multiple Indian and European languages. Experimental results
and in-depth analysis show the effectiveness of the proposed VQA model over the
pre-trained language-vision models on eleven diverse language setups.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04535">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Extracting moral sentiment from text is a vital component in understanding
public opinion, social movements, and policy decisions. The Moral Foundation
Theory identifies five moral foundations, each associated with a positive and
negative polarity. However, moral sentiment is often motivated by its targets,
which can correspond to individuals or collective entities. In this paper, we
introduce morality frames, a representation framework for organizing moral
attitudes directed at different entities, and come up with a novel and
high-quality annotated dataset of tweets written by US politicians. Then, we
propose a relational learning model to predict moral attitudes towards entities
and moral foundations jointly. We do qualitative and quantitative evaluations,
showing that moral sentiment towards entities differs highly across political
ideologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04550">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal Knowledge Graphs store events in the form of subjects, relations,
objects, and timestamps which are often represented by dynamic heterogeneous
graphs. Event forecasting is a critical and challenging task in Temporal
Knowledge Graph reasoning that predicts the subject or object of an event in
the future. To obtain temporal embeddings multi-step away in the future,
existing methods learn generative models that capture the joint distribution of
the observed events. To reduce the high computation costs, these methods rely
on unrealistic assumptions of independence and approximations in training and
inference. In this work, we propose SeDyT, a discriminative framework that
performs sequence modeling on the dynamic entity embeddings to solve the
multi-step event forecasting problem. SeDyT consists of two components: a
Temporal Graph Neural Network that generates dynamic entity embeddings in the
past and a sequence model that predicts the entity embeddings in the future.
Compared with the generative models, SeDyT does not rely on any heuristic-based
probability model and has low computation complexity in both training and
inference. SeDyT is compatible with most Temporal Graph Neural Networks and
sequence models. We also design an efficient training method that trains the
two components in one gradient descent propagation. We evaluate the performance
of SeDyT on five popular datasets. By combining temporal Graph Neural Network
models and sequence models, SeDyT achieves an average of 2.4% MRR improvement
when not using the validation set and more than 10% MRR improvement when using
the validation set.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04787">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Resolving pronouns to their referents has long been studied as a fundamental
natural language understanding problem. Previous works on pronoun coreference
resolution (PCR) mostly focus on resolving pronouns to mentions in text while
ignoring the exophoric scenario. Exophoric pronouns are common in daily
communications, where speakers may directly use pronouns to refer to some
objects present in the environment without introducing the objects first.
Although such objects are not mentioned in the dialogue text, they can often be
disambiguated by the general topics of the dialogue. Motivated by this, we
propose to jointly leverage the local context and global topics of dialogues to
solve the out-of-text PCR problem. Extensive experiments demonstrate the
effectiveness of adding topic regularization for resolving exophoric pronouns.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.02499">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a model to predict fine-grained emotions along the continuous
dimensions of valence, arousal, and dominance (VAD) with a corpus with
categorical emotion annotations. Our model is trained by minimizing the EMD
(Earth Mover&#x27;s Distance) loss between the predicted VAD score distribution and
the categorical emotion distributions sorted along VAD, and it can
simultaneously classify the emotion categories and predict the VAD scores for a
given sentence. We use pre-trained RoBERTa-Large and fine-tune on three
different corpora with categorical labels and evaluate on EmoBank corpus with
VAD scores. We show that our approach reaches comparable performance to that of
the state-of-the-art classifiers in categorical emotion classification and
shows significant positive correlations with the ground truth VAD scores. Also,
further training with supervision of VAD labels leads to improved performance
especially when dataset is small. We also present examples of predictions of
appropriate emotion words that are not part of the original annotations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04939">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In computational linguistics, it has been shown that hierarchical structures
make language models (LMs) more human-like. However, the previous literature
has been agnostic about a parsing strategy of the hierarchical models. In this
paper, we investigated whether hierarchical structures make LMs more
human-like, and if so, which parsing strategy is most cognitively plausible. In
order to address this question, we evaluated three LMs against human reading
times in Japanese with head-final left-branching structures: Long Short-Term
Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars
(RNNGs) with top-down and left-corner parsing strategies as hierarchical
models. Our computational modeling demonstrated that left-corner RNNGs
outperformed top-down RNNGs and LSTM, suggesting that hierarchical and
left-corner architectures are more cognitively plausible than top-down or
sequential architectures. In addition, the relationships between the cognitive
plausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be
discussed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01454">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce a few-shot transfer learning method for keyword spotting in any
language. Leveraging open speech corpora in nine languages, we automate the
extraction of a large multilingual keyword bank and use it to train an
embedding model. With just five training examples, we fine-tune the embedding
model for keyword spotting and achieve an average F1 score of 0.75 on keyword
classification for 180 new keywords unseen by the embedding model in these nine
languages. This embedding model also generalizes to new languages. We achieve
an average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13
new languages unseen by the embedding model. We investigate streaming accuracy
for our 5-shot models in two contexts: keyword spotting and keyword search.
Across 440 keywords in 22 languages, we achieve an average streaming keyword
spotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe
promising initial results on keyword search.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04783">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>When a sufficiently large far-field training data is presented, jointly
optimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech
Recognition (ASR) backend shows promising results. Recent literature has shown
traditional beamformer designs, such as MVDR (Minimum Variance Distortionless
Response) or fixed beamformers can be successfully integrated as the frontend
into an E2E ASR system with learnable parameters. In this work, we propose the
self-attention channel combinator (SACC) ASR frontend, which leverages the
self-attention mechanism to combine multichannel audio signals in the magnitude
spectral domain. Experiments conducted on a multichannel playback test data
shows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed
beamformer-based frontend, both jointly optimized with a ContextNet-based ASR
backend. We also demonstrate the connection between the SACC and the
traditional beamformers, and analyze the intermediate outputs of the SACC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04825">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The impressive capabilities of recent generative models to create texts that
are challenging to distinguish from the human-written ones can be misused for
generating fake news, product reviews, and even abusive content. Despite the
prominent performance of existing methods for artificial text detection, they
still lack interpretability and robustness towards unseen models. To this end,
we propose three novel types of interpretable topological features for this
task based on Topological Data Analysis (TDA) which is currently understudied
in the field of NLP. We empirically show that the features derived from the
BERT model outperform count- and neural-based baselines up to 10\% on three
common datasets, and tend to be the most robust towards unseen GPT-style
generation models as opposed to existing methods. The probing analysis of the
features reveals their sensitivity to the surface and syntactic properties. The
results demonstrate that TDA is a promising line with respect to NLP tasks,
specifically the ones that incorporate surface and structural information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WHOSe Heritage: Classification of UNESCO World Heritage &quot;Outstanding Universal Value&quot; Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05547">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The UNESCO World Heritage List (WHL) includes the exceptionally valuable
cultural and natural heritage to be preserved for mankind. Evaluating and
justifying the Outstanding Universal Value (OUV) is essential for each site
inscribed in the WHL, and yet a complex task, even for experts, since the
selection criteria of OUV are not mutually exclusive. Furthermore, manual
annotation of heritage values and attributes from multi-source textual data,
which is currently dominant in heritage studies, is knowledge-demanding and
time-consuming, impeding systematic analysis of such authoritative documents in
terms of their implications on heritage management. This study applies
state-of-the-art NLP models to build a classifier on a new dataset containing
Statements of OUV, seeking an explainable and scalable automation tool to
facilitate the nomination, evaluation, research, and monitoring processes of
World Heritage sites. Label smoothing is innovatively adapted to improve the
model performance by adding prior inter-class relationship knowledge to
generate soft labels. The study shows that the best models fine-tuned from BERT
and ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation
on the model prediction shows that the models are sufficiently generalizable.
The study is promising to be further developed and applied in heritage research
and practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04689">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporally Coherent Person Matting Trained on Fake-Motion Dataset. (arXiv:2109.04843v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel neural-network-based method to perform matting of videos
depicting people that does not require additional user input such as trimaps.
Our architecture achieves temporal stability of the resulting alpha mattes by
using motion-estimation-based smoothing of image-segmentation algorithm
outputs, combined with convolutional-LSTM modules on U-Net skip connections.

We also propose a fake-motion algorithm that generates training clips for the
video-matting network given photos with ground-truth alpha mattes and
background videos. We apply random motion to photos and their mattes to
simulate movement one would find in real videos and composite the result with
the background clips. It lets us train a deep neural network operating on
videos in an absence of a large annotated video dataset and provides
ground-truth training-clip foreground optical flow for use in loss functions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Better Self-training for Image Classification through Self-supervision. (arXiv:2109.00778v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00778">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Self-training is a simple semi-supervised learning approach: Unlabelled
examples that attract high-confidence predictions are labelled with their
predictions and added to the training set, with this process being repeated
multiple times. Recently, self-supervision -- learning without manual
supervision by solving an automatically-generated pretext task -- has gained
prominence in deep learning. This paper investigates three different ways of
incorporating self-supervision into self-training to improve accuracy in image
classification: self-supervision as pretraining only, self-supervision
performed exclusively in the first iteration of self-training, and
self-supervision added to every iteration of self-training. Empirical results
on the SVHN, CIFAR-10, and PlantVillage datasets, using both training from
scratch, and Imagenet-pretrained weights, show that applying self-supervision
only in the first iteration of self-training can greatly improve accuracy, for
a modest increase in computation time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. (arXiv:2103.16874v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The task of image-based virtual try-on aims to transfer a target clothing
item onto the corresponding region of a person, which is commonly tackled by
fitting the item to the desired body part and fusing the warped item with the
person. While an increasing number of studies have been conducted, the
resolution of synthesized images is still limited to low (e.g., 256x192), which
acts as the critical limitation against satisfying online consumers. We argue
that the limitation stems from several challenges: as the resolution increases,
the artifacts in the misaligned areas between the warped clothes and the
desired clothing regions become noticeable in the final results; the
architectures used in existing methods have low performance in generating
high-quality body parts and maintaining the texture sharpness of the clothes.
To address the challenges, we propose a novel virtual try-on method called
VITON-HD that successfully synthesizes 1024x768 virtual try-on images.
Specifically, we first prepare the segmentation map to guide our virtual try-on
synthesis, and then roughly fit the target clothing item to a given person&#x27;s
body. Next, we propose ALIgnment-Aware Segment (ALIAS) normalization and ALIAS
generator to handle the misaligned areas and preserve the details of 1024x768
inputs. Through rigorous comparison with existing methods, we demonstrate that
VITON-HD highly surpasses the baselines in terms of synthesized image quality
both qualitatively and quantitatively. Code is available at
https://github.com/shadow2496/VITON-HD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13301">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To implement a good Content Based Image Retrieval (CBIR) system, it is
essential to adopt efficient search methods. One way to achieve this results is
by exploiting approximate search techniques. In fact, when we deal with very
large collections of data, using an exact search method makes the system very
slow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to
implement a CBIR system that allows us to perform fast similarity search on
deep features. Specifically, we exploit transfer learning techniques to extract
deep features from images; this phase is done using two famous Convolutional
Neural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both
pre-trained on ImageNet. Then we try out several fully connected deep neural
networks, built on top of both of the previously mentioned CNNs in order to
fine-tuned them on our dataset. In both of previous cases, we index the
features within our LSH index implementation and within a sequential scan, to
better understand how much the introduction of the index affects the results.
Finally, we carry out a performance analysis: we evaluate the relevance of the
result set, computing the mAP (mean Average Precision) value obtained during
the different experiments with respect to the number of done comparison and
varying the hyper-parameter values of the LSH index.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MobileStyleGAN: A Lightweight Convolutional Neural Network for High-Fidelity Image Synthesis. (arXiv:2104.04767v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04767">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, the use of Generative Adversarial Networks (GANs) has become
very popular in generative image modeling. While style-based GAN architectures
yield state-of-the-art results in high-fidelity image synthesis,
computationally, they are highly complex. In our work, we focus on the
performance optimization of style-based generative models. We analyze the most
computationally hard parts of StyleGAN2, and propose changes in the generator
network to make it possible to deploy style-based generative networks in the
edge devices. We introduce MobileStyleGAN architecture, which has x3.5 fewer
parameters and is x9.5 less computationally complex than StyleGAN2, while
providing comparable quality.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04954">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Artificial learning systems aspire to mimic human intelligence by continually
learning from a stream of tasks without forgetting past knowledge. One way to
enable such learning is to store past experiences in the form of input examples
in episodic memory and replay them when learning new tasks. However,
performance of such method suffers as the size of the memory becomes smaller.
In this paper, we propose a new approach for experience replay, where we select
the past experiences by looking at the saliency maps which provide visual
explanations for the model&#x27;s decision. Guided by these saliency maps, we pack
the memory with only the parts or patches of the input images important for the
model&#x27;s prediction. While learning a new task, we replay these memory patches
with appropriate zero-padding to remind the model about its past decisions. We
evaluate our algorithm on diverse image classification datasets and report
better performance than the state-of-the-art approaches. With qualitative and
quantitative analyses we show that our method captures richer summary of past
experiences without any memory increase, and hence performs well with small
episodic memory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04617">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naively training all tasks
together in one model often degrades performance, and exhaustively searching
through combinations of task groupings can be prohibitively expensive. As a
result, efficiently identifying the tasks that would benefit from co-training
remains a challenging design question without a clear solution. In this paper,
we suggest an approach to select which tasks should train together in
multi-task learning models. Our method determines task groupings in a single
training run by co-training all tasks together and quantifying the effect to
which one task&#x27;s gradient would affect another task&#x27;s loss. On the large-scale
Taskonomy computer vision dataset, we find this method can decrease test loss
by 10.0\% compared to simply training all tasks together while operating 11.6
times faster than a state-of-the-art task grouping method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AAformer: Auto-Aligned Transformer for Person Re-Identification. (arXiv:2104.00921v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00921">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In person re-identification, extracting part-level features from person
images has been verified to be crucial. Most of existing CNN-based methods only
locate the human parts coarsely, or rely on pre-trained human parsing models
and fail in locating the identifiable non-human parts (e.g., knapsack). In this
paper, we introduce an alignment scheme in Transformer architecture for the
first time and propose the Auto-Aligned Transformer (AAformer) to automatically
locate both the human parts and non-human ones at patch-level. We introduce the
&quot;part tokens&quot;, which are learnable vectors, to extract part features in
Transformer. A part token only interacts with a local subset of patches in
self-attention and learns to be the part representation. To adaptively group
the image patches into different subsets, we design the Auto-Alignment.
Auto-Alignment employs a fast variant of Optimal Transport algorithm to online
cluster the patch embeddings into several groups with the part tokens as their
prototypes. We harmoniously integrate the part alignment into the
self-attention and the output part tokens can be directly used for retrieval.
Extensive experiments validate the effectiveness of part tokens and the
superiority of AAformer over various state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detection of GAN-synthesized street videos. (arXiv:2109.04991v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04991">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Research on the detection of AI-generated videos has focused almost
exclusively on face videos, usually referred to as deepfakes. Manipulations
like face swapping, face reenactment and expression manipulation have been the
subject of an intense research with the development of a number of efficient
tools to distinguish artificial videos from genuine ones. Much less attention
has been paid to the detection of artificial non-facial videos. Yet, new tools
for the generation of such kind of videos are being developed at a fast pace
and will soon reach the quality level of deepfake videos. The goal of this
paper is to investigate the detectability of a new kind of AI-generated videos
framing driving street sequences (here referred to as DeepStreets videos),
which, by their nature, can not be analysed with the same tools used for facial
deepfakes. Specifically, we present a simple frame-based detector, achieving
very good performance on state-of-the-art DeepStreets videos generated by the
Vid2vid architecture. Noticeably, the detector retains very good performance on
compressed videos, even when the compression level used during training does
not match that used for the test videos.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Object recognition for robotics from tactile time series data utilising different neural network architectures. (arXiv:2109.04573v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04573">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robots need to exploit high-quality information on grasped objects to
interact with the physical environment. Haptic data can therefore be used for
supplementing the visual modality. This paper investigates the use of
Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) neural
network architectures for object classification on Spatio-temporal tactile
grasping data. Furthermore, we compared these methods using data from two
different fingertip sensors (namely the BioTac SP and WTS-FT) in the same
physical setup, allowing for a realistic comparison across methods and sensors
for the same tactile object classification dataset. Additionally, we propose a
way to create more training examples from the recorded data. The results show
that the proposed method improves the maximum accuracy from 82.4% (BioTac SP
fingertips) and 90.7% (WTS-FT fingertips) with complete time-series data to
about 94% for both sensor types.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nerfies: Deformable Neural Radiance Fields. (arXiv:2011.12948v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12948">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present the first method capable of photorealistically reconstructing
deformable scenes using photos/videos captured casually from mobile phones. Our
approach augments neural radiance fields (NeRF) by optimizing an additional
continuous volumetric deformation field that warps each observed point into a
canonical 5D NeRF. We observe that these NeRF-like deformation fields are prone
to local minima, and propose a coarse-to-fine optimization method for
coordinate-based models that allows for more robust optimization. By adapting
principles from geometry processing and physical simulation to NeRF-like
models, we propose an elastic regularization of the deformation field that
further improves robustness. We show that our method can turn casually captured
selfie photos/videos into deformable NeRF models that allow for photorealistic
renderings of the subject from arbitrary viewpoints, which we dub &quot;nerfies.&quot; We
evaluate our method by collecting time-synchronized data using a rig with two
mobile phones, yielding train/validation images of the same pose at different
viewpoints. We show that our method faithfully reconstructs non-rigidly
deforming scenes and reproduces unseen views with high fidelity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Negative Sample Matters: A Renaissance of Metric Learning for Temporal Grounding. (arXiv:2109.04872v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04872">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal grounding aims to temporally localize a video moment in the video
whose semantics are related to a given natural language query. Existing methods
typically apply a detection or regression pipeline on the fused representation
with a focus on designing complicated heads and fusion strategies. Instead,
from a perspective on temporal grounding as a metric-learning problem, we
present a Dual Matching Network (DMN), to directly model the relations between
language queries and video moments in a joint embedding space. This new
metric-learning framework enables fully exploiting negative samples from two
new aspects: constructing negative cross-modal pairs from a dual matching
scheme and mining negative pairs across different videos. These new negative
samples could enhance the joint representation learning of two modalities via
cross-modal pair discrimination to maximize their mutual information.
Experiments show that DMN achieves highly competitive performance compared with
state-of-the-art methods on four video grounding benchmarks. Based on DMN, we
present a winner solution for STVG challenge of the 3rd PIC workshop. This
suggests that metric-learning is still a promising method for temporal
grounding via capturing the essential cross-modal correlation in a joint
embedding space.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Resolving gas bubbles ascending in liquid metal from low-SNR neutron radiography images. (arXiv:2109.04883v1 [physics.flu-dyn])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04883">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We demonstrate a new image processing methodology for resolving gas bubbles
travelling through liquid metal from dynamic neutron radiography images with
intrinsically low signal-to-noise ratio. Image pre-processing, denoising and
bubble segmentation are described in detail, with practical recommendations.
Experimental validation is presented - stationary and moving reference bodies
with neutron-transparent cavities are radiographed with imaging conditions
similar to the cases with bubbles in liquid metal. The new methods are applied
to our experimental data from previous and recent imaging campaigns, and the
performance of the methods proposed in this paper is compared against our
previously developed methods. Significant improvements are observed as well as
the capacity to reliably extract physically meaningful information from
measurements performed under highly adverse imaging conditions. The showcased
image processing solution and separate elements thereof are readily extendable
beyond the present application, and have been made open-source.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02174">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies indicate that hierarchical Vision Transformer with a macro
architecture of interleaved non-overlapped window-based self-attention \&amp;
shifted-window operation is able to achieve state-of-the-art performance in
various visual recognition tasks, and challenges the ubiquitous convolutional
neural networks (CNNs) using densely slid kernels. Most follow-up works attempt
to replace the shifted-window operation with other kinds of cross-window
communication paradigms, while treating self-attention as the de-facto standard
for window-based information aggregation. In this manuscript, we question
whether self-attention is the only choice for hierarchical Vision Transformer
to attain strong performance, and the effects of different kinds of
cross-window communication. To this end, we replace self-attention layers with
embarrassingly simple linear mapping layers, and the resulting proof-of-concept
architecture termed as LinMapper can achieve very strong performance in
ImageNet-1k image recognition. Moreover, we find that LinMapper is able to
better leverage the pre-trained representations from image recognition and
demonstrates excellent transfer learning properties on downstream dense
prediction tasks such as object detection and instance segmentation. We also
experiment with other alternatives to self-attention for content aggregation
inside each non-overlapped window under different cross-window communication
approaches, which all give similar competitive results. Our study reveals that
the \textbf{macro architecture} of Swin model families, other than specific
aggregation layers or specific means of cross-window communication, may be more
responsible for its strong performance and is the real challenger to the
ubiquitous CNN&#x27;s dense sliding window paradigm. Code and models will be
publicly available to facilitate future research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04993">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-training visual and textual representations from large-scale image-text
pairs is becoming a standard approach for many downstream vision-language
tasks. The transformer-based models learn inter and intra-modal attention
through a list of self-supervised learning tasks. This paper proposes LAViTeR,
a novel architecture for visual and textual representation learning. The main
module, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,
GAN-based image synthesis and Image Captioning. We also propose a new
evaluation metric measuring the similarity between the learnt visual and
textual embedding. The experimental results on two public datasets, CUB and
MS-COCO, demonstrate superior visual and textual representation alignment in
the joint feature embedding space</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Features Guidance Network for partial-to-partial point cloud registration. (arXiv:2011.12079v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.12079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To eliminate the problems of large dimensional differences, big semantic gap,
and mutual interference caused by hybrid features, in this paper, we propose a
novel Multi-Features Guidance Network for partial-to-partial point cloud
registration(MFG). The proposed network mainly includes four parts: keypoints&#x27;
feature extraction, correspondences searching, correspondences credibility
computation, and SVD, among which correspondences searching and correspondence
credibility computation are the cores of the network. Unlike the previous work,
we utilize the shape features and the spatial coordinates to guide
correspondences search independently and fusing the matching results to obtain
the final matching matrix. In the correspondences credibility computation
module, based on the conflicted relationship between the features matching
matrix and the coordinates matching matrix, we score the reliability for each
correspondence, which can reduce the impact of mismatched or non-matched
points. Experimental results show that our network outperforms the current
state-of-the-art while maintaining computational efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FineAction: A Fine-Grained Video Dataset for Temporal Action Localization. (arXiv:2105.11107v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal action localization (TAL) is an important and challenging problem in
video understanding. However, most existing TAL benchmarks are built upon the
coarse granularity of action classes, which exhibits two major limitations in
this task. First, coarse-level actions can make the localization models overfit
in high-level context information, and ignore the atomic action details in the
video. Second, the coarse action classes often lead to the ambiguous
annotations of temporal boundaries, which are inappropriate for temporal action
localization. To tackle these problems, we develop a novel large-scale and
fine-grained video dataset, coined as FineAction, for temporal action
localization. In total, FineAction contains 103K temporal instances of 106
action categories, annotated in 17K untrimmed videos. FineAction introduces new
opportunities and challenges for temporal action localization, thanks to its
distinct characteristics of fine action classes with rich diversity, dense
annotations of multiple instances, and co-occurring actions of different
classes. To benchmark FineAction, we systematically investigate the performance
of several popular temporal localization methods on it, and deeply analyze the
influence of short-duration and fine-grained instances in temporal action
localization. We believe that FineAction can advance research of temporal
action localization and beyond.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00794">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural networks have been successfully used as classification models yielding
state-of-the-art results when trained on a large number of labeled samples.
These models, however, are more difficult to train successfully for
semi-supervised problems where small amounts of labeled instances are available
along with a large number of unlabeled instances. This work explores a new
training method for semi-supervised learning that is based on similarity
function learning using a Siamese network to obtain a suitable embedding. The
learned representations are discriminative in Euclidean space, and hence can be
used for labeling unlabeled instances using a nearest-neighbor classifier.
Confident predictions of unlabeled instances are used as true labels for
retraining the Siamese network on the expanded training set. This process is
applied iteratively. We perform an empirical study of this iterative
self-training algorithm. For improving unlabeled predictions, local learning
with global consistency [22] is also evaluated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mean Shift for Self-Supervised Learning. (arXiv:2105.07269v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07269">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most recent self-supervised learning (SSL) algorithms learn features by
contrasting between instances of images or by clustering the images and then
contrasting between the image clusters. We introduce a simple mean-shift
algorithm that learns representations by grouping images together without
contrasting between them or adopting much of prior on the structure of the
clusters. We simply &quot;shift&quot; the embedding of each image to be close to the
&quot;mean&quot; of its neighbors. Since in our setting, the closest neighbor is always
another augmentation of the same image, our model will be identical to BYOL
when using only one nearest neighbor instead of 5 as used in our experiments.
Our model achieves 72.4% on ImageNet linear evaluation with ResNet50 at 200
epochs outperforming BYOL. Our code is available here:
https://github.com/UMBCvision/MSF</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naive formulations often
degrade performance and in particular, identifying the tasks that would benefit
from co-training remains a challenging design question. In this paper, we
analyze the dynamics of information transfer, or transference, across tasks
throughout training. Specifically, we develop a similarity measure that can
quantify transference among tasks and use this quantity to both better
understand the optimization dynamics of multi-task learning as well as improve
overall learning performance. In the latter case, we propose two methods to
leverage our transference metric. The first operates at a macro-level by
selecting which tasks should train together while the second functions at a
micro-level by determining how to combine task gradients at each training step.
We find these methods can lead to significant improvement over prior work on
three supervised multi-task learning benchmarks and one multi-task
reinforcement learning paradigm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning normal appearance for fetal anomaly screening: Application to the unsupervised detection of Hypoplastic Left Heart Syndrome. (arXiv:2012.03679v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03679">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Congenital heart disease is considered as one the most common groups of
congenital malformations which affects $6-11$ per $1000$ newborns. In this
work, an automated framework for detection of cardiac anomalies during
ultrasound screening is proposed and evaluated on the example of Hypoplastic
Left Heart Syndrome (HLHS), a sub-category of congenital heart disease. We
propose an unsupervised approach that learns healthy anatomy exclusively from
clinically confirmed normal control patients. We evaluate a number of known
anomaly detection frameworks together with a model architecture based on the
$\alpha$-GAN network and find evidence that the proposed model performs
significantly better than the state-of-the-art in image-based anomaly
detection, yielding average $0.81$ AUC \emph{and} a better robustness towards
initialisation compared to previous works.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hidden Markov Models (HMMs) comprise a powerful generative approach for
modeling sequential data and time-series in general. However, the commonly
employed assumption of the dependence of the current time frame to a single or
multiple immediately preceding frames is unrealistic; more complicated dynamics
potentially exist in real world scenarios. This paper revisits conventional
sequential modeling approaches, aiming to address the problem of capturing
time-varying temporal dependency patterns. To this end, we propose a different
formulation of HMMs, whereby the dependence on past frames is dynamically
inferred from the data. Specifically, we introduce a hierarchical extension by
postulating an additional latent variable layer; therein, the (time-varying)
temporal dependence patterns are treated as latent variables over which
inference is performed. We leverage solid arguments from the Variational Bayes
framework and derive a tractable inference algorithm based on the
forward-backward algorithm. As we experimentally show, our approach can model
highly complex sequential data and can effectively handle data with missing
values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07566">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recovering texture information from the aliasing regions has always been a
major challenge for Single Image Super Resolution (SISR) task. These regions
are often submerged in noise so that we have to restore texture details while
suppressing noise. To address this issue, we propose a Balanced Attention
Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and
Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to
suppress extreme noise in the large scale feature maps while MSAM preserves
high-frequency texture details. Thanks to the parallel structure, these two
modules not only conduct self-optimization, but also mutual optimization to
obtain the balance of noise reduction and high-frequency texture restoration
during the back propagation process, and the parallel structure makes the
inference faster. To verify the effectiveness and robustness of BAM, we applied
it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently
improve the networks performance, and for those originally with attention
mechanism, the substitution with BAM further reduces the amount of parameters
and increases the inference speed. Moreover, we present a dataset with rich
texture aliasing regions in real scenes, named realSR7. Experiments prove that
BAM achieves better super-resolution results on the aliasing area.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-imaging real-time detection and tracking of fast-moving objects using a single-pixel detector. (arXiv:2108.06009v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06009">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Detection and tracking of fast-moving objects have widespread utility in many
fields. However, fulfilling this demand for fast and efficient detecting and
tracking using image-based techniques is problematic, owing to the complex
calculations and limited data processing capabilities. To tackle this problem,
we propose an image-free method to achieve real-time detection and tracking of
fast-moving objects. It employs the Hadamard pattern to illuminate the
fast-moving object by a spatial light modulator, in which the resulting light
signal is collected by a single-pixel detector. The single-pixel measurement
values are directly used to reconstruct the position information without image
reconstruction. Furthermore, a new sampling method is used to optimize the
pattern projection way for achieving an ultra-low sampling rate. Compared with
the state-of-the-art methods, our approach is not only capable of handling
real-time detection and tracking, but also it has a small amount of calculation
and high efficiency. We experimentally demonstrate that the proposed method,
using a 22kHz digital micro-mirror device, can implement a 105fps frame rate at
a 1.28% sampling rate when tracks. Our method breaks through the traditional
tracking ways, which can implement the object real-time tracking without image
reconstruction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HyperNeRF: A Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. (arXiv:2106.13228v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural Radiance Fields (NeRF) are able to reconstruct scenes with
unprecedented fidelity, and various recent works have extended NeRF to handle
dynamic scenes. A common approach to reconstruct such non-rigid scenes is
through the use of a learned deformation field mapping from coordinates in each
input image into a canonical template coordinate space. However, these
deformation-based approaches struggle to model changes in topology, as
topological changes require a discontinuity in the deformation field, but these
deformation fields are necessarily continuous. We address this limitation by
lifting NeRFs into a higher dimensional space, and by representing the 5D
radiance field corresponding to each individual input image as a slice through
this &quot;hyper-space&quot;. Our method is inspired by level set methods, which model
the evolution of surfaces as slices through a higher dimensional surface. We
evaluate our method on two tasks: (i) interpolating smoothly between &quot;moments&quot;,
i.e., configurations of the scene, seen in the input images while maintaining
visual plausibility, and (ii) novel-view synthesis at fixed moments. We show
that our method, which we dub HyperNeRF, outperforms existing methods on both
tasks. Compared to Nerfies, HyperNeRF reduces average error rates by 4.1% for
interpolation and 8.6% for novel-view synthesis, as measured by LPIPS.
Additional videos, results, and visualizations are available at
https://hypernerf.github.io.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">&quot;Wikily&quot; Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08384">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a simple but effective approach for leveraging Wikipedia for
neural machine translation as well as cross-lingual tasks of image captioning
and dependency parsing without using any direct supervision from external
parallel data or supervised models in the target language. We show that first
sentences and titles of linked Wikipedia pages, as well as cross-lingual image
captions, are strong signals for a seed parallel data to extract bilingual
dictionaries and cross-lingual word embeddings for mining parallel text from
Wikipedia. Our final model achieves high BLEU scores that are close to or
sometimes higher than strong supervised baselines in low-resource languages;
e.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.
Moreover, we tailor our wikily supervised translation models to unsupervised
image captioning, and cross-lingual dependency parser transfer. In image
captioning, we train a multi-tasking machine translation and image captioning
pipeline for Arabic and English from which the Arabic training data is a
translated version of the English captioning data, using our wikily-supervised
translation models. Our captioning results on Arabic are slightly better than
that of its supervised model. In dependency parsing, we translate a large
amount of monolingual text, and use it as artificial training data in an
annotation projection framework. We show that our model outperforms recent work
on cross-lingual transfer of dependency parsers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">View-Invariant, Occlusion-Robust Probabilistic Embedding for Human Pose. (arXiv:2010.13321v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13321">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Recognition of human poses and actions is crucial for autonomous systems to
interact smoothly with people. However, cameras generally capture human poses
in 2D as images and videos, which can have significant appearance variations
across viewpoints that make the recognition tasks challenging. To address this,
we explore recognizing similarity in 3D human body poses from 2D information,
which has not been well-studied in existing works. Here, we propose an approach
to learning a compact view-invariant embedding space from 2D body joint
keypoints, without explicitly predicting 3D poses. Input ambiguities of 2D
poses from projection and occlusion are difficult to represent through a
deterministic mapping, and therefore we adopt a probabilistic formulation for
our embedding space. Experimental results show that our embedding model
achieves higher accuracy when retrieving similar poses across different camera
views, in comparison with 3D pose estimation models. We also show that by
training a simple temporal embedding model, we achieve superior performance on
pose sequence retrieval and largely reduce the embedding dimension from
stacking frame-based embeddings for efficient large-scale retrieval.
Furthermore, in order to enable our embeddings to work with partially visible
input, we further investigate different keypoint occlusion augmentation
strategies during training. We demonstrate that these occlusion augmentations
significantly improve retrieval performance on partial 2D input poses. Results
on action recognition and video alignment demonstrate that using our embeddings
without any additional training achieves competitive performance relative to
other models specifically trained for each task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a
3D solid volume, exhibits advantages in numerous application domains. However,
existing methods generally synthesize solid texture with specific features,
which may result in the failure of capturing diversified textural information.
In this paper, we propose a novel generative adversarial nets-based approach
(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our
multi-scale discriminators evaluate the similarity between patch from exemplar
and slice from the generated volume, promoting the generator to synthesize
realistic solid textures. Experimental results demonstrate that the proposed
method can generate high-quality solid textures with similar visual
characteristics to the exemplar.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Combining Morphological and Histogram based Text Line Segmentation in the OCR Context. (arXiv:2103.08922v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08922">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text line segmentation is one of the pre-stages of modern optical character
recognition systems. The algorithmic approach proposed by this paper has been
designed for this exact purpose. Its main characteristic is the combination of
two different techniques, morphological image operations and horizontal
histogram projections. The method was developed to be applied on a historic
data collection that commonly features quality issues, such as degraded paper,
blurred text, or presence of noise. For that reason, the segmenter in question
could be of particular interest for cultural institutions, that want access to
robust line bounding boxes for a given historic document. Because of the
promising segmentation results that are joined by low computational cost, the
algorithm was incorporated into the OCR pipeline of the National Library of
Luxembourg, in the context of the initiative of reprocessing their historic
newspaper collection. The general contribution of this paper is to outline the
approach and to evaluate the gains in terms of accuracy and speed, comparing it
to the segmentation algorithm bundled with the used open source OCR software.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Binary change detection in bi-temporal co-registered hyperspectral images is
a challenging task due to a large number of spectral bands present in the data.
Researchers, therefore, try to handle it by reducing dimensions. The proposed
work aims to build a novel feature extraction system using a feature fusion
deep convolutional autoencoder for detecting changes between a pair of such
bi-temporal co-registered hyperspectral images. The feature fusion considers
features across successive levels and multiple receptive fields and therefore
adds a competitive edge over the existing feature extraction methods. The
change detection technique described is completely unsupervised and is much
more elegant than other supervised or semi-supervised methods which require
some amount of label information. Different methods have been applied to the
extracted features to find the changes in the two images and it is found that
the proposed method clearly outperformed the state of the art methods in
unsupervised change detection for all the datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach. (arXiv:2108.05060v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05060">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multitask learning is a common approach in machine learning, which allows to
train multiple objectives with a shared architecture. It has been shown that by
training multiple tasks together inference time and compute resources can be
saved, while the objectives performance remains on a similar or even higher
level. However, in perception related multitask networks only closely related
tasks can be found, such as object detection, instance and semantic
segmentation or depth estimation. Multitask networks with diverse tasks and
their effects with respect to efficiency on one another are not well studied.
In this paper we augment the CenterNet anchor-free approach for training
multiple diverse perception related tasks together, including the task of
object detection and semantic segmentation as well as human pose estimation. We
refer to this DNN as Multitask-CenterNet (MCN). Additionally, we study
different MCN settings for efficiency. The MCN can perform several tasks at
once while maintaining, and in some cases even exceeding, the performance
values of its corresponding single task networks. More importantly, the MCN
architecture decreases inference time and reduces network size when compared to
a composition of single task networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05649">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Measuring concept generalization, i.e., the extent to which models trained on
a set of (seen) visual concepts can be leveraged to recognize a new set of
(unseen) concepts, is a popular way of evaluating visual representations,
especially in a self-supervised learning framework. Nonetheless, the choice of
unseen concepts for such an evaluation is usually made arbitrarily, and
independently from the seen concepts used to train representations, thus
ignoring any semantic relationships between the two. In this paper, we argue
that the semantic relationships between seen and unseen concepts affect
generalization performance and propose ImageNet-CoG, a novel benchmark on the
ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in
a principled way. Our benchmark leverages expert knowledge that comes from
WordNet in order to define a sequence of unseen IN-21K concept sets that are
semantically more and more distant from the ImageNet-1K (IN-1K) subset, a
ubiquitous training set. This allows us to benchmark visual representations
learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31
convolution and transformer-based models and show how different architectures,
levels of supervision, regularization techniques and use of web data impact the
concept generalization performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ISD: Self-Supervised Learning by Iterative Similarity Distillation. (arXiv:2012.09259v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09259">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, contrastive learning has achieved great results in self-supervised
learning, where the main idea is to push two augmentations of an image
(positive pairs) closer compared to other random images (negative pairs). We
argue that not all random images are equal. Hence, we introduce a self
supervised learning algorithm where we use a soft similarity for the negative
images rather than a binary distinction between positive and negative pairs. We
iteratively distill a slowly evolving teacher model to the student model by
capturing the similarity of a query image to some random images and
transferring that knowledge to the student. We argue that our method is less
constrained compared to recent contrastive learning methods, so it can learn
better features. Specifically, our method should handle unbalanced and
unlabeled data better than existing contrastive learning methods, because the
randomly chosen negative set might include many samples that are semantically
similar to the query image. In this case, our method labels them as highly
similar while standard contrastive methods label them as negative pairs. Our
method achieves comparable results to the state-of-the-art models. We also show
that our method performs better in the settings where the unlabeled data is
unbalanced. Our code is available here: https://github.com/UMBCvision/ISD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer. (arXiv:2010.08054v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08054">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Keypoint detection is an essential building block for many robotic
applications like motion capture and pose estimation. Historically, keypoints
are detected using uniquely engineered markers such as checkerboards or
fiducials. More recently, deep learning methods have been explored as they have
the ability to detect user-defined keypoints in a marker-less manner. However,
different manually selected keypoints can have uneven performance when it comes
to detection and localization. An example of this can be found on symmetric
robotic tools where DNN detectors cannot solve the correspondence problem
correctly. In this work, we propose a new and autonomous way to define the
keypoint locations that overcomes these challenges. The approach involves
finding the optimal set of keypoints on robotic manipulators for robust visual
detection and localization. Using a robotic simulator as a medium, our
algorithm utilizes synthetic data for DNN training, and the proposed algorithm
is used to optimize the selection of keypoints through an iterative approach.
The results show that when using the optimized keypoints, the detection
performance of the DNNs improved significantly. We further use the optimized
keypoints for real robotic applications by using domain randomization to bridge
the reality gap between the simulator and the physical world. The physical
world experiments show how the proposed method can be applied to the
wide-breadth of robotic applications that require visual feedback, such as
camera-to-robot calibration, robotic tool tracking, and end-effector pose
estimation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transfer of Pretrained Model Weights Substantially Improves Semi-Supervised Image Classification. (arXiv:2109.00788v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00788">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks produce state-of-the-art results when trained on a large
number of labeled examples but tend to overfit when small amounts of labeled
examples are used for training. Creating a large number of labeled examples
requires considerable resources, time, and effort. If labeling new data is not
feasible, so-called semi-supervised learning can achieve better generalisation
than purely supervised learning by employing unlabeled instances as well as
labeled ones. The work presented in this paper is motivated by the observation
that transfer learning provides the opportunity to potentially further improve
performance by exploiting models pretrained on a similar domain. More
specifically, we explore the use of transfer learning when performing
semi-supervised learning using self-learning. The main contribution is an
empirical evaluation of transfer learning using different combinations of
similarity metric learning methods and label propagation algorithms in
semi-supervised learning. We find that transfer learning always substantially
improves the model&#x27;s accuracy when few labeled examples are available,
regardless of the type of loss used for training the neural network. This
finding is obtained by performing extensive experiments on the SVHN, CIFAR10,
and Plant Village image classification datasets and applying pretrained weights
from Imagenet for transfer learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Backbone for Hyperspectral Image Reconstruction. (arXiv:2108.07739v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07739">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The study of 3D hyperspectral image (HSI) reconstruction refers to the
inverse process of snapshot compressive imaging, during which the optical
system, e.g., the coded aperture snapshot spectral imaging (CASSI) system,
captures the 3D spatial-spectral signal and encodes it to a 2D measurement.
While numerous sophisticated neural networks have been elaborated for
end-to-end reconstruction, trade-offs still need to be made among performance,
efficiency (training and inference time), and feasibility (the ability of
restoring high resolution HSI on limited GPU memory). This raises a challenge
to design a new baseline to conjointly meet the above requirements. In this
paper, we fill in this blank by proposing a Spatial/Spectral Invariant Residual
U-Net, namely SSI-ResU-Net. It differentiates with U-Net in three folds--1)
scale/spectral-invariant learning, 2) nested residual learning, and 3)
computational efficiency. Benefiting from these three modules, the proposed
SSI-ResU-Net outperforms the current state-of-the-art method TSA-Net by over 3
dB in PSNR and 0.036 in SSIM while only using 2.82% trainable parameters. To
the greatest extent, SSI-ResU-Net achieves competing performance with over
77.3% reduction in terms of floating-point operations (FLOPs), which for the
first time, makes high-resolution HSI reconstruction feasible under practical
application scenarios. Code and pre-trained models are made available at
https://github.com/Jiamian-Wang/HSI_baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Face-NMS: A Core-set Selection Approach for Efficient Face Recognition. (arXiv:2109.04698v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04698">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, face recognition in the wild has achieved remarkable success and
one key engine is the increasing size of training data. For example, the
largest face dataset, WebFace42M contains about 2 million identities and 42
million faces. However, a massive number of faces raise the constraints in
training time, computing resources, and memory cost. The current research on
this problem mainly focuses on designing an efficient Fully-connected layer
(FC) to reduce GPU memory consumption caused by a large number of identities.
In this work, we relax these constraints by resolving the redundancy problem of
the up-to-date face datasets caused by the greedily collecting operation (i.e.
the core-set selection perspective). As the first attempt in this perspective
on the face recognition problem, we find that existing methods are limited in
both performance and efficiency. For superior cost-efficiency, we contribute a
novel filtering strategy dubbed Face-NMS. Face-NMS works on feature space and
simultaneously considers the local and global sparsity in generating core sets.
In practice, Face-NMS is analogous to Non-Maximum Suppression (NMS) in the
object detection community. It ranks the faces by their potential contribution
to the overall sparsity and filters out the superfluous face in the pairs with
high similarity for local sparsity. With respect to the efficiency aspect,
Face-NMS accelerates the whole pipeline by applying a smaller but sufficient
proxy dataset in training the proxy model. As a result, with Face-NMS, we
successfully scale down the WebFace42M dataset to 60% while retaining its
performance on the main benchmarks, offering a 40% resource-saving and 1.64
times acceleration. The code is publicly available for reference at
https://github.com/HuangJunJie2017/Face-NMS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Emerging AI Security Threats for Autonomous Cars -- Case Studies. (arXiv:2109.04865v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04865">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Artificial Intelligence has made a significant contribution to autonomous
vehicles, from object detection to path planning. However, AI models require a
large amount of sensitive training data and are usually computationally
intensive to build. The commercial value of such models motivates attackers to
mount various attacks. Adversaries can launch model extraction attacks for
monetization purposes or step-ping-stone towards other attacks like model
evasion. In specific cases, it even results in destroying brand reputation,
differentiation, and value proposition. In addition, IP laws and AI-related
legalities are still evolving and are not uniform across countries. We discuss
model extraction attacks in detail with two use-cases and a generic kill-chain
that can compromise autonomous cars. It is essential to investigate strategies
to manage and mitigate the risk of model theft.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA. (arXiv:2109.05014v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05014">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge-based visual question answering (VQA) involves answering questions
that require external knowledge not present in the image. Existing methods
first retrieve knowledge from external resources, then reason over the selected
knowledge, the input image, and question for answer prediction. However, this
two-step approach could lead to mismatches that potentially limit the VQA
performance. For example, the retrieved knowledge might be noisy and irrelevant
to the question, and the re-embedded knowledge features during reasoning might
deviate from their original meanings in the knowledge base (KB). To address
this challenge, we propose PICa, a simple yet effective method that Prompts
GPT3 via the use of Image Captions, for knowledge-based VQA. Inspired by
GPT-3&#x27;s power in knowledge retrieval and question answering, instead of using
structured KBs as in previous work, we treat GPT-3 as an implicit and
unstructured KB that can jointly acquire and process relevant knowledge.
Specifically, we first convert the image into captions (or tags) that GPT-3 can
understand, then adapt GPT-3 to solve the VQA task in a few-shot manner by just
providing a few in-context VQA examples. We further boost performance by
carefully investigating: (i) what text formats best describe the image content,
and (ii) how in-context examples can be better selected and used. PICa unlocks
the first use of GPT-3 for multimodal tasks. By using only 16 examples, PICa
surpasses the supervised state of the art by an absolute +8.6 points on the
OK-VQA dataset. We also benchmark PICa on VQAv2, where PICa also shows a decent
few-shot performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">View Blind-spot as Inpainting: Self-Supervised Denoising with Mask Guided Residual Convolution. (arXiv:2109.04970v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04970">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, self-supervised denoising methods have shown impressive
performance, which circumvent painstaking collection procedure of noisy-clean
image pairs in supervised denoising methods and boost denoising applicability
in real world. One of well-known self-supervised denoising strategies is the
blind-spot training scheme. However, a few works attempt to improve blind-spot
based self-denoiser in the aspect of network architecture. In this paper, we
take an intuitive view of blind-spot strategy and consider its process of using
neighbor pixels to predict manipulated pixels as an inpainting process.
Therefore, we propose a novel Mask Guided Residual Convolution (MGRConv) into
common convolutional neural networks, e.g. U-Net, to promote blind-spot based
denoising. Our MGRConv can be regarded as soft partial convolution and find a
trade-off among partial convolution, learnable attention maps, and gated
convolution. It enables dynamic mask learning with appropriate mask constrain.
Different from partial convolution and gated convolution, it provides moderate
freedom for network learning. It also avoids leveraging external learnable
parameters for mask activation, unlike learnable attention maps. The
experiments show that our proposed plug-and-play MGRConv can assist blind-spot
based denoising network to reach promising results on both existing
single-image based and dataset-based methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LibFewShot: A Comprehensive Library for Few-shot Learning. (arXiv:2109.04898v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04898">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot learning, especially few-shot image classification, has received
increasing attention and witnessed significant advances in recent years. Some
recent studies implicitly show that many generic techniques or &#x60;&#x60;tricks&#x27;&#x27;, such
as data augmentation, pre-training, knowledge distillation, and
self-supervision, may greatly boost the performance of a few-shot learning
method. Moreover, different works may employ different software platforms,
different training schedules, different backbone architectures and even
different input image sizes, making fair comparisons difficult and
practitioners struggle with reproducibility. To address these situations, we
propose a comprehensive library for few-shot learning (LibFewShot) by
re-implementing seventeen state-of-the-art few-shot learning methods in a
unified framework with the same single codebase in PyTorch. Furthermore, based
on LibFewShot, we provide comprehensive evaluations on multiple benchmark
datasets with multiple backbone architectures to evaluate common pitfalls and
effects of different training tricks. In addition, given the recent doubts on
the necessity of meta- or episodic-training mechanism, our evaluation results
show that such kind of mechanism is still necessary especially when combined
with pre-training. We hope our work can not only lower the barriers for
beginners to work on few-shot learning but also remove the effects of the
nontrivial tricks to facilitate intrinsic research on few-shot learning. The
source code is available from https://github.com/RL-VIG/LibFewShot.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TADA: Taxonomy Adaptive Domain Adaptation. (arXiv:2109.04813v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04813">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Traditional domain adaptation addresses the task of adapting a model to a
novel target domain under limited or no additional supervision. While tackling
the input domain gap, the standard domain adaptation settings assume no domain
change in the output space. In semantic prediction tasks, different datasets
are often labeled according to different semantic taxonomies. In many
real-world settings, the target domain task requires a different taxonomy than
the one imposed by the source domain. We therefore introduce the more general
taxonomy adaptive domain adaptation (TADA) problem, allowing for inconsistent
taxonomies between the two domains. We further propose an approach that jointly
addresses the image-level and label-level domain adaptation. On the
label-level, we employ a bilateral mixed sampling strategy to augment the
target domain, and a relabelling method to unify and align the label spaces. We
address the image-level domain gap by proposing an uncertainty-rectified
contrastive learning method, leading to more domain-invariant and class
discriminative features. We extensively evaluate the effectiveness of our
framework under different TADA settings: open taxonomy, coarse-to-fine
taxonomy, and partially-overlapping taxonomy. Our framework outperforms
previous state-of-the-art by a large margin, while capable of adapting to new
target domain taxonomies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Residual 3D Scene Flow Learning with Context-Aware Feature Extraction. (arXiv:2109.04685v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04685">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scene flow estimation is the task to predict the point-wise 3D displacement
vector between two consecutive frames of point clouds, which has important
application in fields such as service robots and autonomous driving. Although
many previous works have explored greatly on scene flow estimation based on
point clouds, we point out two problems that have not been noticed or well
solved before: 1) Points of adjacent frames in repetitive patterns may be
wrongly associated due to similar spatial structure in their neighbourhoods; 2)
Scene flow between adjacent frames of point clouds with long-distance movement
may be inaccurately estimated. To solve the first problem, we propose a novel
context-aware set conv layer to exploit contextual structure information of
Euclidean space and learn soft aggregation weights for local point features.
Our design is inspired by human perception of contextual structure information
during scene understanding. We incorporate the context-aware set conv layer in
a context-aware point feature pyramid module of 3D point clouds for scene flow
estimation. For the second problem, we propose an explicit residual flow
learning structure in the residual flow refinement layer to cope with
long-distance movement. The experiments and ablation study on FlyingThings3D
and KITTI scene flow datasets demonstrate the effectiveness of each proposed
component and that we solve problem of ambiguous inter-frame association and
long-distance movement estimation. Quantitative results on both FlyingThings3D
and KITTI scene flow datasets show that our method achieves state-of-the-art
performance, surpassing all other previous works to the best of our knowledge
by at least 25%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ACFNet: Adaptively-Cooperative Fusion Network for RGB-D Salient Object Detection. (arXiv:2109.04627v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04627">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The reasonable employment of RGB and depth data show great significance in
promoting the development of computer vision tasks and robot-environment
interaction. However, there are different advantages and disadvantages in the
early and late fusion of the two types of data. Besides, due to the diversity
of object information, using a single type of data in a specific scenario tends
to result in semantic misleading. Based on the above considerations, we propose
an adaptively-cooperative fusion network (ACFNet) with ResinRes structure for
salient object detection. This structure is designed to flexibly utilize the
advantages of feature fusion in early and late stages. Secondly, an
adaptively-cooperative semantic guidance (ACG) scheme is designed to suppress
inaccurate features in the guidance phase. Further, we proposed a type-based
attention module (TAM) to optimize the network and enhance the multi-scale
perception of different objects. For different objects, the features generated
by different types of convolution are enhanced or suppressed by the gated
mechanism for segmentation optimization. ACG and TAM optimize the transfer of
feature streams according to their data attributes and convolution attributes,
respectively. Sufficient experiments conducted on RGB-D SOD datasets illustrate
that the proposed network performs favorably against 18 state-of-the-art
algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">S3G-ARM: Highly Compressive Visual Self-localization from Sequential Semantic Scene Graph Using Absolute and Relative Measurements. (arXiv:2109.04569v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04569">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we address the problem of image sequence-based
self-localization (ISS) from a new highly compressive scene representation
called sequential semantic scene graph (S3G). Recent developments in deep graph
convolutional neural networks (GCNs) have enabled a highly compressive visual
place classifier (VPC) that can use a scene graph as the input modality.
However, in such a highly compressive application, the amount of information
lost in the image-to-graph mapping is significant and can damage the
classification performance. To address this issue, we propose a pair of
similarity-preserving mappings, image-to-nodes and image-to-edges, such that
the nodes and edges act as absolute and relative features, respectively, that
complement each other. Moreover, the proposed GCN-VPC is applied to a new task
of viewpoint planning (VP) of the query image sequence, which contributes to
further improvement in the VPC performance. Experiments using the public NCLT
dataset validated the effectiveness of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Portrait Video Matting via Context Motion Network. (arXiv:2109.04598v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04598">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Our automatic portrait video matting method does not require extra inputs.
Most state-of-the-art matting methods rely on semantic segmentation methods to
automatically generate the trimap. Their performance is compromised due to the
lack of temporal information. Our method exploits semantic information as well
as temporal information from optical flow and produces high-quality results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Computational fluid dynamics (CFD) is a valuable tool for personalised,
non-invasive evaluation of hemodynamics in arteries, but its complexity and
time-consuming nature prohibit large-scale use in practice. Recently, the use
of deep learning for rapid estimation of CFD parameters like wall shear stress
(WSS) on surface meshes has been investigated. However, existing approaches
typically depend on a hand-crafted re-parametrisation of the surface mesh to
match convolutional neural network architectures. In this work, we propose to
instead use mesh convolutional neural networks that directly operate on the
same finite-element surface mesh as used in CFD. We train and evaluate our
method on two datasets of synthetic coronary artery models with and without
bifurcation, using a ground truth obtained from CFD simulation. We show that
our flexible deep learning model can accurately predict 3D WSS vectors on this
surface mesh. Our method processes new meshes in less than 5 [s], consistently
achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5
[%] median approximation accuracy over the held-out test set, comparing
favorably to previously published work. This shows the feasibility of CFD
surrogate modelling using mesh convolutional neural networks for hemodynamic
parameter estimation in artery models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation. (arXiv:2109.04871v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04871">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Event camera has offered promising alternative for visual perception,
especially in high speed and high dynamic range scenes. Recently, many deep
learning methods have shown great success in providing model-free solutions to
many event-based problems, such as optical flow estimation. However, existing
deep learning methods did not address the importance of temporal information
well from the perspective of architecture design and cannot effectively extract
spatio-temporal features. Another line of research that utilizes Spiking Neural
Network suffers from training issues for deeper architecture. To address these
points, a novel input representation is proposed that captures the events
temporal distribution for signal enhancement. Moreover, we introduce a
spatio-temporal recurrent encoding-decoding neural network architecture for
event-based optical flow estimation, which utilizes Convolutional Gated
Recurrent Units to extract feature maps from a series of event images. Besides,
our architecture allows some traditional frame-based core modules, such as
correlation layer and iterative residual refine scheme, to be incorporated. The
network is end-to-end trained with self-supervised learning on the
Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms
all the existing state-of-the-art methods by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ReconfigISP: Reconfigurable Camera Image Processing Pipeline. (arXiv:2109.04760v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04760">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image Signal Processor (ISP) is a crucial component in digital cameras that
transforms sensor signals into images for us to perceive and understand.
Existing ISP designs always adopt a fixed architecture, e.g., several
sequential modules connected in a rigid order. Such a fixed ISP architecture
may be suboptimal for real-world applications, where camera sensors, scenes and
tasks are diverse. In this study, we propose a novel Reconfigurable ISP
(ReconfigISP) whose architecture and parameters can be automatically tailored
to specific data and tasks. In particular, we implement several ISP modules,
and enable backpropagation for each module by training a differentiable proxy,
hence allowing us to leverage the popular differentiable neural architecture
search and effectively search for the optimal ISP architecture. A proxy tuning
mechanism is adopted to maintain the accuracy of proxy networks in all cases.
Extensive experiments conducted on image restoration and object detection, with
different sensors, light conditions and efficiency constraints, validate the
effectiveness of ReconfigISP. Only hundreds of parameters need tuning for every
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Panoptic Narrative Grounding. (arXiv:2109.04988v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04988">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes Panoptic Narrative Grounding, a spatially fine and
general formulation of the natural language visual grounding problem. We
establish an experimental framework for the study of this new task, including
new ground truth and metrics, and we propose a strong baseline method to serve
as stepping stone for future work. We exploit the intrinsic semantic richness
in an image by including panoptic categories, and we approach visual grounding
at a fine-grained level by using segmentations. In terms of ground truth, we
propose an algorithm to automatically transfer Localized Narratives annotations
to specific regions in the panoptic segmentations of the MS COCO dataset. To
guarantee the quality of our annotations, we take advantage of the semantic
structure contained in WordNet to exclusively incorporate noun phrases that are
grounded to a meaningfully related panoptic segmentation region. The proposed
baseline achieves a performance of 55.4 absolute Average Recall points. This
result is a suitable foundation to push the envelope further in the development
of methods for Panoptic Narrative Grounding.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering. (arXiv:2109.04735v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04735">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Video question answering (VideoQA) is challenging given its multimodal
combination of visual understanding and natural language understanding. While
existing approaches seldom leverage the appearance-motion information in the
video at multiple temporal scales, the interaction between the question and the
visual information for textual semantics extraction is frequently ignored.
Targeting these issues, this paper proposes a novel Temporal Pyramid
Transformer (TPT) model with multimodal interaction for VideoQA. The TPT model
comprises two modules, namely Question-specific Transformer (QT) and Visual
Inference (VI). Given the temporal pyramid constructed from a video, QT builds
the question semantics from the coarse-to-fine multimodal co-occurrence between
each word and the visual content. Under the guidance of such question-specific
semantics, VI infers the visual clues from the local-to-global multi-level
interactions between the question and the video. Within each module, we
introduce a multimodal attention mechanism to aid the extraction of
question-video interactions, with residual connections adopted for the
information passing across different levels. Through extensive experiments on
three VideoQA datasets, we demonstrate better performances of the proposed
method in comparison with the state-of-the-arts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization. (arXiv:2109.04527v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04527">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Visual localization is the problem of estimating the position and orientation
from which a given image (or a sequence of images) is taken in a known scene.
It is an important part of a wide range of computer vision and robotics
applications, from self-driving cars to augmented/virtual reality systems.
Visual localization techniques should work reliably and robustly under a wide
range of conditions, including seasonal, weather, illumination and man-made
changes. Recent benchmarking efforts model this by providing images under
different conditions, and the community has made rapid progress on these
datasets since their inception. However, they are limited to a few geographical
regions and often recorded with a single device. We propose a new benchmark for
visual localization in outdoor scenes, using crowd-sourced data to cover a wide
range of geographical regions and camera devices with a focus on the failure
cases of current algorithms. Experiments with state-of-the-art localization
approaches show that our dataset is very challenging, with all evaluated
methods failing on its hardest parts. As part of the dataset release, we
provide the tooling used to generate it, enabling efficient and effective 2D
correspondence annotation to obtain reference poses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Per Garment Capture and Synthesis for Real-time Virtual Try-on. (arXiv:2109.04654v1 [cs.GR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04654">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Virtual try-on is a promising application of computer graphics and human
computer interaction that can have a profound real-world impact especially
during this pandemic. Existing image-based works try to synthesize a try-on
image from a single image of a target garment, but it inherently limits the
ability to react to possible interactions. It is difficult to reproduce the
change of wrinkles caused by pose and body size change, as well as pulling and
stretching of the garment by hand. In this paper, we propose an alternative per
garment capture and synthesis workflow to handle such rich interactions by
training the model with many systematically captured images. Our workflow is
composed of two parts: garment capturing and clothed person image synthesis. We
designed an actuated mannequin and an efficient capturing process that collects
the detailed deformations of the target garments under diverse body sizes and
poses. Furthermore, we proposed to use a custom-designed measurement garment,
and we captured paired images of the measurement garment and the target
garments. We then learn a mapping between the measurement garment and the
target garments using deep image-to-image translation. The customer can then
try on the target garments interactively during online shopping.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Displacement and Vibration Measurement in Laboratory Experiments with A Deep Learning Method. (arXiv:2109.04960v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04960">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a pipeline to automatically track and measure
displacement and vibration of structural specimens during laboratory
experiments. The latest Mask Regional Convolutional Neural Network (Mask R-CNN)
can locate the targets and monitor their movement from videos recorded by a
stationary camera. To improve precision and remove the noise, techniques such
as Scale-invariant Feature Transform (SIFT) and various filters for signal
processing are included. Experiments on three small-scale reinforced concrete
beams and a shaking table test are utilized to verify the proposed method.
Results show that the proposed deep learning method can achieve the goal to
automatically and precisely measure the motion of tested structural members
during laboratory experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.14556">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Accurate lung nodules segmentation from Computed Tomography (CT) images is
crucial to the analysis and diagnosis of lung diseases such as COVID-19 and
lung cancer. However, due to the smallness and variety of lung nodules and the
lack of high-quality labeling, accurate lung nodule segmentation is still a
challenging problem. To address these issues, we propose a complete paradigm
for accurate lung nodules segmentation. First, we introduce a new segmentation
mask named Soft Mask which has richer and more accurate edge details
description and better visualization. Correspondingly, we develop a universal
semi-automatic Soft Mask annotation pipeline to deal with different datasets.
Second, a novel Network with Detailed representation transfer and Soft Mask
supervision (DSNet) is proposed to process the input low-resolution images of
lung nodules into high-quality segmentation results. In our DSNet, we design a
novel Selective Detailed Representation Fusion Module to reconstruct the
detailed representation to alleviate the small size of lung nodules images. In
addition, the adversarial training framework with Soft Mask is proposed to
further improve the accuracy of segmentation. Extensive experiments validate
that our DSNet outperforms the state-of-the-art methods for accurate lung
nodules segmentation. And our method also demonstrates competitive results in
other accurate medical segmentation tasks. Besides, we provide a new
challenging lung nodules segmentation dataset for further studies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PIP: Physical Interaction Prediction via Mental Imagery with Span Selection. (arXiv:2109.04683v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04683">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To align advanced artificial intelligence (AI) with human values and promote
safe AI, it is important for AI to predict the outcome of physical
interactions. Even with the ongoing debates on how humans predict the outcomes
of physical interactions among objects in the real world, there are works
attempting to tackle this task via cognitive-inspired AI approaches. However,
there is still a lack of AI approaches that mimic the mental imagery humans use
to predict physical interactions in the real world. In this work, we propose a
novel PIP scheme: Physical Interaction Prediction via Mental Imagery with Span
Selection. PIP utilizes a deep generative model to output future frames of
physical interactions among objects before extracting crucial information for
predicting physical interactions by focusing on salient frames using span
selection. To evaluate our model, we propose a large-scale SPACE+ dataset of
synthetic video frames, including three physical interaction events in a 3D
environment. Our experiments show that PIP outperforms baselines and human
performance in physical interaction prediction for both seen and unseen
objects. Furthermore, PIP&#x27;s span selection scheme can effectively identify the
frames where physical interactions among objects occur within the generated
frames, allowing for added interpretability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04600">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal grounding aims to predict a time interval of a video clip
corresponding to a natural language query input. In this work, we present
EVOQUER, a temporal grounding framework incorporating an existing text-to-video
grounding model and a video-assisted query generation network. Given a query
and an untrimmed video, the temporal grounding model predicts the target
interval, and the predicted video clip is fed into a video translation task by
generating a simplified version of the input query. EVOQUER forms closed-loop
learning by incorporating loss functions from both temporal grounding and query
generation serving as feedback. Our experiments on two widely used datasets,
Charades-STA and ActivityNet, show that EVOQUER achieves promising improvements
by 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could
facilitate error analysis by explaining temporal grounding model behavior.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11861">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We learn a latent space for easy capture, consistent interpolation, and
efficient reproduction of visual material appearance. When users provide a
photo of a stationary natural material captured under flashlight illumination,
first it is converted into a latent material code. Then, in the second step,
conditioned on the material code, our method produces an infinite and diverse
spatial field of BRDF model parameters (diffuse albedo, normals, roughness,
specular albedo) that subsequently allows rendering in complex scenes and
illuminations, matching the appearance of the input photograph. Technically, we
jointly embed all flash images into a latent space using a convolutional
encoder, and -- conditioned on these latent codes -- convert random spatial
fields into fields of BRDF parameters using a convolutional neural network
(CNN). We condition these BRDF parameters to match the visual characteristics
(statistics and spectra of visual features) of the input under matching light.
A user study compares our approach favorably to previous work, even those with
access to BRDF supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Line as a Visual Sentence: Context-aware Line Descriptor for Visual Localization. (arXiv:2109.04753v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Along with feature points for image matching, line features provide
additional constraints to solve visual geometric problems in robotics and
computer vision (CV). Although recent convolutional neural network (CNN)-based
line descriptors are promising for viewpoint changes or dynamic environments,
we claim that the CNN architecture has innate disadvantages to abstract
variable line length into the fixed-dimensional descriptor. In this paper, we
effectively introduce Line-Transformers dealing with variable lines. Inspired
by natural language processing (NLP) tasks where sentences can be understood
and abstracted well in neural nets, we view a line segment as a sentence that
contains points (words). By attending to well-describable points on aline
dynamically, our descriptor performs excellently on variable line length. We
also propose line signature networks sharing the line&#x27;s geometric attributes to
neighborhoods. Performing as group descriptors, the networks enhance line
descriptors by understanding lines&#x27; relative geometries. Finally, we present
the proposed line descriptor and matching in a Point and Line Localization
(PL-Loc). We show that the visual localization with feature points can be
improved using our line features. We validate the proposed method for
homography estimation and visual localization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04553">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As an essential ingredient of modern deep learning, attention mechanism,
especially self-attention, plays a vital role in the global correlation
discovery. However, is hand-crafted attention irreplaceable when modeling the
global context? Our intriguing finding is that self-attention is not better
than the matrix decomposition (MD) model developed 20 years ago regarding the
performance and computational cost for encoding the long-distance dependencies.
We model the global context issue as a low-rank recovery problem and show that
its optimization algorithms can help design global information blocks. This
paper then proposes a series of Hamburgers, in which we employ the optimization
algorithms for solving MDs to factorize the input representations into
sub-matrices and reconstruct a low-rank embedding. Hamburgers with different
MDs can perform favorably against the popular global context module
self-attention when carefully coping with gradients back-propagated through
MDs. Comprehensive experiments are conducted in the vision tasks where it is
crucial to learn the global context, including semantic segmentation and image
generation, demonstrating significant improvements over self-attention and its
variants.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Keeping track of scientific challenges, advances and emerging directions is a
fundamental part of research. However, researchers face a flood of papers that
hinders discovery of important knowledge. In biomedicine, this directly impacts
human lives. To address this problem, we present a novel task of extraction and
search of scientific challenges and directions, to facilitate rapid knowledge
discovery. We construct and release an expert-annotated corpus of texts sampled
from full-length papers, labeled with novel semantic categories that generalize
across many types of challenges and directions. We focus on a large corpus of
interdisciplinary work relating to the COVID-19 pandemic, ranging from
biomedicine to areas such as AI and economics. We apply a model trained on our
data to identify challenges and directions across the corpus and build a
dedicated search engine. In experiments with 19 researchers and clinicians
using our system, we outperform a popular scientific search engine in assisting
knowledge discovery. Finally, we show that models trained on our resource
generalize to the wider biomedical domain and to AI papers, highlighting its
broad utility. We make our data, model and search engine publicly available.
https://challenges.apps.allenai.org/</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Web image search engine based on LSH index and CNN Resnet50. (arXiv:2108.13301v1 [cs.IR] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13301">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To implement a good Content Based Image Retrieval (CBIR) system, it is
essential to adopt efficient search methods. One way to achieve this results is
by exploiting approximate search techniques. In fact, when we deal with very
large collections of data, using an exact search method makes the system very
slow. In this project, we adopt the Locality Sensitive Hashing (LSH) index to
implement a CBIR system that allows us to perform fast similarity search on
deep features. Specifically, we exploit transfer learning techniques to extract
deep features from images; this phase is done using two famous Convolutional
Neural Networks (CNNs) as features extractors: Resnet50 and Resnet50v2, both
pre-trained on ImageNet. Then we try out several fully connected deep neural
networks, built on top of both of the previously mentioned CNNs in order to
fine-tuned them on our dataset. In both of previous cases, we index the
features within our LSH index implementation and within a sequential scan, to
better understand how much the introduction of the index affects the results.
Finally, we carry out a performance analysis: we evaluate the relevance of the
result set, computing the mAP (mean Average Precision) value obtained during
the different experiments with respect to the number of done comparison and
varying the hyper-parameter values of the LSH index.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">You Get What You Chat: Using Conversations to Personalize Search-based Recommendations. (arXiv:2109.04716v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04716">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Prior work on personalized recommendations has focused on exploiting explicit
signals from user-specific queries, clicks, likes, and ratings. This paper
investigates tapping into a different source of implicit signals of interests
and tastes: online chats between users. The paper develops an expressive model
and effective methods for personalizing search-based entity recommendations.
User models derived from chats augment different methods for re-ranking entity
answers for medium-grained queries. The paper presents specific techniques to
enhance the user models by capturing domain-specific vocabularies and by
entity-based expansion. Experiments are based on a collection of online chats
from a controlled user study covering three domains: books, travel, food. We
evaluate different configurations and compare chat-based user models against
concise user profiles from questionnaires. Overall, these two variants perform
on par in terms of NCDG@20, but each has advantages in certain domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12800">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.
Similar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,
Query Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from
55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query
Bank and Relevance Set, where the former contains 1,236 human-paraphrased
queries while the latter contains ~32 human-annotated FAQ items for each query.
We analyze COUGH by testing different FAQ retrieval models built on top of BM25
and BERT, among which the best model achieves 48.8 under P@5, indicating a
great challenge presented by COUGH and encouraging future research for further
improvement. Our COUGH dataset is available at
https://github.com/sunlab-osu/covid-faq.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04726">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural models for low-resource named entity recognition (NER) have shown
impressive results by leveraging distant super-vision or other meta-level
information (e.g. explanation). However, the costs of acquiring such additional
information are generally prohibitive, especially in domains where existing
resources (e.g. databases to be used for distant supervision) may not exist. In
this paper, we present a novel two-stage framework (AutoTriggER) to improve NER
performance by automatically generating and leveraging &quot;entity triggers&quot; which
are essentially human-readable clues in the text that can help guide the model
to make better decisions. Thus, the framework is able to both create and
leverage auxiliary supervision by itself. Through experiments on three
well-studied NER datasets, we show that our automatically extracted triggers
are well-matched to human triggers, and AutoTriggER improves performance over a
RoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a
low resource setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Personalized Entity Search by Sparse and Scrutable User Profiles. (arXiv:2109.04713v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04713">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Prior work on personalizing web search results has focused on considering
query-and-click logs to capture users individual interests. For product search,
extensive user histories about purchases and ratings have been exploited.
However, for general entity search, such as for books on specific topics or
travel destinations with certain features, personalization is largely
underexplored. In this paper, we address personalization of book search, as an
exemplary case of entity search, by exploiting sparse user profiles obtained
through online questionnaires. We devise and compare a variety of re-ranking
methods based on language models or neural learning. Our experiments show that
even very sparse information about individuals can enhance the effectiveness of
the search results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trust your neighbors: A comprehensive survey of neighborhood-based methods for recommender systems. (arXiv:2109.04584v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04584">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Collaborative recommendation approaches based on nearest-neighbors are still
highly popular today due to their simplicity, their efficiency, and their
ability to produce accurate and personalized recommendations. This chapter
offers a comprehensive survey of neighborhood-based methods for the item
recommendation problem. It presents the main characteristics and benefits of
such methods, describes key design choices for implementing a
neighborhood-based recommender system, and gives practical information on how
to make these choices. A broad range of methods is covered in the chapter,
including traditional algorithms like k-nearest neighbors as well as advanced
approaches based on matrix factorization, sparse coding and random walks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05669">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Isolated silos of scientific research and the growing challenge of
information overload limit awareness across the literature and hinder
innovation. Algorithmic curation and recommendation, which often prioritize
relevance, can further reinforce these informational &quot;filter bubbles.&quot; In
response, we describe Bridger, a system for facilitating discovery of scholars
and their work, to explore design tradeoffs between relevant and novel
recommendations. We construct a faceted representation of authors with
information gleaned from their papers and inferred author personas, and use it
to develop an approach that locates commonalities (&quot;bridges&quot;) and contrasts
between scientists -- retrieving partially similar authors rather than aiming
for strict similarity. In studies with computer science researchers, this
approach helps users discover authors considered useful for generating novel
research directions, outperforming a state-of-art neural model. In addition to
recommending new content, we also demonstrate an approach for displaying it in
a manner that boosts researchers&#x27; ability to understand the work of authors
with whom they are unfamiliar. Finally, our analysis reveals that Bridger
connects authors who have different citation profiles, publish in different
venues, and are more distant in social co-authorship networks, raising the
prospect of bridging diverse communities and facilitating discovery.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing the Quality of the Datasets by Identifying Mislabeled Samples. (arXiv:2109.05000v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05000">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Due to the over-emphasize of the quantity of data, the data quality has often
been overlooked. However, not all training data points contribute equally to
learning. In particular, if mislabeled, it might actively damage the
performance of the model and the ability to generalize out of distribution, as
the model might end up learning spurious artifacts present in the dataset. This
problem gets compounded by the prevalence of heavily parameterized and complex
deep neural networks, which can, with their high capacity, end up memorizing
the noise present in the dataset. This paper proposes a novel statistic --
noise score, as a measure for the quality of each data point to identify such
mislabeled samples based on the variations in the latent space representation.
In our work, we use the representations derived by the inference network of
data quality supervised variational autoencoder (AQUAVS). Our method leverages
the fact that samples belonging to the same class will have similar latent
representations. Therefore, by identifying the outliers in the latent space, we
can find the mislabeled samples. We validate our proposed statistic through
experimentation by corrupting MNIST, FashionMNIST, and CIFAR10/100 datasets in
different noise settings for the task of identifying mislabelled samples. We
further show significant improvements in accuracy for the classification task
for each dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04708">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The majority of language domains require prudent use of terminology to ensure
clarity and adequacy of information conveyed. While the correct use of
terminology for some languages and domains can be achieved by adapting
general-purpose MT systems on large volumes of in-domain parallel data, such
quantities of domain-specific data are seldom available for less-resourced
languages and niche domains. Furthermore, as exemplified by COVID-19 recently,
no domain-specific parallel data is readily available for emerging domains.
However, the gravity of this recent calamity created a high demand for reliable
translation of critical information regarding pandemic and infection
prevention. This work is part of WMT2021 Shared Task: Machine Translation using
Terminologies, where we describe Tilde MT systems that are capable of dynamic
terminology integration at the time of translation. Our systems achieve up to
94% COVID-19 term use accuracy on the test set of the EN-FR language pair
without having access to any form of in-domain information during system
training. We conclude our work with a broader discussion considering the Shared
Task itself and terminology translation in MT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04611">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer-based rankers have shown state-of-the-art performance. However,
their self-attention operation is mostly unable to process long sequences. One
of the common approaches to train these rankers is to heuristically select some
segments of each document, such as the first segment, as training data.
However, these segments may not contain the query-related parts of documents.
To address this problem, we propose query-driven segment selection from long
documents to build training data. The segment selector provides relevant
samples with more accurate labels and non-relevant samples which are harder to
be predicted. The experimental results show that the basic BERT-based ranker
trained with the proposed segment selector significantly outperforms that
trained by the heuristically selected segments, and performs equally to the
state-of-the-art model with localized self-attention that can process longer
input sequences. Our findings open up new direction to design efficient
transformer-based rankers.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Labeling data can be an expensive task as it is usually performed manually by
domain experts. This is cumbersome for deep learning, as it is dependent on
large labeled datasets. Active learning (AL) is a paradigm that aims to reduce
labeling effort by only using the data which the used model deems most
informative. Little research has been done on AL in a text classification
setting and next to none has involved the more recent, state-of-the-art NLP
models. Here, we present an empirical study that compares different
uncertainty-based algorithms with BERT$_{base}$ as the used classifier. We
evaluate the algorithms on two NLP classification datasets: Stanford Sentiment
Treebank and KvK-Frontpages. Additionally, we explore heuristics that aim to
solve presupposed problems of uncertainty-based AL; namely, that it is
unscalable and that it is prone to selecting outliers. Furthermore, we explore
the influence of the query-pool size on the performance of AL. Whereas it was
found that the proposed heuristics for AL did not improve performance of AL;
our results show that using uncertainty-based AL with BERT$_{base}$ outperforms
random sampling of data. This difference in performance can decrease as the
query-pool size gets larger.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09697">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The capacity of neural networks like the widely adopted transformer is known
to be very high. Evidence is emerging that they learn successfully due to
inductive bias in the training routine, typically a variant of gradient descent
(GD). To better understand this bias, we study the tendency for transformer
parameters to grow in magnitude ($\ell_2$ norm) during training, and its
implications for the emergent representations within self attention layers.
Empirically, we document norm growth in the training of transformer language
models, including T5 during its pretraining. As the parameters grow in
magnitude, we prove that the network approximates a discretized network with
saturated activation functions. Such &quot;saturated&quot; networks are known to have a
reduced capacity compared to the full network family that can be described in
terms of formal languages and automata. Our results suggest saturation is a new
characterization of an inductive bias implicit in GD of particular interest for
NLP. We leverage the emergent discrete structure in a saturated transformer to
analyze the role of different attention heads, finding that some focus locally
on a small number of positions, while other heads compute global averages,
allowing counting. We believe understanding the interplay between these two
capabilities may shed further light on the structure of computation within
large transformers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05845">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding what sequence of steps are needed to complete a goal can help
artificial intelligence systems reason about human activities. Past work in NLP
has examined the task of goal-step inference for text. We introduce the visual
analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model
is given a textual goal and must choose which of four images represents a
plausible step towards that goal. With a new dataset harvested from wikiHow
consisting of 772,277 images representing human actions, we show that our task
is challenging for state-of-the-art multimodal models. Moreover, the multimodal
representation learned from our data can be effectively transferred to other
datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task
will facilitate multimodal reasoning about procedural events.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04912">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present ReasonBert, a pre-training method that augments language models
with the ability to reason over long-range relations and multiple, possibly
hybrid contexts. Unlike existing pre-training methods that only harvest
learning signals from local contexts of naturally occurring texts, we propose a
generalized notion of distant supervision to automatically connect multiple
pieces of text and tables to create pre-training examples that require
long-range reasoning. Different types of reasoning are simulated, including
intersecting multiple pieces of evidence, bridging from one piece of evidence
to another, and detecting unanswerable cases. We conduct a comprehensive
evaluation on a variety of extractive question answering datasets ranging from
single-hop to multi-hop and from text-only to table-only to hybrid that require
various reasoning capabilities and show that ReasonBert achieves remarkable
improvement over an array of strong baselines. Few-shot experiments further
demonstrate that our pre-training method substantially improves sample
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-label Classification of Aircraft Heading Changes Using Neural Network to Resolve Conflicts. (arXiv:2109.04767v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04767">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An aircraft conflict occurs when two or more aircraft cross at a certain
distance at the same time. Specific air traffic controllers are assigned to
solve such conflicts. A controller needs to consider various types of
information in order to solve a conflict. The most common and preliminary
information is the coordinate position of the involved aircraft. Additionally,
a controller has to take into account more information such as flight planning,
weather, restricted territory, etc. The most important challenges a controller
has to face are: to think about the issues involved and make a decision in a
very short time. Due to the increased number of aircraft, it is crucial to
reduce the workload of the controllers and help them make quick decisions. A
conflict can be solved in many ways, therefore, we consider this problem as a
multi-label classification problem. In doing so, we are proposing a multi-label
classification model which provides multiple heading advisories for a given
conflict. This model we named CRMLnet is based on a novel application of a
multi-layer neural network and helps the controllers in their decisions. When
compared to other machine learning models, our CRMLnet has achieved the best
results with an accuracy of 98.72% and ROC of 0.999. The simulated data set
that we have developed and used in our experiments will be delivered to the
research community.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">C-MinHash: Practically Reducing Two Permutations to Just One. (arXiv:2109.04595v1 [cs.DS])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04595">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Traditional minwise hashing (MinHash) requires applying $K$ independent
permutations to estimate the Jaccard similarity in massive binary (0/1) data,
where $K$ can be (e.g.,) 1024 or even larger, depending on applications. The
recent work on C-MinHash (Li and Li, 2021) has shown, with rigorous proofs,
that only two permutations are needed. An initial permutation is applied to
break whatever structures which might exist in the data, and a second
permutation is re-used $K$ times to produce $K$ hashes, via a circulant
shifting fashion. (Li and Li, 2021) has proved that, perhaps surprisingly, even
though the $K$ hashes are correlated, the estimation variance is strictly
smaller than the variance of the traditional MinHash.

It has been demonstrated in (Li and Li, 2021) that the initial permutation in
C-MinHash is indeed necessary. For the ease of theoretical analysis, they have
used two independent permutations. In this paper, we show that one can actually
simply use one permutation. That is, one single permutation is used for both
the initial pre-processing step to break the structures in the data and the
circulant hashing step to generate $K$ hashes. Although the theoretical
analysis becomes very complicated, we are able to explicitly write down the
expression for the expectation of the estimator. The new estimator is no longer
unbiased but the bias is extremely small and has essentially no impact on the
estimation accuracy (mean square errors). An extensive set of experiments are
provided to verify our claim for using just one permutation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BAM: A Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v3 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07566">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recovering texture information from the aliasing regions has always been a
major challenge for Single Image Super Resolution (SISR) task. These regions
are often submerged in noise so that we have to restore texture details while
suppressing noise. To address this issue, we propose a Balanced Attention
Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and
Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to
suppress extreme noise in the large scale feature maps while MSAM preserves
high-frequency texture details. Thanks to the parallel structure, these two
modules not only conduct self-optimization, but also mutual optimization to
obtain the balance of noise reduction and high-frequency texture restoration
during the back propagation process, and the parallel structure makes the
inference faster. To verify the effectiveness and robustness of BAM, we applied
it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently
improve the networks performance, and for those originally with attention
mechanism, the substitution with BAM further reduces the amount of parameters
and increases the inference speed. Moreover, we present a dataset with rich
texture aliasing regions in real scenes, named realSR7. Experiments prove that
BAM achieves better super-resolution results on the aliasing area.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A conditional one-output likelihood formulation for multitask Gaussian processes. (arXiv:2006.03495v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.03495">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multitask Gaussian processes (MTGP) are the Gaussian process (GP) framework&#x27;s
solution for multioutput regression problems in which the $T$ elements of the
regressors cannot be considered conditionally independent given the
observations. Standard MTGP models assume that there exist both a multitask
covariance matrix as a function of an intertask matrix, and a noise covariance
matrix. These matrices need to be approximated by a low rank simplification of
order $P$ in order to reduce the number of parameters to be learnt from $T^2$
to $TP$. Here we introduce a novel approach that simplifies the multitask
learning by reducing it to a set of conditioned univariate GPs without the need
for any low rank approximations, therefore completely eliminating the
requirement to select an adequate value for hyperparameter $P$. At the same
time, by extending this approach with both a hierarchical and an approximate
model, the proposed extensions are capable of recovering the multitask
covariance and noise matrices after learning only $2T$ parameters, avoiding the
validation of any model hyperparameter and reducing the overall complexity of
the model as well as the risk of overfitting. Experimental results over
synthetic and real problems confirm the advantages of this inference approach
in its ability to accurately recover the original noise and signal matrices, as
well as the achieved performance improvement in comparison to other state of
art MTGP approaches. We have also integrated the model with standard GP
toolboxes, showing that it is computationally competitive with state of the art
options.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rapid Model Architecture Adaption for Meta-Learning. (arXiv:2109.04925v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04925">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Network Architecture Search (NAS) methods have recently gathered much
attention. They design networks with better performance and use a much shorter
search time compared to traditional manual tuning. Despite their efficiency in
model deployments, most NAS algorithms target a single task on a fixed hardware
system. However, real-life few-shot learning environments often cover a great
number of tasks (T ) and deployments on a wide variety of hardware platforms (H
).

The combinatorial search complexity T times H creates a fundamental search
efficiency challenge if one naively applies existing NAS methods to these
scenarios. To overcome this issue, we show, for the first time, how to rapidly
adapt model architectures to new tasks in a many-task many-hardware few-shot
learning setup by integrating Model Agnostic Meta Learning (MAML) into the NAS
flow. The proposed NAS method (H-Meta-NAS) is hardware-aware and performs
optimisation in the MAML framework. H-Meta-NAS shows a Pareto dominance
compared to a variety of NAS and manual baselines in popular few-shot learning
benchmarks with various hardware platforms and constraints. In particular, on
the 5-way 1-shot Mini-ImageNet classification task, the proposed method
outperforms the best manual baseline by a large margin (5.21% in accuracy)
using 60% less computation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text classification systems based on contextual embeddings are not viable
options for many of the low resource languages. On the other hand, recently
introduced capsule networks have shown performance in par with these text
classification models. Thus, they could be considered as a viable alternative
for text classification for languages that do not have pre-trained contextual
embedding models. However, current capsule networks depend upon spatial
patterns without considering the sequential features of the text. They are also
sub-optimal in capturing the context-level information in longer sequences.
This paper presents a novel Dual-State Capsule (DS-Caps) network-based
technique for text classification, which is optimized to mitigate these issues.
Two varieties of states, namely sentence-level and word-level, are integrated
with capsule layers to capture deeper context-level information for language
modeling. The dynamic routing process among capsules was also optimized using
the context-level information obtained through sentence-level states. The
DS-Caps networks outperform the existing capsule network architectures for
multiple datasets, particularly for tasks with longer sequences of text. We
also demonstrate the superiority of DS-Caps in text classification for a low
resource language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-agent deep reinforcement learning (MADRL) meets multi-user MIMO systems. (arXiv:2109.04986v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A multi-agent deep reinforcement learning (MADRL) is a promising approach to
challenging problems in wireless environments involving multiple
decision-makers (or actors) with high-dimensional continuous action space. In
this paper, we present a MADRL-based approach that can jointly optimize
precoders to achieve the outer-boundary, called pareto-boundary, of the
achievable rate region for a multiple-input single-output (MISO) interference
channel (IFC). In order to address two main challenges, namely, multiple actors
(or agents) with partial observability and multi-dimensional continuous action
space in MISO IFC setup, we adopt a multi-agent deep deterministic policy
gradient (MA-DDPG) framework in which decentralized actors with partial
observability can learn a multi-dimensional continuous policy in a centralized
manner with the aid of shared critic with global information. Meanwhile, we
will also address a phase ambiguity issue with the conventional complex
baseband representation of signals widely used in radio communications. In
order to mitigate the impact of phase ambiguity on training performance, we
propose a training method, called phase ambiguity elimination (PAE), that leads
to faster learning and better performance of MA-DDPG in wireless communication
systems. The simulation results exhibit that MA-DDPG is capable of learning a
near-optimal precoding strategy in a MISO IFC environment. To the best of our
knowledge, this is the first work to demonstrate that the MA-DDPG framework can
jointly optimize precoders to achieve the pareto-boundary of achievable rate
region in a multi-cell multi-user multi-antenna system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04593">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent advances in neural machine translation (NMT) have pushed the quality
of machine translation systems to the point where they are becoming widely
adopted to build competitive systems. However, there is still a large number of
languages that are yet to reap the benefits of NMT. In this paper, we provide
the first large-scale case study of the practical application of MT in the
Turkic language family in order to realize the gains of NMT for Turkic
languages under high-resource to extremely low-resource scenarios. In addition
to presenting an extensive analysis that identifies the bottlenecks towards
building competitive systems to ameliorate data scarcity, our study has several
key contributions, including, i) a large parallel corpus covering 22 Turkic
languages consisting of common public datasets in combination with new datasets
of approximately 2 million parallel sentences, ii) bilingual baselines for 26
language pairs, iii) novel high-quality test sets in three different
translation domains and iv) human evaluation scores. All models, scripts, and
data will be released to the public.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07637">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Natural languages display a trade-off among different strategies to convey
syntactic structure, such as word order or inflection. This trade-off, however,
has not appeared in recent simulations of iterated language learning with
neural network agents (Chaabouni et al., 2019b). We re-evaluate this result in
light of three factors that play an important role in comparable experiments
from the Language Evolution field: (i) speaker bias towards efficient
messaging, (ii) non systematic input languages, and (iii) learning bottleneck.
Our simulations show that neural agents mainly strive to maintain the utterance
type distribution observed during learning, instead of developing a more
efficient or systematic language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mesh convolutional neural networks for wall shear stress estimation in 3D artery models. (arXiv:2109.04797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Computational fluid dynamics (CFD) is a valuable tool for personalised,
non-invasive evaluation of hemodynamics in arteries, but its complexity and
time-consuming nature prohibit large-scale use in practice. Recently, the use
of deep learning for rapid estimation of CFD parameters like wall shear stress
(WSS) on surface meshes has been investigated. However, existing approaches
typically depend on a hand-crafted re-parametrisation of the surface mesh to
match convolutional neural network architectures. In this work, we propose to
instead use mesh convolutional neural networks that directly operate on the
same finite-element surface mesh as used in CFD. We train and evaluate our
method on two datasets of synthetic coronary artery models with and without
bifurcation, using a ground truth obtained from CFD simulation. We show that
our flexible deep learning model can accurately predict 3D WSS vectors on this
surface mesh. Our method processes new meshes in less than 5 [s], consistently
achieves a normalised mean absolute error of $\leq$ 1.6 [%], and peaks at 90.5
[%] median approximation accuracy over the held-out test set, comparing
favorably to previously published work. This shows the feasibility of CFD
surrogate modelling using mesh convolutional neural networks for hemodynamic
parameter estimation in artery models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Unsupervised Anomaly Detection with Score-Guided Network. (arXiv:2109.04684v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04684">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Anomaly detection plays a crucial role in various real-world applications,
including healthcare and finance systems. Owing to the limited number of
anomaly labels in these complex systems, unsupervised anomaly detection methods
have attracted great attention in recent years. Two major challenges faced by
the existing unsupervised methods are: (i) distinguishing between normal and
abnormal data in the transition field, where normal and abnormal data are
highly mixed together; (ii) defining an effective metric to maximize the gap
between normal and abnormal data in a hypothesis space, which is built by a
representation learner. To that end, this work proposes a novel scoring network
with a score-guided regularization to learn and enlarge the anomaly score
disparities between normal and abnormal data. With such score-guided strategy,
the representation learner can gradually learn more informative representation
during the model training stage, especially for the samples in the transition
field. We next propose a score-guided autoencoder (SG-AE), incorporating the
scoring network into an autoencoder framework for anomaly detection, as well as
other three state-of-the-art models, to further demonstrate the effectiveness
and transferability of the design. Extensive experiments on both synthetic and
real-world datasets demonstrate the state-of-the-art performance of these
score-guided models (SGMs).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Saliency Guided Experience Packing for Replay in Continual Learning. (arXiv:2109.04954v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04954">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Artificial learning systems aspire to mimic human intelligence by continually
learning from a stream of tasks without forgetting past knowledge. One way to
enable such learning is to store past experiences in the form of input examples
in episodic memory and replay them when learning new tasks. However,
performance of such method suffers as the size of the memory becomes smaller.
In this paper, we propose a new approach for experience replay, where we select
the past experiences by looking at the saliency maps which provide visual
explanations for the model&#x27;s decision. Guided by these saliency maps, we pack
the memory with only the parts or patches of the input images important for the
model&#x27;s prediction. While learning a new task, we replay these memory patches
with appropriate zero-padding to remind the model about its past decisions. We
evaluate our algorithm on diverse image classification datasets and report
better performance than the state-of-the-art approaches. With qualitative and
quantitative analyses we show that our method captures richer summary of past
experiences without any memory increase, and hence performs well with small
episodic memory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Projected State-action Balancing Weights for Offline Reinforcement Learning. (arXiv:2109.04640v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04640">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Offline policy evaluation (OPE) is considered a fundamental and challenging
problem in reinforcement learning (RL). This paper focuses on the value
estimation of a target policy based on pre-collected data generated from a
possibly different policy, under the framework of infinite-horizon Markov
decision processes. Motivated by the recently developed marginal importance
sampling method in RL and the covariate balancing idea in causal inference, we
propose a novel estimator with approximately projected state-action balancing
weights for the policy value estimation. We obtain the convergence rate of
these weights, and show that the proposed value estimator is semi-parametric
efficient under technical conditions. In terms of asymptotics, our results
scale with both the number of trajectories and the number of decision points at
each trajectory. As such, consistency can still be achieved with a limited
number of subjects when the number of decision points diverges. In addition, we
make a first attempt towards characterizing the difficulty of OPE problems,
which may be of independent interest. Numerical experiments demonstrate the
promising performance of our proposed estimator.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Learning using Siamese Networks. (arXiv:2109.00794v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00794">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks have been successfully used as classification models yielding
state-of-the-art results when trained on a large number of labeled samples.
These models, however, are more difficult to train successfully for
semi-supervised problems where small amounts of labeled instances are available
along with a large number of unlabeled instances. This work explores a new
training method for semi-supervised learning that is based on similarity
function learning using a Siamese network to obtain a suitable embedding. The
learned representations are discriminative in Euclidean space, and hence can be
used for labeling unlabeled instances using a nearest-neighbor classifier.
Confident predictions of unlabeled instances are used as true labels for
retraining the Siamese network on the expanded training set. This process is
applied iteratively. We perform an empirical study of this iterative
self-training algorithm. For improving unlabeled predictions, local learning
with global consistency [22] is also evaluated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solid Texture Synthesis using Generative Adversarial Networks. (arXiv:2102.03973v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Solid texture synthesis (STS), as an effective way to extend 2D exemplar to a
3D solid volume, exhibits advantages in numerous application domains. However,
existing methods generally synthesize solid texture with specific features,
which may result in the failure of capturing diversified textural information.
In this paper, we propose a novel generative adversarial nets-based approach
(STS-GAN) to hierarchically learn solid texture with a feature-free nature. Our
multi-scale discriminators evaluate the similarity between patch from exemplar
and slice from the generated volume, promoting the generator to synthesize
realistic solid textures. Experimental results demonstrate that the proposed
method can generate high-quality solid textures with similar visual
characteristics to the exemplar.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-task learning for virtual flow metering. (arXiv:2103.08713v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08713">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Virtual flow metering (VFM) is a cost-effective and non-intrusive technology
for inferring multiphase flow rates in petroleum assets. Inferences about flow
rates are fundamental to decision support systems that operators extensively
rely on. Data-driven VFM, where mechanistic models are replaced with machine
learning models, has recently gained attention due to its promise of lower
maintenance costs. While excellent performances in small sample studies have
been reported in the literature, there is still considerable doubt about the
robustness of data-driven VFM. In this paper, we propose a new multi-task
learning (MTL) architecture for data-driven VFM. Our method differs from
previous methods in that it enables learning across oil and gas wells. We study
the method by modeling 55 wells from four petroleum assets and compare the
results with two single-task baseline models. Our findings show that MTL
improves robustness over single-task methods, without sacrificing performance.
MTL yields a 25-50% error reduction on average for the assets where single-task
architectures are struggling.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Differentiable Implicit Soft-Body Physics. (arXiv:2102.05791v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a differentiable soft-body physics simulator that can be composed
with neural networks as a differentiable layer. In contrast to other
differentiable physics approaches that use explicit forward models to define
state transitions, we focus on implicit state transitions defined via function
minimization. Implicit state transitions appear in implicit numerical
integration methods, which offer the benefits of large time steps and excellent
numerical stability, but require a special treatment to achieve
differentiability due to the absence of an explicit differentiable forward
pass. In contrast to other implicit differentiation approaches that require
explicit formulas for the force function and the force Jacobian matrix, we
present an energy-based approach that allows us to compute these derivatives
automatically and in a matrix-free fashion via reverse-mode automatic
differentiation. This allows for more flexibility and productivity when
defining physical models and is particularly important in the context of neural
network training, which often relies on reverse-mode automatic differentiation
(backpropagation). We demonstrate the effectiveness of our differentiable
simulator in policy optimization for locomotion tasks and show that it achieves
better sample efficiency than model-free reinforcement learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07497">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Codifying commonsense knowledge in machines is a longstanding goal of
artificial intelligence. Recently, much progress toward this goal has been made
with automatic knowledge base (KB) construction techniques. However, such
techniques focus primarily on the acquisition of positive (true) KB statements,
even though negative (false) statements are often also important for
discriminative reasoning over commonsense KBs. As a first step toward the
latter, this paper proposes NegatER, a framework that ranks potential negatives
in commonsense KBs using a contextual language model (LM). Importantly, as most
KBs do not contain negatives, NegatER relies only on the positive knowledge in
the LM and does not require ground-truth negative examples. Experiments
demonstrate that, compared to multiple contrastive data augmentation
approaches, NegatER yields negatives that are more grammatical, coherent, and
informative -- leading to statistically significant accuracy improvements in a
challenging KB completion task and confirming that the positive knowledge in
LMs can be &quot;re-purposed&quot; to generate negative knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Measuring and Harnessing Transference in Multi-Task Learning. (arXiv:2010.15413v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.15413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naive formulations often
degrade performance and in particular, identifying the tasks that would benefit
from co-training remains a challenging design question. In this paper, we
analyze the dynamics of information transfer, or transference, across tasks
throughout training. Specifically, we develop a similarity measure that can
quantify transference among tasks and use this quantity to both better
understand the optimization dynamics of multi-task learning as well as improve
overall learning performance. In the latter case, we propose two methods to
leverage our transference metric. The first operates at a macro-level by
selecting which tasks should train together while the second functions at a
micro-level by determining how to combine task gradients at each training step.
We find these methods can lead to significant improvement over prior work on
three supervised multi-task learning benchmarks and one multi-task
reinforcement learning paradigm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PWPAE: An Ensemble Framework for Concept Drift Adaptation in IoT Data Streams. (arXiv:2109.05013v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05013">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As the number of Internet of Things (IoT) devices and systems have surged,
IoT data analytics techniques have been developed to detect malicious
cyber-attacks and secure IoT systems; however, concept drift issues often occur
in IoT data analytics, as IoT data is often dynamic data streams that change
over time, causing model degradation and attack detection failure. This is
because traditional data analytics models are static models that cannot adapt
to data distribution changes. In this paper, we propose a Performance Weighted
Probability Averaging Ensemble (PWPAE) framework for drift adaptive IoT anomaly
detection through IoT data stream analytics. Experiments on two public datasets
show the effectiveness of our proposed PWPAE method compared against
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Citizen centric optimal electric vehicle charging stations locations in a full city: case of Malaga. (arXiv:2109.04975v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04975">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This article presents the problem of locating electric vehicle (EV) charging
stations in a city by defining the Electric Vehicle Charging Stations Locations
(EV-CSL) problem. The idea is to minimize the distance the citizens have to
travel to charge their vehicles. EV-CSL takes into account the maximum number
of charging stations to install and the electric power requirements. Two
metaheuristics are applied to address the relying optimization problem: a
genetic algorithm (GA) and a variable neighborhood search (VNS). The
experimental analysis over a realistic scenario of Malaga city, Spain, shows
that the metaheuristics are able to find competitive solutions which
dramatically improve the actual installation of the stations in Malaga. GA
provided statistically the best results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07971">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we propose a novel problem formulation for de-identification of
unstructured clinical text. We formulate the de-identification problem as a
sequence to sequence learning problem instead of a token classification
problem. Our approach is inspired by the recent state-of -the-art performance
of sequence to sequence learning models for named entity recognition. Early
experimentation of our proposed approach achieved 98.91% recall rate on i2b2
dataset. This performance is comparable to current state-of-the-art models for
unstructured clinical text de-identification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Modelling of BRDF Textures from Flash Images. (arXiv:2102.11861v2 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11861">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We learn a latent space for easy capture, consistent interpolation, and
efficient reproduction of visual material appearance. When users provide a
photo of a stationary natural material captured under flashlight illumination,
first it is converted into a latent material code. Then, in the second step,
conditioned on the material code, our method produces an infinite and diverse
spatial field of BRDF model parameters (diffuse albedo, normals, roughness,
specular albedo) that subsequently allows rendering in complex scenes and
illuminations, matching the appearance of the input photograph. Technically, we
jointly embed all flash images into a latent space using a convolutional
encoder, and -- conditioned on these latent codes -- convert random spatial
fields into fields of BRDF parameters using a convolutional neural network
(CNN). We condition these BRDF parameters to match the visual characteristics
(statistics and spectra of visual features) of the input under matching light.
A user study compares our approach favorably to previous work, even those with
access to BRDF supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a parameter sharing method for Transformers (Vaswani et al.,
2017). The proposed approach relaxes a widely used technique, which shares
parameters for one layer with all layers such as Universal Transformers
(Dehghani et al., 2019), to increase the efficiency in the computational time.
We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign
parameters to each layer. Experimental results show that the proposed
strategies are efficient in the parameter size and computational time.
Moreover, we indicate that the proposed strategies are also effective in the
configuration where we use many training data such as the recent WMT
competition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Secure Multi-Function Computation with Private Remote Sources. (arXiv:2106.09485v3 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09485">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider a distributed function computation problem in which parties
observing noisy versions of a remote source facilitate the computation of a
function of their observations at a fusion center through public communication.
The distributed function computation is subject to constraints, including not
only reliability and storage but also privacy and secrecy. Specifically, 1) the
remote source should remain private from an eavesdropper and the fusion center,
measured in terms of the information leaked about the remote source; 2) the
function computed should remain secret from the eavesdropper, measured in terms
of the information leaked about the arguments of the function, to ensure
secrecy regardless of the exact function used. We derive the exact rate regions
for lossless and lossy single-function computation and illustrate the lossy
single-function computation rate region for an information bottleneck example,
in which the optimal auxiliary random variables are characterized for
binary-input symmetric-output channels. We extend the approach to lossless and
lossy asynchronous multiple-function computations with joint secrecy and
privacy constraints, in which case inner and outer bounds for the rate regions
differing only in the Markov chain conditions imposed are characterized.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards control of opinion diversity by introducing zealots into a polarised social group. (arXiv:2006.07265v5 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We explore a method to influence or even control the diversity of opinions
within a polarised social group. We leverage the voter model in which users
hold binary opinions and repeatedly update their beliefs based on others they
connect with. Stubborn agents who never change their minds (&quot;zealots&quot;) are also
disseminated through the network, which is modelled by a connected graph.
Building on earlier results, we provide a closed-form expression for the
average opinion of the group at equilibrium. This leads us to a strategy to
inject zealots into a polarised network in order to shift the average opinion
towards any target value. We account for the possible presence of a backfire
effect, which may lead the group to react negatively and reinforce its level of
polarisation in response. Our results are supported by numerical experiments on
synthetic data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning the Hypotheses Space from data: Learning Space and U-curve Property. (arXiv:2001.09532v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.09532">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents an extension of the classical agnostic PAC learning model
in which learning problems are modelled not only by a Hypothesis Space
$\mathcal{H}$, but also by a Learning Space $\mathbb{L}(\mathcal{H})$, which is
a cover of $\mathcal{H}$, constrained by a VC-dimension property, that is a
suitable domain for Model Selection algorithms. Our main contribution is a data
driven general learning algorithm to perform regularized Model Selection on
$\mathbb{L}(\mathcal{H})$. A remarkable, formally proved, consequence of this
approach are conditions on $\mathbb{L}(\mathcal{H})$ and on the loss function
that lead to estimated out-of-sample error surfaces which are true U-curves on
$\mathbb{L}(\mathcal{H})$ chains, enabling a more efficient search on
$\mathbb{L}(\mathcal{H})$. To our knowledge, this is the first rigorous result
asserting that a non exhaustive search of a family of candidate models can
return an optimal solution. In this new framework, an U-curve optimization
algorithm becomes a natural component of Model Selection, hence of learning
algorithms. The abstract general framework proposed here may have important
implications on modern learning models and on areas such as Neural Architecture
Search.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, with the expansion of the Internet and attractive social
media infrastructures, people prefer to follow the news through these media.
Despite the many advantages of these media in the news field, the lack of any
control and verification mechanism has led to the spread of fake news, as one
of the most important threats to democracy, economy, journalism and freedom of
expression. Designing and using automatic methods to detect fake news on social
media has become a significant challenge. In this paper, we examine the
publishers&#x27; role in detecting fake news on social media. We also suggest a high
accurate multi-modal framework, namely FR-Detect, using user-related and
content-related features with early detection capability. For this purpose, two
new user-related features, namely Activity Credibility and Influence, have been
introduced for publishers. Furthermore, a sentence-level convolutional neural
network is provided to combine these features with latent textual content
features properly. Experimental results have shown that the publishers&#x27;
features can improve the performance of content-based models by up to 13% and
29% in accuracy and F1-score, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Change Detection in Hyperspectral Images using Feature Fusion Deep Convolutional Autoencoders. (arXiv:2109.04990v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Binary change detection in bi-temporal co-registered hyperspectral images is
a challenging task due to a large number of spectral bands present in the data.
Researchers, therefore, try to handle it by reducing dimensions. The proposed
work aims to build a novel feature extraction system using a feature fusion
deep convolutional autoencoder for detecting changes between a pair of such
bi-temporal co-registered hyperspectral images. The feature fusion considers
features across successive levels and multiple receptive fields and therefore
adds a competitive edge over the existing feature extraction methods. The
change detection technique described is completely unsupervised and is much
more elegant than other supervised or semi-supervised methods which require
some amount of label information. Different methods have been applied to the
extracted features to find the changes in the two images and it is found that
the proposed method clearly outperformed the state of the art methods in
unsupervised change detection for all the datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04689">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by suggested question generation in conversational news
recommendation systems, we propose a model for generating question-answer pairs
(QA pairs) with self-contained, summary-centric questions and
length-constrained, article-summarizing answers. We begin by collecting a new
dataset of news articles with questions as titles and pairing them with
summaries of varying length. This dataset is used to learn a QA pair generation
model producing summaries as answers that balance brevity with sufficiency
jointly with their corresponding questions. We then reinforce the QA pair
generation process with a differentiable reward function to mitigate exposure
bias, a common problem in natural language generation. Both automatic metrics
and human evaluation demonstrate these QA pairs successfully capture the
central gists of the articles and achieve high answer accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04660">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the growth of deep neural networks (DNN), the number of DNN parameters
has drastically increased. This makes DNN models hard to be deployed on
resource-limited embedded systems. To alleviate this problem, dynamic pruning
methods have emerged, which try to find diverse sparsity patterns during
training by utilizing Straight-Through-Estimator (STE) to approximate gradients
of pruned weights. STE can help the pruned weights revive in the process of
finding dynamic sparsity patterns. However, using these coarse gradients causes
training instability and performance degradation owing to the unreliable
gradient signal of the STE approximation. In this work, to tackle this issue,
we introduce refined gradients to update the pruned weights by forming dual
forwarding paths from two sets (pruned and unpruned) of weights. We propose a
novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the
learning synergy between the collective intelligence of both weight sets. We
verify the usefulness of the refined gradients by showing enhancements in the
training stability and the model performance on the CIFAR and ImageNet
datasets. DCIL outperforms various previously proposed pruning schemes
including other dynamic pruning methods with enhanced stability during
training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04672">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The transformer-based pre-trained language models have been tremendously
successful in most of the conventional NLP tasks. But they often struggle in
those tasks where numerical understanding is required. Some possible reasons
can be the tokenizers and pre-training objectives which are not specifically
designed to learn and preserve numeracy. Here we investigate the ability of
text-to-text transfer learning model (T5), which has outperformed its
predecessors in the conventional NLP tasks, to learn numeracy. We consider four
numeracy tasks: numeration, magnitude order prediction, finding minimum and
maximum in a series, and sorting. We find that, although T5 models perform
reasonably well in the interpolation setting, they struggle considerably in the
extrapolation setting across all four tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Locally Optimal Number Set Partitioning for Scheduling, Allocation and Fair Selection. (arXiv:2109.04809v1 [cs.DS])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the optimization version of the set partition problem (where the
difference between the partition sums are minimized), which has numerous
applications in decision theory literature. While the set partitioning problem
is NP-hard and requires exponential complexity to solve (i.e., intractable); we
formulate a weaker version of this NP-hard problem, where the goal is to find a
locally optimal solution. We show that our proposed algorithms can find a
locally optimal solution in near linear time. Our algorithms require neither
positive nor integer elements in the input set, hence, they are more widely
applicable.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04783">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>When a sufficiently large far-field training data is presented, jointly
optimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech
Recognition (ASR) backend shows promising results. Recent literature has shown
traditional beamformer designs, such as MVDR (Minimum Variance Distortionless
Response) or fixed beamformers can be successfully integrated as the frontend
into an E2E ASR system with learnable parameters. In this work, we propose the
self-attention channel combinator (SACC) ASR frontend, which leverages the
self-attention mechanism to combine multichannel audio signals in the magnitude
spectral domain. Experiments conducted on a multichannel playback test data
shows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed
beamformer-based frontend, both jointly optimized with a ContextNet-based ASR
backend. We also demonstrate the connection between the SACC and the
traditional beamformers, and analyze the intermediate outputs of the SACC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ReLU Regression with Massart Noise. (arXiv:2109.04623v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04623">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the fundamental problem of ReLU regression, where the goal is to fit
Rectified Linear Units (ReLUs) to data. This supervised learning task is
efficiently solvable in the realizable setting, but is known to be
computationally hard with adversarial label noise. In this work, we focus on
ReLU regression in the Massart noise model, a natural and well-studied
semi-random noise model. In this model, the label of every point is generated
according to a function in the class, but an adversary is allowed to change
this value arbitrarily with some probability, which is {\em at most} $\eta &lt;
1/2$. We develop an efficient algorithm that achieves exact parameter recovery
in this model under mild anti-concentration assumptions on the underlying
distribution. Such assumptions are necessary for exact recovery to be
information-theoretically possible. We demonstrate that our algorithm
significantly outperforms naive applications of $\ell_1$ and $\ell_2$
regression on both synthetic and real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the validity of pre-trained transformers for natural language processing in the software engineering domain. (arXiv:2109.04738v1 [cs.SE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04738">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers are the current state-of-the-art of natural language processing
in many domains and are using traction within software engineering research as
well. Such models are pre-trained on large amounts of data, usually from the
general domain. However, we only have a limited understanding regarding the
validity of transformers within the software engineering domain, i.e., how good
such models are at understanding words and sentences within a software
engineering context and how this improves the state-of-the-art. Within this
article, we shed light on this complex, but crucial issue. We compare BERT
transformer models trained with software engineering data with transformers
based on general domain data in multiple dimensions: their vocabulary, their
ability to understand which words are missing, and their performance in
classification tasks. Our results show that for tasks that require
understanding of the software engineering context, pre-training with software
engineering data is valuable, while general domain models are sufficient for
general language understanding, also within the software engineering domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervising the Decoder of Variational Autoencoders to Improve Scientific Utility. (arXiv:2109.04561v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04561">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Probabilistic generative models are attractive for scientific modeling
because their inferred parameters can be used to generate hypotheses and design
experiments. This requires that the learned model provide an accurate
representation of the input data and yield a latent space that effectively
predicts outcomes relevant to the scientific question. Supervised Variational
Autoencoders (SVAEs) have previously been used for this purpose, where a
carefully designed decoder can be used as an interpretable generative model
while the supervised objective ensures a predictive latent representation.
Unfortunately, the supervised objective forces the encoder to learn a biased
approximation to the generative posterior distribution, which renders the
generative parameters unreliable when used in scientific models. This issue has
remained undetected as reconstruction losses commonly used to evaluate model
performance do not detect bias in the encoder. We address this
previously-unreported issue by developing a second order supervision framework
(SOS-VAE) that influences the decoder to induce a predictive latent
representation. This ensures that the associated encoder maintains a reliable
generative interpretation. We extend this technique to allow the user to
trade-off some bias in the generative parameters for improved predictive
performance, acting as an intermediate option between SVAEs and our new
SOS-VAE. We also use this methodology to address missing data issues that often
arise when combining recordings from multiple scientific experiments. We
demonstrate the effectiveness of these developments using synthetic data and
electrophysiological recordings with an emphasis on how our learned
representations can be used to design scientific experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04552">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Selective rationalization aims to produce decisions along with rationales
(e.g., text highlights or word alignments between two sentences). Commonly,
rationales are modeled as stochastic binary masks, requiring sampling-based
gradient estimators, which complicates training and requires careful
hyperparameter tuning. Sparse attention mechanisms are a deterministic
alternative, but they lack a way to regularize the rationale extraction (e.g.,
to control the sparsity of a text highlight or the number of alignments). In
this paper, we present a unified framework for deterministic extraction of
structured explanations via constrained inference on a factor graph, forming a
differentiable layer. Our approach greatly eases training and rationale
regularization, generally outperforming previous work on what comes to
performance and plausibility of the extracted rationales. We further provide a
comparative study of stochastic and deterministic methods for rationale
extraction for classification and natural language inference tasks, jointly
assessing their predictive power, quality of the explanations, and model
variability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SanitAIs: Unsupervised Data Augmentation to Sanitize Trojaned Neural Networks. (arXiv:2109.04566v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04566">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of self-supervised methods has resulted in broad improvements
to neural network performance by leveraging large, untapped collections of
unlabeled data to learn generalized underlying structure. In this work, we
harness unsupervised data augmentation (UDA) to mitigate backdoor or Trojan
attacks on deep neural networks. We show that UDA is more effective at removing
the effects of a trigger than current state-of-the-art methods for both feature
space and point triggers. These results demonstrate that UDA is both an
effective and practical approach to mitigating the effects of backdoors on
neural networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04550">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal Knowledge Graphs store events in the form of subjects, relations,
objects, and timestamps which are often represented by dynamic heterogeneous
graphs. Event forecasting is a critical and challenging task in Temporal
Knowledge Graph reasoning that predicts the subject or object of an event in
the future. To obtain temporal embeddings multi-step away in the future,
existing methods learn generative models that capture the joint distribution of
the observed events. To reduce the high computation costs, these methods rely
on unrealistic assumptions of independence and approximations in training and
inference. In this work, we propose SeDyT, a discriminative framework that
performs sequence modeling on the dynamic entity embeddings to solve the
multi-step event forecasting problem. SeDyT consists of two components: a
Temporal Graph Neural Network that generates dynamic entity embeddings in the
past and a sequence model that predicts the entity embeddings in the future.
Compared with the generative models, SeDyT does not rely on any heuristic-based
probability model and has low computation complexity in both training and
inference. SeDyT is compatible with most Temporal Graph Neural Networks and
sequence models. We also design an efficient training method that trains the
two components in one gradient descent propagation. We evaluate the performance
of SeDyT on five popular datasets. By combining temporal Graph Neural Network
models and sequence models, SeDyT achieves an average of 2.4% MRR improvement
when not using the validation set and more than 10% MRR improvement when using
the validation set.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04707">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Meta-learning has achieved great success in leveraging the historical learned
knowledge to facilitate the learning process of the new task. However, merely
learning the knowledge from the historical tasks, adopted by current
meta-learning algorithms, may not generalize well to testing tasks when they
are not well-supported by training tasks. This paper studies a low-resource
text classification problem and bridges the gap between meta-training and
meta-testing tasks by leveraging the external knowledge bases. Specifically, we
propose KGML to introduce additional representation for each sentence learned
from the extracted sentence-specific knowledge graph. The extensive experiments
on three datasets demonstrate the effectiveness of KGML under both supervised
adaptation and unsupervised adaptation settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transfer Learning and Curriculum Learning in Sokoban. (arXiv:2105.11702v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11702">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transfer learning can speed up training in machine learning and is regularly
used in classification tasks. It reuses prior knowledge from other tasks to
pre-train networks for new tasks. In reinforcement learning, learning actions
for a behavior policy that can be applied to new environments is still a
challenge, especially for tasks that involve much planning. Sokoban is a
challenging puzzle game. It has been used widely as a benchmark in
planning-based reinforcement learning. In this paper, we show how prior
knowledge improves learning in Sokoban tasks. We find that reusing feature
representations learned previously can accelerate learning new, more complex,
instances. In effect, we show how curriculum learning, from simple to complex
tasks, works in Sokoban. Furthermore, feature representations learned in
simpler instances are more general, and thus lead to positive transfers towards
more complex tasks, but not vice versa. We have also studied which part of the
knowledge is most important for transfer to succeed, and identify which layers
should be used for pre-training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global Optimization of Objective Functions Represented by ReLU Networks. (arXiv:2010.03258v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03258">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks can learn complex, non-convex functions, and it is
challenging to guarantee their correct behavior in safety-critical contexts.
Many approaches exist to find failures in networks (e.g., adversarial
examples), but these cannot guarantee the absence of failures. Verification
algorithms address this need and provide formal guarantees about a neural
network by answering &quot;yes or no&quot; questions. For example, they can answer
whether a violation exists within certain bounds. However, individual &quot;yes or
no&quot; questions cannot answer qualitative questions such as &quot;what is the largest
error within these bounds&quot;; the answers to these lie in the domain of
optimization. Therefore, we propose strategies to extend existing verifiers to
perform optimization and find: (i) the most extreme failure in a given input
region and (ii) the minimum input perturbation required to cause a failure. A
naive approach using a bisection search with an off-the-shelf verifier results
in many expensive and overlapping calls to the verifier. Instead, we propose an
approach that tightly integrates the optimization process into the verification
procedure, achieving better runtime performance than the naive approach. We
evaluate our approach implemented as an extension of Marabou, a
state-of-the-art neural network verifier, and compare its performance with the
bisection approach and MIPVerify, an optimization-based verifier. We observe
complementary performance between our extension of Marabou and MIPVerify.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15781">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Influence functions approximate the &quot;influences&quot; of training data-points for
test predictions and have a wide variety of applications. Despite the
popularity, their computational cost does not scale well with model and
training data size. We present FastIF, a set of simple modifications to
influence functions that significantly improves their run-time. We use
k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good
candidate data points, identify the configurations that best balance the
speed-quality trade-off in estimating the inverse Hessian-vector product, and
introduce a fast parallel variant. Our proposed method achieves about 80X
speedup while being highly correlated with the original influence values. With
the availability of the fast influence functions, we demonstrate their
usefulness in four applications. First, we examine whether influential
data-points can &quot;explain&quot; test time behavior using the framework of
simulatability. Second, we visualize the influence interactions between
training and test data-points. Third, we show that we can correct model errors
by additional fine-tuning on certain influential data-points, improving the
accuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we
experiment with a similar setup but fine-tuning on datapoints not seen during
training, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI
datasets respectively. Overall, our fast influence functions can be efficiently
applied to large models and datasets, and our experiments demonstrate the
potential of influence functions in model interpretation and correcting model
errors. Code is available at
https://github.com/salesforce/fast-influence-functions</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03432">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Concept-to-text Natural Language Generation is the task of expressing an
input meaning representation in natural language. Previous approaches in this
task have been able to generalise to rare or unseen instances by relying on a
delexicalisation of the input. However, this often requires that the input
appears verbatim in the output text. This poses challenges in multilingual
settings, where the task expands to generate the output text in multiple
languages given the same input. In this paper, we explore the application of
multilingual models in concept-to-text and propose Language Agnostic
Delexicalisation, a novel delexicalisation method that uses multilingual
pretrained embeddings, and employs a character-level post-editing model to
inflect words in their correct form during relexicalisation. Our experiments
across five datasets and five languages show that multilingual models
outperform monolingual models in concept-to-text and that our framework
outperforms previous approaches, especially for low resource languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09574">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans use commonsense reasoning (CSR) implicitly to produce natural and
coherent responses in conversations. Aiming to close the gap between current
response generation (RG) models and human communication abilities, we want to
understand why RG models respond as they do by probing RG model&#x27;s understanding
of commonsense reasoning that elicits proper responses. We formalize the
problem by framing commonsense as a latent variable in the RG task and using
explanations for responses as textual form of commonsense. We collect 6k
annotated explanations justifying responses from four dialogue datasets and ask
humans to verify them and propose two probing settings to evaluate RG models&#x27;
CSR capabilities. Probing results show that models fail to capture the logical
relations between commonsense explanations and responses and fine-tuning on
in-domain data and increasing model sizes do not lead to understanding of CSR
for RG. We hope our study motivates more research in making RG models emulate
the human reasoning process in pursuit of smooth human-AI communication.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04611">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based rankers have shown state-of-the-art performance. However,
their self-attention operation is mostly unable to process long sequences. One
of the common approaches to train these rankers is to heuristically select some
segments of each document, such as the first segment, as training data.
However, these segments may not contain the query-related parts of documents.
To address this problem, we propose query-driven segment selection from long
documents to build training data. The segment selector provides relevant
samples with more accurate labels and non-relevant samples which are harder to
be predicted. The experimental results show that the basic BERT-based ranker
trained with the proposed segment selector significantly outperforms that
trained by the heuristically selected segments, and performs equally to the
state-of-the-art model with localized self-attention that can process longer
input sequences. Our findings open up new direction to design efficient
transformer-based rankers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Relational knowledge bases (KBs) are commonly used to represent world
knowledge in machines. However, while advantageous for their high degree of
precision and interpretability, KBs are usually organized according to
manually-defined schemas, which limit their expressiveness and require
significant human efforts to engineer and maintain. In this review, we take a
natural language processing perspective to these limitations, examining how
they may be addressed in part by training deep contextual language models (LMs)
to internalize and express relational knowledge in more flexible forms. We
propose to organize knowledge representation strategies in LMs by the level of
KB supervision provided, from no KB supervision at all to entity- and
relation-level supervision. Our contributions are threefold: (1) We provide a
high-level, extensible taxonomy for knowledge representation in LMs; (2) Within
our taxonomy, we highlight notable models, evaluation tasks, and findings, in
order to provide an up-to-date review of current knowledge representation
capabilities in LMs; and (3) We suggest future research directions that build
upon the complementary aspects of LMs and KBs as knowledge representations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimised one-class classification performance. (arXiv:2102.02618v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02618">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We provide a thorough treatment of one-class classification with
hyperparameter optimisation for five data descriptors: Support Vector Machine
(SVM), Nearest Neighbour Distance (NND), Localised Nearest Neighbour Distance
(LNND), Local Outlier Factor (LOF) and Average Localised Proximity (ALP). The
hyperparameters of SVM and LOF have to be optimised through cross-validation,
while NND, LNND and ALP allow an efficient form of leave-one-out validation and
the reuse of a single nearest-neighbour query. We experimentally evaluate the
effect of hyperparameter optimisation with 246 classification problems drawn
from 50 datasets. From a selection of optimisation algorithms, the recent
Malherbe-Powell proposal optimises the hyperparameters of all data descriptors
most efficiently. We calculate the increase in test AUROC and the amount of
overfitting as a function of the number of hyperparameter evaluations. After 50
evaluations, ALP and SVM significantly outperform LOF, NND and LNND, and LOF
and NND outperform LNND. The performance of ALP and SVM is comparable, but ALP
can be optimised more efficiently so constitutes a good default choice.
Alternatively, using validation AUROC as a selection criterion between ALP or
SVM gives the best overall result, and NND is the least computationally
demanding option. We thus end up with a clear trade-off between three choices,
allowing practitioners to make an informed decision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16410">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To alleviate human efforts from obtaining large-scale annotations,
Semi-Supervised Relation Extraction methods aim to leverage unlabeled data in
addition to learning from limited samples. Existing self-training methods
suffer from the gradual drift problem, where noisy pseudo labels on unlabeled
data are incorporated during training. To alleviate the noise in pseudo labels,
we propose a method called MetaSRE, where a Relation Label Generation Network
generates quality assessment on pseudo labels by (meta) learning from the
successful and failed attempts on Relation Classification Network as an
additional meta-objective. To reduce the influence of noisy pseudo labels,
MetaSRE adopts a pseudo label selection and exploitation scheme which assesses
pseudo label quality on unlabeled samples and only exploits high-quality pseudo
labels in a self-training fashion to incrementally augment labeled samples for
both robustness and accuracy. Experimental results on two public datasets
demonstrate the effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Networks for Latent Budget Analysis of Compositional Data. (arXiv:2109.04875v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04875">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Compositional data are non-negative data collected in a rectangular matrix
with a constant row sum. Due to the non-negativity the focus is on conditional
proportions that add up to 1 for each row. A row of conditional proportions is
called an observed budget. Latent budget analysis (LBA) assumes a mixture of
latent budgets that explains the observed budgets. LBA is usually fitted to a
contingency table, where the rows are levels of one or more explanatory
variables and the columns the levels of a response variable. In prospective
studies, there is only knowledge about the explanatory variables of individuals
and interest goes out to predicting the response variable. Thus, a form of LBA
is needed that has the functionality of prediction. Previous studies proposed a
constrained neural network (NN) extension of LBA that was hampered by an
unsatisfying prediction ability. Here we propose LBA-NN, a feed forward NN
model that yields a similar interpretation to LBA but equips LBA with a better
ability of prediction. A stable and plausible interpretation of LBA-NN is
obtained through the use of importance plots and table, that show the relative
importance of all explanatory variables on the response variable. An LBA-NN-K-
means approach that applies K-means clustering on the importance table is used
to produce K clusters that are comparable to K latent budgets in LBA. Here we
provide different experiments where LBA-NN is implemented and compared with
LBA. In our analysis, LBA-NN outperforms LBA in prediction in terms of
accuracy, specificity, recall and mean square error. We provide open-source
software at GitHub.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04825">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The impressive capabilities of recent generative models to create texts that
are challenging to distinguish from the human-written ones can be misused for
generating fake news, product reviews, and even abusive content. Despite the
prominent performance of existing methods for artificial text detection, they
still lack interpretability and robustness towards unseen models. To this end,
we propose three novel types of interpretable topological features for this
task based on Topological Data Analysis (TDA) which is currently understudied
in the field of NLP. We empirically show that the features derived from the
BERT model outperform count- and neural-based baselines up to 10\% on three
common datasets, and tend to be the most robust towards unseen GPT-style
generation models as opposed to existing methods. The probing analysis of the
features reveals their sensitivity to the surface and syntactic properties. The
results demonstrate that TDA is a promising line with respect to NLP tasks,
specifically the ones that incorporate surface and structural information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Best-Arm Identification in Correlated Multi-Armed Bandits. (arXiv:2109.04941v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper we consider the problem of best-arm identification in
multi-armed bandits in the fixed confidence setting, where the goal is to
identify, with probability $1-\delta$ for some $\delta&gt;0$, the arm with the
highest mean reward in minimum possible samples from the set of arms
$\mathcal{K}$. Most existing best-arm identification algorithms and analyses
operate under the assumption that the rewards corresponding to different arms
are independent of each other. We propose a novel correlated bandit framework
that captures domain knowledge about correlation between arms in the form of
upper bounds on expected conditional reward of an arm, given a reward
realization from another arm. Our proposed algorithm C-LUCB, which generalizes
the LUCB algorithm utilizes this partial knowledge of correlations to sharply
reduce the sample complexity of best-arm identification. More interestingly, we
show that the total samples obtained by C-LUCB are of the form
$\mathcal{O}\left(\sum_{k \in \mathcal{C}}
\log\left(\frac{1}{\delta}\right)\right)$ as opposed to the typical
$\mathcal{O}\left(\sum_{k \in \mathcal{K}}
\log\left(\frac{1}{\delta}\right)\right)$ samples required in the independent
reward setting. The improvement comes, as the $\mathcal{O}(\log(1/\delta))$
term is summed only for the set of competitive arms $\mathcal{C}$, which is a
subset of the original set of arms $\mathcal{K}$. The size of the set
$\mathcal{C}$, depending on the problem setting, can be as small as $2$, and
hence using C-LUCB in the correlated bandits setting can lead to significant
performance improvements. Our theoretical findings are supported by experiments
on the Movielens and Goodreads recommendation datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While large scale pre-training has achieved great achievements in bridging
the gap between vision and language, it still faces several challenges. First,
the cost for pre-training is expensive. Second, there is no efficient way to
handle the data noise which degrades model performance. Third, previous methods
only leverage limited image-text paired data, while ignoring richer
single-modal data, which may result in poor generalization to single-modal
downstream tasks. In this work, we propose an EfficientCLIP method via Ensemble
Confident Learning to obtain a less noisy data subset. Extra rich non-paired
single-modal text data is used for boosting the generalization of text branch.
We achieve the state-of-the-art performance on Chinese cross-modal retrieval
tasks with only 1/10 training resources compared to CLIP and WenLan, while
showing excellent generalization to single-modal tasks, including text
retrieval and text classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepOPF: A Feasibility-Optimized Deep Neural Network Approach for AC Optimal Power Flow Problems. (arXiv:2007.01002v4 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.01002">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>High percentage penetrations of renewable energy generations introduce
significant uncertainty into power systems. It requires grid operators to solve
alternative current optimal power flow (AC-OPF) problems more frequently for
economical and reliable operation in both transmission and distribution grids.
In this paper, we develop a Deep Neural Network (DNN) approach, called DeepOPF,
for solving AC-OPF problems in a fraction of the time used by conventional
solvers. A key difficulty for applying machine learning techniques for solving
AC-OPF problems lies in ensuring that the obtained solutions respect the
equality and inequality physical and operational constraints. Generalized the
2-stage procedure in [1], [2], DeepOPF first trains a DNN model to predict a
set of independent operating variables and then directly compute the remaining
dependable ones by solving power flow equations. Such an approach not only
preserves the power-flow balance equality constraints but also reduces the
number of variables to predict by the DNN, cutting down the number of neurons
and training data needed. DeepOPF then employs a penalty approach with a
zero-order gradient estimation technique in the training process to preserve
the remaining inequality constraints. As another contribution, we drive a
condition for tuning the size of the DNN according to the desired approximation
accuracy, which measures the DNN generalization capability. It provides
theoretical justification for using DNN to solve the AC-OPF problem. Simulation
results of IEEE 30/118/300-bus and a synthetic 2000-bus test cases show that
DeepOPF speeds up the computing time by up to two orders of magnitude as
compared to a state-of-the-art solver, at the expense of $&lt;$0.1% cost
difference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Highdicom: A Python library for standardized encoding of image annotations and machine learning model outputs in pathology and radiology. (arXiv:2106.07806v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Machine learning is revolutionizing image-based diagnostics in pathology and
radiology. ML models have shown promising results in research settings, but
their lack of interoperability has been a major barrier for clinical
integration and evaluation. The DICOM a standard specifies Information Object
Definitions and Services for the representation and communication of digital
images and related information, including image-derived annotations and
analysis results. However, the complexity of the standard represents an
obstacle for its adoption in the ML community and creates a need for software
libraries and tools that simplify working with data sets in DICOM format. Here
we present the highdicom library, which provides a high-level application
programming interface for the Python programming language that abstracts
low-level details of the standard and enables encoding and decoding of
image-derived information in DICOM format in a few lines of Python code. The
highdicom library ties into the extensive Python ecosystem for image processing
and machine learning. Simultaneously, by simplifying creation and parsing of
DICOM-compliant files, highdicom achieves interoperability with the medical
imaging systems that hold the data used to train and run ML models, and
ultimately communicate and store model outputs for clinical use. We demonstrate
through experiments with slide microscopy and computed tomography imaging,
that, by bridging these two ecosystems, highdicom enables developers to train
and evaluate state-of-the-art ML models in pathology and radiology while
remaining compliant with the DICOM standard and interoperable with clinical
systems at all stages. To promote standardization of ML research and streamline
the ML model development and deployment process, we made the library available
free and open-source.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Adversarial Learning with Representation Interpolation. (arXiv:2109.04746v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04746">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep learning models exhibit a preference for statistical fitting over
logical reasoning. Spurious correlations might be memorized when there exists
statistical bias in training data, which severely limits the model performance
especially in small data scenarios. In this work, we introduce Counterfactual
Adversarial Training framework (CAT) to tackle the problem from a causality
perspective. Particularly, for a specific sample, CAT first generates a
counterfactual representation through latent space interpolation in an
adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on
each original-counterfactual pair to adjust sample-wise loss weight
dynamically, which encourages the model to explore the true causal effect.
Extensive experiments demonstrate that CAT achieves substantial performance
improvement over SOTA across different downstream tasks, including sentence
classification, natural language inference and question answering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Federated Learning. (arXiv:2109.04833v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04833">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning is proposed as an alternative to centralized machine
learning since its client-server structure provides better privacy protection
and scalability in real-world applications. In many applications, such as smart
homes with IoT devices, local data on clients are generated from different
modalities such as sensory, visual, and audio data. Existing federated learning
systems only work on local data from a single modality, which limits the
scalability of the systems.

In this paper, we propose a multimodal and semi-supervised federated learning
framework that trains autoencoders to extract shared or correlated
representations from different local data modalities on clients. In addition,
we propose a multimodal FedAvg algorithm to aggregate local autoencoders
trained on different data modalities. We use the learned global autoencoder for
a downstream classification task with the help of auxiliary labelled data on
the server. We empirically evaluate our framework on different modalities
including sensory data, depth camera videos, and RGB camera videos. Our
experimental results demonstrate that introducing data from multiple modalities
into federated learning can improve its accuracy. In addition, we can use
labelled data from only one modality for supervised learning on the server and
apply the learned model to testing data from other modalities to achieve decent
accuracy (e.g., approximately 70% as the best performance), especially when
combining contributions from both unimodal clients and multimodal clients.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04953">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Pretraining techniques leveraging enormous datasets have driven recent
advances in text summarization. While folk explanations suggest that knowledge
transfer accounts for pretraining&#x27;s benefits, little is known about why it
works or what makes a pretraining task or dataset suitable. In this paper, we
challenge the knowledge transfer story, showing that pretraining on documents
consisting of character n-grams selected at random, we can nearly match the
performance of models pretrained on real corpora. This work holds the promise
of eliminating upstream corpora, which may alleviate some concerns over
offensive language, bias, and copyright issues. To see whether the small
residual benefit of using real data could be accounted for by the structure of
the pretraining task, we design several tasks motivated by a qualitative study
of summarization corpora. However, these tasks confer no appreciable benefit,
leaving open the possibility of a small role for knowledge transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Teach with Student Feedback. (arXiv:2109.04641v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04641">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Knowledge distillation (KD) has gained much attention due to its
effectiveness in compressing large-scale pre-trained models. In typical KD
methods, the small student model is trained to match the soft targets generated
by the big teacher model. However, the interaction between student and teacher
is one-way. The teacher is usually fixed once trained, resulting in static soft
targets to be distilled. This one-way interaction leads to the teacher&#x27;s
inability to perceive the characteristics of the student and its training
progress. To address this issue, we propose Interactive Knowledge Distillation
(IKD), which also allows the teacher to learn to teach from the feedback of the
student. In particular, IKD trains the teacher model to generate specific soft
target at each training step for a certain student. Joint optimization for both
teacher and student is achieved by two iterative steps: a course step to
optimize student with the soft target of teacher, and an exam step to optimize
teacher with the feedback of student. IKD is a general framework that is
orthogonal to most existing knowledge distillation methods. Experimental
results show that IKD outperforms traditional KD methods on various NLP tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Makes for Hierarchical Vision Transformer?. (arXiv:2107.02174v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02174">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies indicate that hierarchical Vision Transformer with a macro
architecture of interleaved non-overlapped window-based self-attention \&amp;
shifted-window operation is able to achieve state-of-the-art performance in
various visual recognition tasks, and challenges the ubiquitous convolutional
neural networks (CNNs) using densely slid kernels. Most follow-up works attempt
to replace the shifted-window operation with other kinds of cross-window
communication paradigms, while treating self-attention as the de-facto standard
for window-based information aggregation. In this manuscript, we question
whether self-attention is the only choice for hierarchical Vision Transformer
to attain strong performance, and the effects of different kinds of
cross-window communication. To this end, we replace self-attention layers with
embarrassingly simple linear mapping layers, and the resulting proof-of-concept
architecture termed as LinMapper can achieve very strong performance in
ImageNet-1k image recognition. Moreover, we find that LinMapper is able to
better leverage the pre-trained representations from image recognition and
demonstrates excellent transfer learning properties on downstream dense
prediction tasks such as object detection and instance segmentation. We also
experiment with other alternatives to self-attention for content aggregation
inside each non-overlapped window under different cross-window communication
approaches, which all give similar competitive results. Our study reveals that
the \textbf{macro architecture} of Swin model families, other than specific
aggregation layers or specific means of cross-window communication, may be more
responsible for its strong performance and is the real challenger to the
ubiquitous CNN&#x27;s dense sliding window paradigm. Code and models will be
publicly available to facilitate future research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Less is More: Pre-training a Strong Siamese Encoder Using a Weak Decoder. (arXiv:2102.09206v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dense retrieval requires high-quality text sequence embeddings to support
effective search in the representation space. Autoencoder-based language models
are appealing in dense retrieval as they train the encoder to output
high-quality embedding that can reconstruct the input texts. However, in this
paper, we provide theoretical analyses and show empirically that an autoencoder
language model with a low reconstruction loss may not provide good sequence
representations because the decoder may take shortcuts by exploiting language
patterns. To address this, we propose a new self-learning method that
pre-trains the autoencoder using a \textit{weak} decoder, with restricted
capacity and attention flexibility to push the encoder to provide better text
representations. Our experiments on web search, news recommendation, and open
domain question answering show that our pre-trained model significantly boosts
the effectiveness and few-shot ability of dense retrieval models. Our code is
available at https://github.com/microsoft/SEED-Encoder/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal Clustering from Noisy Binary Feedback. (arXiv:1910.06002v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.06002">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the problem of clustering a set of items from binary user feedback.
Such a problem arises in crowdsourcing platforms solving large-scale labeling
tasks with minimal effort put on the users. For example, in some of the recent
reCAPTCHA systems, users clicks (binary answers) can be used to efficiently
label images. In our inference problem, items are grouped into initially
unknown non-overlapping clusters. To recover these clusters, the learner
sequentially presents to users a finite list of items together with a question
with a binary answer selected from a fixed finite set. For each of these items,
the user provides a noisy answer whose expectation is determined by the item
cluster and the question and by an item-specific parameter characterizing the
{\it hardness} of classifying the item. The objective is to devise an algorithm
with a minimal cluster recovery error rate. We derive problem-specific
information-theoretical lower bounds on the error rate satisfied by any
algorithm, for both uniform and adaptive (list, question) selection strategies.
For uniform selection, we present a simple algorithm built upon the K-means
algorithm and whose performance almost matches the fundamental limits. For
adaptive selection, we develop an adaptive algorithm that is inspired by the
derivation of the information-theoretical error lower bounds, and in turn
allocates the budget in an efficient way. The algorithm learns to select items
hard to cluster and relevant questions more often. We compare the performance
of our algorithms with or without the adaptive selection strategy numerically
and illustrate the gain achieved by being adaptive.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">KNODE-MPC: A Knowledge-based Data-driven Predictive Control Framework for Aerial Robots. (arXiv:2109.04821v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04821">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we consider the problem of deriving and incorporating accurate
dynamic models for model predictive control (MPC) with an application to
quadrotor control. MPC relies on precise dynamic models to achieve the desired
closed-loop performance. However, the presence of uncertainties in complex
systems and the environments they operate in poses a challenge in obtaining
sufficiently accurate representations of the system dynamics. In this work, we
make use of a deep learning tool, knowledge-based neural ordinary differential
equations (KNODE), to augment a model obtained from first principles. The
resulting hybrid model encompasses both a nominal first-principle model and a
neural network learnt from simulated or real-world experimental data. Using a
quadrotor, we benchmark our hybrid model against a state-of-the-art Gaussian
Process (GP) model and show that the hybrid model provides more accurate
predictions of the quadrotor dynamics and is able to generalize beyond the
training data. To improve closed-loop performance, the hybrid model is
integrated into a novel MPC framework, known as KNODE-MPC. Results show that
the integrated framework achieves 73% improvement in simulations and more than
14% in physical experiments, in terms of trajectory tracking performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unfolding Projection-free SDP Relaxation of Binary Graph Classifier via GDPA Linearization. (arXiv:2109.04697v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04697">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Algorithm unfolding creates an interpretable and parsimonious neural network
architecture by implementing each iteration of a model-based algorithm as a
neural layer. However, unfolding a proximal splitting algorithm with a positive
semi-definite (PSD) cone projection operator per iteration is expensive, due to
the required full matrix eigen-decomposition. In this paper, leveraging a
recent linear algebraic theorem called Gershgorin disc perfect alignment
(GDPA), we unroll a projection-free algorithm for semi-definite programming
relaxation (SDR) of a binary graph classifier, where the PSD cone constraint is
replaced by a set of &quot;tightest possible&quot; linear constraints per iteration. As a
result, each iteration only requires computing a linear program (LP) and one
extreme eigenvector. Inside the unrolled network, we optimize parameters via
stochastic gradient descent (SGD) that determine graph edge weights in two
ways: i) a metric matrix that computes feature distances, and ii) a sparse
weight matrix computed via local linear embedding (LLE). Experimental results
show that our unrolled network outperformed pure model-based graph classifiers,
and achieved comparable performance to pure data-driven networks but using far
fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Memorization for Fast Learning New Knowledge without Forgetting. (arXiv:2108.12596v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The ability to quickly learn new knowledge (e.g. new classes or data
distributions) is a big step towards human-level intelligence. In this paper,
we consider scenarios that require learning new classes or data distributions
quickly and incrementally over time, as it often occurs in real-world dynamic
environments. We propose &quot;Memory-based Hebbian Parameter Adaptation&quot; (Hebb) to
tackle the two major challenges (i.e., catastrophic forgetting and sample
efficiency) towards this goal in a unified framework. To mitigate catastrophic
forgetting, Hebb augments a regular neural classifier with a continuously
updated memory module to store representations of previous data. To improve
sample efficiency, we propose a parameter adaptation method based on the
well-known Hebbian theory, which directly &quot;wires&quot; the output network&#x27;s
parameters with similar representations retrieved from the memory. We
empirically verify the superior performance of Hebb through extensive
experiments on a wide range of learning tasks (image classification, language
model) and learning scenarios (continual, incremental, online). We demonstrate
that Hebb effectively mitigates catastrophic forgetting, and it indeed learns
new knowledge better and faster than the current state-of-the-art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05094">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive Learning has emerged as a powerful representation learning method
and facilitates various downstream tasks especially when supervised data is
limited. How to construct efficient contrastive samples through data
augmentation is key to its success. Unlike vision tasks, the data augmentation
method for contrastive learning has not been investigated sufficiently in
language tasks. In this paper, we propose a novel approach to construct
contrastive samples for language tasks using text summarization. We use these
samples for supervised contrastive learning to gain better text representations
which greatly benefit text classification tasks with limited annotations. To
further improve the method, we mix up samples from different classes and add an
extra regularization, named Mixsum, in addition to the cross-entropy-loss.
Experiments on real-world text classification datasets (Amazon-5, Yelp-5, AG
News, and IMDb) demonstrate the effectiveness of the proposed contrastive
learning framework with summarization-based data augmentation and Mixsum
regularization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Foreseeing the Benefits of Incidental Supervision. (arXiv:2006.05500v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.05500">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Real-world applications often require improved models by leveraging a range
of cheap incidental supervision signals. These could include partial labels,
noisy labels, knowledge-based constraints, and cross-domain or cross-task
annotations -- all having statistical associations with gold annotations but
not exactly the same. However, we currently lack a principled way to measure
the benefits of these signals to a given target task, and the common practice
of evaluating these benefits is through exhaustive experiments with various
models and hyperparameters. This paper studies whether we can, in a single
framework, quantify the benefits of various types of incidental signals for a
given target task without going through combinatorial experiments. We propose a
unified PAC-Bayesian motivated informativeness measure, PABI, that
characterizes the uncertainty reduction provided by incidental supervision
signals. We demonstrate PABI&#x27;s effectiveness by quantifying the value added by
various types of incidental signals to sequence tagging tasks. Experiments on
named entity recognition (NER) and question answering (QA) show that PABI&#x27;s
predictions correlate well with learning performance, providing a promising way
to determine, ahead of learning, which supervision signals would be beneficial.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing the Quality of the Datasets by Identifying Mislabeled Samples. (arXiv:2109.05000v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05000">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Due to the over-emphasize of the quantity of data, the data quality has often
been overlooked. However, not all training data points contribute equally to
learning. In particular, if mislabeled, it might actively damage the
performance of the model and the ability to generalize out of distribution, as
the model might end up learning spurious artifacts present in the dataset. This
problem gets compounded by the prevalence of heavily parameterized and complex
deep neural networks, which can, with their high capacity, end up memorizing
the noise present in the dataset. This paper proposes a novel statistic --
noise score, as a measure for the quality of each data point to identify such
mislabeled samples based on the variations in the latent space representation.
In our work, we use the representations derived by the inference network of
data quality supervised variational autoencoder (AQUAVS). Our method leverages
the fact that samples belonging to the same class will have similar latent
representations. Therefore, by identifying the outliers in the latent space, we
can find the mislabeled samples. We validate our proposed statistic through
experimentation by corrupting MNIST, FashionMNIST, and CIFAR10/100 datasets in
different noise settings for the task of identifying mislabelled samples. We
further show significant improvements in accuracy for the classification task
for each dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We find that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we propose a supervised
approach, which incorporates annotated pairs from natural language inference
datasets into our contrastive learning framework, by using &quot;entailment&quot; pairs
as positives and &quot;contradiction&quot; pairs as hard negatives. We evaluate SimCSE on
standard semantic textual similarity (STS) tasks, and our unsupervised and
supervised models using BERT base achieve an average of 76.3% and 81.6%
Spearman&#x27;s correlation respectively, a 4.2% and 2.2% improvement compared to
previous best results. We also show -- both theoretically and empirically --
that contrastive learning objective regularizes pre-trained embeddings&#x27;
anisotropic space to be more uniform, and it better aligns positive pairs when
supervised signals are available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised classification of simulated magnetospheric regions. (arXiv:2109.04916v1 [physics.space-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04916">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In magnetospheric missions, burst mode data sampling should be triggered in
the presence of processes of scientific or operational interest. We present an
unsupervised classification method for magnetospheric regions, that could
constitute the first-step of a multi-step method for the automatic
identification of magnetospheric processes of interest. Our method is based on
Self Organizing Maps (SOMs), and we test it preliminarily on data points from
global magnetospheric simulations obtained with the OpenGGCM-CTIM-RCM code. The
dimensionality of the data is reduced with Principal Component Analysis before
classification. The classification relies exclusively on local plasma
properties at the selected data points, without information on their
neighborhood or on their temporal evolution. We classify the SOM nodes into an
automatically selected number of classes, and we obtain clusters that map to
well defined magnetospheric regions. We validate our classification results by
plotting the classified data in the simulated space and by comparing with
K-means classification. For the sake of result interpretability, we examine the
SOM feature maps (magnetospheric variables are called features in the context
of classification), and we use them to unlock information on the clusters. We
repeat the classification experiments using different sets of features, we
quantitatively compare different classification results, and we obtain insights
on which magnetospheric variables make more effective features for unsupervised
classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoPilot: Automating SoC Design Space Exploration for SWaP Constrained Autonomous UAVs. (arXiv:2102.02988v3 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02988">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Building domain-specific accelerators for autonomous unmanned aerial vehicles
(UAVs) is challenging due to a lack of systematic methodology for designing
onboard compute. Balancing a computing system for a UAV requires considering
both the cyber (e.g., sensor rate, compute performance) and physical (e.g.,
payload weight) characteristics that affect overall performance. Iterating over
the many component choices results in a combinatorial explosion of the number
of possible combinations: from 10s of thousands to billions, depending on
implementation details. Manually selecting combinations of these components is
tedious and expensive. To navigate the {cyber-physical design space}
efficiently, we introduce \emph{AutoPilot}, a framework that automates
full-system UAV co-design. AutoPilot uses Bayesian optimization to navigate a
large design space and automatically select a combination of autonomy algorithm
and hardware accelerator while considering the cross-product effect of other
cyber and physical UAV components. We show that the AutoPilot methodology
consistently outperforms general-purpose hardware selections like Xavier NX and
Jetson TX2, as well as dedicated hardware accelerators built for autonomous
UAVs, across a range of representative scenarios (three different UAV types and
three deployment environments). Designs generated by AutoPilot increase the
number of missions on average by up to 2.25x, 1.62x, and 1.43x for nano, micro,
and mini-UAVs respectively over baselines. Our work demonstrates the need for
holistic full-UAV co-design to achieve maximum overall UAV performance and the
need for automated flows to simplify the design process for autonomous
cyber-physical systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hybrid modeling of the human cardiovascular system using NeuralFMUs. (arXiv:2109.04880v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hybrid modeling, the combination of first principle and machine learning
models, is an emerging research field that gathers more and more attention.
Even if hybrid models produce formidable results for academic examples, there
are still different technical challenges that hinder the use of hybrid modeling
in real-world applications. By presenting NeuralFMUs, the fusion of a FMU, a
numerical ODE solver and an ANN, we are paving the way for the use of a variety
of first principle models from different modeling tools as parts of hybrid
models. This contribution handles the hybrid modeling of a complex, real-world
example: Starting with a simplified 1D-fluid model of the human cardiovascular
system (arterial side), the aim is to learn neglected physical effects like
arterial elasticity from data. We will show that the hybrid modeling process is
more comfortable, needs less system knowledge and is therefore less error-prone
compared to modeling solely based on first principle. Further, the resulting
hybrid model has improved in computation performance, compared to a pure first
principle white-box model, while still fulfilling the requirements regarding
accuracy of the considered hemodynamic quantities. The use of the presented
techniques is explained in a general manner and the considered use-case can
serve as example for other modeling and simulation applications in and beyond
the medical domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trust your neighbors: A comprehensive survey of neighborhood-based methods for recommender systems. (arXiv:2109.04584v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04584">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Collaborative recommendation approaches based on nearest-neighbors are still
highly popular today due to their simplicity, their efficiency, and their
ability to produce accurate and personalized recommendations. This chapter
offers a comprehensive survey of neighborhood-based methods for the item
recommendation problem. It presents the main characteristics and benefits of
such methods, describes key design choices for implementing a
neighborhood-based recommender system, and gives practical information on how
to make these choices. A broad range of methods is covered in the chapter,
including traditional algorithms like k-nearest neighbors as well as advanced
approaches based on matrix factorization, sparse coding and random walks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Heading Estimation Using Ultra-Wideband Received Signal Strength and Gaussian Processes. (arXiv:2109.04868v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04868">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>It is essential that a robot has the ability to determine its position and
orientation to execute tasks autonomously. Heading estimation is especially
challenging in indoor environments where magnetic distortions make
magnetometer-based heading estimation difficult. Ultra-wideband (UWB)
transceivers are common in indoor localization problems. This letter
experimentally demonstrates how to use UWB range and received signal strength
(RSS) measurements to estimate robot heading. The RSS of a UWB antenna varies
with its orientation. As such, a Gaussian process (GP) is used to learn a
data-driven relationship from UWB range and RSS inputs to orientation outputs.
Combined with a gyroscope in an invariant extended Kalman filter, this realizes
a heading estimation method that uses only UWB and gyroscope measurements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness without the sensitive attribute via Causal Variational Autoencoder. (arXiv:2109.04999v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04999">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, most fairness strategies in machine learning models focus on
mitigating unwanted biases by assuming that the sensitive information is
observed. However this is not always possible in practice. Due to privacy
purposes and var-ious regulations such as RGPD in EU, many personal sensitive
attributes are frequently not collected. We notice a lack of approaches for
mitigating bias in such difficult settings, in particular for achieving
classical fairness objectives such as Demographic Parity and Equalized Odds. By
leveraging recent developments for approximate inference, we propose an
approach to fill this gap. Based on a causal graph, we rely on a new
variational auto-encoding based framework named SRCVAE to infer a sensitive
information proxy, that serve for bias mitigation in an adversarial fairness
approach. We empirically demonstrate significant improvements over existing
works in the field. We observe that the generated proxy&#x27;s latent space recovers
sensitive information and that our approach achieves a higher accuracy while
obtaining the same level of fairness on two real datasets, as measured using
com-mon fairness definitions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automated Machine Learning, Bounded Rationality, and Rational Metareasoning. (arXiv:2109.04744v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04744">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The notion of bounded rationality originated from the insight that perfectly
rational behavior cannot be realized by agents with limited cognitive or
computational resources. Research on bounded rationality, mainly initiated by
Herbert Simon, has a longstanding tradition in economics and the social
sciences, but also plays a major role in modern AI and intelligent agent
design. Taking actions under bounded resources requires an agent to reflect on
how to use these resources in an optimal way - hence, to reason and make
decisions on a meta-level. In this paper, we will look at automated machine
learning (AutoML) and related problems from the perspective of bounded
rationality, essentially viewing an AutoML tool as an agent that has to train a
model on a given set of data, and the search for a good way of doing so (a
suitable &quot;ML pipeline&quot;) as deliberation on a meta-level.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema(definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5)is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Fast PC Algorithm with Reversed-order Pruning and A Parallelization Strategy. (arXiv:2109.04626v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04626">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The PC algorithm is the state-of-the-art algorithm for causal structure
discovery on observational data. It can be computationally expensive in the
worst case due to the conditional independence tests are performed in an
exhaustive-searching manner. This makes the algorithm computationally
intractable when the task contains several hundred or thousand nodes,
particularly when the true underlying causal graph is dense. We propose a
critical observation that the conditional set rendering two nodes independent
is non-unique, and including certain redundant nodes do not sacrifice result
accuracy. Based on this finding, the innovations of our work are two-folds.
First, we innovate on a reserve order linkage pruning PC algorithm which
significantly increases the algorithm&#x27;s efficiency. Second, we propose a
parallel computing strategy for statistical independence tests by leveraging
tensor computation, which brings further speedup. We also prove the proposed
algorithm does not induce statistical power loss under mild graph and data
dimensionality assumptions. Experimental results show that the single-threaded
version of the proposed algorithm can achieve a 6-fold speedup compared to the
PC algorithm on a dense 95-node graph, and the parallel version can make a
825-fold speed-up. We also provide proof that the proposed algorithm is
consistent under the same set of conditions with conventional PC algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The problem of representing the atomic elements of language in modern neural
learning systems is one of the central challenges of the field of natural
language processing. I present a survey of the distributional, compositional,
and relational approaches to addressing this task, and discuss various means of
integrating them into systems, with special emphasis on the word level and the
out-of-vocabulary phenomenon.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MEAL: Manifold Embedding-based Active Learning. (arXiv:2106.11858v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image segmentation is a common and challenging task in autonomous driving.
Availability of sufficient pixel-level annotations for the training data is a
hurdle. Active learning helps learning from small amounts of data by suggesting
the most promising samples for labeling. In this work, we propose a new
pool-based method for active learning, which proposes promising patches
extracted from full image, in each acquisition step. The problem is framed in
an exploration-exploitation framework by combining an embedding based on
Uniform Manifold Approximation to model representativeness with entropy as
uncertainty measure to model informativeness. We applied our proposed method to
the autonomous driving datasets CamVid and Cityscapes and performed a
quantitative comparison with state-of-the-art baselines. We find that our
active learning method achieves better performance compared to previous
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimates on the generalization error of Physics Informed Neural Networks (PINNs) for approximating PDEs. (arXiv:2006.16144v2 [math.NA] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.16144">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Physics informed neural networks (PINNs) have recently been widely used for
robust and accurate approximation of PDEs. We provide rigorous upper bounds on
the generalization error of PINNs approximating solutions of the forward
problem for PDEs. An abstract formalism is introduced and stability properties
of the underlying PDE are leveraged to derive an estimate for the
generalization error in terms of the training error and number of training
samples. This abstract framework is illustrated with several examples of
nonlinear PDEs. Numerical experiments, validating the proposed theory, are also
presented.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">In Defense of Uniform Convergence: Generalization via derandomization with an application to interpolating predictors. (arXiv:1912.04265v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.04265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose to study the generalization error of a learned predictor $\hat h$
in terms of that of a surrogate (potentially randomized) predictor that is
coupled to $\hat h$ and designed to trade empirical risk for control of
generalization error. In the case where $\hat h$ interpolates the data, it is
interesting to consider theoretical surrogate classifiers that are partially
derandomized or rerandomized, e.g., fit to the training data but with modified
label noise. We also show that replacing $\hat h$ by its conditional
distribution with respect to an arbitrary $\sigma$-field is a convenient way to
derandomize. We study two examples, inspired by the work of Nagarajan and
Kolter (2019) and Bartlett et al. (2019), where the learned classifier $\hat h$
interpolates the training data with high probability, has small risk, and, yet,
does not belong to a nonrandom class with a tight uniform bound on two-sided
generalization error. At the same time, we bound the risk of $\hat h$ in terms
of surrogates constructed by conditioning and denoising, respectively, and
shown to belong to nonrandom classes with uniformly small generalization error.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04997">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A major factor contributing to the success of modern representation learning
is the ease of performing various vector operations. Recently, objects with
geometric structures (eg. distributions, complex or hyperbolic vectors, or
regions such as cones, disks, or boxes) have been explored for their
alternative inductive biases and additional representational capacities. In
this work, we introduce Box Embeddings, a Python library that enables
researchers to easily apply and extend probabilistic box embeddings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Armed Bandits with Correlated Arms. (arXiv:1911.03959v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03959">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider a multi-armed bandit framework where the rewards obtained by
pulling different arms are correlated. We develop a unified approach to
leverage these reward correlations and present fundamental generalizations of
classic bandit algorithms to the correlated setting. We present a unified proof
technique to analyze the proposed algorithms. Rigorous analysis of C-UCB (the
correlated bandit version of Upper-confidence-bound) reveals that the algorithm
ends up pulling certain sub-optimal arms, termed as non-competitive, only O(1)
times, as opposed to the O(log T) pulls required by classic bandit algorithms
such as UCB, TS etc. We present regret-lower bound and show that when arms are
correlated through a latent random source, our algorithms obtain order-optimal
regret. We validate the proposed algorithms via experiments on the MovieLens
and Goodreads datasets, and show significant improvement over classical bandit
algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A possible explanation for the impressive performance of masked language
model (MLM) pre-training is that such models have learned to represent the
syntactic structures prevalent in classical NLP pipelines. In this paper, we
propose a different explanation: MLMs succeed on downstream tasks almost
entirely due to their ability to model higher-order word co-occurrence
statistics. To demonstrate this, we pre-train MLMs on sentences with randomly
shuffled word order, and show that these models still achieve high accuracy
after fine-tuning on many downstream tasks -- including on tasks specifically
designed to be challenging for models that ignore word order. Our models
perform surprisingly well according to some parametric syntactic probes,
indicating possible deficiencies in how we test representations for syntactic
information. Overall, our results show that purely distributional information
largely explains the success of pre-training, and underscore the importance of
curating challenging evaluation datasets that require deeper linguistic
knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05003">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the problem of training named entity recognition (NER) models using
only distantly-labeled data, which can be automatically obtained by matching
entity mentions in the raw text with entity types in a knowledge base. The
biggest challenge of distantly-supervised NER is that the distant supervision
may induce incomplete and noisy labels, rendering the straightforward
application of supervised learning ineffective. In this paper, we propose (1) a
noise-robust learning scheme comprised of a new loss function and a noisy label
removal step, for training NER models on distantly-labeled data, and (2) a
self-training method that uses contextualized augmentations created by
pre-trained language models to improve the generalization ability of the NER
model. On three benchmark datasets, our method achieves superior performance,
outperforming existing distantly-supervised NER models by significant margins.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We show that Transformer encoder architectures can be sped up, with limited
accuracy costs, by replacing the self-attention sublayers with simple linear
transformations that &quot;mix&quot; input tokens. These linear mixers, along with
standard nonlinearities in feed-forward layers, prove competent at modeling
semantic relationships in several text classification tasks. Most surprisingly,
we find that replacing the self-attention sublayer in a Transformer encoder
with a standard, unparameterized Fourier Transform achieves 92-97% of the
accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on
GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input
lengths, our FNet model is significantly faster: when compared to the
&quot;efficient&quot; Transformers on the Long Range Arena benchmark, FNet matches the
accuracy of the most accurate models, while outpacing the fastest models across
all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).
Finally, FNet has a light memory footprint and is particularly efficient at
smaller model sizes; for a fixed speed and accuracy budget, small FNet models
outperform Transformer counterparts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spike2Vec: An Efficient and Scalable Embedding Approach for COVID-19 Spike Sequences. (arXiv:2109.05019v1 [q-bio.GN])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05019">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>With the rapid global spread of COVID-19, more and more data related to this
virus is becoming available, including genomic sequence data. The total number
of genomic sequences that are publicly available on platforms such as GISAID is
currently several million, and is increasing with every day. The availability
of such \textit{Big Data} creates a new opportunity for researchers to study
this virus in detail. This is particularly important with all of the dynamics
of the COVID-19 variants which emerge and circulate. This rich data source will
give us insights on the best ways to perform genomic surveillance for this and
future pandemic threats, with the ultimate goal of mitigating or eliminating
such threats. Analyzing and processing the several million genomic sequences is
a challenging task. Although traditional methods for sequence classification
are proven to be effective, they are not designed to deal with these specific
types of genomic sequences. Moreover, most of the existing methods also face
the issue of scalability. Previous studies which were tailored to coronavirus
genomic data proposed to use spike sequences (corresponding to a subsequence of
the genome), rather than using the complete genomic sequence, to perform
different machine learning (ML) tasks such as classification and clustering.
However, those methods suffer from scalability issues. In this paper, we
propose an approach called Spike2Vec, an efficient and scalable feature vector
representation for each spike sequence that can be used for downstream ML
tasks. Through experiments, we show that Spike2Vec is not only scalable on
several million spike sequences, but also outperforms the baseline models in
terms of prediction accuracy, F1-score, etc.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generalizing to unseen domains via distribution matching. (arXiv:1911.00804v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.00804">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Supervised learning results typically rely on assumptions of i.i.d. data.
Unfortunately, those assumptions are commonly violated in practice. In this
work, we tackle such problem by focusing on domain generalization: a
formalization where the data generating process at test time may yield samples
from never-before-seen domains (distributions). Our work relies on the
following lemma: by minimizing a notion of discrepancy between all pairs from a
set of given domains, we also minimize the discrepancy between any pairs of
mixtures of domains. Using this result, we derive a generalization bound for
our setting. We then show that low risk over unseen domains can be achieved by
representing the data in a space where (i) the training distributions are
indistinguishable, and (ii) relevant information for the task at hand is
preserved. Minimizing the terms in our bound yields an adversarial formulation
which estimates and minimizes pairwise discrepancies. We validate our proposed
strategy on standard domain generalization benchmarks, outperforming a number
of recently introduced methods. Notably, we tackle a real-world application
where the underlying data corresponds to multi-channel electroencephalography
time series from different subjects, each considered as a distinct domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Neural Tangent Kernel Perspective of Infinite Tree Ensembles. (arXiv:2109.04983v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04983">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In practical situations, the ensemble tree model is one of the most popular
models along with neural networks. A soft tree is one of the variants of a
decision tree. Instead of using a greedy method for searching splitting rules,
the soft tree is trained using a gradient method in which the whole splitting
operation is formulated in a differentiable form. Although ensembles of such
soft trees have been increasingly used in recent years, little theoretical work
has been done for understanding their behavior. In this paper, by considering
an ensemble of infinite soft trees, we introduce and study the Tree Neural
Tangent Kernel (TNTK), which provides new insights into the behavior of the
infinite ensemble of soft trees. Using the TNTK, we succeed in theoretically
finding several non-trivial properties, such as the effect of the oblivious
tree structure and the degeneracy of the TNTK induced by the deepening of the
trees. Moreover, we empirically examine the performance of an ensemble of
infinite soft trees using the TNTK.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Differentially Private Federated Learning with Laplacian Smoothing. (arXiv:2005.00218v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00218">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning aims to protect data privacy by collaboratively learning a
model without sharing private data among users. However, an adversary may still
be able to infer the private training data by attacking the released model.
Differential privacy provides a statistical protection against such attacks at
the price of significantly degrading the accuracy or utility of the trained
models. In this paper, we investigate a utility enhancement scheme based on
Laplacian smoothing for differentially private federated learning (DP-Fed-LS),
where the parameter aggregation with injected Gaussian noise is improved in
statistical precision without losing privacy budget. Our key observation is
that the aggregated gradients in federated learning often enjoy a type of
smoothness, i.e. sparsity in the graph Fourier basis with polynomial decays of
Fourier coefficients as frequency grows, which can be exploited by the
Laplacian smoothing efficiently. Under a prescribed differential privacy
budget, convergence error bounds with tight rates are provided for DP-Fed-LS
with uniform subsampling of heterogeneous Non-IID data, revealing possible
utility improvement of Laplacian smoothing in effective dimensionality and
variance reduction, among others. Experiments over MNIST, SVHN, and Shakespeare
datasets show that the proposed method can improve model accuracy with
DP-guarantee and membership privacy under both uniform and Poisson subsampling
mechanisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model&#x27;s decision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Concept Generalization in Visual Representation Learning. (arXiv:2012.05649v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05649">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Measuring concept generalization, i.e., the extent to which models trained on
a set of (seen) visual concepts can be leveraged to recognize a new set of
(unseen) concepts, is a popular way of evaluating visual representations,
especially in a self-supervised learning framework. Nonetheless, the choice of
unseen concepts for such an evaluation is usually made arbitrarily, and
independently from the seen concepts used to train representations, thus
ignoring any semantic relationships between the two. In this paper, we argue
that the semantic relationships between seen and unseen concepts affect
generalization performance and propose ImageNet-CoG, a novel benchmark on the
ImageNet-21K (IN-21K) dataset that enables measuring concept generalization in
a principled way. Our benchmark leverages expert knowledge that comes from
WordNet in order to define a sequence of unseen IN-21K concept sets that are
semantically more and more distant from the ImageNet-1K (IN-1K) subset, a
ubiquitous training set. This allows us to benchmark visual representations
learned on IN-1K out-of-the box. We conduct a large-scale study encompassing 31
convolution and transformer-based models and show how different architectures,
levels of supervision, regularization techniques and use of web data impact the
concept generalization performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning the Hypotheses Space from data Part II: Convergence and Feasibility. (arXiv:2001.11578v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.11578">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In part \textit{I} we proposed a structure for a general Hypotheses Space
$\mathcal{H}$, the Learning Space $\mathbb{L}(\mathcal{H})$, which can be
employed to avoid \textit{overfitting} when estimating in a complex space with
relative shortage of examples. Also, we presented the U-curve property, which
can be taken advantage of in order to select a Hypotheses Space without
exhaustively searching $\mathbb{L}(\mathcal{H})$. In this paper, we carry
further our agenda, by showing the consistency of a model selection framework
based on Learning Spaces, in which one selects from data the Hypotheses Space
on which to learn. The method developed in this paper adds to the
state-of-the-art in model selection, by extending Vapnik-Chervonenkis Theory to
\textit{random} Hypotheses Spaces, i.e., Hypotheses Spaces learned from data.
In this framework, one estimates a random subspace $\hat{\mathcal{M}} \in
\mathbb{L}(\mathcal{H})$ which converges with probability one to a target
Hypotheses Space $\mathcal{M}^{\star} \in \mathbb{L}(\mathcal{H})$ with desired
properties. As the convergence implies asymptotic unbiased estimators, we have
a consistent framework for model selection, showing that it is feasible to
learn the Hypotheses Space from data. Furthermore, we show that the
generalization errors of learning on $\hat{\mathcal{M}}$ are lesser than those
we commit when learning on $\mathcal{H}$, so it is more efficient to learn on a
subspace learned from data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Conditional Dependence Hidden Markov Models for Skeleton-Based Action Recognition. (arXiv:2002.05809v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.05809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hidden Markov Models (HMMs) comprise a powerful generative approach for
modeling sequential data and time-series in general. However, the commonly
employed assumption of the dependence of the current time frame to a single or
multiple immediately preceding frames is unrealistic; more complicated dynamics
potentially exist in real world scenarios. This paper revisits conventional
sequential modeling approaches, aiming to address the problem of capturing
time-varying temporal dependency patterns. To this end, we propose a different
formulation of HMMs, whereby the dependence on past frames is dynamically
inferred from the data. Specifically, we introduce a hierarchical extension by
postulating an additional latent variable layer; therein, the (time-varying)
temporal dependence patterns are treated as latent variables over which
inference is performed. We leverage solid arguments from the Variational Bayes
framework and derive a tractable inference algorithm based on the
forward-backward algorithm. As we experimentally show, our approach can model
highly complex sequential data and can effectively handle data with missing
values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Dynamic Multi-Modal Data Fusion: A Model Uncertainty Perspective. (arXiv:2105.06018v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06018">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper is concerned with multi-modal data fusion (MMDF) under unexpected
modality failures in nonlinear non-Gaussian dynamic processes. An efficient
framework to tackle this problem is proposed. In particular, a notion termed
modality &quot;\emph{usefulness}&quot;, which takes a value of 1 or 0, is used for
indicating whether the observation of this modality is useful or not. For $n$
modalities involved, $2^n$ combinations of their &quot;\emph{usefulness}&quot; values
exist. Each combination defines one hypothetical model of the true data
generative process. Then the problem of concern is formalized as a task of
nonlinear non-Gaussian state filtering under model uncertainty, which is
addressed by a dynamic model averaging (DMA) based particle filter (PF)
algorithm. This DMA algorithm employs $2^n$ models, while all models share the
same state-transition function and a unique set of particle values. That makes
the computational complexity of this algorithm only slightly larger than a
single model based PF algorithm, especially for scenarios in which $n$ is
small. Experimental results show that the proposed solution outperforms
remarkably state-of-the-art methods. Code and data are available at
https://github.com/robinlau1981/fusion.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SplitAVG: A heterogeneity-aware federated deep learning method for medical imaging. (arXiv:2107.02375v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.02375">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning is an emerging research paradigm for enabling
collaboratively training deep learning models without sharing patient data.
However, the data from different institutions are usually heterogeneous across
institutions, which may reduce the performance of models trained using
federated learning. In this study, we propose a novel heterogeneity-aware
federated learning method, SplitAVG, to overcome the performance drops from
data heterogeneity in federated learning. Unlike previous federated methods
that require complex heuristic training or hyper parameter tuning, our SplitAVG
leverages the simple network split and feature map concatenation strategies to
encourage the federated model training an unbiased estimator of the target data
distribution. We compare SplitAVG with seven state-of-the-art federated
learning methods, using centrally hosted training data as the baseline on a
suite of both synthetic and real-world federated datasets. We find that the
performance of models trained using all the comparison federated learning
methods degraded significantly with the increasing degrees of data
heterogeneity. In contrast, SplitAVG method achieves comparable results to the
baseline method under all heterogeneous settings, that it achieves 96.2% of the
accuracy and 110.4% of the mean absolute error obtained by the baseline in a
diabetic retinopathy binary classification dataset and a bone age prediction
dataset, respectively, on highly heterogeneous data partitions. We conclude
that SplitAVG method can effectively overcome the performance drops from
variability in data distributions across institutions. Experimental results
also show that SplitAVG can be adapted to different base networks and
generalized to various types of medical imaging tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RL-IoT: Reinforcement Learning to Interact with IoT Devices. (arXiv:2105.00884v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00884">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Our life is getting filled by Internet of Things (IoT) devices. These devices
often rely on closed or poorly documented protocols, with unknown formats and
semantics. Learning how to interact with such devices in an autonomous manner
is the key for interoperability and automatic verification of their
capabilities. In this paper, we propose RL-IoT, a system that explores how to
automatically interact with possibly unknown IoT devices. We leverage
reinforcement learning (RL) to recover the semantics of protocol messages and
to take control of the device to reach a given goal, while minimizing the
number of interactions. We assume to know only a database of possible IoT
protocol messages, whose semantics are however unknown. RL-IoT exchanges
messages with the target IoT device, learning those commands that are useful to
reach the given goal. Our results show that RL-IoT is able to solve both simple
and complex tasks. With properly tuned parameters, RL-IoT learns how to perform
actions with the target device, a Yeelight smart bulb in our case study,
completing non-trivial patterns with as few as 400 interactions. RL-IoT paves
the road for automatic interactions with poorly documented IoT protocols, thus
enabling interoperable systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Unifying View on Implicit Bias in Training Linear Neural Networks. (arXiv:2010.02501v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.02501">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the implicit bias of gradient flow (i.e., gradient descent with
infinitesimal step size) on linear neural network training. We propose a tensor
formulation of neural networks that includes fully-connected, diagonal, and
convolutional networks as special cases, and investigate the linear version of
the formulation called linear tensor networks. With this formulation, we can
characterize the convergence direction of the network parameters as singular
vectors of a tensor defined by the network. For $L$-layer linear tensor
networks that are orthogonally decomposable, we show that gradient flow on
separable classification finds a stationary point of the $\ell_{2/L}$
max-margin problem in a &quot;transformed&quot; input space defined by the network. For
underdetermined regression, we prove that gradient flow finds a global minimum
which minimizes a norm-like function that interpolates between weighted
$\ell_1$ and $\ell_2$ norms in the transformed input space. Our theorems
subsume existing results in the literature while removing standard convergence
assumptions. We also provide experiments that corroborate our analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06127">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers that are pre-trained on multilingual corpora, such as, mBERT and
XLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In
the zero-shot transfer setting, only English training data is used, and the
fine-tuned model is evaluated on another target language. While this works
surprisingly well, substantial variance has been observed in target language
performance between different fine-tuning runs, and in the zero-shot setup, no
target-language development data is available to select among multiple
fine-tuned models. Prior work has relied on English dev data to select among
models that are fine-tuned with different learning rates, number of steps and
other hyperparameters, often resulting in suboptimal choices. In this paper, we
show that it is possible to select consistently better models when small
amounts of annotated data are available in auxiliary pivot languages. We
propose a machine learning approach to model selection that uses the fine-tuned
model&#x27;s own internal representations to predict its cross-lingual capabilities.
In extensive experiments we find that this method consistently selects better
models than English validation data across twenty five languages (including
eight low-resource languages), and often achieves results that are comparable
to model selection using target language development data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hamiltonian neural networks for solving equations of motion. (arXiv:2001.11107v3 [physics.comp-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.11107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There has been a wave of interest in applying machine learning to study
dynamical systems. In particular, neural networks have been applied to solve
the equations of motion, and therefore, track the evolution of a system. In
contrast to other applications of neural networks and machine learning,
dynamical systems possess invariants such as energy, momentum, and angular
momentum, depending on their underlying symmetries. Traditional numerical
integration methods sometimes violate these conservation laws, propagating
errors in time, ultimately reducing the predictability of the method. We
present a data-free Hamiltonian neural network that solves the differential
equations that govern dynamical systems. This is an equation-driven
unsupervised learning method where the optimization process of the network
depends solely on the predicted functions without using any ground truth data.
This unsupervised model learns solutions that satisfy identically, up to an
arbitrarily small error, Hamilton&#x27;s equations and, therefore, conserve the
Hamiltonian invariants. Once the network is optimized, the proposed
architecture is considered a symplectic unit due to the introduction of an
efficient parametric form of solutions. In addition, the choice of an
appropriate activation function drastically improves the predictability of the
network. An error analysis is derived and states that the numerical errors
depend on the overall network performance. The symplectic architecture is then
employed to solve the equations for the nonlinear oscillator and the chaotic
Henon-Heiles dynamical system. In both systems, a symplectic Euler integrator
requires two orders more evaluation points than the Hamiltonian network in
order to achieve the same order of the numerical error in the predicted phase
space trajectories.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ProcK: Machine Learning for Knowledge-Intensive Processes. (arXiv:2109.04881v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04881">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Process mining deals with extraction of knowledge from business process
execution logs. Traditional process mining tasks, like process model generation
or conformance checking, rely on a minimalistic feature set where each event is
characterized only by its case identifier, activity type, and timestamp. In
contrast, the success of modern machine learning is based on models that take
any available data as direct input and build layers of features automatically
during training. In this work, we introduce ProcK (Process &amp; Knowledge), a
novel pipeline to build business process prediction models that take into
account both sequential data in the form of event logs and rich semantic
information represented in a graph-structured knowledge base. The hybrid
approach enables ProcK to flexibly make use of all information residing in the
databases of organizations. Components to extract inter-linked event logs and
knowledge bases from relational databases are part of the pipeline. We
demonstrate the power of ProcK by training it for prediction tasks on the OULAD
e-learning dataset, where we achieve state-of-the-art performance on the tasks
of predicting student dropout from courses and predicting their success. We
also apply our method on a number of additional machine learning tasks,
including exam score prediction and early predictions that only take into
account data recorded during the first weeks of the courses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Study of Joint Graph Inference and Forecasting. (arXiv:2109.04979v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04979">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study a recent class of models which uses graph neural networks (GNNs) to
improve forecasting in multivariate time series.

The core assumption behind these models is that there is a latent graph
between the time series (nodes) that governs the evolution of the multivariate
time series.

By parameterizing a graph in a differentiable way, the models aim to improve
forecasting quality.

We compare four recent models of this class on the forecasting task. Further,
we perform ablations to study their behavior under changing conditions, e.g.,
when disabling the graph-learning modules and providing the ground-truth
relations instead. Based on our findings, we propose novel ways of combining
the existing architectures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A health outcome is a measurement or an observation used to capture and
assess the effect of a treatment. Automatic detection of health outcomes from
text would undoubtedly speed up access to evidence necessary in healthcare
decision making. Prior work on outcome detection has modelled this task as
either (a) a sequence labelling task, where the goal is to detect which text
spans describe health outcomes, or (b) a classification task, where the goal is
to classify a text into a pre-defined set of categories depending on an outcome
that is mentioned somewhere in that text. However, this decoupling of span
detection and classification is problematic from a modelling perspective and
ignores global structural correspondences between sentence-level and word-level
information present in a given text. To address this, we propose a method that
uses both word-level and sentence-level information to simultaneously perform
outcome span detection and outcome type classification. In addition to
injecting contextual information to hidden vectors, we use label attention to
appropriately weight both word and sentence level information. Experimental
results on several benchmark datasets for health outcome detection show that
our proposed method consistently outperforms decoupled methods, reporting
competitive results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04994">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Unlike well-structured text, such as news reports and encyclopedia articles,
dialogue content often comes from two or more interlocutors, exchanging
information with each other. In such a scenario, the topic of a conversation
can vary upon progression and the key information for a certain topic is often
scattered across multiple utterances of different speakers, which poses
challenges to abstractly summarize dialogues. To capture the various topic
information of a conversation and outline salient facts for the captured
topics, this work proposes two topic-aware contrastive learning objectives,
namely coherence detection and sub-summary generation objectives, which are
expected to implicitly model the topic change and handle information scattering
challenges for the dialogue summarization task. The proposed contrastive
objectives are framed as auxiliary tasks for the primary dialogue summarization
task, united via an alternative parameter updating strategy. Extensive
experiments on benchmark datasets demonstrate that the proposed simple method
significantly outperforms strong baselines and achieves new state-of-the-art
performance. The code and trained models are publicly available via
\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ECO: Enabling Energy-Neutral IoT Devices through Runtime Allocation of Harvested Energy. (arXiv:2102.13605v2 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13605">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Energy harvesting offers an attractive and promising mechanism to power
low-energy devices. However, it alone is insufficient to enable an
energy-neutral operation, which can eliminate tedious battery charging and
replacement requirements. Achieving an energy-neutral operation is challenging
since the uncertainties in harvested energy undermine the quality of service
requirements. To address this challenge, we present a runtime energy-allocation
framework that optimizes the utility of the target device under energy
constraints using a rollout algorithm, which is a sequential approach to solve
dynamic optimization problems. The proposed framework uses an efficient
iterative algorithm to compute initial energy allocations at the beginning of a
day. The initial allocations are then corrected at every interval to compensate
for the deviations from the expected energy harvesting pattern. We evaluate
this framework using solar and motion energy harvesting modalities and American
Time Use Survey data from 4772 different users. Compared to prior techniques,
the proposed framework achieves up to 35% higher utility even under
energy-limited scenarios. Moreover, measurements on a wearable device prototype
show that the proposed framework has 1000x smaller energy overhead than
iterative approaches with a negligible loss in utility.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedProf: Selective Federated Learning with Representation Profiling. (arXiv:2102.01733v6 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated Learning (FL) has shown great potential as a privacy-preserving
solution to learning from decentralized data that are only accessible to end
devices (i.e., clients). In many scenarios however, a large proportion of the
clients are probably in possession of low-quality data that are biased, noisy
or even irrelevant. As a result, they could significantly slow down the
convergence of the global model we aim to build and also compromise its
quality. In light of this, we propose FedProf, a novel algorithm for optimizing
FL under such circumstances without breaching data privacy. The key of our
approach is a data representation profiling and matching scheme that uses the
global model to dynamically profile data representations and allows for
low-cost, lightweight representation matching. Based on the scheme we
adaptively score each client and adjust its participation probability so as to
mitigate the impact of low-value clients on the training process. We have
conducted extensive experiments on public datasets using various FL settings.
The results show that FedProf effectively reduces the number of communication
rounds and overall time (up to 4.5x speedup) for the global model to converge
and provides accuracy gain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Differential Privacy in Personalized Pricing with Nonparametric Demand Models. (arXiv:2109.04615v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04615">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In the recent decades, the advance of information technology and abundant
personal data facilitate the application of algorithmic personalized pricing.
However, this leads to the growing concern of potential violation of privacy
due to adversarial attack. To address the privacy issue, this paper studies a
dynamic personalized pricing problem with \textit{unknown} nonparametric demand
models under data privacy protection. Two concepts of data privacy, which have
been widely applied in practices, are introduced: \textit{central differential
privacy (CDP)} and \textit{local differential privacy (LDP)}, which is proved
to be stronger than CDP in many cases. We develop two algorithms which make
pricing decisions and learn the unknown demand on the fly, while satisfying the
CDP and LDP gurantees respectively. In particular, for the algorithm with CDP
guarantee, the regret is proved to be at most $\tilde
O(T^{(d+2)/(d+4)}+\varepsilon^{-1}T^{d/(d+4)})$. Here, the parameter $T$
denotes the length of the time horizon, $d$ is the dimension of the
personalized information vector, and the key parameter $\varepsilon&gt;0$ measures
the strength of privacy (smaller $\varepsilon$ indicates a stronger privacy
protection). On the other hand, for the algorithm with LDP guarantee, its
regret is proved to be at most $\tilde
O(\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$, which is near-optimal as we prove a
lower bound of $\Omega(\varepsilon^{-2/(d+2)}T^{(d+1)/(d+2)})$ for any
algorithm with LDP guarantee.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedCon: A Contrastive Framework for Federated Semi-Supervised Learning. (arXiv:2109.04533v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04533">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated Semi-Supervised Learning (FedSSL) has gained rising attention from
both academic and industrial researchers, due to its unique characteristics of
co-training machine learning models with isolated yet unlabeled data. Most
existing FedSSL methods focus on the classical scenario, i.e, the labeled and
unlabeled data are stored at the client side. However, in real world
applications, client users may not provide labels without any incentive. Thus,
the scenario of labels at the server side is more practical. Since unlabeled
data and labeled data are decoupled, most existing FedSSL approaches may fail
to deal with such a scenario. To overcome this problem, in this paper, we
propose FedCon, which introduces a new learning paradigm, i.e., contractive
learning, to FedSSL. Experimental results on three datasets show that FedCon
achieves the best performance with the contractive framework compared with
state-of-the-art baselines under both IID and Non-IID settings. Besides,
ablation studies demonstrate the characteristics of the proposed FedCon
framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Inverse design of 3d molecular structures with conditional generative neural networks. (arXiv:2109.04824v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rational design of molecules with desired properties is a long-standing
challenge in chemistry. Generative neural networks have emerged as a powerful
approach to sample novel molecules from a learned distribution. Here, we
propose a conditional generative neural network for 3d molecular structures
with specified structural and chemical properties. This approach is agnostic to
chemical bonding and enables targeted sampling of novel molecules from
conditional distributions, even in domains where reference calculations are
sparse. We demonstrate the utility of our method for inverse design by
generating molecules with specified composition or motifs, discovering
particularly stable molecules, and jointly targeting multiple electronic
properties beyond the training regime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pre-training has improved model accuracy for both classification and
generation tasks at the cost of introducing much larger and slower models.
Pruning methods have proven to be an effective way of reducing model size,
whereas distillation methods are proven for speeding up inference. We introduce
a block pruning approach targeting both small and fast models. Our approach
extends structured methods by considering blocks of any size and integrates
this structure into the movement pruning paradigm for fine-tuning. We find that
this approach learns to prune out full components of the underlying model, such
as attention heads. Experiments consider classification and generation tasks,
yielding among other results a pruned model that is a 2.4x faster, 74% smaller
BERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models
in speed and pruned models in size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Asynchronous Iterations in Optimization: New Sequence Results and Sharper Algorithmic Guarantees. (arXiv:2109.04522v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04522">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce novel convergence results for asynchronous iterations which
appear in the analysis of parallel and distributed optimization algorithms. The
results are simple to apply and give explicit estimates for how the degree of
asynchrony impacts the convergence rates of the iterates. Our results shorten,
streamline and strengthen existing convergence proofs for several asynchronous
optimization methods, and allow us to establish convergence guarantees for
popular algorithms that were thus far lacking a complete theoretical
understanding. Specifically, we use our results to derive better iteration
complexity bounds for proximal incremental aggregated gradient methods, to
provide less conservative analyses of the speedup conditions for asynchronous
block-coordinate implementations of Krasnoselskii-Mann iterations, and to
quantify the convergence rates for totally asynchronous iterations under
various assumptions on communication delays and update rates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Feature-based Individual Fairness in k-Clustering. (arXiv:2109.04554v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04554">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Ensuring fairness in machine learning algorithms is a challenging and
important task. We consider the problem of clustering a set of points while
ensuring fairness constraints. While there have been several attempts to
capture group fairness in the k-clustering problem, fairness at an individual
level is not well-studied. We introduce a new notion of individual fairness in
k-clustering based on features that are not necessarily used for clustering. We
show that this problem is NP-hard and does not admit a constant factor
approximation. We then design a randomized algorithm that guarantees
approximation both in terms of minimizing the clustering distance objective as
well as individual fairness under natural restrictions on the distance metric
and fairness constraints. Finally, our experimental results validate that our
algorithm produces lower clustering costs compared to existing algorithms while
being competitive in individual fairness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TENET: Temporal CNN with Attention for Anomaly Detection in Automotive Cyber-Physical Systems. (arXiv:2109.04565v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04565">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Modern vehicles have multiple electronic control units (ECUs) that are
connected together as part of a complex distributed cyber-physical system
(CPS). The ever-increasing communication between ECUs and external electronic
systems has made these vehicles particularly susceptible to a variety of
cyber-attacks. In this work, we present a novel anomaly detection framework
called TENET to detect anomalies induced by cyber-attacks on vehicles. TENET
uses temporal convolutional neural networks with an integrated attention
mechanism to detect anomalous attack patterns. TENET is able to achieve an
improvement of 32.70% in False Negative Rate, 19.14% in the Mathews Correlation
Coefficient, and 17.25% in the ROC-AUC metric, with 94.62% fewer model
parameters, 86.95% decrease in memory footprint, and 48.14% lower inference
time when compared to the best performing prior work on automotive anomaly
detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatially Focused Attack against Spatiotemporal Graph Neural Networks. (arXiv:2109.04608v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04608">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Spatiotemporal forecasting plays an essential role in various applications in
intelligent transportation systems (ITS), such as route planning, navigation,
and traffic control and management. Deep Spatiotemporal graph neural networks
(GNNs), which capture both spatial and temporal patterns, have achieved great
success in traffic forecasting applications. Understanding how GNNs-based
forecasting work and the vulnerability and robustness of these models becomes
critical to real-world applications. For example, if spatiotemporal GNNs are
vulnerable in real-world traffic prediction applications, a hacker can easily
manipulate the results and cause serious traffic congestion and even a
city-scale breakdown. However, despite that recent studies have demonstrated
that deep neural networks (DNNs) are vulnerable to carefully designed
perturbations in multiple domains like objection classification and graph
representation, current adversarial works cannot be directly applied to
spatiotemporal forecasting due to the causal nature and spatiotemporal
mechanisms in forecasting models. To fill this gap, in this paper we design
Spatially Focused Attack (SFA) to break spatiotemporal GNNs by attacking a
single vertex. To achieve this, we first propose the inverse estimation to
address the causality issue; then, we apply genetic algorithms with a universal
attack method as the evaluation function to locate the weakest vertex; finally,
perturbations are generated by solving an inverse estimation-based optimization
problem. We conduct experiments on real-world traffic data and our results show
that perturbations in one vertex designed by SA can be diffused into a large
part of the graph.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Simulating the Effects of Eco-Friendly Transportation Selections for Air Pollution Reduction. (arXiv:2109.04831v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04831">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reducing air pollution, such as CO2 and PM2.5 emissions, is one of the most
important issues for many countries worldwide. Selecting an environmentally
friendly transport mode can be an effective approach of individuals to reduce
air pollution in daily life. In this study, we propose a method to simulate the
effectiveness of an eco-friendly transport mode selection for reducing air
pollution by using map search logs. We formulate the transport mode selection
as a combinatorial optimization problem with the constraints regarding the
total amount of CO2 emissions as an example of air pollution and the average
travel time. The optimization results show that the total amount of CO2
emissions can be reduced by 9.23%, whereas the average travel time can in fact
be reduced by 9.96%. Our research proposal won first prize in Regular Machine
Learning Competition Track Task 2 at KDD Cup 2019.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Causal Binary Concepts Discovery with VAE for Black-box Model Explanation. (arXiv:2109.04518v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04518">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We aim to explain a black-box classifier with the form: &#x60;data X is classified
as class Y because X \textit{has} A, B and \textit{does not have} C&#x27; in which
A, B, and C are high-level concepts. The challenge is that we have to discover
in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful
for the explaining the classifier. We first introduce a structural generative
model that is suitable to express and discover such concepts. We then propose a
learning process that simultaneously learns the data distribution and
encourages certain concepts to have a large causal influence on the classifier
output. Our method also allows easy integration of user&#x27;s prior knowledge to
induce high interpretability of concepts. Using multiple datasets, we
demonstrate that our method can discover useful binary concepts for
explanation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Is Attention Better Than Matrix Decomposition?. (arXiv:2109.04553v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04553">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As an essential ingredient of modern deep learning, attention mechanism,
especially self-attention, plays a vital role in the global correlation
discovery. However, is hand-crafted attention irreplaceable when modeling the
global context? Our intriguing finding is that self-attention is not better
than the matrix decomposition (MD) model developed 20 years ago regarding the
performance and computational cost for encoding the long-distance dependencies.
We model the global context issue as a low-rank recovery problem and show that
its optimization algorithms can help design global information blocks. This
paper then proposes a series of Hamburgers, in which we employ the optimization
algorithms for solving MDs to factorize the input representations into
sub-matrices and reconstruct a low-rank embedding. Hamburgers with different
MDs can perform favorably against the popular global context module
self-attention when carefully coping with gradients back-propagated through
MDs. Comprehensive experiments are conducted in the vision tasks where it is
crucial to learn the global context, including semantic segmentation and image
generation, demonstrating significant improvements over self-attention and its
variants.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Truth Discovery in Sequence Labels from Crowds. (arXiv:2109.04470v1 [cs.HC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04470">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Annotations quality and quantity positively affect the performance of
sequence labeling, a vital task in Natural Language Processing. Hiring domain
experts to annotate a corpus set is very costly in terms of money and time.
Crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have been
deployed to assist in this purpose. However, these platforms are prone to human
errors due to the lack of expertise; hence, one worker&#x27;s annotations cannot be
directly used to train the model. Existing literature in annotation aggregation
more focuses on binary or multi-choice problems. In recent years, handling the
sequential label aggregation tasks on imbalanced datasets with complex
dependencies between tokens has been challenging. To conquer the challenge, we
propose an optimization-based method that infers the best set of aggregated
annotations using labels provided by workers. The proposed Aggregation method
for Sequential Labels from Crowds ($AggSLC$) jointly considers the
characteristics of sequential labeling tasks, workers&#x27; reliabilities, and
advanced machine learning techniques. We evaluate $AggSLC$ on different
crowdsourced data for Named Entity Recognition (NER), Information Extraction
tasks in biomedical (PICO), and the simulated dataset. Our results show that
the proposed method outperforms the state-of-the-art aggregation methods. To
achieve insights into the framework, we study $AggSLC$ components&#x27;
effectiveness through ablation studies by evaluating our model in the absence
of the prediction module and inconsistency loss function. Theoretical analysis
of our algorithm&#x27;s convergence points that the proposed $AggSLC$ halts after a
finite number of iterations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficiently Identifying Task Groupings for Multi-Task Learning. (arXiv:2109.04617v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04617">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-task learning can leverage information learned by one task to benefit
the training of other tasks. Despite this capacity, naively training all tasks
together in one model often degrades performance, and exhaustively searching
through combinations of task groupings can be prohibitively expensive. As a
result, efficiently identifying the tasks that would benefit from co-training
remains a challenging design question without a clear solution. In this paper,
we suggest an approach to select which tasks should train together in
multi-task learning models. Our method determines task groupings in a single
training run by co-training all tasks together and quantifying the effect to
which one task&#x27;s gradient would affect another task&#x27;s loss. On the large-scale
Taskonomy computer vision dataset, we find this method can decrease test loss
by 10.0\% compared to simply training all tasks together while operating 11.6
times faster than a state-of-the-art task grouping method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bootstrapped Meta-Learning. (arXiv:2109.04504v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04504">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Meta-learning empowers artificial intelligence to increase its efficiency by
learning how to learn. Unlocking this potential involves overcoming a
challenging meta-optimisation problem that often exhibits ill-conditioning, and
myopic meta-objectives. We propose an algorithm that tackles these issues by
letting the meta-learner teach itself. The algorithm first bootstraps a target
from the meta-learner, then optimises the meta-learner by minimising the
distance to that target under a chosen (pseudo-)metric. Focusing on
meta-learning with gradients, we establish conditions that guarantee
performance improvements and show that the improvement is related to the target
distance. Thus, by controlling curvature, the distance measure can be used to
ease meta-optimization, for instance by reducing ill-conditioning. Further, the
bootstrapping mechanism can extend the effective meta-learning horizon without
requiring backpropagation through all updates. The algorithm is versatile and
easy to implement. We achieve a new state-of-the art for model-free agents on
the Atari ALE benchmark, improve upon MAML in few-shot learning, and
demonstrate how our approach opens up new possibilities by meta-learning
efficient exploration in a Q-learning agent.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">6MapNet: Representing soccer players from tracking data by a triplet network. (arXiv:2109.04720v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04720">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Although the values of individual soccer players have become astronomical,
subjective judgments still play a big part in the player analysis. Recently,
there have been new attempts to quantitatively grasp players&#x27; styles using
video-based event stream data. However, they have some limitations in
scalability due to high annotation costs and sparsity of event stream data. In
this paper, we build a triplet network named 6MapNet that can effectively
capture the movement styles of players using in-game GPS data. Without any
annotation of soccer-specific actions, we use players&#x27; locations and velocities
to generate two types of heatmaps. Our subnetworks then map these heatmap pairs
into feature vectors whose similarity corresponds to the actual similarity of
playing styles. The experimental results show that players can be accurately
identified with only a small number of matches by our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Style Pooling: Automatic Text Style Obfuscation for Improved Classification Fairness. (arXiv:2109.04624v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04624">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text style can reveal sensitive attributes of the author (e.g. race or age)
to the reader, which can, in turn, lead to privacy violations and bias in both
human and algorithmic decisions based on text. For example, the style of
writing in job applications might reveal protected attributes of the candidate
which could lead to bias in hiring decisions, regardless of whether hiring
decisions are made algorithmically or by humans. We propose a VAE-based
framework that obfuscates stylistic features of human-generated text through
style transfer by automatically re-writing the text itself. Our framework
operationalizes the notion of obfuscated style in a flexible way that enables
two distinct notions of obfuscated style: (1) a minimal notion that effectively
intersects the various styles seen in training, and (2) a maximal notion that
seeks to obfuscate by adding stylistic features of all sensitive attributes to
text, in effect, computing a union of styles. Our style-obfuscation framework
can be used for multiple purposes, however, we demonstrate its effectiveness in
improving the fairness of downstream classifiers. We also conduct a
comprehensive study on style pooling&#x27;s effect on fluency, semantic consistency,
and attribute removal from text, in two and three domain style obfuscation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deciphering Environmental Air Pollution with Large Scale City Data. (arXiv:2109.04572v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04572">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Out of the numerous hazards posing a threat to sustainable environmental
conditions in the 21st century, only a few have a graver impact than air
pollution. Its importance in determining the health and living standards in
urban settings is only expected to increase with time. Various factors ranging
from emissions from traffic and power plants, household emissions, natural
causes are known to be primary causal agents or influencers behind rising air
pollution levels. However, the lack of large scale data involving the major
factors has hindered the research on the causes and relations governing the
variability of the different air pollutants. Through this work, we introduce a
large scale city-wise dataset for exploring the relationships among these
agents over a long period of time. We analyze and explore the dataset to bring
out inferences which we can derive by modeling the data. Also, we provide a set
of benchmarks for the problem of estimating or forecasting pollutant levels
with a set of diverse models and methodologies. Through our paper, we seek to
provide a ground base for further research into this domain that will demand
critical attention of ours in the near future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Notes on Generalizing the Maximum Entropy Principle to Uncertain Data. (arXiv:2109.04530v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04530">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The principle of maximum entropy is a broadly applicable technique for
computing a distribution with the least amount of information possible while
commonly constrained to match empirically estimated feature expectations. We
seek to generalize this principle to scenarios where the empirical feature
expectations cannot be computed because the model variables are only partially
observed, which introduces a dependency on the learned model. Extending and
generalizing the principle of latent maximum entropy, we introduce uncertain
maximum entropy and describe an expectation-maximization based solution to
approximately solve these problems. We show that our technique generalizes the
principle of maximum entropy and latent maximum entropy and discuss a generally
applicable regularization technique for adding error terms to feature
expectation constraints in the event of limited data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04535">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Extracting moral sentiment from text is a vital component in understanding
public opinion, social movements, and policy decisions. The Moral Foundation
Theory identifies five moral foundations, each associated with a positive and
negative polarity. However, moral sentiment is often motivated by its targets,
which can correspond to individuals or collective entities. In this paper, we
introduce morality frames, a representation framework for organizing moral
attitudes directed at different entities, and come up with a novel and
high-quality annotated dataset of tweets written by US politicians. Then, we
propose a relational learning model to predict moral attitudes towards entities
and moral foundations jointly. We do qualitative and quantitative evaluations,
showing that moral sentiment towards entities differs highly across political
ideologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Ergodic Limits, Relaxations, and Geometric Properties of Random Walk Node Embeddings. (arXiv:2109.04526v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04526">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Random walk based node embedding algorithms learn vector representations of
nodes by optimizing an objective function of node embedding vectors and
skip-bigram statistics computed from random walks on the network. They have
been applied to many supervised learning problems such as link prediction and
node classification and have demonstrated state-of-the-art performance. Yet,
their properties remain poorly understood. This paper studies properties of
random walk based node embeddings in the unsupervised setting of discovering
hidden block structure in the network, i.e., learning node representations
whose cluster structure in Euclidean space reflects their adjacency structure
within the network. We characterize the ergodic limits of the embedding
objective, its generalization, and related convex relaxations to derive
corresponding non-randomized versions of the node embedding objectives. We also
characterize the optimal node embedding Grammians of the non-randomized
objectives for the expected graph of a two-community Stochastic Block Model
(SBM). We prove that the solution Grammian has rank $1$ for a suitable nuclear
norm relaxation of the non-randomized objective. Comprehensive experimental
results on SBM random networks reveal that our non-randomized ergodic
objectives yield node embeddings whose distribution is Gaussian-like, centered
at the node embeddings of the expected network within each community, and
concentrate in the linear degree-scaling regime as the number of nodes
increases.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-10">2021-09-10</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08836">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00720">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most existing NER methods rely on extensive labeled data for model training,
which struggles in the low-resource scenarios with limited training data.
Recently, prompt-tuning methods for pre-trained language models have achieved
remarkable performance in few-shot learning by exploiting prompts as task
guidance to reduce the gap between training progress and downstream tuning.
Inspired by prompt learning, we propose a novel lightweight generative
framework with prompt-guided attention for low-resource NER (LightNER).
Specifically, we construct the semantic-aware answer space of entity categories
for prompt learning to generate the entity span sequence and entity categories
without any label-specific classifiers. We further propose prompt-guided
attention by incorporating continuous prompts into the self-attention layer to
re-modulate the attention and adapt pre-trained weights. Note that we only tune
those continuous prompts with the whole parameter of the pre-trained language
model fixed, thus, making our approach lightweight and flexible for
low-resource scenarios and can better transfer knowledge across domains.
Experimental results show that LightNER can obtain comparable performance in
the standard supervised setting and outperform strong baselines in low-resource
settings by tuning only a small part of the parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we introduce back-training, an alternative to self-training for
unsupervised domain adaptation (UDA) from source to target domain. While
self-training generates synthetic training data where natural inputs are
aligned with noisy outputs, back-training results in natural outputs aligned
with noisy inputs. This significantly reduces the gap between the target domain
and synthetic data distribution, and reduces model overfitting to the source
domain. We run UDA experiments on question generation and passage retrieval
from the \textit{Natural Questions} domain to machine learning and biomedical
domains. We find that back-training vastly outperforms self-training by a mean
improvement of 7.8 BLEU-4 points on generation, and 17.6\% top-20 retrieval
accuracy across both domains. We further propose consistency filters to remove
low-quality synthetic data before training. We also release a new
domain-adaptation dataset- \textit{MLQuestions} containing 35K unaligned
questions, 50K unaligned passages, and 3K aligned question-passage pairs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, prompt-tuning has achieved promising results for certain few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces
(i.e., templates) into the input and transform a classification task into a
masked language modeling problem. However, for relation extraction, determining
an appropriate prompt template requires domain expertise, and it is cumbersome
and time-consuming to obtain a suitable label word. Furthermore, there exist
abundant semantic knowledge among the entities and relations that cannot be
ignored. To this end, we focus on incorporating knowledge into prompt-tuning
for relation extraction and propose a knowledge-aware prompt-tuning approach
with synergistic optimization (KnowPrompt). Specifically, we inject entity and
relation knowledge into prompt construction with learnable virtual template
words as well as answer words and synergistically optimize their representation
with knowledge constraints. Extensive experimental results on five datasets
with standard and low-resource settings demonstrate the effectiveness of our
approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Latent-State GPT for Semi-supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, two approaches, fine-tuning large pre-trained language models and
variational training, have attracted significant interests, separately, for
semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,
we propose Variational Latent-State GPT model (VLS-GPT), which is the first to
combine the strengths of the two approaches. Among many options of models, we
propose the generative model and the inference model for variational learning
of the end-to-end TOD system, both as auto-regressive language models based on
GPT-2, which can be further trained over a mix of labeled and unlabeled dialog
data in a semi-supervised manner. We develop the strategy of
sampling-then-forward-computation, which successfully overcomes the memory
explosion issue of using GPT in variational learning and speeds up training.
Semi-supervised TOD experiments are conducted on two benchmark multi-domain
datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to
significantly outperform both supervised-only and semi-supervised baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04404">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Similarity measures are a vital tool for understanding how language models
represent and process language. Standard representational similarity measures
such as cosine similarity and Euclidean distance have been successfully used in
static word embedding models to understand how words cluster in semantic space.
Recently, these measures have been applied to embeddings from contextualized
models such as BERT and GPT-2. In this work, we call into question the
informativity of such measures for contextualized language models. We find that
a small number of rogue dimensions, often just 1-3, dominate these measures.
Moreover, we find a striking mismatch between the dimensions that dominate
similarity measures and those which are important to the behavior of the model.
We show that simple postprocessing techniques such as standardization are able
to correct for rogue dimensions and reveal underlying representational quality.
We argue that accounting for rogue dimensions is essential for any
similarity-based analysis of contextual language models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents a sparse-attention Transformer architecture for modeling
documents that contain large tables. Tables are ubiquitous on the web, and are
rich in information. However, more than 20% of relational tables on the web
have 20 or more rows (Cafarella et al., 2008), and these large tables present a
challenge for current Transformer models, which are typically limited to 512
tokens. Here we propose MATE, a novel Transformer architecture designed to
model the structure of web tables. MATE uses sparse attention in a way that
allows heads to efficiently attend to either rows or columns in a table. This
architecture scales linearly with respect to speed and memory, and can handle
documents containing more than 8000 tokens with current accelerators. MATE also
has a more appropriate inductive bias for tabular data, and sets a new
state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,
2020b), a dataset that involves large documents containing tables, we improve
the best prior result by 19 points.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models. (arXiv:2109.04413v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite their success in a variety of NLP tasks, pre-trained language models,
due to their heavy reliance on compositionality, fail in effectively capturing
the meanings of multiword expressions (MWEs), especially idioms. Therefore,
datasets and methods to improve the representation of MWEs are urgently needed.
Existing datasets are limited to providing the degree of idiomaticity of
expressions along with the literal and, where applicable, (a single)
non-literal interpretation of MWEs. This work presents a novel dataset of
naturally occurring sentences containing MWEs manually classified into a
fine-grained set of meanings, spanning both English and Portuguese. We use this
dataset in two tasks designed to test i) a language model&#x27;s ability to detect
idiom usage, and ii) the effectiveness of a language model in generating
representations of sentences containing idioms. Our experiments demonstrate
that, on the task of detecting idiomatic usage, these models perform reasonably
well in the one-shot and few-shot scenarios, but that there is significant
scope for improvement in the zero-shot scenario. On the task of representing
idiomaticity, we find that pre-training is not always effective, while
fine-tuning could provide a sample efficient method of learning representations
of sentences containing MWEs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution. (arXiv:2009.09363v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09363">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite significant recent progress in coreference resolution, the quality of
current state-of-the-art systems still considerably trails behind human-level
performance. Using the CoNLL-2012 and PreCo datasets, we dissect the best
instantiation of the mainstream end-to-end coreference resolution model that
underlies most current best-performing coreference systems, and empirically
analyze the behavior of its two components: mention detector and mention
linker. While the detector traditionally focuses heavily on recall as a design
decision, we demonstrate the importance of precision, calling for their
balance. However, we point out the difficulty in building a precise detector
due to its inability to make important anaphoricity decisions. We also
highlight the enormous room for improving the linker and show that the rest of
its errors mainly involve pronoun resolution. We propose promising next steps
and hope our findings will help future research in coreference resolution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Compositional Generalization via Semantic Tagging. (arXiv:2010.11818v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Although neural sequence-to-sequence models have been successfully applied to
semantic parsing, they fail at compositional generalization, i.e., they are
unable to systematically generalize to unseen compositions of seen components.
Motivated by traditional semantic parsing where compositionality is explicitly
accounted for by symbolic grammars, we propose a new decoding framework that
preserves the expressivity and generality of sequence-to-sequence models while
featuring lexicon-style alignments and disentangled information processing.
Specifically, we decompose decoding into two phases where an input utterance is
first tagged with semantic symbols representing the meaning of individual
words, and then a sequence-to-sequence model is used to predict the final
meaning representation conditioning on the utterance and the predicted tag
sequence. Experimental results on three semantic parsing datasets show that the
proposed approach consistently improves compositional generalization across
model architectures, domains, and semantic formalisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning. (arXiv:2109.04144v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04144">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent prompt-based approaches allow pretrained language models to achieve
strong performances on few-shot finetuning by reformulating downstream tasks as
a language modeling problem. In this work, we demonstrate that, despite its
advantages on low data regimes, finetuned prompt-based models for sentence pair
classification tasks still suffer from a common pitfall of adopting inference
heuristics based on lexical overlap, e.g., models incorrectly assuming a
sentence pair is of the same meaning because they consist of the same set of
words. Interestingly, we find that this particular inference heuristic is
significantly less present in the zero-shot evaluation of the prompt-based
model, indicating how finetuning can be destructive to useful knowledge learned
during the pretraining. We then show that adding a regularization that
preserves pretraining weights is effective in mitigating this destructive
tendency of few-shot finetuning. Our evaluation on three datasets demonstrates
promising improvements on the three corresponding challenge datasets used to
diagnose the inference heuristics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04152">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text classification tasks have improved substantially during the last years
by the usage of transformers. However, the majority of researches focus on
prose texts, with poetry receiving less attention, specially for Spanish
language. In this paper, we propose a semi-supervised learning approach for
inferring 21 psychological categories evoked by a corpus of 4572 sonnets, along
with 10 affective and lexico-semantic multiclass ones. The subset of poems used
for training an evaluation includes 270 sonnets. With our approach, we achieve
an AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65
for 60% on the multiclass ones. The sonnets are modelled using transformers,
through sentence embeddings, along with lexico-semantic and affective features,
obtained by using external lexicons. Consequently, we see that this approach
provides an AUC increase of up to 0.12, as opposed to using transformers alone.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Word-Level Coreference Resolution. (arXiv:2109.04127v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04127">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent coreference resolution models rely heavily on span representations to
find coreference links between word spans. As the number of spans is $O(n^2)$
in the length of text and the number of potential links is $O(n^4)$, various
pruning techniques are necessary to make this approach computationally
feasible. We propose instead to consider coreference links between individual
words rather than word spans and then reconstruct the word spans. This reduces
the complexity of the coreference model to $O(n^2)$ and allows it to consider
all potential mentions without pruning any of them out. We also demonstrate
that, with these changes, SpanBERT for coreference resolution will be
significantly outperformed by RoBERTa. While being highly efficient, our model
performs competitively with recent coreference resolution systems on the
OntoNotes benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04400">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In cross-lingual text classification, it is required that task-specific
training data in high-resource source languages are available, where the task
is identical to that of a low-resource target language. However, collecting
such training data can be infeasible because of the labeling cost, task
characteristics, and privacy concerns. This paper proposes an alternative
solution that uses only task-independent word embeddings of high-resource
languages and bilingual dictionaries. First, we construct a dictionary-based
heterogeneous graph (DHG) from bilingual dictionaries. This opens the
possibility to use graph neural networks for cross-lingual transfer. The
remaining challenge is the heterogeneity of DHG because multiple languages are
considered. To address this challenge, we propose dictionary-based
heterogeneous graph neural network (DHGNet) that effectively handles the
heterogeneity of DHG by two-step aggregations, which are word-level and
language-level aggregations. Experimental results demonstrate that our method
outperforms pretrained models even though it does not access to large corpora.
Furthermore, it can perform well even though dictionaries contain many
incorrect translations. Its robustness allows the usage of a wider range of
dictionaries such as an automatically constructed dictionary and crowdsourced
dictionary, which are convenient for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tracking Turbulence Through Financial News During COVID-19. (arXiv:2109.04369v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04369">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Grave human toll notwithstanding, the COVID-19 pandemic created uniquely
unstable conditions in financial markets. In this work we uncover and discuss
relationships involving sentiment in financial publications during the 2020
pandemic-motivated U.S. financial crash. First, we introduce a set of expert
annotations of financial sentiment for articles from major American financial
news publishers. After an exploratory data analysis, we then describe a
CNN-based architecture to address the task of predicting financial sentiment in
this anomalous, tumultuous setting. Our best performing model achieves a
maximum weighted F1 score of 0.746, establishing a strong performance
benchmark. Using predictions from our top performing model, we close by
conducting a statistical correlation study with real stock market data, finding
interesting and strong relationships between financial news and the S\&amp;P 500
index, trading volume, market volatility, and different single-factor ETFs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance. (arXiv:2109.04349v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04349">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The ability to identify and resolve uncertainty is crucial for the robustness
of a dialogue system. Indeed, this has been confirmed empirically on systems
that utilise Bayesian approaches to dialogue belief tracking. However, such
systems consider only confidence estimates and have difficulty scaling to more
complex settings. Neural dialogue systems, on the other hand, rarely take
uncertainties into account. They are therefore overconfident in their decisions
and less robust. Moreover, the performance of the tracking task is often
evaluated in isolation, without consideration of its effect on the downstream
policy optimisation. We propose the use of different uncertainty measures in
neural belief tracking. The effects of these measures on the downstream task of
policy optimisation are evaluated by adding selected measures of uncertainty to
the feature space of the policy and training policies through interaction with
a user simulator. Both human and simulated user results show that incorporating
these measures leads to improvements both of the performance and of the
robustness of the downstream dialogue policy. This highlights the importance of
developing neural dialogue belief trackers that take uncertainty into account.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04422">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),
requires an alignment of semantic concepts across domains. Despite the
widespread success of end-to-end learning, today&#x27;s multimodal pipelines by and
large leverage pre-extracted, fixed features from object detectors, typically
Faster R-CNN, as representations of the visual world. The obvious downside is
that the visual representation is not specifically tuned to the multimodal task
at hand. At the same time, while transformer-based object detectors have gained
popularity, they have not been employed in today&#x27;s multimodal pipelines. We
address both shortcomings with TxT, a transformer-based crossmodal pipeline
that enables fine-tuning both language and visual components on the downstream
task in a fully end-to-end manner. We overcome existing limitations of
transformer-based detectors for multimodal reasoning regarding the integration
of global context and their scalability. Our transformer-based multimodal model
achieves considerable gains from end-to-end learning for multimodal question
answering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection. (arXiv:2109.04292v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04292">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper considers the unsupervised domain adaptation problem for neural
machine translation (NMT), where we assume the access to only monolingual text
in either the source or target language in the new domain. We propose a
cross-lingual data selection method to extract in-domain sentences in the
missing language side from a large generic monolingual corpus. Our proposed
method trains an adaptive layer on top of multilingual BERT by contrastive
learning to align the representation between the source and target language.
This then enables the transferability of the domain classifier between the
languages in a zero-shot manner. Once the in-domain data is detected by the
classifier, the NMT model is then adapted to the new domain by jointly learning
translation and domain discrimination tasks. We evaluate our cross-lingual data
selection method on NMT across five diverse domains in three language pairs, as
well as a real-world scenario of translation for COVID-19. The results show
that our proposed method outperforms other selection baselines up to +1.5 BLEU
score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12463">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A new metric \texttt{BaryScore} to evaluate text generation based on deep
contextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric
is motivated by a new framework relying on optimal transport tools, i.e.,
Wasserstein distance and barycenter. By modelling the layer output of deep
contextualized embeddings as a probability distribution rather than by a vector
embedding; this framework provides a natural way to aggregate the different
outputs through the Wasserstein space topology. In addition, it provides
theoretical grounds to our metric and offers an alternative to available
solutions e.g., MoverScore and BertScore). Numerical evaluation is performed on
four different tasks: machine translation, summarization, data2text generation
and image captioning. Our results show that \texttt{BaryScore} outperforms
other BERT based metrics and exhibits more consistent behaviour in particular
for text summarization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Topic Confusion Task: A Novel Scenario for Authorship Attribution. (arXiv:2104.08530v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08530">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Authorship attribution is the problem of identifying the most plausible
author of an anonymous text from a set of candidate authors. Researchers have
investigated same-topic and cross-topic scenarios of authorship attribution,
which differ according to whether new, unseen topics are used in the testing
phase. However, neither scenario allows us to explain whether errors are caused
by a failure to capture authorship writing style or by a topic shift. Motivated
by this, we propose the \emph{topic confusion} task where we switch the
author-topic configuration between the training and testing sets. This setup
allows us to distinguish two types of errors: those caused by the topic shift
and those caused by the features&#x27; inability to capture the writing styles. We
show that stylometric features with part-of-speech tags are the least
susceptible to topic variations. We further show that combining them with other
features leads to significantly lower topic confusion and higher attribution
accuracy. Finally, we show that pretrained language models such as BERT and
RoBERTa perform poorly on this task and are surpassed by simple features such
as word-level $n$-grams.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07705">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While large language models a la BERT are used ubiquitously in NLP,
pretraining them is considered a luxury that only a few well-funded industry
labs can afford. How can one train such models with a more modest budget? We
present a recipe for pretraining a masked language model in 24 hours using a
single low-end deep learning server. We demonstrate that through a combination
of software optimizations, design choices, and hyperparameter tuning, it is
possible to produce models that are competitive with BERT-base on GLUE tasks at
a fraction of the original pretraining cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v3 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we introduce a novel semi-supervised learning framework for
end-to-end speech separation. The proposed method first uses mixtures of
unseparated sources and the mixture invariant training (MixIT) criterion to
train a teacher model. The teacher model then estimates separated sources that
are used to train a student model with standard permutation invariant training
(PIT). The student model can be fine-tuned with supervised data, i.e., paired
artificial mixtures and clean speech sources, and further improved via model
distillation. Experiments with single and multi channel mixtures show that the
teacher-student training resolves the over-separation problem observed in the
original MixIT method. Further, the semisupervised performance is comparable to
a fully-supervised separation system trained using ten times the amount of
supervised data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. (arXiv:2104.08202v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural knowledge-grounded generative models for dialogue often produce
content that is factually inconsistent with the knowledge they rely on, making
them unreliable and limiting their applicability. Inspired by recent work on
evaluating factual consistency in abstractive summarization, we propose an
automatic evaluation metric for factual consistency in knowledge-grounded
dialogue using automatic question generation and question answering. Our
metric, denoted $Q^2$, compares answer spans using natural language inference
(NLI), instead of token-based matching as done in previous work. To foster
proper evaluation, we curate a novel dataset of dialogue system outputs for the
Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We
perform a thorough meta-evaluation of $Q^2$ against other metrics using this
dataset and two others, where it consistently shows higher correlation with
human judgements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01198">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we present to the NLP community, and to the wider research
community as a whole, an application for the diachronic analysis of research
corpora. We open source an easy-to-use tool coined: DRIFT, which allows
researchers to track research trends and development over the years. The
analysis methods are collated from well-cited research works, with a few of our
own methods added for good measure. Succinctly put, some of the analysis
methods are: keyword extraction, word clouds, predicting
declining/stagnant/growing trends using Productivity, tracking bi-grams using
Acceleration plots, finding the Semantic Drift of words, tracking trends using
similarity, etc. To demonstrate the utility and efficacy of our tool, we
perform a case study on the cs.CL corpus of the arXiv repository and draw
inferences from the analysis methods. The toolkit and the associated code are
available here: https://github.com/rajaswa/DRIFT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12465">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Spoken dialog systems need to be able to handle both multiple languages and
multilinguality inside a conversation (\textit{e.g} in case of code-switching).
In this work, we introduce new pretraining losses tailored to learn
multilingual spoken dialog representations. The goal of these losses is to
expose the model to code-switched language. To scale up training, we
automatically build a pretraining corpus composed of multilingual conversations
in five different languages (French, Italian, English, German and Spanish) from
\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We
test the generic representations on \texttt{MIAM}, a new benchmark composed of
five dialog act corpora on the same aforementioned languages as well as on two
novel multilingual downstream tasks (\textit{i.e} multilingual mask utterance
retrieval and multilingual inconsistency identification). Our experiments show
that our new code switched-inspired losses achieve a better performance in both
monolingual and multilingual settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent
years. However, previous work has indicated that off-the-shelf MLMs are not
effective as universal lexical or sentence encoders without further
task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks
using annotated task data. In this work, we demonstrate that it is possible to
turn MLMs into effective universal lexical and sentence encoders even without
any additional data and without any supervision. We propose an extremely
simple, fast and effective contrastive learning technique, termed Mirror-BERT,
which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30
seconds without any additional external knowledge. Mirror-BERT relies on fully
identical or slightly modified string pairs as positive (i.e., synonymous)
fine-tuning examples, and aims to maximise their similarity during identity
fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in
both lexical-level and sentence-level tasks, across different domains and
different languages. Notably, in the standard sentence semantic similarity
(STS) tasks, our self-supervised Mirror-BERT model even matches the performance
of the task-tuned Sentence-BERT models from prior work. Finally, we delve
deeper into the inner workings of MLMs, and suggest some evidence on why this
simple approach can yield effective universal lexical and sentence encoders.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00922">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal sentiment analysis is a trending area of research, and the
multimodal fusion is one of its most active topic. Acknowledging humans
communicate through a variety of channels (i.e visual, acoustic, linguistic),
multimodal systems aim at integrating different unimodal representations into a
synthetic one. So far, a consequent effort has been made on developing complex
architectures allowing the fusion of these modalities. However, such systems
are mainly trained by minimising simple losses such as $L_1$ or cross-entropy.
In this work, we investigate unexplored penalties and propose a set of new
objectives that measure the dependency between modalities. We demonstrate that
our new penalties lead to a consistent improvement (up to $4.3$ on accuracy)
across a large variety of state-of-the-art models on two well-known sentiment
analysis datasets: \texttt{CMU-MOSI} and \texttt{CMU-MOSEI}. Our method not
only achieves a new SOTA on both datasets but also produces representations
that are more robust to modality drops. Finally, a by-product of our methods
includes a statistical network which can be used to interpret the high
dimensional representations learnt by the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many implicit inferences exist in text depending on how it is structured that
can critically impact the text&#x27;s interpretation and meaning. One such
structural aspect present in text with chronology is the order of its
presentation. For narratives or stories, this is known as the narrative order.
Reordering a narrative can impact the temporal, causal, event-based, and other
inferences readers draw from it, which in turn can have strong effects both on
its interpretation and interestingness. In this paper, we propose and
investigate the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that although our models can perform decently, NAREOR is a challenging task
with potential for further exploration. We also investigate two applications of
NAREOR: generation of more interesting variations of stories and serving as
adversarial sets for temporal/event-related tasks, besides discussing other
prospective ones, such as for pedagogical setups related to language skills
like essay writing and applications to medicine involving clinical narratives.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12870">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Extractive text summarization aims at extracting the most representative
sentences from a given document as its summary. To extract a good summary from
a long text document, sentence embedding plays an important role. Recent
studies have leveraged graph neural networks to capture the inter-sentential
relationship (e.g., the discourse graph) to learn contextual sentence
embedding. However, those approaches neither consider multiple types of
inter-sentential relationships (e.g., semantic similarity &amp; natural
connection), nor model intra-sentential relationships (e.g, semantic &amp;
syntactic relationship among words). To address these problems, we propose a
novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model
different types of relationships among sentences and words. Based on Multi-GCN,
we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive
text summarization. Finally, we evaluate the proposed models on the
CNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation. (arXiv:2109.04096v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural conversation models have shown great potentials towards generating
fluent and informative responses by introducing external background knowledge.
Nevertheless, it is laborious to construct such knowledge-grounded dialogues,
and existing models usually perform poorly when transfer to new domains with
limited training samples. Therefore, building a knowledge-grounded dialogue
system under the low-resource setting is a still crucial issue. In this paper,
we propose a novel three-stage learning framework based on weakly supervised
learning which benefits from large scale ungrounded dialogues and unstructured
knowledge base. To better cooperate with this framework, we devise a variant of
Transformer with decoupled decoder which facilitates the disentangled learning
of response generation and knowledge incorporation. Evaluation results on two
benchmarks indicate that our approach can outperform other state-of-the-art
methods with less training data, and even in zero-resource scenario, our
approach still performs well.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04240">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Albeit the universal representational power of pre-trained language models,
adapting them onto a specific NLP task still requires a considerably large
amount of labeled data. Effective task fine-tuning meets challenges when only a
few labeled examples are present for the task. In this paper, we aim to the
address of the problem of few shot task learning by exploiting and transferring
from a different task which admits a related but disparate label space.
Specifically, we devise a label transfer network (LTN) to transform the labels
from source task to the target task of interest for training. Both the LTN and
the model for task prediction are learned via a bi-level optimization
framework, which we term as MetaXT. MetaXT offers a principled solution to best
adapt a pre-trained language model to the target task by transferring knowledge
from the source task. Empirical evaluations on cross-task transfer settings for
four NLP tasks, from two different types of label space disparities,
demonstrate the effectiveness of MetaXT, especially when the labeled data in
the target task is limited.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction. (arXiv:2109.04108v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04108">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural relation extraction models have shown promising results in recent
years; however, the model performance drops dramatically given only a few
training samples. Recent works try leveraging the advance in few-shot learning
to solve the low resource problem, where they train label-agnostic models to
directly compare the semantic similarities among context sentences in the
embedding space. However, the label-aware information, i.e., the relation label
that contains the semantic knowledge of the relation itself, is often neglected
for prediction. In this work, we propose a framework considering both
label-agnostic and label-aware semantic mapping information for low resource
relation extraction. We show that incorporating the above two types of mapping
information in both pretraining and fine-tuning can significantly improve the
model performance on low-resource relation extraction tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04443">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Back-translation (BT) of target monolingual corpora is a widely used data
augmentation strategy for neural machine translation (NMT), especially for
low-resource language pairs. To improve effectiveness of the available BT data,
we introduce HintedBT -- a family of techniques which provides hints (through
tags) to the encoder and decoder. First, we propose a novel method of using
both high and low quality BT data by providing hints (as source tags on the
encoder) to the model about the quality of each source-target pair. We don&#x27;t
filter out low quality data but instead show that these hints enable the model
to learn effectively from noisy data. Second, we address the problem of
predicting whether a source token needs to be translated or transliterated to
the target language, which is common in cross-script translation tasks (i.e.,
where source and target do not share the written script). For such cases, we
propose training the model with additional hints (as target tags on the
decoder) that provide information about the operation required on the source
(translation or both translation and transliteration). We conduct experiments
and detailed analyses on standard WMT benchmarks for three cross-script
low/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our
methods compare favorably with five strong and well established baselines. We
show that using these hints, both separately and together, significantly
improves translation quality and leads to state-of-the-art performance in all
three language pairs in corresponding bilingual settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04101">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal knowledge graph (TKG) reasoning is a crucial task that has gained
increasing research interest in recent years. Most existing methods focus on
reasoning at past timestamps to complete the missing facts, and there are only
a few works of reasoning on known TKGs to forecast future facts. Compared with
the completion task, the forecasting task is more difficult that faces two main
challenges: (1) how to effectively model the time information to handle future
timestamps? (2) how to make inductive inference to handle previously unseen
entities that emerge over time? To address these challenges, we propose the
first reinforcement learning method for forecasting. Specifically, the agent
travels on historical knowledge graph snapshots to search for the answer. Our
method defines a relative time encoding function to capture the timespan
information, and we design a novel time-shaped reward based on Dirichlet
distribution to guide the model learning. Furthermore, we propose a novel
representation method for unseen entities to improve the inductive inference
ability of the model. We evaluate our method for this link prediction task at
future timestamps. Extensive experiments on four benchmark datasets demonstrate
substantial performance improvement meanwhile with higher explainability, less
calculation, and fewer parameters when compared with existing state-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Debiasing Methods in Natural Language Understanding Make Bias More Accessible. (arXiv:2109.04095v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04095">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Model robustness to bias is often determined by the generalization on
carefully designed out-of-distribution datasets. Recent debiasing methods in
natural language understanding (NLU) improve performance on such datasets by
pressuring models into making unbiased predictions. An underlying assumption
behind such methods is that this also leads to the discovery of more robust
features in the model&#x27;s inner representations. We propose a general
probing-based framework that allows for post-hoc interpretation of biases in
language models, and use an information-theoretic approach to measure the
extractability of certain biases from the model&#x27;s representations. We
experiment with several NLU datasets and known biases, and show that,
counter-intuitively, the more a language model is pushed towards a debiased
regime, the more bias is actually encoded in its inner representations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2109.04321v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04321">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive learning has been gradually applied to learn high-quality
unsupervised sentence embedding. Among the previous un-supervised methods, the
latest state-of-the-art method, as far as we know, is unsupervised SimCSE
(unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training
stage by pulling semantically similar sentences together and pushing apart
dis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE
to get more adequate comparisons among samples and avoid overfitting. However,
increasing the batch size does not always lead to improvements, but instead
even lead to performance degradation when the batch size exceeds a threshold.
Through statistical observation, we find that this is probably due to the
introduction of low-confidence negative pairs after in-creasing the batch size.
To alleviate this problem, we introduce a simple smoothing strategy upon the
InfoNCE loss function, termedGaussian Smoothing InfoNCE
(GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative
samples, which act asa smoothing of the negative sample space.Though being
simple, the proposed smooth-ing strategy brings substantial improvements to
unsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity
(STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an
average Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of
BERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04448">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained vision-and-language BERTs aim to learn representations that
combine information from both modalities. We propose a diagnostic method based
on cross-modal input ablation to assess the extent to which these models
actually integrate cross-modal information. This method involves ablating
inputs from one modality, either entirely or selectively based on cross-modal
grounding alignments, and evaluating the model prediction performance on the
other modality. Model performance is measured by modality-specific tasks that
mirror the model pretraining objectives (e.g. masked language modelling for
text). Models that have learned to construct cross-modal representations using
both modalities are expected to perform worse when inputs are missing from a
modality. We find that recently proposed models have much greater relative
difficulty predicting text when visual information is ablated, compared to
predicting visual object categories when text is ablated, indicating that these
models are not symmetrically cross-modal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Biomedical Question Answering: A Survey of Approaches and Challenges. (arXiv:2102.05281v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatic Question Answering (QA) has been successfully applied in various
domains such as search engines and chatbots. Biomedical QA (BQA), as an
emerging QA task, enables innovative applications to effectively perceive,
access and understand complex biomedical knowledge. There have been tremendous
developments of BQA in the past two decades, which we classify into 5
distinctive approaches: classic, information retrieval, machine reading
comprehension, knowledge base and question entailment approaches. In this
survey, we introduce available datasets and representative methods of each BQA
approach in detail. Despite the developments, BQA systems are still immature
and rarely used in real-life settings. We identify and characterize several key
challenges in BQA that might lead to this issue, and discuss some potential
future directions to explore.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04332">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Prompts for pre-trained language models (PLMs) have shown remarkable
performance by bridging the gap between pre-training tasks and various
downstream tasks. Among these methods, prompt tuning, which freezes PLMs and
only tunes soft prompts, provides an efficient and effective solution for
adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to
be fully explored. In our pilot experiments, we find that prompt tuning
performs comparably with conventional full-model fine-tuning when downstream
data are sufficient, whereas it performs much worse under few-shot learning
settings, which may hinder the application of prompt tuning in practice. We
attribute this low performance to the manner of initializing soft prompts.
Therefore, in this work, we propose to pre-train prompts by adding soft prompts
into the pre-training stage to obtain a better initialization. We name this
Pre-trained Prompt Tuning framework &quot;PPT&quot;. To ensure the generalization of PPT,
we formulate similar classification tasks into a unified task form and
pre-train soft prompts for this unified task. Extensive experiments show that
tuning pre-trained prompts for downstream tasks can reach or even outperform
full-model fine-tuning under both full-data and few-shot settings. Our approach
is effective and efficient for using large-scale PLMs in practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04367">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, the textual adversarial attack models become increasingly popular
due to their successful in estimating the robustness of NLP models. However,
existing works have obvious deficiencies. (1) They usually consider only a
single granularity of modification strategies (e.g. word-level or
sentence-level), which is insufficient to explore the holistic textual space
for generation; (2) They need to query victim models hundreds of times to make
a successful attack, which is highly inefficient in practice. To address such
problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to
effectively generate high-quality adversarial samples with fewer queries to
victim models. Furthermore, we propose a reinforcement-learning based method to
train a multi-granularity attack agent through behavior cloning with the expert
knowledge from our MAYA algorithm to further reduce the query times.
Additionally, we also adapt the agent to attack black-box models that only
output labels without confidence scores. We conduct comprehensive experiments
to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two
different black-box attack settings and three benchmark datasets. Experimental
results show that our models achieve overall better attacking performance and
produce more fluent and grammatical adversarial samples compared to baseline
models. Besides, our adversarial attack agent significantly reduces the query
times in both attack settings. Our codes are released at
https://github.com/Yangyi-Chen/MAYA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations. (arXiv:2010.06196v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06196">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There is an increasing interest in the use of mathematical word problem (MWP)
generation in educational assessment. Different from standard natural question
generation, MWP generation needs to maintain the underlying mathematical
operations between quantities and variables, while at the same time ensuring
the relevance between the output and the given topic. To address above problem,
we develop an end-to-end neural model to generate diverse MWPs in real-world
scenarios from commonsense knowledge graph and equations. The proposed model
(1) learns both representations from edge-enhanced Levi graphs of symbolic
equations and commonsense knowledge; (2) automatically fuses equation and
commonsense knowledge information via a self-planning module when generating
the MWPs. Experiments on an educational gold-standard set and a large-scale
generated MWP set show that our approach is superior on the MWP generation
task, and it outperforms the SOTA models in terms of both automatic evaluation
metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e.,
equation relevance, topic relevance, and language coherence. To encourage
reproducible results, we make our code and MWP dataset public available at
\url{https://github.com/tal-ai/MaKE_EMNLP2021}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03437">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transfer learning has fundamentally changed the landscape of natural language
processing (NLP) research. Many existing state-of-the-art models are first
pre-trained on a large text corpus and then fine-tuned on downstream tasks.
However, due to limited data resources from downstream tasks and the extremely
large capacity of pre-trained models, aggressive fine-tuning often causes the
adapted model to overfit the data of downstream tasks and forget the knowledge
of the pre-trained model. To address the above issue in a more principled
manner, we propose a new computational framework for robust and efficient
fine-tuning for pre-trained language models. Specifically, our proposed
framework contains two important ingredients: 1. Smoothness-inducing
regularization, which effectively manages the capacity of the model; 2. Bregman
proximal point optimization, which is a class of trust-region methods and can
prevent knowledge forgetting. Our experiments demonstrate that our proposed
method achieves the state-of-the-art performance on multiple NLP benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization. (arXiv:2109.04098v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04098">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Abstractive text summarization is one of the areas influenced by the
emergence of pre-trained language models. Current pre-training works in
abstractive summarization give more points to the summaries with more words in
common with the main text and pay less attention to the semantic similarity
between generated sentences and the original document. We propose ARMAN, a
Transformer-based encoder-decoder model pre-trained with three novel objectives
to address this issue. In ARMAN, salient sentences from a document are selected
according to a modified semantic score to be masked and form a pseudo summary.
To summarize more accurately and similar to human writing patterns, we applied
modified sentence reordering. We evaluated our proposed models on six
downstream Persian summarization tasks. Experimental results show that our
proposed model achieves state-of-the-art performance on all six summarization
tasks measured by ROUGE and BERTScore. Our models also outperform prior works
in textual entailment, question paraphrasing, and multiple choice question
answering. Finally, we established a human evaluation and show that using the
semantic score significantly improves summarization results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08164">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The factual knowledge acquired during pre-training and stored in the
parameters of Language Models (LMs) can be useful in downstream tasks (e.g.,
question answering or textual inference). However, some facts can be
incorrectly induced or become obsolete over time. We present KnowledgeEditor, a
method which can be used to edit this knowledge and, thus, fix &#x27;bugs&#x27; or
unexpected predictions without the need for expensive re-training or
fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not
require any modifications in LM pre-training (e.g., the use of meta-learning).
In our approach, we train a hyper-network with constrained optimization to
modify a fact without affecting the rest of the knowledge; the trained
hyper-network is then used to predict the weight update at test time. We show
KnowledgeEditor&#x27;s efficacy with two popular architectures and
knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and
ii) a sequence-to-sequence BART model for question answering. With our method,
changing a prediction on the specific wording of a query tends to result in a
consistent change in predictions also for its paraphrases. We show that this
can be further encouraged by exploiting (e.g., automatically-generated)
paraphrases during training. Interestingly, our hyper-network can be regarded
as a &#x27;probe&#x27; revealing which components need to be changed to manipulate
factual knowledge; our analysis shows that the updates tend to be concentrated
on a small subset of components. Source code available at
https://github.com/nicola-decao/KnowledgeEditor</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization. (arXiv:2103.12235v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12235">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Question Answering (QA) tasks requiring information from multiple documents
often rely on a retrieval model to identify relevant information for reasoning.
The retrieval model is typically trained to maximize the likelihood of the
labeled supporting evidence. However, when retrieving from large text corpora
such as Wikipedia, the correct answer can often be obtained from multiple
evidence candidates. Moreover, not all such candidates are labeled as positive
during annotation, rendering the training signal weak and noisy. This problem
is exacerbated when the questions are unanswerable or when the answers are
Boolean, since the model cannot rely on lexical overlap to make a connection
between the answer and supporting evidence. We develop a new parameterization
of set-valued retrieval that handles unanswerable queries, and we show that
marginalizing over this set during training allows a model to mitigate false
negatives in supporting evidence annotations. We test our method on two
multi-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint
modeling with marginalization improves model performance by 5.5 F1 points and
achieves a new state-of-the-art performance of 50.5 F1. We also show that
retrieval marginalization results in 4.1 QA F1 improvement over a
non-marginalized baseline on HotpotQA in the fullwiki setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension. (arXiv:2109.04066v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04066">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-party multi-turn dialogue comprehension brings unprecedented challenges
on handling the complicated scenarios from multiple speakers and criss-crossed
discourse relationship among speaker-aware utterances. Most existing methods
deal with dialogue contexts as plain texts and pay insufficient attention to
the crucial speaker-aware clues. In this work, we propose an enhanced
speaker-aware model with masking attention and heterogeneous graph networks to
comprehensively capture discourse clues from both sides of speaker property and
speaker-aware relationships. With such comprehensive speaker-aware modeling,
experimental results show that our speaker-aware model helps achieves
state-of-the-art performance on the benchmark dataset Molweni. Case analysis
shows that our model enhances the connections between utterances and their own
speakers and captures the speaker-aware discourse relations, which are critical
for dialogue modeling.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding. (arXiv:2109.04380v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04380">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contrastive learning has been attracting much attention for learning
unsupervised sentence embeddings. The current state-of-the-art unsupervised
method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as
a minimal data augmentation method, and passes the same input sentence to a
pre-trained Transformer encoder (with dropout turned on) twice to obtain the
two corresponding embeddings to build a positive pair. As the length
information of a sentence will generally be encoded into the sentence
embeddings due to the usage of position embedding in Transformer, each positive
pair in unsup-SimCSE actually contains the same length information. And thus
unsup-SimCSE trained with these positive pairs is probably biased, which would
tend to consider that sentences of the same or similar length are more similar
in semantics. Through statistical observations, we find that unsup-SimCSE does
have such a problem. To alleviate it, we apply a simple repetition operation to
modify the input sentence, and then pass the input sentence and its modified
counterpart to the pre-trained Transformer encoder, respectively, to get the
positive pair. Additionally, we draw inspiration from the community of computer
vision and introduce a momentum contrast, enlarging the number of negative
pairs without additional calculations. The proposed two modifications are
applied on positive and negative pairs separately, and build a new sentence
embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the
proposed ESimCSE on several benchmark datasets w.r.t the semantic text
similarity (STS) task. Experimental results show that ESimCSE outperforms the
state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on
BERT-base.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04114">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring. (arXiv:2109.04411v1 [eess.AS])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04411">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This article describes an efficient end-to-end speech translation (E2E-ST)
framework based on non-autoregressive (NAR) models. End-to-end speech
translation models have several advantages over traditional cascade systems
such as inference latency reduction. However, conventional AR decoding methods
are not fast enough because each token is generated incrementally. NAR models,
however, can accelerate the decoding speed by generating multiple tokens in
parallel on the basis of the token-wise conditional independence assumption. We
propose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder
and an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary
shallow AR decoder selects the best hypothesis by rescoring multiple candidates
generated from the NAR decoder in parallel (parallel AR rescoring). We adopt
conditional masked language model (CMLM) and a connectionist temporal
classification (CTC)-based model as NAR decoders for Orthros, referred to as
Orthros-CMLM and Orthros-CTC, respectively. We also propose two training
methods to enhance the CMLM decoder. Experimental evaluations on three
benchmark datasets with six language directions demonstrated that Orthros
achieved large improvements in translation quality with a very small overhead
compared with the baseline NAR model. Moreover, the Conformer encoder
architecture enabled large quality improvements, especially for CTC-based
models. Orthros-CTC with the Conformer encoder increased decoding speed by
3.63x on CPU with translation quality comparable to that of an AR model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Opinion summarization has been traditionally approached with unsupervised,
weakly-supervised and few-shot learning techniques. In this work, we collect a
large dataset of summaries paired with user reviews for over 31,000 products,
enabling supervised training. However, the number of reviews per product is
large (320 on average), making summarization - and especially training a
summarizer - impractical. Moreover, the content of many reviews is not
reflected in the human-written summaries, and, thus, the summarizer trained on
random review subsets hallucinates. In order to deal with both of these
challenges, we formulate the task as jointly learning to select informative
subsets of reviews and summarizing the opinions expressed in these subsets. The
choice of the review subset is treated as a latent variable, predicted by a
small and simple selector. The subset is then fed into a more powerful
summarizer. For joint training, we use amortized variational inference and
policy gradient methods. Our experiments demonstrate the importance of
selecting informative reviews resulting in improved quality of summaries and
reduced hallucinations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification. (arXiv:2109.04385v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04385">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Research shows that natural language processing models are generally
considered to be vulnerable to adversarial attacks; but recent work has drawn
attention to the issue of validating these adversarial inputs against certain
criteria (e.g., the preservation of semantics and grammaticality). Enforcing
constraints to uphold such criteria may render attacks unsuccessful, raising
the question of whether valid attacks are actually feasible. In this work, we
investigate this through the lens of human language ability. We report on
crowdsourcing studies in which we task humans with iteratively modifying words
in an input text, while receiving immediate model feedback, with the aim of
causing a sentiment classification model to misclassify the example. Our
findings suggest that humans are capable of generating a substantial amount of
adversarial examples using semantics-preserving word substitutions. We analyze
how human-generated adversarial examples compare to the recently proposed
TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions
naturalness, preservation of sentiment, grammaticality and substitution rate.
Our findings suggest that human-generated adversarial examples are not more
able than the best algorithms to generate natural-reading, sentiment-preserving
examples, though they do so by being much more computationally efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Few-Shot Commonsense Knowledge Models. (arXiv:2101.00297v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00297">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Providing natural language processing systems with commonsense knowledge is a
critical challenge for achieving language understanding. Recently, commonsense
knowledge models have emerged as a suitable approach for hypothesizing
situation-relevant commonsense knowledge on-demand in natural language
applications. However, these systems are limited by the fixed set of relations
captured by schemas of the knowledge bases on which they&#x27;re trained.

To address this limitation, we investigate training commonsense knowledge
models in a few-shot setting with limited tuples per commonsense relation in
the graph. We perform five separate studies on different dimensions of few-shot
commonsense knowledge learning, providing a roadmap on best practices for
training these systems efficiently. Importantly, we find that human quality
ratings for knowledge produced from a few-shot trained system can achieve
performance within 6% of knowledge produced from fully supervised systems. This
few-shot performance enables coverage of a wide breadth of relations in future
commonsense systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Translate &amp; Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04319">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While multilingual pretrained language models (LMs) fine-tuned on a single
language have shown substantial cross-lingual task transfer capabilities, there
is still a wide performance gap in semantic parsing tasks when target language
supervision is available. In this paper, we propose a novel Translate-and-Fill
(TaF) method to produce silver training data for a multilingual semantic
parser. This method simplifies the popular Translate-Align-Project (TAP)
pipeline and consists of a sequence-to-sequence filler model that constructs a
full parse conditioned on an utterance and a view of the same parse. Our filler
is trained on English data only but can accurately complete instances in other
languages (i.e., translations of the English training utterances), in a
zero-shot fashion. Experimental results on three multilingual semantic parsing
datasets show that data augmentation with TaF reaches accuracies competitive
with similar systems which rely on traditional alignment techniques.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What&#x27;s Hidden in a One-layer Randomly Weighted Transformer?. (arXiv:2109.03939v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03939">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We demonstrate that, hidden within one-layer randomly weighted neural
networks, there exist subnetworks that can achieve impressive performance,
without ever modifying the weight initializations, on machine translation
tasks. To find subnetworks for one-layer randomly weighted neural networks, we
apply different binary masks to the same weight matrix to generate different
layers. Hidden within a one-layer randomly weighted Transformer, we find that
subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed
pre-trained embedding layer, the previously found subnetworks are smaller than,
but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained
Transformer small/base on IWSLT14/WMT14. Furthermore, we demonstrate the
effectiveness of larger and deeper transformers in this setting, as well as the
impact of different initialization methods. We released the source code at
https://github.com/sIncerass/one_layer_lottery_ticket.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Competence-based Curriculum Learning for Multilingual Machine Translation. (arXiv:2109.04002v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04002">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Currently, multilingual machine translation is receiving more and more
attention since it brings better performance for low resource languages (LRLs)
and saves more space. However, existing multilingual machine translation models
face a severe challenge: imbalance. As a result, the translation performance of
different languages in multilingual translation models are quite different. We
argue that this imbalance problem stems from the different learning
competencies of different languages. Therefore, we focus on balancing the
learning competencies of different languages and propose Competence-based
Curriculum Learning for Multilingual Machine Translation, named CCL-M.
Specifically, we firstly define two competencies to help schedule the high
resource languages (HRLs) and the low resource languages: 1) Self-evaluated
Competence, evaluating how well the language itself has been learned; and 2)
HRLs-evaluated Competence, evaluating whether an LRL is ready to be learned
according to HRLs&#x27; Self-evaluated Competence. Based on the above competencies,
we utilize the proposed CCL-M algorithm to gradually add new languages into the
training set in a curriculum learning manner. Furthermore, we propose a novel
competenceaware dynamic balancing sampling strategy for better selecting
training samples in multilingual training. Experimental results show that our
approach has achieved a steady and significant performance gain compared to the
previous state-of-the-art approach on the TED talks dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Nearest Neighbor Language Models. (arXiv:2109.04212v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04212">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Non-parametric neural language models (NLMs) learn predictive distributions
of text utilizing an external datastore, which allows them to learn through
explicitly memorizing the training datapoints. While effective, these models
often require retrieval from a large datastore at test time, significantly
increasing the inference overhead and thus limiting the deployment of
non-parametric NLMs in practical applications. In this paper, we take the
recently proposed $k$-nearest neighbors language model (Khandelwal et al.,
2019) as an example, exploring methods to improve its efficiency along various
dimensions. Experiments on the standard WikiText-103 benchmark and
domain-adaptation datasets show that our methods are able to achieve up to a 6x
speed-up in inference speed while retaining comparable performance. The
empirical analysis we present may provide guidelines for future research
seeking to develop or deploy more efficient non-parametric NLMs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Flexible Generation of Natural Language Deductions. (arXiv:2104.08825v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08825">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An interpretable system for open-domain reasoning needs to express its
reasoning process in a transparent form. Natural language is an attractive
representation for this purpose -- it is both highly expressive and easy for
humans to understand. However, manipulating natural language statements in
logically consistent ways is hard: models must cope with variation in how
meaning is expressed while remaining precise. In this paper, we describe
ParaPattern, a method for building models to generate deductive inferences from
diverse natural language inputs without direct human supervision. We train
BART-based models (Lewis et al., 2020) to generate the result of applying a
particular logical operation to one or more premise statements. Crucially, we
develop a largely automated pipeline for constructing suitable training
examples from Wikipedia. We evaluate our models using out-of-domain sentence
compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et
al., 2021) datasets as well as targeted perturbation sets. Our results show
that our models are substantially more accurate and flexible than baseline
systems. ParaPattern achieves 85% validity on examples of the &#x27;substitution&#x27;
operation from EntailmentBank without the use of any in-domain training data,
matching the performance of a model fine-tuned for EntailmentBank. The full
source code for our method is publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Position Information in Transformers: An Overview. (arXiv:2102.11090v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11090">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers are arguably the main workhorse in recent Natural Language
Processing research. By definition a Transformer is invariant with respect to
reordering of the input. However, language is inherently sequential and word
order is essential to the semantics and syntax of an utterance. In this
article, we provide an overview and theoretical comparison of existing methods
to incorporate position information into Transformer models. The objectives of
this survey are to (1) showcase that position information in Transformer is a
vibrant and extensive research area; (2) enable the reader to compare existing
methods by providing a unified notation and systematization of different
approaches along important model dimensions; (3) indicate what characteristics
of an application should be taken into account when selecting a position
encoding; (4) provide stimuli for future research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04080">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the rapid increase in the volume of dialogue data from daily life, there
is a growing demand for dialogue summarization. Unfortunately, training a large
summarization model is generally infeasible due to the inadequacy of dialogue
data with annotated summaries. Most existing works for low-resource dialogue
summarization directly pretrain models in other domains, e.g., the news domain,
but they generally neglect the huge difference between dialogues and
conventional articles. To bridge the gap between out-of-domain pretraining and
in-domain fine-tuning, in this work, we propose a multi-source pretraining
paradigm to better leverage the external summary data. Specifically, we exploit
large-scale in-domain non-summary data to separately pretrain the dialogue
encoder and the summary decoder. The combined encoder-decoder model is then
pretrained on the out-of-domain summary data using adversarial critics, aiming
to facilitate domain-agnostic summarization. The experimental results on two
public datasets show that with only limited training data, our approach
achieves competitive performance and generalizes well in different dialogue
scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04223">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Incorporating factual knowledge into pre-trained language models (PLM) such
as BERT is an emerging trend in recent NLP studies. However, most of the
existing methods combine the external knowledge integration module with a
modified pre-training loss and re-implement the pre-training process on the
large-scale corpus. Re-pretraining these models is usually resource-consuming,
and difficult to adapt to another domain with a different knowledge graph (KG).
Besides, those works either cannot embed knowledge context dynamically
according to textual context or struggle with the knowledge ambiguity issue. In
this paper, we propose a novel knowledge-aware language model framework based
on fine-tuning process, which equips PLM with a unified knowledge-enhanced text
graph that contains both text and multi-relational sub-graphs extracted from
KG. We design a hierarchical relational-graph-based message passing mechanism,
which can allow the representations of injected KG and text to mutually update
each other and can dynamically select ambiguous mentioned entities that share
the same text. Our empirical results show that our model can efficiently
incorporate world knowledge from KGs into existing language models such as
BERT, and achieve significant improvement on the machine reading comprehension
(MRC) task compared with other knowledge-enhanced models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. (arXiv:2103.00453v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00453">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>When trained on large, unfiltered crawls from the internet, language models
pick up and reproduce all kinds of undesirable biases that can be found in the
data: they often generate racist, sexist, violent or otherwise toxic language.
As large models require millions of training examples to achieve good
performance, it is difficult to completely prevent them from being exposed to
such content. In this paper, we first demonstrate a surprising finding:
pretrained language models recognize, to a considerable degree, their
undesirable biases and the toxicity of the content they produce. We refer to
this capability as self-diagnosis. Based on this finding, we then propose a
decoding algorithm that, given only a textual description of the undesired
behavior, reduces the probability of a language model producing problematic
text. We refer to this approach as self-debiasing. Self-debiasing does not rely
on manually curated word lists, nor does it require any training data or
changes to the model&#x27;s parameters. While we by no means eliminate the issue of
language models generating biased text, we believe our approach to be an
important step in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Linguistic Capacity of Real-Time Counter Automata. (arXiv:2004.06866v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06866">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Counter machines have achieved a newfound relevance to the field of natural
language processing (NLP): recent work suggests some strong-performing
recurrent neural networks utilize their memory as counters. Thus, one potential
way to understand the success of these networks is to revisit the theory of
counter computation. Therefore, we study the abilities of real-time counter
machines as formal grammars, focusing on formal properties that are relevant
for NLP models. We first show that several variants of the counter machine
converge to express the same class of formal languages. We also prove that
counter languages are closed under complement, union, intersection, and many
other common set operations. Next, we show that counter machines cannot
evaluate boolean expressions, even though they can weakly validate their
syntax. This has implications for the interpretability and evaluation of neural
network systems: successfully matching syntactic patterns does not guarantee
that counter memory accurately encodes compositional semantics. Finally, we
consider whether counter languages are semilinear. This work makes general
contributions to the theory of formal languages that are of potential interest
for understanding recurrent neural networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation. (arXiv:2108.04556v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04556">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Code representation learning, which aims to encode the semantics of source
code into distributed vectors, plays an important role in recent
deep-learning-based models for code intelligence. Recently, many pre-trained
language models for source code (e.g., CuBERT and CodeBERT) have been proposed
to model the context of code and serve as a basis for downstream code
intelligence tasks such as code search, code clone detection, and program
translation. Current approaches typically consider the source code as a plain
sequence of tokens, or inject the structure information (e.g., AST and
data-flow) into the sequential model pre-training. To further explore the
properties of programming languages, this paper proposes SynCoBERT, a
syntax-guided multi-modal contrastive pre-training approach for better code
representations. Specially, we design two novel pre-training objectives
originating from the symbolic and syntactic properties of source code, i.e.,
Identifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to
predict identifiers, and edges between two nodes of AST, respectively.
Meanwhile, to exploit the complementary information in semantically equivalent
modalities (i.e., code, comment, AST) of the code, we propose a multi-modal
contrastive learning strategy to maximize the mutual information among
different modalities. Extensive experiments on four downstream tasks related to
code intelligence show that SynCoBERT advances the state-of-the-art with the
same pre-training corpus and model size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04408">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
label annotation distribution schemes, assigning multiple labels per example
for a small subset of training examples. Introducing such multi label examples
at the cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from uneven training examples (with zero, one, or multiple labels). This
algorithm efficiently combines signals from uneven training data and brings
additional gains in low annotation budget and cross domain settings. Together,
our method achieves consistent gains in both accuracy and label distribution
metrics in two tasks, suggesting training with uneven training data can be
beneficial for many NLP tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions. (arXiv:2010.12758v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12758">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing conversational systems are mostly agent-centric, which assumes the
user utterances would closely follow the system ontology (for NLU or dialogue
state tracking). However, in real-world scenarios, it is highly desirable that
the users can speak freely in their own way. It is extremely hard, if not
impossible, for the users to adapt to the unknown system ontology. In this
work, we attempt to build a user-centric dialogue system. As there is no clean
mapping for a user&#x27;s free form utterance to an ontology, we first model the
user preferences as estimated distributions over the system ontology and map
the users&#x27; utterances to such distributions. Learning such a mapping poses new
challenges on reasoning over existing knowledge, ranging from factoid
knowledge, commonsense knowledge to the users&#x27; own situations. To this end, we
build a new dataset named NUANCED that focuses on such realistic settings for
conversational recommendation. Collected via dialogue simulation and
paraphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user
responses. We conduct experiments, showing both the usefulness and challenges
of our problem setting. We believe NUANCED can serve as a valuable resource to
push existing research from the agent-centric system to the user-centric
system. The code and data is publicly available at
\url{https://github.com/facebookresearch/nuanced}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems. (arXiv:2109.04084v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human dialogue contains evolving concepts, and speakers naturally associate
multiple concepts to compose a response. However, current dialogue models with
the seq2seq framework lack the ability to effectively manage concept
transitions and can hardly introduce multiple concepts to responses in a
sequential decoding manner. To facilitate a controllable and coherent dialogue,
in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for
open-domain dialogue generation. The proposed model comprises a multi-concept
planning module that learns to identify multiple associated concepts from a
concept graph and a customized Insertion Transformer that performs
concept-guided non-autoregressive generation to complete a response. The
experimental results on two public datasets show that CG-nAR can produce
diverse and coherent responses, outperforming state-of-the-art baselines in
both automatic and human evaluations with substantially faster inference speed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The goal of building intelligent dialogue systems has largely been
\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)
systems, which perform goal-oriented functions, and open-domain dialogue (ODD)
systems, which focus on non-goal-oriented chitchat. The two dialogue modes can
potentially be intertwined together seamlessly in the same conversation, as
easily done by a friendly human assistant. Such ability is desirable in
conversational agents, as the integration makes them more accessible and
useful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn
dialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset
FusedChat, by rewriting the existing TOD turns and adding new ODD turns. This
procedure constructs conversation sessions containing exchanges from both
dialogue modes. It features inter-mode contextual dependency, i.e., the
dialogue turns from the two modes depend on each other. Rich dependency
patterns including co-reference and ellipsis are features. The new dataset,
with 60k new human-written ODD turns and 5k re-written TOD turns, offers a
benchmark to test a dialogue model&#x27;s ability to perform inter-mode
conversations. This is a more challenging task since the model has to determine
the appropriate dialogue mode and generate the response based on the inter-mode
context. But such models would better mimic human-level conversation
capabilities. We evaluate baseline models on this task, including
\textit{classification-based} two-stage models and \textit{two-in-one} fused
models. We publicly release FusedChat and the baselines to propel future work
on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08803">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We develop a novel approach for confidently accelerating inference in the
large and expensive multilayer Transformers that are now ubiquitous in natural
language processing (NLP). Amortized or approximate computational methods
increase efficiency, but can come with unpredictable performance costs. In this
work, we present CATs -- Confident Adaptive Transformers -- in which we
simultaneously increase computational efficiency, while guaranteeing a
specifiable degree of consistency with the original model with high confidence.
Our method trains additional prediction heads on top of intermediate layers,
and dynamically decides when to stop allocating computational effort to each
input using a meta consistency classifier. To calibrate our early prediction
stopping rule, we formulate a unique extension of conformal prediction. We
demonstrate the effectiveness of this approach on four classification and
regression tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of Language Change in Collaborative Instruction Following. (arXiv:2109.04452v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04452">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We analyze language change over time in a collaborative, goal-oriented
instructional task, where utility-maximizing participants form conventions and
increase their expertise. Prior work studied such scenarios mostly in the
context of reference games, and consistently found that language complexity is
reduced along multiple dimensions, such as utterance length, as conventions are
formed. In contrast, we find that, given the ability to increase instruction
utility, instructors increase language complexity along these previously
studied dimensions to better collaborate with increasingly skilled instruction
followers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06402">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Truth can vary over time. Fact-checking decisions on claim veracity should
therefore take into account temporal information of both the claim and
supporting or refuting evidence. In this work, we investigate the hypothesis
that the timestamp of a Web page is crucial to how it should be ranked for a
given claim. We delineate four temporal ranking methods that constrain evidence
ranking differently and simulate hypothesis-specific evidence rankings given
the evidence timestamps as gold standard. Evidence ranking in three
fact-checking models is ultimately optimized using a learning-to-rank loss
function. Our study reveals that time-aware evidence ranking not only surpasses
relevance assumptions based purely on semantic similarity or position in a
search results list, but also improves veracity predictions of time-sensitive
claims in particular.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal Embedding Calibration for Symbolic Music Similarity. (arXiv:2103.07656v2 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07656">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In natural language processing (NLP), the semantic similarity task requires
large-scale, high-quality human-annotated labels for fine-tuning or evaluation.
By contrast, in cases of music similarity, such labels are expensive to collect
and largely dependent on the annotator&#x27;s artistic preferences. Recent research
has demonstrated that embedding calibration technique can greatly increase
semantic similarity performance of the pre-trained language model without
fine-tuning. However, it is yet unknown which calibration method is the best
and how much performance improvement can be achieved. To address these issues,
we propose using composer information to construct labels for automatically
evaluating music similarity. Under this paradigm, we discover the optimal
combination of embedding calibration which achieves superior metrics than the
baseline methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03926">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Representation of linguistic phenomena in computational language models is
typically assessed against the predictions of existing linguistic theories of
these phenomena. Using the notion of polarity as a case study, we show that
this is not always the most adequate set-up. We probe polarity via so-called
&#x27;negative polarity items&#x27; (in particular, English &#x27;any&#x27;) in two pre-trained
Transformer-based models (BERT and GPT-2). We show that -- at least for
polarity -- metrics derived from language models are more consistent with data
from psycholinguistic experiments than linguistic theory predictions.
Establishing this allows us to more adequately evaluate the performance of
language models and also to use language models to discover new insights into
natural language grammar beyond existing linguistic theories. Overall, our
results encourage a closer tie between experiments with human subjects and with
language models. We propose methods to enable this closer tie, with language
models as part of experimental pipeline, and show this pipeline at work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have recently become very popular for sequence-to-sequence
applications such as machine translation and speech recognition. In this work,
we propose a multi-task learning-based transformer model for low-resource
multilingual speech recognition for Indian languages. Our proposed model
consists of a conformer [1] encoder and two parallel transformer decoders. We
use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme
decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme
recognition task as an auxiliary task for our multi-task learning framework. We
jointly optimize the network for both phoneme and grapheme recognition tasks
using Joint CTC-Attention [2] training. We use a conditional decoding scheme to
inject the language information into the model before predicting the grapheme
sequence. Our experiments show that our proposed approach can obtain
significant improvement over previous approaches [4]. We also show that our
conformer-based dual-decoder approach outperforms both the transformer-based
dual-decoder approach and single decoder approach. Finally, We compare
monolingual ASR models with our proposed multilingual ASR approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent works have found evidence of gender bias in models of machine
translation and coreference resolution using mostly synthetic diagnostic
datasets. While these quantify bias in a controlled experiment, they often do
so on a small scale and consist mostly of artificial, out-of-distribution
sentences. In this work, we find grammatical patterns indicating stereotypical
and non-stereotypical gender-role assignments (e.g., female nurses versus male
dancers) in corpora from three domains, resulting in a first large-scale gender
bias dataset of 108K diverse real-world English sentences. We manually verify
the quality of our corpus and use it to evaluate gender bias in various
coreference resolution and machine translation models. We find that all tested
models tend to over-rely on gender stereotypes when presented with natural
inputs, which may be especially harmful when deployed in commercial systems.
Finally, we show that our dataset lends itself to finetuning a coreference
resolution model, finding it mitigates bias on a held out set. Our dataset and
models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will
spur future research into gender bias evaluation mitigation techniques in
realistic settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering. (arXiv:2109.04014v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04014">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge-based visual question answering (VQA) requires answering questions
with external knowledge in addition to the content of images. One dataset that
is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold
standard knowledge corpus for retrieval. Existing work leverage different
knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge.
Because of varying knowledge bases, it is hard to fairly compare models&#x27;
performance. To address this issue, we collect a natural language knowledge
base that can be used for any VQA system. Moreover, we propose a Visual
Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever
aims to retrieve relevant knowledge, and the visual reader seeks to predict
answers based on given knowledge. We introduce various ways to retrieve
knowledge using text and images and two reader styles: classification and
extraction. Both the retriever and reader are trained with weak supervision.
Our experimental results show that a good retriever can significantly improve
the reader&#x27;s performance on the OK-VQA challenge. The code and corpus are
provided in https://github.com/luomancs/retriever\_reader\_for\_okvqa.git</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03892">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We investigate the use of multimodal information contained in images as an
effective method for enhancing the commonsense of Transformer models for text
generation. We perform experiments using BART and T5 on concept-to-text
generation, specifically the task of generative commonsense reasoning, or
CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text
Generation. VisCTG involves captioning images representing appropriate everyday
scenarios, and using these captions to enrich and steer the generation process.
Comprehensive evaluation and analysis demonstrate that VisCTG noticeably
improves model performance while successfully addressing several issues of the
baseline generations, including poor commonsense, fluency, and specificity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Based Network with Contextualized Representations of Turns in Dialogue. (arXiv:2109.04008v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dialogue-based relation extraction (RE) aims to extract relation(s) between
two arguments that appear in a dialogue. Because dialogues have the
characteristics of high personal pronoun occurrences and low information
density, and since most relational facts in dialogues are not supported by any
single sentence, dialogue-based relation extraction requires a comprehensive
understanding of dialogue. In this paper, we propose the TUrn COntext awaRE
Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way
people understand dialogues. In addition, we propose a novel approach which
treats the task of emotion recognition in conversations (ERC) as a
dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC
datasets demonstrate that our model is very effective in various dialogue-based
natural language understanding tasks. In these experiments, TUCORE-GCN
outperforms the state-of-the-art models on most of the benchmark datasets. Our
code is available at https://github.com/BlackNoodle/TUCORE-GCN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ELIT: Emory Language and Information Toolkit. (arXiv:2109.03903v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce ELIT, the Emory Language and Information Toolkit, which is a
comprehensive NLP framework providing transformer-based end-to-end models for
core tasks with a special focus on memory efficiency while maintaining
state-of-the-art accuracy and speed. Compared to existing toolkits, ELIT
features an efficient Multi-Task Learning (MTL) model with many downstream
tasks that include lemmatization, part-of-speech tagging, named entity
recognition, dependency parsing, constituency parsing, semantic role labeling,
and AMR parsing. The backbone of ELIT&#x27;s MTL framework is a pre-trained
transformer encoder that is shared across tasks to speed up their inference.
ELIT provides pre-trained models developed on a remix of eight datasets. To
scale up its service, ELIT also integrates a RESTful Client/Server combination.
On the server side, ELIT extends its functionality to cover other tasks such as
tokenization and coreference resolution, providing an end user with agile
research experience. All resources including the source codes, documentation,
and pre-trained models are publicly available at
https://github.com/emorynlp/elit.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04053">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tables provide valuable knowledge that can be used to verify textual
statements. While a number of works have considered table-based fact
verification, direct alignments of tabular data with tokens in textual
statements are rarely available. Moreover, training a generalized fact
verification model requires abundant labeled training data. In this paper, we
propose a novel system to address these problems. Inspired by counterfactual
causality, our system identifies token-level salience in the statement with
probing-based salience estimation. Salience estimation allows enhanced learning
of fact verification from two perspectives. From one perspective, our system
conducts masked salient token prediction to enhance the model for alignment and
reasoning between the table and the statement. From the other perspective, our
system applies salience-aware data augmentation to generate a more diverse set
of training instances by replacing non-salient terms. Experimental results on
TabFact show the effective improvement by the proposed salience-aware learning
techniques, leading to the new SOTA performance on the benchmark. Our code is
publicly available at https://github.com/luka-group/Salience-aware-Learning .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Bayesian Framework for Information-Theoretic Probing. (arXiv:2109.03853v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03853">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pimentel et al. (2020) recently analysed probing from an
information-theoretic perspective. They argue that probing should be seen as
approximating a mutual information. This led to the rather unintuitive
conclusion that representations encode exactly the same information about a
target task as the original sentences. The mutual information, however, assumes
the true probability distribution of a pair of random variables is known,
leading to unintuitive results in settings where it is not. This paper proposes
a new framework to measure what we term Bayesian mutual information, which
analyses information from the perspective of Bayesian agents -- allowing for
more intuitive findings in scenarios with finite data. For instance, under
Bayesian MI we have that data can add information, processing can help, and
information can hurt, which makes it more intuitive for machine learning
applications. Finally, we apply our framework to probing where we believe
Bayesian mutual information naturally operationalises ease of extraction by
explicitly limiting the available background knowledge to solve a task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Ensemble Fine-tuned mBERT for Translation Quality Estimation. (arXiv:2109.03914v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03914">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quality Estimation (QE) is an important component of the machine translation
workflow as it assesses the quality of the translated output without consulting
reference translations. In this paper, we discuss our submission to the WMT
2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that
challenge participants to predict the HTER score for sentence-level
post-editing effort. Our proposed system is an ensemble of multilingual BERT
(mBERT)-based regression models, which are generated by fine-tuning on
different input settings. It demonstrates comparable performance with respect
to the Pearson&#x27;s correlation and beats the baseline system in MAE/ RMSE for
several language pairs. In addition, we adapt our system for the zero-shot
setting by exploiting target language-relevant language pairs and
pseudo-reference translations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While recent research on natural language inference has considerably
benefited from large annotated datasets, the amount of inference-related
knowledge (including commonsense) provided in the annotated data is still
rather limited. There have been two lines of approaches that can be used to
further address the limitation: (1) unsupervised pretraining can leverage
knowledge in much larger unstructured text data; (2) structured (often
human-curated) knowledge has started to be considered in neural-network-based
models for NLI. An immediate question is whether these two approaches
complement each other, or how to develop models that can bring together their
advantages. In this paper, we propose models that leverage structured knowledge
in different components of pre-trained models. Our results show that the
proposed models perform better than previous BERT-based state-of-the-art
models. Although our models are proposed for NLI, they can be easily extended
to other sentence or sentence-pair classification problems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study Comparative Preference Classification (CPC) which aims at predicting
whether a preference comparison exists between two entities in a given sentence
and, if so, which entity is preferred over the other. High-quality CPC models
can significantly benefit applications such as comparative question answering
and review-based recommendations. Among the existing approaches, non-deep
learning methods suffer from inferior performances. The state-of-the-art graph
neural network-based ED-GAT (Ma et al., 2020) only considers syntactic
information while ignoring the critical semantic relations and the sentiments
to the compared entities. We proposed sentiment Analysis Enhanced COmparative
Network (SAECON) which improves CPC ac-curacy with a sentiment analyzer that
learns sentiments to individual entities via domain adaptive knowledge
transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset
present a significant improvement on the F1 scores over the best existing CPC
approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03910">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we leverage large language models (LMs) to perform zero-shot
text style transfer. We present a prompting method that we call augmented
zero-shot learning, which frames style transfer as a sentence rewriting task
and requires only a natural language instruction, without model fine-tuning or
exemplars in the target style. Augmented zero-shot learning is simple and
demonstrates promising results not just on standard style transfer tasks such
as sentiment, but also on arbitrary transformations such as &quot;make this
melodramatic&quot; or &quot;insert a metaphor.&quot;</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems. (arXiv:2109.03888v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03888">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer models have achieved state-of-the-art results in a wide range of
NLP tasks including summarization. Training and inference using large
transformer models can be computationally expensive. Previous work has focused
on one important bottleneck, the quadratic self-attention mechanism in the
encoder. Modified encoder architectures such as LED or LoBART use local
attention patterns to address this problem for summarization. In contrast, this
work focuses on the transformer&#x27;s encoder-decoder attention mechanism. The cost
of this attention becomes more significant in inference or training approaches
that require model-generated histories. First, we examine the complexity of the
encoder-decoder attention. We demonstrate empirically that there is a sparse
sentence structure in document summarization that can be exploited by
constraining the attention mechanism to a subset of input sentences, whilst
maintaining system performance. Second, we propose a modified architecture that
selects the subset of sentences to constrain the encoder-decoder attention.
Experiments are carried out on abstractive summarization tasks, including
CNN/DailyMail, XSum, Spotify Podcast, and arXiv.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graphine: A Dataset for Graph-aware Terminology Definition Generation. (arXiv:2109.04018v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04018">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Precisely defining the terminology is the first step in scientific
communication. Developing neural text generation models for definition
generation can circumvent the labor-intensity curation, further accelerating
scientific discovery. Unfortunately, the lack of large-scale terminology
definition dataset hinders the process toward definition generation. In this
paper, we present a large-scale terminology definition dataset Graphine
covering 2,010,648 terminology definition pairs, spanning 227 biomedical
subdisciplines. Terminologies in each subdiscipline further form a directed
acyclic graph, opening up new avenues for developing graph-aware text
generation models. We then proposed a novel graph-aware definition generation
model Graphex that integrates transformer with graph neural network. Our model
outperforms existing text generation models by exploiting the graph structure
of terminologies. We further demonstrated how Graphine can be used to evaluate
pretrained language models, compare graph representation learning methods and
predict sentence granularity. We envision Graphine to be a unique resource for
definition generation and many other NLP tasks in biomedicine.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multilingual neural machine translation (MNMT) learns to translate multiple
language pairs with a single model, potentially improving both the accuracy and
the memory-efficiency of deployed models. However, the heavy data imbalance
between languages hinders the model from performing uniformly across language
pairs. In this paper, we propose a new learning objective for MNMT based on
distributionally robust optimization, which minimizes the worst-case expected
loss over the set of language pairs. We further show how to practically
optimize this objective for large translation corpora using an iterated best
response scheme, which is both effective and incurs negligible additional
computational cost compared to standard empirical risk minimization. We perform
extensive experiments on three sets of languages from two datasets and show
that our method consistently outperforms strong baseline methods in terms of
average and per-language performance under both many-to-one and one-to-many
translation settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Formal Description of Sorani Kurdish Morphology. (arXiv:2109.03942v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03942">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sorani Kurdish, also known as Central Kurdish, has a complex morphology,
particularly due to the patterns in which morphemes appear. Although several
aspects of Kurdish morphology have been studied, such as pronominal endoclitics
and Izafa constructions, Sorani Kurdish morphology has received trivial
attention in computational linguistics. Moreover, some morphemes, such as the
emphasis endoclitic &#x3D;\^i\c{s}, and derivational morphemes have not been
previously studied. To tackle the complex morphology of Sorani, we provide a
thorough description of Sorani Kurdish morphological and morphophonological
constructions in a formal way such that they can be used as finite-state
transducers for morphological analysis and synthesis.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.12205">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural network models are resource hungry. It is difficult to deploy such
deep networks on devices with limited resources, like smart wearables,
cellphones, drones, and autonomous vehicles. Low bit quantization such as
binary and ternary quantization is a common approach to alleviate this resource
requirements. Ternary quantization provides a more flexible model and
outperforms binary quantization in terms of accuracy, however doubles the
memory footprint and increases the computational cost. Contrary to these
approaches, mixed quantized models allow a trade-off between accuracy and
memory footprint. In such models, quantization depth is often chosen manually,
or is tuned using a separate optimization routine. The latter requires training
a quantized network multiple times. Here, we propose an adaptive combination of
binary and ternary quantization, namely Smart Quantization (SQ), in which the
quantization depth is modified directly via a regularization function, so that
the model is trained only once. Our experimental results show that the proposed
method adapts quantization depth successfully while keeping the model accuracy
high on MNIST and CIFAR10 benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Video-Text Retrieval by Multi-Stream Corpus Alignment and Dual Softmax Loss. (arXiv:2109.04290v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04290">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Employing large-scale pre-trained model CLIP to conduct video-text retrieval
task (VTR) has become a new trend, which exceeds previous VTR methods. Though,
due to the heterogeneity of structures and contents between video and text,
previous CLIP-based models are prone to overfitting in the training phase,
resulting in relatively poor retrieval performance. In this paper, we propose a
multi-stream Corpus Alignment network with single gate Mixture-of-Experts
(CAMoE) and a novel Dual Softmax Loss (DSL) to solve the two heterogeneity. The
CAMoE employs Mixture-of-Experts (MoE) to extract multi-perspective video
representations, including action, entity, scene, etc., then align them with
the corresponding part of the text. In this stage, we conduct massive
explorations towards the feature extraction module and feature alignment
module. DSL is proposed to avoid the one-way optimum-match which occurs in
previous contrastive methods. Introducing the intrinsic prior of each pair in a
batch, DSL serves as a reviser to correct the similarity matrix and achieves
the dual optimal match. DSL is easy to implement with only one-line code but
improves significantly. The results show that the proposed CAMoE and DSL are of
strong efficiency, and each of them is capable of achieving State-of-The-Art
(SOTA) individually on various benchmarks such as MSR-VTT, MSVD, and LSMDC.
Further, with both of them, the performance is advanced to a big extend,
surpassing the previous SOTA methods for around 4.6\% R@1 in MSR-VTT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">General Partial Label Learning via Dual Bipartite Graph Autoencoder. (arXiv:2001.01290v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.01290">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We formulate a practical yet challenging problem: General Partial Label
Learning (GPLL). Compared to the traditional Partial Label Learning (PLL)
problem, GPLL relaxes the supervision assumption from instance-level -- a label
set partially labels an instance -- to group-level: 1) a label set partially
labels a group of instances, where the within-group instance-label link
annotations are missing, and 2) cross-group links are allowed -- instances in a
group may be partially linked to the label set from another group. Such
ambiguous group-level supervision is more practical in real-world scenarios as
additional annotation on the instance-level is no longer required, e.g.,
face-naming in videos where the group consists of faces in a frame, labeled by
a name set in the corresponding caption. In this paper, we propose a novel
graph convolutional network (GCN) called Dual Bipartite Graph Autoencoder
(DB-GAE) to tackle the label ambiguity challenge of GPLL. First, we exploit the
cross-group correlations to represent the instance groups as dual bipartite
graphs: within-group and cross-group, which reciprocally complements each other
to resolve the linking ambiguities. Second, we design a GCN autoencoder to
encode and decode them, where the decodings are considered as the refined
results. It is worth noting that DB-GAE is self-supervised and transductive, as
it only uses the group-level supervision without a separate offline training
stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE
significantly outperforms the best baseline over absolute 0.159 F1-score and
24.8% accuracy. We further offer analysis on various levels of label
ambiguities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Continuous Event-Line Constraint for Closed-Form Velocity Initialization. (arXiv:2109.04313v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04313">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Event cameras trigger events asynchronously and independently upon a
sufficient change of the logarithmic brightness level. The neuromorphic sensor
has several advantages over standard cameras including low latency, absence of
motion blur, and high dynamic range. Event cameras are particularly well suited
to sense motion dynamics in agile scenarios. We propose the continuous
event-line constraint, which relies on a constant-velocity motion assumption as
well as trifocal tensor geometry in order to express a relationship between
line observations given by event clusters as well as first-order camera
dynamics. Our core result is a closed-form solver for up-to-scale linear camera
velocity {with known angular velocity}. Nonlinear optimization is adopted to
improve the performance of the algorithm. The feasibility of the approach is
demonstrated through a careful analysis on both simulated and real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modified Supervised Contrastive Learning for Detecting Anomalous Driving Behaviours. (arXiv:2109.04021v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04021">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Detecting distracted driving behaviours is important to reduce millions of
deaths and injuries occurring worldwide. Distracted or anomalous driving
behaviours are deviations from the &#x27;normal&#x27; driving that need to be identified
correctly to alert the driver. However, these driving behaviours do not
comprise of one specific type of driving style and their distribution can be
different during training and testing phases of a classifier. We formulate this
problem as a supervised contrastive learning approach to learn a visual
representation to detect normal, and seen and unseen anomalous driving
behaviours. We made a change to the standard contrastive loss function to
adjust the similarity of negative pairs to aid the optimization. Normally, the
(self) supervised contrastive framework contains an encoder followed by a
projection head, which is omitted during testing phase as the encoding layers
are considered to contain general visual representative information. However,
we assert that for supervised contrastive learning task, including projection
head will be beneficial. We showed our results on a Driver Anomaly Detection
dataset that contains 783 minutes of video recordings of normal and anomalous
driving behaviours of 31 drivers from various from top and front cameras (both
depth and infrared). We also performed an extra step of fine tuning the labels
in this dataset. Out of 9 video modalities combinations, our modified
contrastive approach improved the ROC AUC on 7 in comparison to the baseline
models (from 3.12% to 8.91% for different modalities); the remaining two models
also had manual labelling. We performed statistical tests that showed evidence
that our modifications perform better than the baseline contrastive models.
Finally, the results showed that the fusion of depth and infrared modalities
from top and front view achieved the best AUC ROC of 0.9738 and AUC PR of
0.9772.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neighborhood Consensus Contrastive Learning for Backward-Compatible Representation. (arXiv:2108.03372v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03372">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In object re-identification (ReID), the development of deep learning
techniques often involves model updates and deployment. It is unbearable to
re-embedding and re-index with the system suspended when deploying new models.
Therefore, backward-compatible representation is proposed to enable &quot;new&quot;
features to be compared with &quot;old&quot; features directly, which means that the
database is active when there are both &quot;new&quot; and &quot;old&quot; features in it. Thus we
can scroll-refresh the database or even do nothing on the database to update.

The existing backward-compatible methods either require a strong overlap
between old and new training data or simply conduct constraints at the instance
level. Thus they are difficult in handling complicated cluster structures and
are limited in eliminating the impact of outliers in old embeddings, resulting
in a risk of damaging the discriminative capability of new features. In this
work, we propose a Neighborhood Consensus Contrastive Learning (NCCL) method.
With no assumptions about the new training data, we estimate the sub-cluster
structures of old embeddings. A new embedding is constrained with multiple old
embeddings in both embedding space and discrimination space at the sub-class
level. The effect of outliers diminished, as the multiple samples serve as
&quot;mean teachers&quot;. Besides, we also propose a scheme to filter the old embeddings
with low credibility, further improving the compatibility robustness. Our
method ensures backward compatibility without impairing the accuracy of the new
model. And it can even improve the new model&#x27;s accuracy in most scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NEAT: Neural Attention Fields for End-to-End Autonomous Driving. (arXiv:2109.04456v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04456">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Efficient reasoning about the semantic, spatial, and temporal structure of a
scene is a crucial prerequisite for autonomous driving. We present NEural
ATtention fields (NEAT), a novel representation that enables such reasoning for
end-to-end imitation learning models. NEAT is a continuous function which maps
locations in Bird&#x27;s Eye View (BEV) scene coordinates to waypoints and
semantics, using intermediate attention maps to iteratively compress
high-dimensional 2D image features into a compact representation. This allows
our model to selectively attend to relevant regions in the input while ignoring
information irrelevant to the driving task, effectively associating the images
with the BEV representation. In a new evaluation setting involving adverse
environmental conditions and challenging scenarios, NEAT outperforms several
strong baselines and achieves driving scores on par with the privileged CARLA
expert used to generate its training data. Furthermore, visualizing the
attention maps for models with NEAT intermediate representations provides
improved interpretability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DSAM: A Distance Shrinking with Angular Marginalizing Loss for High Performance Vehicle Re-identificatio. (arXiv:2011.06228v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vehicle Re-identification (ReID) is an important yet challenging problem in
computer vision. Compared to other visual objects like faces and persons,
vehicles simultaneously exhibit much larger intraclass viewpoint variations and
interclass visual similarities, making most exiting loss functions designed for
face recognition and person ReID unsuitable for vehicle ReID. To obtain a
high-performance vehicle ReID model, we present a novel Distance Shrinking with
Angular Marginalizing (DSAM) loss function to perform hybrid learning in both
the Original Feature Space (OFS) and the Feature Angular Space (FAS) using the
local verification and the global identification information. Specifically, it
shrinks the distance between samples of the same class locally in the Original
Feature Space while keeps samples of different classes far away in the Feature
Angular Space. The shrinking and marginalizing operations are performed during
each iteration of the training process and are suitable for different SoftMax
based loss functions. We evaluate the DSAM loss function on three large vehicle
ReID datasets with detailed analyses and extensive comparisons with many
competing vehicle ReID methods. Experimental results show that our DSAM loss
enhances the SoftMax loss by a large margin on the PKU-VD1-Large dataset:
10.41% for mAP, 5.29% for cmc1, and 4.60% for cmc5. Moreover, the mAP is
increased by 9.34% on the PKU-VehicleID dataset and 6.13% on the VeRi-776
dataset. Source code will be released to facilitate further studies in this
research direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PIMNet: A Parallel, Iterative and Mimicking Network for Scene Text Recognition. (arXiv:2109.04145v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04145">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Nowadays, scene text recognition has attracted more and more attention due to
its various applications. Most state-of-the-art methods adopt an
encoder-decoder framework with attention mechanism, which generates text
autoregressively from left to right. Despite the convincing performance, the
speed is limited because of the one-by-one decoding strategy. As opposed to
autoregressive models, non-autoregressive models predict the results in
parallel with a much shorter inference time, but the accuracy falls behind the
autoregressive counterpart considerably. In this paper, we propose a Parallel,
Iterative and Mimicking Network (PIMNet) to balance accuracy and efficiency.
Specifically, PIMNet adopts a parallel attention mechanism to predict the text
faster and an iterative generation mechanism to make the predictions more
accurate. In each iteration, the context information is fully explored. To
improve learning of the hidden layer, we exploit the mimicking learning in the
training phase, where an additional autoregressive decoder is adopted and the
parallel decoder mimics the autoregressive decoder with fitting outputs of the
hidden layer. With the shared backbone between the two decoders, the proposed
PIMNet can be trained end-to-end without pre-training. During inference, the
branch of the autoregressive decoder is removed for a faster speed. Extensive
experiments on public benchmarks demonstrate the effectiveness and efficiency
of PIMNet. Our code will be available at https://github.com/Pay20Y/PIMNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning has the potential to augment many components of the clinical
workflow, such as medical image interpretation. However, the translation of
these black box algorithms into clinical practice has been marred by the
relative lack of transparency compared to conventional machine learning
methods, hindering in clinician trust in the systems for critical medical
decision-making. Specifically, common deep learning approaches do not have
intuitive ways of expressing uncertainty with respect to cases that might
require further human review. Furthermore, the possibility of algorithmic bias
has caused hesitancy regarding the use of developed algorithms in clinical
settings. To these ends, we explore how conformal methods can complement deep
learning models by providing both clinically intuitive way (by means of
confidence prediction sets) of expressing model uncertainty as well as
facilitating model transparency in clinical workflows. In this paper, we
conduct a field survey with clinicians to assess clinical use-cases of
conformal predictions. Next, we conduct experiments with a mammographic breast
density and dermatology photography datasets to demonstrate the utility of
conformal predictions in &quot;rule-in&quot; and &quot;rule-out&quot; disease scenarios. Further,
we show that conformal predictors can be used to equalize coverage with respect
to patient demographics such as race and skin tone. We find that a conformal
predictions to be a promising framework with potential to increase clinical
usability and transparency for better collaboration between deep learning
algorithms and clinicians.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sharing Matters for Generalization in Deep Metric Learning. (arXiv:2004.05582v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.05582">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning the similarity between images constitutes the foundation for
numerous vision tasks. The common paradigm is discriminative metric learning,
which seeks an embedding that separates different training classes. However,
the main challenge is to learn a metric that not only generalizes from training
to novel, but related, test samples. It should also transfer to different
object classes. So what complementary information is missed by the
discriminative paradigm? Besides finding characteristics that separate between
classes, we also need them to likely occur in novel categories, which is
indicated if they are shared across training classes. This work investigates
how to learn such characteristics without the need for extra annotations or
training data. By formulating our approach as a novel triplet sampling
strategy, it can be easily applied on top of recent ranking loss frameworks.
Experiments show that, independent of the underlying network architecture and
the specific ranking loss, our approach significantly improves performance in
deep metric learning, leading to new the state-of-the-art results on various
standard benchmark datasets. Preliminary early access page can be found here:
https://ieeexplore.ieee.org/document/9141449</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Building Segmentation for Off-Nadir Satellite Imagery. (arXiv:2109.03961v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03961">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatic building segmentation is an important task for satellite imagery
analysis and scene understanding. Most existing segmentation methods focus on
the case where the images are taken from directly overhead (i.e., low
off-nadir/viewing angle). These methods often fail to provide accurate results
on satellite images with larger off-nadir angles due to the higher noise level
and lower spatial resolution. In this paper, we propose a method that is able
to provide accurate building segmentation for satellite imagery captured from a
large range of off-nadir angles. Based on Bayesian deep learning, we explicitly
design our method to learn the data noise via aleatoric and epistemic
uncertainty modeling. Satellite image metadata (e.g., off-nadir angle and
ground sample distance) is also used in our model to further improve the
result. We show that with uncertainty modeling and metadata injection, our
method achieves better performance than the baseline method, especially for
noisy images taken from large off-nadir angles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. (arXiv:2101.12699v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we introduce the \textit{Layer-Peeled Model}, a nonconvex yet
analytically tractable optimization program, in a quest to better understand
deep neural networks that are trained for a sufficiently long time. As the name
suggests, this new model is derived by isolating the topmost layer from the
remainder of the neural network, followed by imposing certain constraints
separately on the two parts of the network. We demonstrate that the
Layer-Peeled Model, albeit simple, inherits many characteristics of
well-trained neural networks, thereby offering an effective tool for explaining
and predicting common empirical patterns of deep learning training. First, when
working on class-balanced datasets, we prove that any solution to this model
forms a simplex equiangular tight frame, which in part explains the recently
discovered phenomenon of neural collapse \cite{papyan2020prevalence}. More
importantly, when moving to the imbalanced case, our analysis of the
Layer-Peeled Model reveals a hitherto unknown phenomenon that we term
\textit{Minority Collapse}, which fundamentally limits the performance of deep
learning models on the minority classes. In addition, we use the Layer-Peeled
Model to gain insights into how to mitigate Minority Collapse. Interestingly,
this phenomenon is first predicted by the Layer-Peeled Model before being
confirmed by our computational experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System. (arXiv:2109.04202v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Like many scientific fields, new chemistry literature has grown at a
staggering pace, with thousands of papers released every month. A large portion
of chemistry literature focuses on new molecules and reactions between
molecules. Most vital information is conveyed through 2-D images of molecules,
representing the underlying molecules or reactions described. In order to
ensure reproducible and machine-readable molecule representations, text-based
molecule descriptors like SMILES and SELFIES were created. These text-based
molecule representations provide molecule generation but are unfortunately
rarely present in published literature. In the absence of molecule descriptors,
the generation of molecule descriptors from the 2-D images present in the
literature is necessary to understand chemistry literature at scale. Successful
methods such as Optical Structure Recognition Application (OSRA), and
ChemSchematicResolver are able to extract the locations of molecules structures
in chemistry papers and infer molecular descriptions and reactions. While
effective, existing systems expect chemists to correct outputs, making them
unsuitable for unsupervised large-scale data mining. Leveraging the task
formulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a
model which leverages Deep Residual Networks for image feature extraction and
an encoder-decoder Transformer layers for molecule description generation.
Unlike previous Neural Network-based systems, IMG2SMI builds around the task of
molecule description generation, which enables IMG2SMI to outperform OSRA-based
systems by 163% in molecule similarity prediction as measured by the molecular
MACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further
research on this task, we release a new molecule prediction dataset. including
81 million molecules for molecule description generation</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Just Noticeable Difference for Machine Perception and Generation of Regularized Adversarial Images with Minimal Perturbation. (arXiv:2102.08079v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this study, we introduce a measure for machine perception, inspired by the
concept of Just Noticeable Difference (JND) of human perception. Based on this
measure, we suggest an adversarial image generation algorithm, which
iteratively distorts an image by an additive noise until the model detects the
change in the image by outputting a false label. The noise added to the
original image is defined as the gradient of the cost function of the model. A
novel cost function is defined to explicitly minimize the amount of
perturbation applied to the input image while enforcing the perceptual
similarity between the adversarial and input images. For this purpose, the cost
function is regularized by the well-known total variation and bounded range
terms to meet the natural appearance of the adversarial image. We evaluate the
adversarial images generated by our algorithm both qualitatively and
quantitatively on CIFAR10, ImageNet, and MS COCO datasets. Our experiments on
image classification and object detection tasks show that adversarial images
generated by our JND method are both more successful in deceiving the
recognition/detection models and less perturbed compared to the images
generated by the state-of-the-art methods, namely, FGV, FSGM, and DeepFool
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ACP++: Action Co-occurrence Priors for Human-Object Interaction Detection. (arXiv:2109.04047v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04047">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A common problem in the task of human-object interaction (HOI) detection is
that numerous HOI classes have only a small number of labeled examples,
resulting in training sets with a long-tailed distribution. The lack of
positive labels can lead to low classification accuracy for these classes.
Towards addressing this issue, we observe that there exist natural correlations
and anti-correlations among human-object interactions. In this paper, we model
the correlations as action co-occurrence matrices and present techniques to
learn these priors and leverage them for more effective training, especially on
rare classes. The efficacy of our approach is demonstrated experimentally,
where the performance of our approach consistently improves over the
state-of-the-art methods on both of the two leading HOI detection benchmark
datasets, HICO-Det and V-COCO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01390">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Vision transformers (ViTs) have recently received explosive popularity, but
the huge computational cost is still a severe issue. Since the computation
complexity of ViT is quadratic with respect to the input sequence length, a
mainstream paradigm for computation reduction is to reduce the number of
tokens. Existing designs include structured spatial compression that uses a
progressive shrinking pyramid to reduce the computations of large feature maps,
and unstructured token pruning that dynamically drops redundant tokens.
However, the limitation of existing token pruning lies in two folds: 1) the
incomplete spatial structure caused by pruning is not compatible with
structured spatial compression that is commonly used in modern deep-narrow
transformers; 2) it usually requires a time-consuming pre-training procedure.
To tackle the limitations and expand the applicable scenario of token pruning,
we present Evo-ViT, a self-motivated slow-fast token evolution approach for
vision transformers. Specifically, we conduct unstructured instance-wise token
selection by taking advantage of the simple and effective global class
attention that is native to vision transformers. Then, we propose to update the
selected informative tokens and uninformative tokens with different computation
paths, namely, slow-fast updating. Since slow-fast updating mechanism maintains
the spatial structure and information flow, Evo-ViT can accelerate vanilla
transformers of both flat and deep-narrow structures from the very beginning of
the training process. Experimental results demonstrate that our method
significantly reduces the computational cost of vision transformers while
maintaining comparable performance on image classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Weight Pruning using Pre-trained Lottery Jackpots. (arXiv:2104.08700v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08700">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Network pruning is an effective approach to reduce network complexity without
performance compromise. Existing studies achieve the sparsity of neural
networks via time-consuming weight tuning or complex search on networks with
expanded width, which greatly limits the applications of network pruning. In
this paper, we show that high-performing and sparse sub-networks without the
involvement of weight tuning, termed &quot;lottery jackpots&quot;, exist in pre-trained
models with unexpanded width. For example, we obtain a lottery jackpot that has
only 10% parameters and still reaches the performance of the original dense
VGGNet-19 without any modifications on the pre-trained weights. Furthermore, we
observe that the sparse masks derived from many existing pruning criteria have
a high overlap with the searched mask of our lottery jackpot, among which, the
magnitude-based pruning results in the most similar mask with ours. Based on
this insight, we initialize our sparse mask using the magnitude pruning,
resulting in at least 3x cost reduction on the lottery jackpot search while
achieves comparable or even better performance. Specifically, our
magnitude-based lottery jackpot removes 90% weights in the ResNet-50, while
easily obtains more than 70% top-1 accuracy using only 10 searching epochs on
ImageNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Energy-Efficient Mobile Robot Control via Run-time Monitoring of Environmental Complexity and Computing Workload. (arXiv:2109.04285v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04285">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose an energy-efficient controller to minimize the energy consumption
of a mobile robot by dynamically manipulating the mechanical and computational
actuators of the robot. The mobile robot performs real-time vision-based
applications based on an event-based camera. The actuators of the controller
are CPU voltage/frequency for the computation part and motor voltage for the
mechanical part. We show that independently considering speed control of the
robot and voltage/frequency control of the CPU does not necessarily result in
an energy-efficient solution. In fact, to obtain the highest efficiency, the
computation and mechanical parts should be controlled together in synergy. We
propose a fast hill-climbing optimization algorithm to allow the controller to
find the best CPU/motor configuration at run-time and whenever the mobile robot
is facing a new environment during its travel. Experimental results on a robot
with Brushless DC Motors, Jetson TX2 board as the computing unit, and a
DAVIS-346 event-based camera show that the proposed control algorithm can save
battery energy by an average of 50.5%, 41%, and 30%, in low-complexity,
medium-complexity, and high-complexity environments, over baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. (arXiv:2109.04379v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04379">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Preserving maximal information is one of principles of designing
self-supervised learning methodologies. To reach this goal, contrastive
learning adopts an implicit way which is contrasting image pairs. However, we
believe it is not fully optimal to simply use the contrastive estimation for
preservation. Moreover, it is necessary and complemental to introduce an
explicit solution to preserve more information. From this perspective, we
introduce Preservational Learning to reconstruct diverse image contexts in
order to preserve more information in learned representations. Together with
the contrastive loss, we present Preservational Contrastive Representation
Learning (PCRL) for learning self-supervised medical representations. PCRL
provides very competitive results under the pretraining-finetuning protocol,
outperforming both self-supervised and supervised counterparts in 5
classification/segmentation tasks substantially.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural-IMLS: Learning Implicit Moving Least-Squares for Surface Reconstruction from Unoriented Point clouds. (arXiv:2109.04398v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04398">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Surface reconstruction from noisy, non-uniformly, and unoriented point clouds
is a fascinating yet difficult problem in computer vision and computer
graphics. In this paper, we propose Neural-IMLS, a novel approach that learning
noise-resistant signed distance function (SDF) for reconstruction. Instead of
explicitly learning priors with the ground-truth signed distance values, our
method learns the SDF from raw point clouds directly in a self-supervised
fashion by minimizing the loss between the couple of SDFs, one obtained by the
implicit moving least-square function (IMLS) and the other by our network.
Finally, a watertight and smooth 2-manifold triangle mesh is yielded by running
Marching Cubes. We conduct extensive experiments on various benchmarks to
demonstrate the performance of Neural-IMLS, especially for point clouds with
noise.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconstructing and grounding narrated instructional videos in 3D. (arXiv:2109.04409v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04409">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Narrated instructional videos often show and describe manipulations of
similar objects, e.g., repairing a particular model of a car or laptop. In this
work we aim to reconstruct such objects and to localize associated narrations
in 3D. Contrary to the standard scenario of instance-level 3D reconstruction,
where identical objects or scenes are present in all views, objects in
different instructional videos may have large appearance variations given
varying conditions and versions of the same product. Narrations may also have
large variation in natural language expressions. We address these challenges by
three contributions. First, we propose an approach for correspondence
estimation combining learnt local features and dense flow. Second, we design a
two-step divide and conquer reconstruction approach where the initial 3D
reconstructions of individual videos are combined into a 3D alignment graph.
Finally, we propose an unsupervised approach to ground natural language in
obtained 3D reconstructions. We demonstrate the effectiveness of our approach
for the domain of car maintenance. Given raw instructional videos and no manual
supervision, our method successfully reconstructs engines of different car
models and associates textual descriptions with corresponding objects in 3D.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels. (arXiv:2105.03857v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03857">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Detection faults in seismic data is a crucial step for seismic structural
interpretation, reservoir characterization and well placement. Some recent
works regard it as an image segmentation task. The task of image segmentation
requires huge labels, especially 3D seismic data, which has a complex structure
and lots of noise. Therefore, its annotation requires expert experience and a
huge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to
effectively train 3D-CNN by some slices from 3D seismic data, so that the model
can learn the segmentation of 3D seismic data from a few 2D slices. In order to
fully extract information from limited data and suppress seismic noise, we
propose an attention module that can be used for active supervision training
and embedded in the network. The attention heatmap label is generated by the
original label, and letting it supervise the attention module using the
lambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss
function, the method can extract 3D seismic features from a few 2D slice
labels. And it also shows the advanced performance of the attention module,
which can significantly suppress the noise in the seismic data while increasing
the model&#x27;s sensitivity to the foreground. Finally, on the public test set, we
only use the 2D slice labels training that accounts for 3.3% of the 3D volume
label, and achieve similar performance to the 3D volume label training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Superpoint-guided Semi-supervised Semantic Segmentation of 3D Point Clouds. (arXiv:2107.03601v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03601">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>3D point cloud semantic segmentation is a challenging topic in the computer
vision field. Most of the existing methods in literature require a large amount
of fully labeled training data, but it is extremely time-consuming to obtain
these training data by manually labeling massive point clouds. Addressing this
problem, we propose a superpoint-guided semi-supervised segmentation network
for 3D point clouds, which jointly utilizes a small portion of labeled scene
point clouds and a large number of unlabeled point clouds for network training.
The proposed network is iteratively updated with its predicted pseudo labels,
where a superpoint generation module is introduced for extracting superpoints
from 3D point clouds, and a pseudo-label optimization module is explored for
automatically assigning pseudo labels to the unlabeled points under the
constraint of the extracted superpoints. Additionally, there are some 3D points
without pseudo-label supervision. We propose an edge prediction module to
constrain features of edge points. A superpoint feature aggregation module and
a superpoint feature consistency loss function are introduced to smooth
superpoint features. Extensive experimental results on two 3D public datasets
demonstrate that our method can achieve better performance than several
state-of-the-art point cloud segmentation networks and several popular
semi-supervised segmentation methods with few labeled scenes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study. (arXiv:2108.05170v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05170">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Development in the field of Single Board Computers (SBC) have been increasing
for several years. They provide a good balance between computing performance
and power consumption which is usually required for mobile platforms, like
application in vehicles for Advanced Driver Assistance Systems (ADAS) and
Autonomous Driving (AD). However, there is an ever-increasing need of more
powerful and efficient SBCs which can run power intensive Deep Neural Networks
(DNNs) in real-time and can also satisfy necessary functional safety
requirements such as Automotive Safety Integrity Level (ASIL). ProAI is being
developed by ZF mainly to run powerful and efficient applications such as
multitask DNNs and on top of that it also has the required safety certification
for AD. In this work, we compare and discuss state of the art SBC on the basis
of power intensive multitask DNN architecture called Multitask-CenterNet with
respect to performance measures such as, FPS and power efficiency. As an
automotive supercomputer, ProAI delivers an excellent combination of
performance and efficiency, managing nearly twice the number of FPS per watt
than a modern workstation laptop and almost four times compared to the Jetson
Nano. Furthermore, it was also shown that there is still power in reserve for
further and more complex tasks on the ProAI, based on the CPU and GPU
utilization during the benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.15607">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Since many safety-critical systems, such as surgical robots and autonomous
driving cars operate in unstable environments with sensor noise and incomplete
data, it is desirable for object detectors to take the localization uncertainty
into account. However, there are several limitations of the existing
uncertainty estimation methods for anchor-based object detection. 1) They model
the uncertainty of the heterogeneous object properties with different
characteristics and scales, such as location (center point) and scale (width,
height), which could be difficult to estimate. 2) They model box offsets as
Gaussian distributions, which is not compatible with the ground truth bounding
boxes that follow the Dirac delta distribution. 3) Since anchor-based methods
are sensitive to anchor hyperparameters, the localization uncertainty for them
could be also highly sensitive to the choice of hyperparameters as well. To
tackle these limitations, we propose a new localization uncertainty estimation
method called UAD for anchor-free object detection. Our method captures the
uncertainty in four directions of box offsets~(left, right, top, bottom) that
are homogeneous, so that it can tell which direction is uncertain, and provides
a quantitative value of uncertainty in $[0, 1]$. To enable such uncertainty
estimation, we design a new uncertainty loss, negative power log-likelihood
loss, to measure the localization uncertainty by weighting the likelihood loss
by its IoU, which alleviates the model misspecification problem. Furthermore,
we propose an uncertainty-aware focal loss for reflecting the estimated
uncertainty to the classification score. Experimental results on COCO datasets
demonstrate that our method significantly improves FCOS, by up to 1.8 points,
without sacrificing computational efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IICNet: A Generic Framework for Reversible Image Conversion. (arXiv:2109.04242v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04242">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reversible image conversion (RIC) aims to build a reversible transformation
between specific visual content (e.g., short videos) and an embedding image,
where the original content can be restored from the embedding when necessary.
This work develops Invertible Image Conversion Net (IICNet) as a generic
solution to various RIC tasks due to its strong capacity and task-independent
design. Unlike previous encoder-decoder based methods, IICNet maintains a
highly invertible structure based on invertible neural networks (INNs) to
better preserve the information during conversion. We use a relation module and
a channel squeeze layer to improve the INN nonlinearity to extract cross-image
relations and the network flexibility, respectively. Experimental results
demonstrate that IICNet outperforms the specifically-designed methods on
existing RIC tasks and can generalize well to various newly-explored tasks.
With our generic IICNet, we no longer need to hand-engineer task-specific
embedding networks for rapidly occurring visual content. Our source codes are
available at: https://github.com/felixcheng97/IICNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency. (arXiv:2105.08667v2 [cs.CY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08667">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Twitter uses machine learning to crop images, where crops are centered around
the part predicted to be the most salient. In fall 2020, Twitter users raised
concerns that the automated image cropping system on Twitter favored
light-skinned over dark-skinned individuals, as well as concerns that the
system favored cropping woman&#x27;s bodies instead of their heads. In order to
address these concerns, we conduct an extensive analysis using formalized group
fairness metrics. We find systematic disparities in cropping and identify
contributing factors, including the fact that the cropping based on the single
most salient point can amplify the disparities because of an effect we term
argmax bias. However, we demonstrate that formalized fairness metrics and
quantitative analysis on their own are insufficient for capturing the risk of
representational harm in automatic cropping. We suggest the removal of
saliency-based cropping in favor of a solution that better preserves user
agency. For developing a new solution that sufficiently address concerns
related to representational harm, our critique motivates a combination of
quantitative and qualitative methods that include human-centered design.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. (arXiv:2109.04378v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tactile sensing is critical for humans to perform everyday tasks. While
significant progress has been made in analyzing object grasping from vision, it
remains unclear how we can utilize tactile sensing to reason about and model
the dynamics of hand-object interactions. In this work, we employ a
high-resolution tactile glove to perform four different interactive activities
on a diversified set of objects. We build our model on a cross-modal learning
framework and generate the labels using a visual processing pipeline to
supervise the tactile model, which can then be used on its own during the test
time. The tactile model aims to predict the 3d locations of both the hand and
the object purely from the touch data by combining a predictive model and a
contrastive learning module. This framework can reason about the interaction
patterns from the tactile data, hallucinate the changes in the environment,
estimate the uncertainty of the prediction, and generalize to unseen objects.
We also provide detailed ablation studies regarding different system designs as
well as visualizations of the predicted trajectories. This work takes a step on
dynamics modeling in hand-object interactions from dense tactile sensing, which
opens the door for future applications in activity learning, human-computer
interactions, and imitation learning for robotics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HoHoNet: 360 Indoor Holistic Understanding with Latent Horizontal Features. (arXiv:2011.11498v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11498">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present HoHoNet, a versatile and efficient framework for holistic
understanding of an indoor 360-degree panorama using a Latent Horizontal
Feature (LHFeat). The compact LHFeat flattens the features along the vertical
direction and has shown success in modeling per-column modality for room layout
reconstruction. HoHoNet advances in two important aspects. First, the deep
architecture is redesigned to run faster with improved accuracy. Second, we
propose a novel horizon-to-dense module, which relaxes the per-column output
shape constraint, allowing per-pixel dense prediction from LHFeat. HoHoNet is
fast: It runs at 52 FPS and 110 FPS with ResNet-50 and ResNet-34 backbones
respectively, for modeling dense modalities from a high-resolution $512 \times
1024$ panorama. HoHoNet is also accurate. On the tasks of layout estimation and
semantic segmentation, HoHoNet achieves results on par with current
state-of-the-art. On dense depth estimation, HoHoNet outperforms all the prior
arts by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image-to-image (i2i) networks struggle to capture local changes because they
do not affect the global scene structure. For example, translating from highway
scenes to offroad, i2i networks easily focus on global color features but
ignore obvious traits for humans like the absence of lane markings. In this
paper, we leverage human knowledge about spatial domain characteristics which
we refer to as &#x27;local domains&#x27; and demonstrate its benefit for image-to-image
translation. Relying on a simple geometrical guidance, we train a patch-based
GAN on few source data and hallucinate a new unseen domain which subsequently
eases transfer learning to target. We experiment on three tasks ranging from
unstructured environments to adverse weather. Our comprehensive evaluation
setting shows we are able to generate realistic translations, with minimal
priors, and training only on a few images. Furthermore, when trained on our
translations images we show that all tested proxy tasks are significantly
improved, without ever seeing target domain at training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adversarial Attacks are Reversible with Natural Supervision. (arXiv:2103.14222v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14222">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We find that images contain intrinsic structure that enables the reversal of
many adversarial attacks. Attack vectors cause not only image classifiers to
fail, but also collaterally disrupt incidental structure in the image. We
demonstrate that modifying the attacked image to restore the natural structure
will reverse many types of attacks, providing a defense. Experiments
demonstrate significantly improved robustness for several state-of-the-art
models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results
show that our defense is still effective even if the attacker is aware of the
defense mechanism. Since our defense is deployed during inference instead of
training, it is compatible with pre-trained networks as well as most other
defenses. Our results suggest deep networks are vulnerable to adversarial
examples partly because their representations do not enforce the natural
structure of images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single Image 3D Object Estimation with Primitive Graph Networks. (arXiv:2109.04153v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04153">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reconstructing 3D object from a single image (RGB or depth) is a fundamental
problem in visual scene understanding and yet remains challenging due to its
ill-posed nature and complexity in real-world scenes. To address those
challenges, we adopt a primitive-based representation for 3D object, and
propose a two-stage graph network for primitive-based 3D object estimation,
which consists of a sequential proposal module and a graph reasoning module.
Given a 2D image, our proposal module first generates a sequence of 3D
primitives from input image with local feature attention. Then the graph
reasoning module performs joint reasoning on a primitive graph to capture the
global shape context for each primitive. Such a framework is capable of taking
into account rich geometry and semantic constraints during 3D structure
recovery, producing 3D objects with more coherent structure even under
challenging viewing conditions. We train the entire graph neural network in a
stage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and
NYU Depth V2. Extensive experiments show that our approach outperforms the
previous state of the arts with a considerable margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LightSAL: Lightweight Sign Agnostic Learning for Implicit Surface Representation. (arXiv:2103.14273v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14273">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, several works have addressed modeling of 3D shapes using deep
neural networks to learn implicit surface representations. Up to now, the
majority of works have concentrated on reconstruction quality, paying little or
no attention to model size or training time. This work proposes LightSAL, a
novel deep convolutional architecture for learning 3D shapes; the proposed work
concentrates on efficiency both in network training time and resulting model
size. We build on the recent concept of Sign Agnostic Learning for training the
proposed network, relying on signed distance fields, with unsigned distance as
ground truth. In the experimental section of the paper, we demonstrate that the
proposed architecture outperforms previous work in model size and number of
required training iterations, while achieving equivalent accuracy. Experiments
are based on the D-Faust dataset that contains 41k 3D scans of human shapes.
The proposed model has been implemented in PyTorch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">M5Product: A Multi-modal Pretraining Benchmark for E-commercial Product Downstream Tasks. (arXiv:2109.04275v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04275">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we aim to advance the research of multi-modal pre-training on
E-commerce and subsequently contribute a large-scale dataset, named M5Product,
which consists of over 6 million multimodal pairs, covering more than 6,000
categories and 5,000 attributes. Generally, existing multi-modal datasets are
either limited in scale or modality diversity. Differently, our M5Product is
featured from the following aspects. First, the M5Product dataset is 500 times
larger than the public multimodal dataset with the same number of modalities
and nearly twice larger compared with the largest available text-image
cross-modal dataset. Second, the dataset contains rich information of multiple
modalities including image, text, table, video and audio, in which each
modality can capture different views of semantic information (e.g. category,
attributes, affordance, brand, preference) and complements the other. Third, to
better accommodate with real-world problems, a few portion of M5Product
contains incomplete modality pairs and noises while having the long-tailed
distribution, which aligns well with real-world scenarios. Finally, we provide
a baseline model M5-MMT that makes the first attempt to integrate the different
modality configuration into an unified model for feature fusion to address the
great challenge for semantic alignment. We also evaluate various multi-model
pre-training state-of-the-arts for benchmarking their capabilities in learning
from unlabeled data under the different number of modalities on the M5Product
dataset. We conduct extensive experiments on four downstream tasks and provide
some interesting findings on these modalities. Our dataset and related code are
available at https://xiaodongsuper.github.io/M5Product_dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Copy-Move Image Forgery Detection Based on Evolving Circular Domains Coverage. (arXiv:2109.04381v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The aim of this paper is to improve the accuracy of copy-move forgery
detection (CMFD) in image forensics by proposing a novel scheme. The proposed
scheme integrates both block-based and keypoint-based forgery detection
methods. Firstly, speed-up robust feature (SURF) descriptor in log-polar space
and scale invariant feature transform (SIFT) descriptor are extracted from an
entire forged image. Secondly, generalized 2 nearest neighbor (g2NN) is
employed to get massive matched pairs. Then, random sample consensus (RANSAC)
algorithm is employed to filter out mismatched pairs, thus allowing rough
localization of the counterfeit areas. To present more accurately these forgery
areas more accurately, we propose an efficient and accurate algorithm, evolving
circular domains coverage (ECDC), to cover present them. This algorithm aims to
find satisfactory threshold areas by extracting block features from jointly
evolving circular domains, which are centered on the matched pairs. Finally,
morphological operation is applied to refine the detected forgery areas. The
experimental results indicate that the proposed CMFD scheme can achieve better
detection performance under various attacks compared with other
state-of-the-art CMFD schemes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Correctly capturing intraoperative brain shift in image-guided neurosurgical
procedures is a critical task for aligning preoperative data with
intraoperative geometry, ensuring effective surgical navigation and optimal
surgical precision. While the finite element method (FEM) is a proven technique
to effectively approximate soft tissue deformation through biomechanical
formulations, their degree of success boils down to a trade-off between
accuracy and speed. To circumvent this problem, the most recent works in this
domain have proposed leveraging data-driven models obtained by training various
machine learning algorithms, e.g. random forests, artificial neural networks
(ANNs), with the results of finite element analysis (FEA) to speed up tissue
deformation approximations by prediction. These methods, however, do not
account for the structure of the finite element (FE) mesh during training that
provides information on node connectivities as well as the distance between
them, which can aid with approximating tissue deformation based on the
proximity of force load points with the rest of the mesh nodes. Therefore, this
work proposes a novel framework, PhysGNN, a data-driven model that approximates
the solution of FEA by leveraging graph neural networks (GNNs), which are
capable of accounting for the mesh structural information and inductive
learning over unstructured grids and complex topological structures.
Empirically, we demonstrate that the proposed architecture, PhysGNN, promises
accurate and fast soft tissue deformation approximations while remaining
computationally feasible, suitable for neurosurgical settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14166">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Indoor panorama typically consists of human-made structures parallel or
perpendicular to gravity. We leverage this phenomenon to approximate the scene
in a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this
end, we propose an effective divide-and-conquer strategy that divides pixels
based on their plane orientation estimation; then, the succeeding instance
segmentation module conquers the task of planes clustering more easily in each
plane orientation group. Besides, parameters of V-planes depend on camera yaw
rotation, but translation-invariant CNNs are less aware of the yaw change. We
thus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We
create a benchmark for indoor panorama planar reconstruction by extending
existing 360 depth datasets with ground truth H\&amp;V-planes (referred to as
PanoH&amp;V dataset) and adopt state-of-the-art planar reconstruction methods to
predict H\&amp;V-planes as our baselines. Our method outperforms the baselines by a
large margin on the proposed dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Energy Attack: On Transferring Adversarial Examples. (arXiv:2109.04300v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04300">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work we propose Energy Attack, a transfer-based black-box
$L_\infty$-adversarial attack. The attack is parameter-free and does not
require gradient approximation. In particular, we first obtain white-box
adversarial perturbations of a surrogate model and divide these perturbations
into small patches. Then we extract the unit component vectors and eigenvalues
of these patches with principal component analysis (PCA). Base on the
eigenvalues, we can model the energy distribution of adversarial perturbations.
We then perform black-box attacks by sampling from the perturbation patches
according to their energy distribution, and tiling the sampled patches to form
a full-size adversarial perturbation. This can be done without the available
access to victim models. Extensive experiments well demonstrate that the
proposed Energy Attack achieves state-of-the-art performance in black-box
attacks on various models and several datasets. Moreover, the extracted
distribution is able to transfer among different model architectures and
different datasets, and is therefore intrinsic to vision architectures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ConvMLP: Hierarchical Convolutional MLPs for Vision. (arXiv:2109.04454v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04454">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>MLP-based architectures, which consist of a sequence of consecutive
multi-layer perceptron blocks, have recently been found to reach comparable
results to convolutional and transformer-based methods. However, most adopt
spatial MLPs which take fixed dimension inputs, therefore making it difficult
to apply them to downstream tasks, such as object detection and semantic
segmentation. Moreover, single-stage designs further limit performance in other
computer vision tasks and fully connected layers bear heavy computation. To
tackle these problems, we propose ConvMLP: a hierarchical Convolutional MLP for
visual recognition, which is a light-weight, stage-wise, co-design of
convolution layers, and MLPs. In particular, ConvMLP-S achieves 76.8% top-1
accuracy on ImageNet-1k with 9M parameters and 2.4G MACs (15% and 19% of
MLP-Mixer-B/16, respectively). Experiments on object detection and semantic
segmentation further show that visual representation learned by ConvMLP can be
seamlessly transferred and achieve competitive results with fewer parameters.
Our code and pre-trained models are publicly available at
https://github.com/SHI-Labs/Convolutional-MLPs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Label Efficient Visual Abstractions for Autonomous Driving. (arXiv:2005.10091v2 [cs.CV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.10091">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>It is well known that semantic segmentation can be used as an effective
intermediate representation for learning driving policies. However, the task of
street scene semantic segmentation requires expensive annotations. Furthermore,
segmentation algorithms are often trained irrespective of the actual driving
task, using auxiliary image-space loss functions which are not guaranteed to
maximize driving metrics such as safety or distance traveled per intervention.
In this work, we seek to quantify the impact of reducing segmentation
annotation costs on learned behavior cloning agents. We analyze several
segmentation-based intermediate representations. We use these visual
abstractions to systematically study the trade-off between annotation
efficiency and driving performance, i.e., the types of classes labeled, the
number of image samples used to learn the visual abstraction model, and their
granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers
several practical insights into how segmentation-based visual abstractions can
be exploited in a more label efficient manner. Surprisingly, we find that
state-of-the-art driving performance can be achieved with orders of magnitude
reduction in annotation cost. Beyond label efficiency, we find several
additional training benefits when leveraging visual abstractions, such as a
significant reduction in the variance of the learned policy when compared to
state-of-the-art end-to-end driving models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04386">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve
the network performance significantly compared to the widely used activations
like ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have
5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in
CIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on
PreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04422">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),
requires an alignment of semantic concepts across domains. Despite the
widespread success of end-to-end learning, today&#x27;s multimodal pipelines by and
large leverage pre-extracted, fixed features from object detectors, typically
Faster R-CNN, as representations of the visual world. The obvious downside is
that the visual representation is not specifically tuned to the multimodal task
at hand. At the same time, while transformer-based object detectors have gained
popularity, they have not been employed in today&#x27;s multimodal pipelines. We
address both shortcomings with TxT, a transformer-based crossmodal pipeline
that enables fine-tuning both language and visual components on the downstream
task in a fully end-to-end manner. We overcome existing limitations of
transformer-based detectors for multimodal reasoning regarding the integration
of global context and their scalability. Our transformer-based multimodal model
achieves considerable gains from end-to-end learning for multimodal question
answering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ISCL: Interdependent Self-Cooperative Learning for Unpaired Image Denoising. (arXiv:2102.09858v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the advent of advances in self-supervised learning, paired clean-noisy
data are no longer required in deep learning-based image denoising. However,
existing blind denoising methods still require the assumption with regard to
noise characteristics, such as zero-mean noise distribution and pixel-wise
noise-signal independence; this hinders wide adaptation of the method in the
medical domain. On the other hand, unpaired learning can overcome limitations
related to the assumption on noise characteristics, which makes it more
feasible for collecting the training data in real-world scenarios. In this
paper, we propose a novel image denoising scheme, Interdependent
Self-Cooperative Learning (ISCL), that leverages unpaired learning by combining
cyclic adversarial learning with self-supervised residual learning. Unlike the
existing unpaired image denoising methods relying on matching data
distributions in different domains, the two architectures in ISCL, designed for
different tasks, complement each other and boost the learning process. To
assess the performance of the proposed method, we conducted extensive
experiments in various biomedical image degradation scenarios, such as noise
caused by physical characteristics of electron microscopy (EM) devices (film
and charging noise), and structural noise found in low-dose computer tomography
(CT). We demonstrate that the image quality of our method is superior to
conventional and current state-of-the-art deep learning-based image denoising
methods, including supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Domain Generalizable Person Re-Identification. (arXiv:2108.05045v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05045">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing person re-identification (re-id) methods are stuck when deployed to
a new unseen scenario despite the success in cross-camera person matching.
Recent efforts have been substantially devoted to domain adaptive person re-id
where extensive unlabeled data in the new scenario are utilized in a
transductive learning manner. However, for each scenario, it is required to
first collect enough data and then train such a domain adaptive re-id model,
thus restricting their practical application. Instead, we aim to explore
multiple labeled datasets to learn generalized domain-invariant representations
for person re-id, which is expected universally effective for each new-coming
re-id scenario. To pursue practicability in real-world systems, we collect all
the person re-id datasets (20 datasets) in this field and select the three most
frequently used datasets (i.e., Market1501, DukeMTMC, and MSMT17) as unseen
target domains. In addition, we develop DataHunter that collects over 300K+
weak annotated images named YouTube-Human from YouTube street-view videos,
which joins 17 remaining full labeled datasets to form multiple source domains.
On such a large and challenging benchmark called FastHuman (~440K+ labeled
images), we further propose a simple yet effective Semi-Supervised Knowledge
Distillation (SSKD) framework. SSKD effectively exploits the weakly annotated
data by assigning soft pseudo labels to YouTube-Human to improve models&#x27;
generalization ability. Experiments on several protocols verify the
effectiveness of the proposed SSKD framework on domain generalizable person
re-id, which is even comparable to supervised learning on the target domains.
Lastly, but most importantly, we hope the proposed benchmark FastHuman could
bring the next development of domain generalizable person re-id algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Deep Metric Learning by Divide and Conquer. (arXiv:2109.04003v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04003">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep metric learning (DML) is a cornerstone of many computer vision
applications. It aims at learning a mapping from the input domain to an
embedding space, where semantically similar objects are located nearby and
dissimilar objects far from another. The target similarity on the training data
is defined by user in form of ground-truth class labels. However, while the
embedding space learns to mimic the user-provided similarity on the training
data, it should also generalize to novel categories not seen during training.
Besides user-provided groundtruth training labels, a lot of additional visual
factors (such as viewpoint changes or shape peculiarities) exist and imply
different notions of similarity between objects, affecting the generalization
on the images unseen during training. However, existing approaches usually
directly learn a single embedding space on all available training data,
struggling to encode all different types of relationships, and do not
generalize well. We propose to build a more expressive representation by
jointly splitting the embedding space and the data hierarchically into smaller
sub-parts. We successively focus on smaller subsets of the training data,
reducing its variance and learning a different embedding subspace for each data
subset. Moreover, the subspaces are learned jointly to cover not only the
intricacies, but the breadth of the data as well. Only after that, we build the
final embedding from the subspaces in the conquering stage. The proposed
algorithm acts as a transparent wrapper that can be placed around arbitrary
existing DML methods. Our approach significantly improves upon the
state-of-the-art on image retrieval, clustering, and re-identification tasks
evaluated using CUB200-2011, CARS196, Stanford Online Products, In-shop
Clothes, and PKU VehicleID datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04448">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained vision-and-language BERTs aim to learn representations that
combine information from both modalities. We propose a diagnostic method based
on cross-modal input ablation to assess the extent to which these models
actually integrate cross-modal information. This method involves ablating
inputs from one modality, either entirely or selectively based on cross-modal
grounding alignments, and evaluating the model prediction performance on the
other modality. Model performance is measured by modality-specific tasks that
mirror the model pretraining objectives (e.g. masked language modelling for
text). Models that have learned to construct cross-modal representations using
both modalities are expected to perform worse when inputs are missing from a
modality. We find that recently proposed models have much greater relative
difficulty predicting text when visual information is ablated, compared to
predicting visual object categories when text is ablated, indicating that these
models are not symmetrically cross-modal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Transferable Adversarial Attacks on Vision Transformers. (arXiv:2109.04176v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04176">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vision transformers (ViTs) have demonstrated impressive performance on a
series of computer vision tasks, yet they still suffer from adversarial
examples. In this paper, we posit that adversarial attacks on transformers
should be specially tailored for their architecture, jointly considering both
patches and self-attention, in order to achieve high transferability. More
specifically, we introduce a dual attack framework, which contains a Pay No
Attention (PNA) attack and a PatchOut attack, to improve the transferability of
adversarial samples across different ViTs. We show that skipping the gradients
of attention during backpropagation can generate adversarial examples with high
transferability. In addition, adversarial perturbations generated by optimizing
randomly sampled subsets of patches at each iteration achieve higher attack
success rates than attacks using all patches. We evaluate the transferability
of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The
results of these experiments demonstrate that the proposed dual attack can
greatly boost transferability between ViTs and from ViTs to CNNs. In addition,
the proposed method can easily be combined with existing transfer methods to
boost performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Talk-to-Edit: Fine-Grained Facial Editing via Dialog. (arXiv:2109.04425v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04425">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Facial editing is an important task in vision and graphics with numerous
applications. However, existing works are incapable to deliver a continuous and
fine-grained editing mode (e.g., editing a slightly smiling face to a big
laughing one) with natural interactions with users. In this work, we propose
Talk-to-Edit, an interactive facial editing framework that performs
fine-grained attribute manipulation through dialog between the user and the
system. Our key insight is to model a continual &quot;semantic field&quot; in the GAN
latent space. 1) Unlike previous works that regard the editing as traversing
straight lines in the latent space, here the fine-grained editing is formulated
as finding a curving trajectory that respects fine-grained attribute landscape
on the semantic field. 2) The curvature at each step is location-specific and
determined by the input image as well as the users&#x27; language requests. 3) To
engage the users in a meaningful dialog, our system generates language feedback
by considering both the user request and the current state of the semantic
field.

We also contribute CelebA-Dialog, a visual-language facial editing dataset to
facilitate large-scale study. Specifically, each image has manually annotated
fine-grained attribute annotations as well as template-based textual
descriptions in natural language. Extensive quantitative and qualitative
experiments demonstrate the superiority of our framework in terms of 1) the
smoothness of fine-grained editing, 2) the identity/attribute preservation, and
3) the visual photorealism and dialog fluency. Notably, user study validates
that our overall system is consistently favored by around 80% of the
participants. Our project page is https://www.mmlab-ntu.com/project/talkedit/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tiny CNN for feature point description for document analysis: approach and dataset. (arXiv:2109.04134v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04134">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we study the problem of feature points description in the
context of document analysis and template matching. Our study shows that the
specific training data is required for the task especially if we are to train a
lightweight neural network that will be usable on devices with limited
computational resources. In this paper, we construct and provide a dataset with
a method of training patches retrieval. We prove the effectiveness of this data
by training a lightweight neural network and show how it performs in both
documents and general patches matching. The training was done on the provided
dataset in comparison with HPatches training dataset and for the testing we use
HPatches testing framework and two publicly available datasets with various
documents pictured on complex backgrounds: MIDV-500 and MIDV-2019.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self Supervision to Distillation for Long-Tailed Visual Recognition. (arXiv:2109.04075v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04075">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning has achieved remarkable progress for visual recognition on
large-scale balanced datasets but still performs poorly on real-world
long-tailed data. Previous methods often adopt class re-balanced training
strategies to effectively alleviate the imbalance issue, but might be a risk of
over-fitting tail classes. The recent decoupling method overcomes over-fitting
issues by using a multi-stage training scheme, yet, it is still incapable of
capturing tail class information in the feature learning stage. In this paper,
we show that soft label can serve as a powerful solution to incorporate label
correlation into a multi-stage training scheme for long-tailed recognition. The
intrinsic relation between classes embodied by soft labels turns out to be
helpful for long-tailed recognition by transferring knowledge from head to tail
classes.

Specifically, we propose a conceptually simple yet particularly effective
multi-stage training scheme, termed as Self Supervised to Distillation (SSD).
This scheme is composed of two parts. First, we introduce a self-distillation
framework for long-tailed recognition, which can mine the label relation
automatically. Second, we present a new distillation label generation module
guided by self-supervision. The distilled labels integrate information from
both label and data domains that can model long-tailed distribution
effectively. We conduct extensive experiments and our method achieves the
state-of-the-art results on three long-tailed recognition benchmarks:
ImageNet-LT, CIFAR100-LT and iNaturalist 2018. Our SSD outperforms the strong
LWS baseline by from $2.7\%$ to $4.5\%$ on various datasets. The code is
available at https://github.com/MCG-NJU/SSD-LT.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions. (arXiv:2108.13055v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13055">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Visual recognition is currently one of the most important and active research
areas in computer vision, pattern recognition, and even the general field of
artificial intelligence. It has great fundamental importance and strong
industrial needs. Deep neural networks (DNNs) have largely boosted their
performances on many concrete tasks, with the help of large amounts of training
data and new powerful computation resources. Though recognition accuracy is
usually the first concern for new progresses, efficiency is actually rather
important and sometimes critical for both academic research and industrial
applications. Moreover, insightful views on the opportunities and challenges of
efficiency are also highly required for the entire community. While general
surveys on the efficiency issue of DNNs have been done from various
perspectives, as far as we are aware, scarcely any of them focused on visual
recognition systematically, and thus it is unclear which progresses are
applicable to it and what else should be concerned. In this paper, we present
the review of the recent advances with our suggestions on the new possible
directions towards improving the efficiency of DNN-related visual recognition
approaches. We investigate not only from the model but also the data point of
view (which is not the case in existing surveys), and focus on three most
studied data types (images, videos and points). This paper attempts to provide
a systematic summary via a comprehensive survey which can serve as a valuable
reference and inspire both researchers and practitioners who work on visual
recognition problems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IFBiD: Inference-Free Bias Detection. (arXiv:2109.04374v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04374">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper is the first to explore an automatic way to detect bias in deep
convolutional neural networks by simply looking at their weights. Furthermore,
it is also a step towards understanding neural networks and how they work. We
show that it is indeed possible to know if a model is biased or not simply by
looking at its weights, without the model inference for an specific input. We
analyze how bias is encoded in the weights of deep networks through a toy
example using the Colored MNIST database and we also provide a realistic case
study in gender detection from face images using state-of-the-art methods and
experimental resources. To do so, we generated two databases with 36K and 48K
biased models each. In the MNIST models we were able to detect whether they
presented a strong or low bias with more than 99% accuracy, and we were also
able to classify between four levels of bias with more than 70% accuracy. For
the face models, we achieved 90% accuracy in distinguishing between models
biased towards Asian, Black, or Caucasian ethnicity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04335">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most recent semantic segmentation methods adopt a U-Net framework with an
encoder-decoder architecture. It is still challenging for U-Net with a simple
skip connection scheme to model the global multi-scale context: 1) Not each
skip connection setting is effective due to the issue of incompatible feature
sets of encoder and decoder stage, even some skip connection negatively
influence the segmentation performance; 2) The original U-Net is worse than the
one without any skip connection on some datasets. Based on our findings, we
propose a new segmentation framework, named UCTransNet (with a proposed CTrans
module in U-Net), from the channel perspective with attention mechanism.
Specifically, the CTrans module is an alternate of the U-Net skip connections,
which consists of a sub-module to conduct the multi-scale Channel Cross fusion
with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention
(named CCA) to guide the fused multi-scale channel-wise information to
effectively connect to the decoder features for eliminating the ambiguity.
Hence, the proposed connection consisting of the CCT and CCA is able to replace
the original skip connection to solve the semantic gaps for an accurate
automatic medical image segmentation. The experimental results suggest that our
UCTransNet produces more precise segmentation performance and achieves
consistent improvements over the state-of-the-art for semantic segmentation
across different datasets and conventional architectures involving transformer
or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multilingual Audio-Visual Smartphone Dataset And Evaluation. (arXiv:2109.04138v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04138">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Smartphones have been employed with biometric-based verification systems to
provide security in highly sensitive applications. Audio-visual biometrics are
getting popular due to the usability and also it will be challenging to spoof
because of multi-modal nature. In this work, we present an audio-visual
smartphone dataset captured in five different recent smartphones. This new
dataset contains 103 subjects captured in three different sessions considering
the different real-world scenarios. Three different languages are acquired in
this dataset to include the problem of language dependency of the speaker
recognition systems. These unique characteristics of this dataset will pave the
way to implement novel state-of-the-art unimodal or audio-visual speaker
recognition systems. We also report the performance of the bench-marked
biometric verification systems on our dataset. The robustness of biometric
algorithms is evaluated towards multiple dependencies like signal noise,
device, language and presentation attacks like replay and synthesized signals
with extensive experiments. The obtained results raised many concerns about the
generalization properties of state-of-the-art biometrics methods in
smartphones.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SORNet: Spatial Object-Centric Representations for Sequential Manipulation. (arXiv:2109.03891v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03891">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sequential manipulation tasks require a robot to perceive the state of an
environment and plan a sequence of actions leading to a desired goal state,
where the ability to reason about spatial relationships among object entities
from raw sensor inputs is crucial. Prior works relying on explicit state
estimation or end-to-end learning struggle with novel objects. In this work, we
propose SORNet (Spatial Object-Centric Representation Network), which extracts
object-centric representations from RGB images conditioned on canonical views
of the objects of interest. We show that the object embeddings learned by
SORNet generalize zero-shot to unseen object entities on three spatial
reasoning tasks: spatial relationship classification, skill precondition
classification and relative direction regression, significantly outperforming
baselines. Further, we present real-world robotic experiments demonstrating the
usage of the learned object embeddings in task planning for sequential
manipulation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OSSR-PID: One-Shot Symbol Recognition in P&amp;ID Sheets using Path Sampling and GCN. (arXiv:2109.03849v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03849">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Piping and Instrumentation Diagrams (P&amp;ID) are ubiquitous in several
manufacturing, oil and gas enterprises for representing engineering schematics
and equipment layout. There is an urgent need to extract and digitize
information from P&amp;IDs without the cost of annotating a varying set of symbols
for each new use case. A robust one-shot learning approach for symbol
recognition i.e., localization followed by classification, would therefore go a
long way towards this goal. Our method works by sampling pixels sequentially
along the different contour boundaries in the image. These sampled points form
paths which are used in the prototypical line diagram to construct a graph that
captures the structure of the contours. Subsequently, the prototypical graphs
are fed into a Dynamic Graph Convolutional Neural Network (DGCNN) which is
trained to classify graphs into one of the given symbol classes. Further, we
append embeddings from a Resnet-34 network which is trained on symbol images
containing sampled points to make the classification network more robust.
Since, many symbols in P&amp;ID are structurally very similar to each other, we
utilize Arcface loss during DGCNN training which helps in maximizing symbol
class separability by producing highly discriminative embeddings. The images
consist of components attached on the pipeline (straight line). The sampled
points segregated around the symbol regions are used for the classification
task. The proposed pipeline, named OSSR-PID, is fast and gives outstanding
performance for recognition of symbols on a synthetic dataset of 100 P&amp;ID
diagrams. We also compare our method against prior-work on a real-world private
dataset of 12 P&amp;ID sheets and obtain comparable/superior results. Remarkably,
it is able to achieve such excellent performance using only one prototypical
example per symbol.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal. (arXiv:2109.04284v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Deep learning models usually require a large amount of labeled data to
achieve satisfactory performance. In multimedia analysis, domain adaptation
studies the problem of cross-domain knowledge transfer from a label rich source
domain to a label scarce target domain, thus potentially alleviates the
annotation requirement for deep learning models. However, we find that
contemporary domain adaptation methods for cross-domain image understanding
perform poorly when source domain is noisy. Weakly Supervised Domain Adaptation
(WSDA) studies the domain adaptation problem under the scenario where source
data can be noisy. Prior methods on WSDA remove noisy source data and align the
marginal distribution across domains without considering the fine-grained
semantic structure in the embedding space, which have the problem of class
misalignment, e.g., features of cats in the target domain might be mapped near
features of dogs in the source domain. In this paper, we propose a novel
method, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we
adopt the cluster assumption and learn cluster discriminatively with class
prototypes in the embedding space. We propose to leverage the location
information of the data points in the embedding space and model the location
information with a Gaussian mixture model to identify noisy source data. We
then design a network which incorporates the Gaussian mixture noise model as a
sub-module for unsupervised noise removal and propose a novel cluster-level
adversarial adaptation method which aligns unlabeled target data with the less
noisy class prototypes for mapping the semantic structure across domains. We
conduct extensive experiments to evaluate the effectiveness of our method on
both general images and medical images from COVID-19 and e-commerce datasets.
The results show that our method significantly outperforms state-of-the-art
WSDA methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-grained Data Distribution Alignment for Post-Training Quantization. (arXiv:2109.04186v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04186">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While post-training quantization receives popularity mostly due to its
evasion in accessing the original complete training dataset, its poor
performance also stems from this limitation. To alleviate this limitation, in
this paper, we leverage the synthetic data introduced by zero-shot quantization
with calibration dataset and we propose a fine-grained data distribution
alignment (FDDA) method to boost the performance of post-training quantization.
The method is based on two important properties of batch normalization
statistics (BNS) we observed in deep layers of the trained network, i.e.,
inter-class separation and intra-class incohesion. To preserve this
fine-grained distribution information: 1) We calculate the per-class BNS of the
calibration dataset as the BNS centers of each class and propose a
BNS-centralized loss to force the synthetic data distributions of different
classes to be close to their own centers. 2) We add Gaussian noise into the
centers to imitate the incohesion and propose a BNS-distorted loss to force the
synthetic data distribution of the same class to be close to the distorted
centers. By introducing these two fine-grained losses, our method shows the
state-of-the-art performance on ImageNet, especially when the first and last
layers are quantized to low-bit as well. Our project is available at
https://github.com/viperit/FDDA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Hough Voting for Robust Global Registration. (arXiv:2109.04310v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04310">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Point cloud registration is the task of estimating the rigid transformation
that aligns a pair of point cloud fragments. We present an efficient and robust
framework for pairwise registration of real-world 3D scans, leveraging Hough
voting in the 6D transformation parameter space. First, deep geometric features
are extracted from a point cloud pair to compute putative correspondences. We
then construct a set of triplets of correspondences to cast votes on the 6D
Hough space, representing the transformation parameters in sparse tensors.
Next, a fully convolutional refinement module is applied to refine the noisy
votes. Finally, we identify the consensus among the correspondences from the
Hough space, which we use to predict our final transformation parameters. Our
method outperforms state-of-the-art methods on 3DMatch and 3DLoMatch benchmarks
while achieving comparable performance on KITTI odometry dataset. We further
demonstrate the generalizability of our approach by setting a new
state-of-the-art on ICL-NUIM dataset, where we integrate our module into a
multi-way registration pipeline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Tensor Network Representation for High-Order Tensor Completion. (arXiv:2109.04022v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work studies the problem of high-dimensional data (referred to tensors)
completion from partially observed samplings. We consider that a tensor is a
superposition of multiple low-rank components. In particular, each component
can be represented as multilinear connections over several latent factors and
naturally mapped to a specific tensor network (TN) topology. In this paper, we
propose a fundamental tensor decomposition (TD) framework: Multi-Tensor Network
Representation (MTNR), which can be regarded as a linear combination of a range
of TD models, e.g., CANDECOMP/PARAFAC (CP) decomposition, Tensor Train (TT),
and Tensor Ring (TR). Specifically, MTNR represents a high-order tensor as the
addition of multiple TN models, and the topology of each TN is automatically
generated instead of manually pre-designed. For the optimization phase, an
adaptive topology learning (ATL) algorithm is presented to obtain latent
factors of each TN based on a rank incremental strategy and a projection error
measurement strategy. In addition, we theoretically establish the fundamental
multilinear operations for the tensors with TN representation, and reveal the
structural transformation of MTNR to a single TN. Finally, MTNR is applied to a
typical task, tensor completion, and two effective algorithms are proposed for
the exact recovery of incomplete data based on the Alternating Least Squares
(ALS) scheme and Alternating Direction Method of Multiplier (ADMM) framework.
Extensive numerical experiments on synthetic data and real-world datasets
demonstrate the effectiveness of MTNR compared with the start-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HSMD: An object motion detection algorithm using a Hybrid Spiking Neural Network Architecture. (arXiv:2109.04119v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The detection of moving objects is a trivial task performed by vertebrate
retinas, yet a complex computer vision task. Object-motion-sensitive ganglion
cells (OMS-GC) are specialised cells in the retina that sense moving objects.
OMS-GC take as input continuous signals and produce spike patterns as output,
that are transmitted to the Visual Cortex via the optic nerve. The Hybrid
Sensitive Motion Detector (HSMD) algorithm proposed in this work enhances the
GSOC dynamic background subtraction (DBS) algorithm with a customised 3-layer
spiking neural network (SNN) that outputs spiking responses akin to the OMS-GC.
The algorithm was compared against existing background subtraction (BS)
approaches, available on the OpenCV library, specifically on the 2012 change
detection (CDnet2012) and the 2014 change detection (CDnet2014) benchmark
datasets. The results show that the HSMD was ranked overall first among the
competing approaches and has performed better than all the other algorithms on
four of the categories across all the eight test metrics. Furthermore, the HSMD
proposed in this paper is the first to use an SNN to enhance an existing state
of the art DBS (GSOC) algorithm and the results demonstrate that the SNN
provides near real-time performance in realistic applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Application of the Singular Spectrum Analysis on electroluminescence images of thin-film photovoltaic modules. (arXiv:2109.04048v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper discusses an application of the singular spectrum analysis method
(SSA) in the context of electroluminescence (EL) images of thin-film
photovoltaic (PV) modules. We propose an EL image decomposition as a sum of
three components: global intensity, cell, and aperiodic components. A
parametric model of the extracted signal is used to perform several image
processing tasks. The cell component is used to identify interconnection lines
between PV cells at sub-pixel accuracy, as well as to correct incorrect
stitching of EL images. Furthermore, an explicit expression of the cell
component signal is used to estimate the inverse characteristic length, a
physical parameter related to the resistances in a PV module.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Fully Automated Segmentation of Rat Cardiac MRI by Leveraging Deep Learning Frameworks. (arXiv:2109.04188v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04188">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automated segmentation of human cardiac magnetic resonance datasets has been
steadily improving during recent years. However, these methods are not directly
applicable in preclinical context due to limited datasets and lower image
resolution. Successful application of deep architectures for rat cardiac
segmentation, although of critical importance for preclinical evaluation of
cardiac function, has to our knowledge not yet been reported. We developed
segmentation models that expand on the standard U-Net architecture and
evaluated separate models for systole and diastole phases, 2MSA, and one model
for all timepoints, 1MSA. Furthermore, we calibrated model outputs using a
Gaussian Process (GP)-based prior to improve phase selection. Resulting models
approach human performance in terms of left ventricular segmentation quality
and ejection fraction (EF) estimation in both 1MSA and 2MSA settings
(S{\o}rensen-Dice score 0.91 +/- 0.072 and 0.93 +/- 0.032, respectively). 2MSA
achieved a mean absolute difference between estimated and reference EF of 3.5
+/- 2.5 %, while 1MSA resulted in 4.1 +/- 3.0 %. Applying Gaussian Processes to
1MSA allows to automate the selection of systole and diastole phases. Combined
with a novel cardiac phase selection strategy, our work presents an important
first step towards a fully automated segmentation pipeline in the context of
rat cardiac analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automated LoD-2 Model Reconstruction from Very-HighResolution Satellite-derived Digital Surface Model and Orthophoto. (arXiv:2109.03876v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a model-driven method that reconstructs LoD-2
building models following a &quot;decomposition-optimization-fitting&quot; paradigm. The
proposed method starts building detection results through a deep learning-based
detector and vectorizes individual segments into polygons using a &quot;three-step&quot;
polygon extraction method, followed by a novel grid-based decomposition method
that decomposes the complex and irregularly shaped building polygons to tightly
combined elementary building rectangles ready to fit elementary building
models. We have optionally introduced OpenStreetMap (OSM) and Graph-Cut (GC)
labeling to further refine the orientation of 2D building rectangle. The 3D
modeling step takes building-specific parameters such as hip lines, as well as
non-rigid and regularized transformations to optimize the flexibility for using
a minimal set of elementary models. Finally, roof type of building models s
refined and adjacent building models in one building segment are merged into
the complex polygonal model. Our proposed method has addressed a few technical
caveats over existing methods, resulting in practically high-quality results,
based on our evaluation and comparative study on a diverse set of experimental
datasets of cities with different urban patterns.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Cross-Scale Visual Representations for Real-Time Image Geo-Localization. (arXiv:2109.04087v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04087">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robot localization remains a challenging task in GPS denied environments.
State estimation approaches based on local sensors, e.g. cameras or IMUs, are
drifting-prone for long-range missions as error accumulates. In this study, we
aim to address this problem by localizing image observations in a 2D
multi-modal geospatial map. We introduce the cross-scale dataset and a
methodology to produce additional data from cross-modality sources. We propose
a framework that learns cross-scale visual representations without supervision.
Experiments are conducted on data from two different domains, underwater and
aerial. In contrast to existing studies in cross-view image geo-localization,
our approach a) performs better on smaller-scale multi-modal maps; b) is more
computationally efficient for real-time applications; c) can serve directly in
concert with state estimation pipelines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Taming Self-Supervised Learning for Presentation Attack Detection: In-Image De-Folding and Out-of-Image De-Mixing. (arXiv:2109.04100v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04100">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Biometric systems are vulnerable to the Presentation Attacks (PA) performed
using various Presentation Attack Instruments (PAIs). Even though there are
numerous Presentation Attack Detection (PAD) techniques based on both deep
learning and hand-crafted features, the generalization of PAD for unknown PAI
is still a challenging problem. The common problem with existing deep
learning-based PAD techniques is that they may struggle with local optima,
resulting in weak generalization against different PAs. In this work, we
propose to use self-supervised learning to find a reasonable initialization
against local trap, so as to improve the generalization ability in detecting
PAs on the biometric system.The proposed method, denoted as IF-OM, is based on
a global-local view coupled with De-Folding and De-Mixing to derive the
task-specific representation for PAD.During De-Folding, the proposed technique
will learn region-specific features to represent samples in a local pattern by
explicitly maximizing cycle consistency. While, De-Mixing drives detectors to
obtain the instance-specific features with global information for more
comprehensive representation by maximizing topological consistency. Extensive
experimental results show that the proposed method can achieve significant
improvements in terms of both face and fingerprint PAD in more complicated and
hybrid datasets, when compared with the state-of-the-art methods. Specifically,
when training in CASIA-FASD and Idiap Replay-Attack, the proposed method can
achieve 18.60% Equal Error Rate (EER) in OULU-NPU and MSU-MFSD, exceeding
baseline performance by 9.54%. Code will be made publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Representation Learning for Trip Recommendation. (arXiv:2109.00968v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00968">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Trip recommendation is a significant and engaging location-based service that
can help new tourists make more customized travel plans. It often attempts to
suggest a sequence of point of interests (POIs) for a user who requests a
personalized travel demand. Conventional methods either leverage the heuristic
algorithms (e.g., dynamic programming) or statistical analysis (e.g., Markov
models) to search or rank a POI sequence. These procedures may fail to capture
the diversity of human needs and transitional regularities. They even provide
recommendations that deviate from tourists&#x27; real travel intention when the trip
data is sparse. Although recent deep recursive models (e.g., RNN) are capable
of alleviating these concerns, existing solutions hardly recognize the
practical reality, such as the diversity of tourist demands, uncertainties in
the trip generation, and the complex visiting preference. Inspired by the
advance in deep learning, we introduce a novel self-supervised representation
learning framework for trip recommendation -- SelfTrip, aiming at tackling the
aforementioned challenges. Specifically, we propose a two-step contrastive
learning mechanism concerning the POI representation, as well as trip
representation. Furthermore, we present four trip augmentation methods to
capture the visiting uncertainties in trip planning. We evaluate our SelfTrip
on four real-world datasets, and extensive results demonstrate the promising
gain compared with several cutting-edge benchmarks, e.g., up to 4% and 12% on
F1 and pair-F1, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation. (arXiv:2108.10511v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10511">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Practical recommender systems experience a cold-start problem when observed
user-item interactions in the history are insufficient. Meta learning,
especially gradient based one, can be adopted to tackle this problem by
learning initial parameters of the model and thus allowing fast adaptation to a
specific task from limited data examples. Though with significant performance
improvement, it commonly suffers from two critical issues: the
non-compatibility with mainstream industrial deployment and the heavy
computational burdens, both due to the inner-loop gradient operation. These two
issues make them hard to be applied in practical recommender systems. To enjoy
the benefits of meta learning framework and mitigate these problems, we propose
a recommendation framework called Contextual Modulation Meta Learning (CMML).
CMML is composed of fully feed-forward operations so it is computationally
efficient and completely compatible with the mainstream industrial deployment.
CMML consists of three components, including a context encoder that can
generate context embedding to represent a specific task, a hybrid context
generator that aggregates specific user-item features with task-level context,
and a contextual modulation network, which can modulate the recommendation
model to adapt effectively. We validate our approach on both scenario-specific
and user-specific cold-start setting on various real-world datasets, showing
CMML can achieve comparable or even better performance with gradient based
methods yet with much higher computational efficiency and better
interpretability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Compression Network with Transformer for Approximate Nearest Neighbor Search. (arXiv:2107.14415v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14415">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a generic feature compression method for Approximate Nearest
Neighbor Search (ANNS) problems, which speeds up existing ANNS methods in a
plug-and-play manner. Specifically, we propose a new network structure called
Compression Network with Transformer (CNT) to compress the feature into a low
dimensional space, and an inhomogeneous neighborhood relationship preserving
(INRP) loss that aims to maintain high search accuracy. In CNT, we use multiple
compression projections to cast the feature into many low dimensional spaces,
and then use transformer to globally optimize these projections such that the
features are well compressed following the guidance from our loss function. The
loss function is designed to assign high weights on point pairs that are close
in original feature space, and keep their distances in projected space. Keeping
these distances helps maintain the eventual top-k retrieval accuracy, and down
weighting others creates room for feature compression. In experiments, we run
our compression method on public datasets, and use the compressed features in
graph based, product quantization and scalar quantization based ANNS solutions.
Experimental results show that our compression method can significantly improve
the efficiency of these methods while preserves or even improves search
accuracy, suggesting its broad potential impact on real world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Gumbel-based activation function for imbalanced datasets. (arXiv:2012.05009v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05009">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Rating prediction is a core problem in recommender systems to quantify users
preferences towards different items. Due to the imbalanced rating distributions
in training data, existing recommendation methods suffer from the biased
prediction problem that generates biased prediction results. Thus, their
performance on predicting ratings which rarely appear in training data is
unsatisfactory. In this paper, inspired by the superior capability of Extreme
Value Distribution (EVD)-based methods in modeling the distribution of rare
data, we propose a novel \underline{\emph{G}}umbel Distribution-based
\underline{\emph{R}}ating \underline{\emph{P}}rediction framework (GRP) which
can accurately predict both frequent and rare ratings between users and items.
In our approach, we first define different Gumbel distributions for each rating
level, which can be learned by historical rating statistics of users and items.
Second, we incorporate the Gumbel-based representations of users and items with
their original representations learned from the rating matrix and/or reviews to
enrich the representations of users and items via a proposed multi-scale
convolutional fusion layer. Third, we propose a data-driven rating prediction
module to predict the ratings of user-item pairs. It&#x27;s worthy to note that our
approach can be readily applied to existing recommendation methods for
addressing their biased prediction problem. To verify the effectiveness of GRP,
we conduct extensive experiments on eight benchmark datasets. Compared with
several baseline models, the results show that: 1) GRP achieves
state-of-the-art overall performance on all eight datasets; 2) GRP makes a
substantial improvement in predicting rare ratings, which shows the
effectiveness of our model in addressing the bias prediction problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mining Points of Interest via Address Embeddings: An Unsupervised Approach. (arXiv:2109.04467v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04467">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Digital maps are commonly used across the globe for exploring places that
users are interested in, commonly referred to as points of interest (PoI). In
online food delivery platforms, PoIs could represent any major private
compounds where customers could order from such as hospitals, residential
complexes, office complexes, educational institutes and hostels. In this work,
we propose an end-to-end unsupervised system design for obtaining polygon
representations of PoIs (PoI polygons) from address locations and address
texts. We preprocess the address texts using locality names and generate
embeddings for the address texts using a deep learning-based architecture, viz.
RoBERTa, trained on our internal address dataset. The PoI candidates are
identified by jointly clustering the anonymised customer phone GPS locations
(obtained during address onboarding) and the embeddings of the address texts.
The final list of PoI polygons is obtained from these PoI candidates using
novel post-processing steps. This algorithm identified 74.8 % more PoIs than
those obtained using the Mummidi-Krumm baseline algorithm run on our internal
dataset. The proposed algorithm achieves a median area precision of 98 %, a
median area recall of 8 %, and a median F-score of 0.15. In order to improve
the recall of the algorithmic polygons, we post-process them using building
footprint polygons from the OpenStreetMap (OSM) database. The post-processing
algorithm involves reshaping the algorithmic polygon using intersecting
polygons and closed private roads from the OSM database, and accounting for
intersection with public roads on the OSM database. We achieve a median area
recall of 70 %, a median area precision of 69 %, and a median F-score of 0.69
on these post-processed polygons.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUINT: Node embedding using network hashing. (arXiv:2109.04206v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Representation learning using network embedding has received tremendous
attention due to its efficacy to solve downstream tasks. Popular embedding
methods (such as deepwalk, node2vec, LINE) are based on a neural architecture,
thus unable to scale on large networks both in terms of time and space usage.
Recently, we proposed BinSketch, a sketching technique for compressing binary
vectors to binary vectors. In this paper, we show how to extend BinSketch and
use it for network hashing. Our proposal named QUINT is built upon BinSketch,
and it embeds nodes of a sparse network onto a low-dimensional space using
simple bi-wise operations. QUINT is the first of its kind that provides
tremendous gain in terms of speed and space usage without compromising much on
the accuracy of the downstream tasks. Extensive experiments are conducted to
compare QUINT with seven state-of-the-art network embedding methods for two end
tasks - link prediction and node classification. We observe huge performance
gain for QUINT in terms of speedup (up to 7000x) and space saving (up to 800x)
due to its bit-wise nature to obtain node embedding. Moreover, QUINT is a
consistent top-performer for both the tasks among the baselines across all the
datasets. Our empirical observations are backed by rigorous theoretical
analysis to justify the effectiveness of QUINT. In particular, we prove that
QUINT retains enough structural information which can be used further to
approximate many topological properties of networks with high confidence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents a sparse-attention Transformer architecture for modeling
documents that contain large tables. Tables are ubiquitous on the web, and are
rich in information. However, more than 20% of relational tables on the web
have 20 or more rows (Cafarella et al., 2008), and these large tables present a
challenge for current Transformer models, which are typically limited to 512
tokens. Here we propose MATE, a novel Transformer architecture designed to
model the structure of web tables. MATE uses sparse attention in a way that
allows heads to efficiently attend to either rows or columns in a table. This
architecture scales linearly with respect to speed and memory, and can handle
documents containing more than 8000 tokens with current accelerators. MATE also
has a more appropriate inductive bias for tabular data, and sets a new
state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,
2020b), a dataset that involves large documents containing tables, we improve
the best prior result by 19 points.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NU:BRIEF -- A Privacy-aware Newsletter Personalization Engine for Publishers. (arXiv:2109.03955v1 [cs.DL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03955">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Newsletters have (re-) emerged as a powerful tool for publishers to engage
with their readers directly and more effectively. Despite the diversity in
their audiences, publishers&#x27; newsletters remain largely a one-size-fits-all
offering, which is suboptimal. In this paper, we present NU:BRIEF, a web
application for publishers that enables them to personalize their newsletters
without harvesting personal data. Personalized newsletters build a habit and
become a great conversion tool for publishers, providing an alternative
readers-generated revenue model to a declining ad/clickbait-centered business
model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting and Mitigating Test-time Failure Risks via Model-agnostic Uncertainty Learning. (arXiv:2109.04432v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04432">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reliably predicting potential failure risks of machine learning (ML) systems
when deployed with production data is a crucial aspect of trustworthy AI. This
paper introduces Risk Advisor, a novel post-hoc meta-learner for estimating
failure risks and predictive uncertainties of any already-trained black-box
classification model. In addition to providing a risk score, the Risk Advisor
decomposes the uncertainty estimates into aleatoric and epistemic uncertainty
components, thus giving informative insights into the sources of uncertainty
inducing the failures. Consequently, Risk Advisor can distinguish between
failures caused by data variability, data shifts and model limitations and
advise on mitigation actions (e.g., collecting more data to counter data
shift). Extensive experiments on various families of black-box classification
models and on real-world and synthetic datasets covering common ML failure
scenarios show that the Risk Advisor reliably predicts deployment-time failure
risks in all the scenarios, and outperforms strong baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. (arXiv:2109.04200v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the prevalence of social media, there has recently been a proliferation
of recommenders that shift their focus from individual modeling to group
recommendation. Since the group preference is a mixture of various
predilections from group members, the fundamental challenge of group
recommendation is to model the correlations among members. Existing methods
mostly adopt heuristic or attention-based preference aggregation strategies to
synthesize group preferences. However, these models mainly focus on the
pairwise connections of users and ignore the complex high-order interactions
within and beyond groups. Besides, group recommendation suffers seriously from
the problem of data sparsity due to severely sparse group-item interactions. In
this paper, we propose a self-supervised hypergraph learning framework for
group recommendation to achieve two goals: (1) capturing the intra- and
inter-group interactions among users; (2) alleviating the data sparsity issue
with the raw data itself. Technically, for (1), a hierarchical hypergraph
convolutional network based on the user- and group-level hypergraphs is
developed to model the complex tuplewise correlations among users within and
beyond groups. For (2), we design a double-scale node dropout strategy to
create self-supervision signals that can regularize user representations with
different granularities against the sparsity issue. The experimental analysis
on multiple benchmark datasets demonstrates the superiority of the proposed
model and also elucidates the rationality of the hypergraph modeling and the
double-scale self-supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Follow the guides: disentangling human and algorithmic curation in online music consumption. (arXiv:2109.03915v1 [cs.CY])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03915">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>The role of recommendation systems in the diversity of content consumption on
platforms is a much-debated issue. The quantitative state of the art often
overlooks the existence of individual attitudes toward guidance, and eventually
of different categories of users in this regard. Focusing on the case of music
streaming, we analyze the complete listening history of about 9k users over one
year and demonstrate that there is no blanket answer to the intertwinement of
recommendation use and consumption diversity: it depends on users. First we
compute for each user the relative importance of different access modes within
their listening history, introducing a trichotomy distinguishing so-called
&#x60;organic&#x27; use from algorithmic and editorial guidance. We thereby identify four
categories of users. We then focus on two scales related to content diversity,
both in terms of dispersion -- how much users consume the same content
repeatedly -- and popularity -- how popular is the content they consume. We
show that the two types of recommendation offered by music platforms --
algorithmic and editorial -- may drive the consumption of more or less diverse
content in opposite directions, depending also strongly on the type of users.
Finally, we compare users&#x27; streaming histories with the music programming of a
selection of popular French radio stations during the same period. While radio
programs are usually more tilted toward repetition than users&#x27; listening
histories, they often program more songs from less popular artists. On the
whole, our results highlight the nontrivial effects of platform-mediated
recommendation on consumption, and lead us to speak of &#x60;filter niches&#x27; rather
than &#x60;filter bubbles&#x27;. They hint at further ramifications for the study and
design of recommendation systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction. (arXiv:2109.03821v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03821">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Compliments and concerns in reviews are valuable for understanding users&#x27;
shopping interests and their opinions with respect to specific aspects of
certain items. Existing review-based recommenders favor large and complex
language encoders that can only learn latent and uninterpretable text
representations. They lack explicit user attention and item property modeling,
which however could provide valuable information beyond the ability to
recommend items. Therefore, we propose a tightly coupled two-stage approach,
including an Aspect-Sentiment Pair Extractor (ASPE) and an
Attention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines
Aspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as
concrete aspect-level evidence. Extensive experiments on seven real-world
Amazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs
which enable APRE to deliver superior accuracy over the leading baselines.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">People Still Care About Facts: Twitter Users Engage More with Factual Discourse than Misinformation--A Comparison Between COVID and General Narratives on Twitter. (arXiv:2012.02164v3 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02164">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Misinformation entails the dissemination of falsehoods that leads to the slow
fracturing of society via decreased trust in democratic processes,
institutions, and science. The public has grown aware of the role of social
media as a superspreader of untrustworthy information, where even pandemics
have not been immune. In this paper, we focus on COVID-19 misinformation and
examine a subset of 2.1M tweets to understand misinformation as a function of
engagement, tweet content (COVID-19- vs. non-COVID-19-related), and veracity
(misleading or factual). Using correlation analysis, we show the most relevant
feature subsets among over 126 features that most heavily correlate with
misinformation or facts. We found that (i) factual tweets, regardless of
whether COVID-related, were more engaging than misinformation tweets; and (ii)
features that most heavily correlated with engagement varied depending on the
veracity and content of the tweet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Extreme Bandits using Robust Statistics. (arXiv:2109.04433v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04433">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider a multi-armed bandit problem motivated by situations where only
the extreme values, as opposed to expected values in the classical bandit
setting, are of interest. We propose distribution free algorithms using robust
statistics and characterize the statistical properties. We show that the
provided algorithms achieve vanishing extremal regret under weaker conditions
than existing algorithms. Performance of the algorithms is demonstrated for the
finite-sample setting using numerical experiments. The results show superior
performance of the proposed algorithms compared to the well known algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detection of Epileptic Seizures on EEG Signals Using ANFIS Classifier, Autoencoders and Fuzzy Entropies. (arXiv:2109.04364v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04364">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Epilepsy is one of the most crucial neurological disorders, and its early
diagnosis will help the clinicians to provide accurate treatment for the
patients. The electroencephalogram (EEG) signals are widely used for epileptic
seizures detection, which provides specialists with substantial information
about the functioning of the brain. In this paper, a novel diagnostic procedure
using fuzzy theory and deep learning techniques are introduced. The proposed
method is evaluated on the Bonn University dataset with six classification
combinations and also on the Freiburg dataset. The tunable-Q wavelet transform
(TQWT) is employed to decompose the EEG signals into different sub-bands. In
the feature extraction step, 13 different fuzzy entropies are calculated from
different sub-bands of TQWT, and their computational complexities are
calculated to help researchers choose the best feature sets. In the following,
an autoencoder (AE) with six layers is employed for dimensionality reduction.
Finally, the standard adaptive neuro-fuzzy inference system (ANFIS), and also
its variants with grasshopper optimization algorithm (ANFIS-GOA), particle
swarm optimization (ANFIS-PSO), and breeding swarm optimization (ANFIS-BS)
methods are used for classification. Using our proposed method, ANFIS-BS method
has obtained an accuracy of 99.74% in classifying into two classes and an
accuracy of 99.46% in ternary classification on the Bonn dataset and 99.28% on
the Freiburg dataset, reaching state-of-the-art performances on both of them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04408">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
label annotation distribution schemes, assigning multiple labels per example
for a small subset of training examples. Introducing such multi label examples
at the cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from uneven training examples (with zero, one, or multiple labels). This
algorithm efficiently combines signals from uneven training data and brings
additional gains in low annotation budget and cross domain settings. Together,
our method achieves consistent gains in both accuracy and label distribution
metrics in two tasks, suggesting training with uneven training data can be
beneficial for many NLP tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Latents Benchmark &#x27;21: Evaluating latent variable models of neural population activity. (arXiv:2109.04463v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04463">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Advances in neural recording present increasing opportunities to study neural
activity in unprecedented detail. Latent variable models (LVMs) are promising
tools for analyzing this rich activity across diverse neural systems and
behaviors, as LVMs do not depend on known relationships between the activity
and external experimental variables. However, progress in latent variable
modeling is currently impeded by a lack of standardization, resulting in
methods being developed and compared in an ad hoc manner. To coordinate these
modeling efforts, we introduce a benchmark suite for latent variable modeling
of neural population activity. We curate four datasets of neural spiking
activity from cognitive, sensory, and motor areas to promote models that apply
to the wide variety of activity seen across these areas. We identify
unsupervised evaluation as a common framework for evaluating models across
datasets, and apply several baselines that demonstrate benchmark diversity. We
release this benchmark through EvalAI. this http URL</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adversarial Attacks are Reversible with Natural Supervision. (arXiv:2103.14222v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14222">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We find that images contain intrinsic structure that enables the reversal of
many adversarial attacks. Attack vectors cause not only image classifiers to
fail, but also collaterally disrupt incidental structure in the image. We
demonstrate that modifying the attacked image to restore the natural structure
will reverse many types of attacks, providing a defense. Experiments
demonstrate significantly improved robustness for several state-of-the-art
models across the CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Our results
show that our defense is still effective even if the attacker is aware of the
defense mechanism. Since our defense is deployed during inference instead of
training, it is compatible with pre-trained networks as well as most other
defenses. Our results suggest deep networks are vulnerable to adversarial
examples partly because their representations do not enforce the natural
structure of images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing Machine Learning Approaches to Address IoT Sensor Drift. (arXiv:2109.04356v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04356">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The proliferation of IoT sensors and their deployment in various industries
and applications has brought about numerous analysis opportunities in this Big
Data era. However, drift of those sensor measurements poses major challenges to
automate data analysis and the ability to effectively train and deploy models
on a continuous basis. In this paper we study and test several approaches from
the literature with regard to their ability to cope with and adapt to sensor
drift under realistic conditions. Most of these approaches are recent and thus
are representative of the current state-of-the-art. The testing was performed
on a publicly available gas sensor dataset exhibiting drift over time. The
results show substantial drops in sensing performance due to sensor drift in
spite of the approaches. We then discuss several issues identified with current
approaches and outline directions for future research to tackle them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent
years. However, previous work has indicated that off-the-shelf MLMs are not
effective as universal lexical or sentence encoders without further
task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks
using annotated task data. In this work, we demonstrate that it is possible to
turn MLMs into effective universal lexical and sentence encoders even without
any additional data and without any supervision. We propose an extremely
simple, fast and effective contrastive learning technique, termed Mirror-BERT,
which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30
seconds without any additional external knowledge. Mirror-BERT relies on fully
identical or slightly modified string pairs as positive (i.e., synonymous)
fine-tuning examples, and aims to maximise their similarity during identity
fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in
both lexical-level and sentence-level tasks, across different domains and
different languages. Notably, in the standard sentence semantic similarity
(STS) tasks, our self-supervised Mirror-BERT model even matches the performance
of the task-tuned Sentence-BERT models from prior work. Finally, we delve
deeper into the inner workings of MLMs, and suggest some evidence on why this
simple approach can yield effective universal lexical and sentence encoders.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantizing data for distributed learning. (arXiv:2012.07913v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07913">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We consider machine learning applications that train a model by leveraging
data distributed over a trusted network, where communication constraints can
create a performance bottleneck. A number of recent approaches propose to
overcome this bottleneck through compression of gradient updates. However, as
models become larger, so does the size of the gradient updates. In this paper,
we propose an alternate approach to learn from distributed data that quantizes
data instead of gradients, and can support learning over applications where the
size of gradient updates is prohibitive. Our approach leverages the dependency
of the computed gradient on data samples, which lie in a much smaller space in
order to perform the quantization in the smaller dimension data space. At the
cost of an extra gradient computation, the gradient estimate can be refined by
conveying the difference between the gradient at the quantized data point and
the original gradient using a small number of bits. Lastly, in order to save
communication, our approach adds a layer that decides whether to transmit a
quantized data sample or not based on its importance for learning. We analyze
the convergence of the proposed approach for smooth convex and non-convex
objective functions and show that we can achieve order optimal convergence
rates with communication that mostly depends on the data rather than the model
(gradient) dimension. We use our proposed algorithm to train ResNet models on
the CIFAR-10 and ImageNet datasets, and show that we can achieve an order of
magnitude savings over gradient compression methods. These communication
savings come at the cost of increasing computation at the learning agent, and
thus our approach is beneficial in scenarios where communication load is the
main problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fisher Task Distance and Its Applications in Transfer Learning and Neural Architecture Search. (arXiv:2103.12827v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12827">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We formulate an asymmetric (or non-commutative) distance between tasks based
on Fisher Information Matrices. We provide proof of consistency for our
distance through theorems and experiments on various classification tasks. We
then apply our proposed measure of task distance in transfer learning on visual
tasks in the Taskonomy dataset. Additionally, we show how the proposed distance
between a target task and a set of baseline tasks can be used to reduce the
neural architecture search space for the target task. The complexity reduction
in search space for task-specific architectures is achieved by building on the
optimized architectures for similar tasks instead of doing a full search
without using this side information. Experimental results demonstrate the
efficacy of the proposed approach and its improvements over other methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06402">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Truth can vary over time. Fact-checking decisions on claim veracity should
therefore take into account temporal information of both the claim and
supporting or refuting evidence. In this work, we investigate the hypothesis
that the timestamp of a Web page is crucial to how it should be ranked for a
given claim. We delineate four temporal ranking methods that constrain evidence
ranking differently and simulate hypothesis-specific evidence rankings given
the evidence timestamps as gold standard. Evidence ranking in three
fact-checking models is ultimately optimized using a learning-to-rank loss
function. Our study reveals that time-aware evidence ranking not only surpasses
relevance assumptions based purely on semantic similarity or position in a
search results list, but also improves veracity predictions of time-sensitive
claims in particular.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer. (arXiv:2109.04335v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04335">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most recent semantic segmentation methods adopt a U-Net framework with an
encoder-decoder architecture. It is still challenging for U-Net with a simple
skip connection scheme to model the global multi-scale context: 1) Not each
skip connection setting is effective due to the issue of incompatible feature
sets of encoder and decoder stage, even some skip connection negatively
influence the segmentation performance; 2) The original U-Net is worse than the
one without any skip connection on some datasets. Based on our findings, we
propose a new segmentation framework, named UCTransNet (with a proposed CTrans
module in U-Net), from the channel perspective with attention mechanism.
Specifically, the CTrans module is an alternate of the U-Net skip connections,
which consists of a sub-module to conduct the multi-scale Channel Cross fusion
with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention
(named CCA) to guide the fused multi-scale channel-wise information to
effectively connect to the decoder features for eliminating the ambiguity.
Hence, the proposed connection consisting of the CCT and CCA is able to replace
the original skip connection to solve the semantic gaps for an accurate
automatic medical image segmentation. The experimental results suggest that our
UCTransNet produces more precise segmentation performance and achieves
consistent improvements over the state-of-the-art for semantic segmentation
across different datasets and conventional architectures involving transformer
or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Conformal Predictors for Applications in Medical Imaging. (arXiv:2109.04392v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning has the potential to augment many components of the clinical
workflow, such as medical image interpretation. However, the translation of
these black box algorithms into clinical practice has been marred by the
relative lack of transparency compared to conventional machine learning
methods, hindering in clinician trust in the systems for critical medical
decision-making. Specifically, common deep learning approaches do not have
intuitive ways of expressing uncertainty with respect to cases that might
require further human review. Furthermore, the possibility of algorithmic bias
has caused hesitancy regarding the use of developed algorithms in clinical
settings. To these ends, we explore how conformal methods can complement deep
learning models by providing both clinically intuitive way (by means of
confidence prediction sets) of expressing model uncertainty as well as
facilitating model transparency in clinical workflows. In this paper, we
conduct a field survey with clinicians to assess clinical use-cases of
conformal predictions. Next, we conduct experiments with a mammographic breast
density and dermatology photography datasets to demonstrate the utility of
conformal predictions in &quot;rule-in&quot; and &quot;rule-out&quot; disease scenarios. Further,
we show that conformal predictors can be used to equalize coverage with respect
to patient demographics such as race and skin tone. We find that a conformal
predictions to be a promising framework with potential to increase clinical
usability and transparency for better collaboration between deep learning
algorithms and clinicians.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NeuralFMU: Towards Structural Integration of FMUs into Neural Networks. (arXiv:2109.04351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper covers two major subjects: First, the presentation of a new
open-source library called FMI.jl for integrating FMI into the Julia
programming environment by providing the possibility to load, parameterize and
simulate FMUs. Further, an extension to this library called FMIFlux.jl is
introduced, that allows the integration of FMUs into a neural network topology
to obtain a NeuralFMU. This structural combination of an industry typical
black-box model and a data-driven machine learning model combines the different
advantages of both modeling approaches in one single development environment.
This allows for the usage of advanced data driven modeling techniques for
physical effects that are difficult to model based on first principles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A fast and simple modification of Newton&#x27;s method helping to avoid saddle points. (arXiv:2006.01512v4 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.01512">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose in this paper New Q-Newton&#x27;s method. The update rule is very
simple conceptually, for example $x_{n+1}&#x3D;x_n-w_n$ where
$w_n&#x3D;pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n&#x3D;\nabla ^2f(x_n)+\delta
_n||\nabla f(x_n)||^2.Id$ and $v_n&#x3D;A_n^{-1}.\nabla f(x_n)$. Here $\delta _n$ is
an appropriate real number so that $A_n$ is invertible, and $pr_{A_n,\pm}$ are
projections to the vector subspaces generated by eigenvectors of positive
(correspondingly negative) eigenvalues of $A_n$.

The main result of this paper roughly says that if $f$ is $C^3$ (can be
unbounded from below) and a sequence $\{x_n\}$, constructed by the New
Q-Newton&#x27;s method from a random initial point $x_0$, {\bf converges}, then the
limit point is a critical point and is not a saddle point, and the convergence
rate is the same as that of Newton&#x27;s method. The first author has recently been
successful incorporating Backtracking line search to New Q-Newton&#x27;s method,
thus resolving the convergence guarantee issue observed for some (non-smooth)
cost functions. An application to quickly finding zeros of a univariate
meromorphic function will be discussed. Various experiments are performed,
against well known algorithms such as BFGS and Adaptive Cubic Regularization
are presented.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PhysGNN: A Physics-Driven Graph Neural Network Based Model for Predicting Soft Tissue Deformation in Image-Guided Neurosurgery. (arXiv:2109.04352v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04352">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Correctly capturing intraoperative brain shift in image-guided neurosurgical
procedures is a critical task for aligning preoperative data with
intraoperative geometry, ensuring effective surgical navigation and optimal
surgical precision. While the finite element method (FEM) is a proven technique
to effectively approximate soft tissue deformation through biomechanical
formulations, their degree of success boils down to a trade-off between
accuracy and speed. To circumvent this problem, the most recent works in this
domain have proposed leveraging data-driven models obtained by training various
machine learning algorithms, e.g. random forests, artificial neural networks
(ANNs), with the results of finite element analysis (FEA) to speed up tissue
deformation approximations by prediction. These methods, however, do not
account for the structure of the finite element (FE) mesh during training that
provides information on node connectivities as well as the distance between
them, which can aid with approximating tissue deformation based on the
proximity of force load points with the rest of the mesh nodes. Therefore, this
work proposes a novel framework, PhysGNN, a data-driven model that approximates
the solution of FEA by leveraging graph neural networks (GNNs), which are
capable of accounting for the mesh structural information and inductive
learning over unstructured grids and complex topological structures.
Empirically, we demonstrate that the proposed architecture, PhysGNN, promises
accurate and fast soft tissue deformation approximations while remaining
computationally feasible, suitable for neurosurgical settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting and Mitigating Test-time Failure Risks via Model-agnostic Uncertainty Learning. (arXiv:2109.04432v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04432">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reliably predicting potential failure risks of machine learning (ML) systems
when deployed with production data is a crucial aspect of trustworthy AI. This
paper introduces Risk Advisor, a novel post-hoc meta-learner for estimating
failure risks and predictive uncertainties of any already-trained black-box
classification model. In addition to providing a risk score, the Risk Advisor
decomposes the uncertainty estimates into aleatoric and epistemic uncertainty
components, thus giving informative insights into the sources of uncertainty
inducing the failures. Consequently, Risk Advisor can distinguish between
failures caused by data variability, data shifts and model limitations and
advise on mitigation actions (e.g., collecting more data to counter data
shift). Extensive experiments on various families of black-box classification
models and on real-world and synthetic datasets covering common ML failure
scenarios show that the Risk Advisor reliably predicts deployment-time failure
risks in all the scenarios, and outperforms strong baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Safe Learning Reference Governor: Theory and Application to Fuel Truck Rollover Avoidance. (arXiv:2101.09298v2 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.09298">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a learning reference governor (LRG) approach to enforce
state and control constraints in systems for which an accurate model is
unavailable, and this approach enables the reference governor to gradually
improve command tracking performance through learning while enforcing the
constraints during learning and after learning is completed. The learning can
be performed either on a black-box type model of the system or directly on the
hardware. After introducing the LRG algorithm and outlining its theoretical
properties, this paper investigates LRG application to fuel truck (tank truck)
rollover avoidance. Through simulations based on a fuel truck model that
accounts for liquid fuel sloshing effects, we show that the proposed LRG can
effectively protect fuel trucks from rollover accidents under various operating
conditions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fundamental Limits and Tradeoffs in Invariant Representation Learning. (arXiv:2012.10713v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.10713">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A wide range of machine learning applications such as privacy-preserving
learning, algorithmic fairness, and domain adaptation/generalization among
others, involve learning \emph{invariant representations} of the data that aim
to achieve two competing goals: (a) maximize information or accuracy with
respect to a target response, and (b) maximize invariance or independence with
respect to a set of protected features (e.g.\ for fairness, privacy, etc).
Despite their wide applicability, theoretical understanding of the optimal
tradeoffs -- with respect to accuracy, and invariance -- achievable by
invariant representations is still severely lacking. In this paper, we provide
precisely such an information-theoretic analysis of such tradeoffs under both
classification and regression settings. We provide a geometric characterization
of the accuracy and invariance achievable by any representation of the data; we
term this feasible region the information plane. We provide a lower bound for
this feasible region for the classification case, and an exact characterization
for the regression case, which allows us to either bound or exactly
characterize the Pareto optimal frontier between accuracy and invariance.
Although our contributions are mainly theoretical, a key practical application
of our results is in certifying the potential sub-optimality of any given
representation learning algorithm for either classification or regression
tasks. Our results shed new light on the fundamental interplay between accuracy
and invariance, and may be useful in guiding the design of future
representation learning algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Wave-Informed Matrix Factorization with Global Optimality Guarantees. (arXiv:2107.09144v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09144">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the recent success of representation learning methods, which includes
deep learning as a special case, there has been considerable interest in
developing representation learning techniques that can incorporate known
physical constraints into the learned representation. As one example, in many
applications that involve a signal propagating through physical media (e.g.,
optics, acoustics, fluid dynamics, etc), it is known that the dynamics of the
signal must satisfy constraints imposed by the wave equation. Here we propose a
matrix factorization technique that decomposes such signals into a sum of
components, where each component is regularized to ensure that it satisfies
wave equation constraints. Although our proposed formulation is non-convex, we
prove that our model can be efficiently solved to global optimality in
polynomial time. We demonstrate the benefits of our work by applications in
structural health monitoring, where prior work has attempted to solve this
problem using sparse dictionary learning approaches that do not come with any
theoretical guarantees regarding convergence to global optimality and employ
heuristics to capture desired physical constraints.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Price is (Probably) Right: Learning Market Equilibria from Samples. (arXiv:2012.14838v3 [cs.GT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Equilibrium computation in markets usually considers settings where player
valuation functions are known. We consider the setting where player valuations
are unknown; using a PAC learning-theoretic framework, we analyze some classes
of common valuation functions, and provide algorithms which output direct PAC
equilibrium allocations, not estimates based on attempting to learn valuation
functions. Since there exist trivial PAC market outcomes with an unbounded
worst-case efficiency loss, we lower-bound the efficiency of our algorithms.
While the efficiency loss under general distributions is rather high, we show
that in some cases (e.g., unit-demand valuations), it is possible to find a PAC
market equilibrium with significantly better utility.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Leveraging Local Domains for Image-to-Image Translation. (arXiv:2109.04468v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image-to-image (i2i) networks struggle to capture local changes because they
do not affect the global scene structure. For example, translating from highway
scenes to offroad, i2i networks easily focus on global color features but
ignore obvious traits for humans like the absence of lane markings. In this
paper, we leverage human knowledge about spatial domain characteristics which
we refer to as &#x27;local domains&#x27; and demonstrate its benefit for image-to-image
translation. Relying on a simple geometrical guidance, we train a patch-based
GAN on few source data and hallucinate a new unseen domain which subsequently
eases transfer learning to target. We experiment on three tasks ranging from
unstructured environments to adverse weather. Our comprehensive evaluation
setting shows we are able to generate realistic translations, with minimal
priors, and training only on a few images. Furthermore, when trained on our
translations images we show that all tested proxy tasks are significantly
improved, without ever seeing target domain at training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast-Convergent Dynamics for Distributed Allocation of Resources Over Switching Sparse Networks with Quantized Communication Links. (arXiv:2012.08181v3 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.08181">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes networked dynamics to solve resource allocation problems
over time-varying multi-agent networks. The state of each agent represents the
amount of used resources (or produced utilities) while the total amount of
resources is fixed. The idea is to optimally allocate the resources among the
group of agents by minimizing the overall cost function subject to fixed sum of
resources. Each agents&#x27; information is restricted to its own state and cost
function and those of its immediate in-neighbors. This is motivated by
distributed applications such as mobile edge-computing, economic dispatch over
smart grids, and multi-agent coverage control. This work provides a fast
convergent solution (in comparison with linear dynamics) while considering
relaxed network connectivity with quantized communication links. The proposed
dynamics reaches optimal solution over switching (possibly disconnected)
undirected networks as far as their union over some bounded non-overlapping
time-intervals has a spanning-tree. We prove feasibility of the solution,
uniqueness of the optimal state, and convergence to the optimal value under the
proposed dynamics, where the analysis is applicable to similar 1st-order
allocation dynamics with strongly sign-preserving nonlinearities, such as
actuator saturation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Protein Folding Neural Networks Are Not Robust. (arXiv:2109.04460v1 [q-bio.BM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04460">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks such as AlphaFold and RoseTTAFold predict remarkably
accurate structures of proteins compared to other algorithmic approaches. It is
known that biologically small perturbations in the protein sequence do not lead
to drastic changes in the protein structure. In this paper, we demonstrate that
RoseTTAFold does not exhibit such a robustness despite its high accuracy, and
biologically small perturbations for some input sequences result in radically
different predicted protein structures. This raises the challenge of detecting
when these predicted protein structures cannot be trusted. We define the
robustness measure for the predicted structure of a protein sequence to be the
inverse of the root-mean-square distance (RMSD) in the predicted structure and
the structure of its adversarially perturbed sequence. We use adversarial
attack methods to create adversarial protein sequences, and show that the RMSD
in the predicted protein structure ranges from 0.119\r{A} to 34.162\r{A} when
the adversarial perturbations are bounded by 20 units in the BLOSUM62 distance.
This demonstrates very high variance in the robustness measure of the predicted
structures. We show that the magnitude of the correlation (0.917) between our
robustness measure and the RMSD between the predicted structure and the ground
truth is high, that is, the predictions with low robustness measure cannot be
trusted. This is the first paper demonstrating the susceptibility of
RoseTTAFold to adversarial attacks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Is a Classification Procedure Good Enough? A Goodness-of-Fit Assessment Tool for Classification Learning. (arXiv:1911.03063v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03063">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, many non-traditional classification methods, such as Random
Forest, Boosting, and neural network, have been widely used in applications.
Their performance is typically measured in terms of classification accuracy.
While the classification error rate and the like are important, they do not
address a fundamental question: Is the classification method underfitted? To
our best knowledge, there is no existing method that can assess the
goodness-of-fit of a general classification procedure. Indeed, the lack of a
parametric assumption makes it challenging to construct proper tests. To
overcome this difficulty, we propose a methodology called BAGofT that splits
the data into a training set and a validation set. First, the classification
procedure to assess is applied to the training set, which is also used to
adaptively find a data grouping that reveals the most severe regions of
underfitting. Then, based on this grouping, we calculate a test statistic by
comparing the estimated success probabilities and the actual observed responses
from the validation set. The data splitting guarantees that the size of the
test is controlled under the null hypothesis, and the power of the test goes to
one as the sample size increases under the alternative hypothesis. For testing
parametric classification models, the BAGofT has a broader scope than the
existing methods since it is not restricted to specific parametric models
(e.g., logistic regression). Extensive simulation studies show the utility of
the BAGofT when assessing general classification procedures and its strengths
over some existing methods when testing parametric classification models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08164">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The factual knowledge acquired during pre-training and stored in the
parameters of Language Models (LMs) can be useful in downstream tasks (e.g.,
question answering or textual inference). However, some facts can be
incorrectly induced or become obsolete over time. We present KnowledgeEditor, a
method which can be used to edit this knowledge and, thus, fix &#x27;bugs&#x27; or
unexpected predictions without the need for expensive re-training or
fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not
require any modifications in LM pre-training (e.g., the use of meta-learning).
In our approach, we train a hyper-network with constrained optimization to
modify a fact without affecting the rest of the knowledge; the trained
hyper-network is then used to predict the weight update at test time. We show
KnowledgeEditor&#x27;s efficacy with two popular architectures and
knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and
ii) a sequence-to-sequence BART model for question answering. With our method,
changing a prediction on the specific wording of a query tends to result in a
consistent change in predictions also for its paraphrases. We show that this
can be further encouraged by exploiting (e.g., automatically-generated)
paraphrases during training. Interestingly, our hyper-network can be regarded
as a &#x27;probe&#x27; revealing which components need to be changed to manipulate
factual knowledge; our analysis shows that the updates tend to be concentrated
on a small subset of components. Source code available at
https://github.com/nicola-decao/KnowledgeEditor</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Supervised Machine Learning Model For Imputing Missing Boarding Stops In Smart Card Data. (arXiv:2003.05285v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.05285">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Public transport has become an essential part of urban existence with
increased population densities and environmental awareness. Large quantities of
data are currently generated, allowing for more robust methods to understand
travel behavior by harvesting smart card usage. However, public transport
datasets suffer from data integrity problems; boarding stop information may be
missing due to imperfect acquirement processes or inadequate reporting. We
developed a supervised machine learning method to impute missing boarding stops
based on ordinal classification using GTFS timetable, smart card, and
geospatial datasets. A new metric, Pareto Accuracy, is suggested to evaluate
algorithms where classes have an ordinal nature. Results are based on a case
study in the city of Beer Sheva, Israel, consisting of one month of smart card
data. We show that our proposed method is robust to irregular travelers and
significantly outperforms well-known imputation methods without the need to
mine any additional datasets. Validation of data from another Israeli city
using transfer learning shows the presented model is general and context-free.
The implications for transportation planning and travel behavior research are
further discussed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Deep Neural Networks via Layer-Peeled Model: Minority Collapse in Imbalanced Training. (arXiv:2101.12699v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we introduce the \textit{Layer-Peeled Model}, a nonconvex yet
analytically tractable optimization program, in a quest to better understand
deep neural networks that are trained for a sufficiently long time. As the name
suggests, this new model is derived by isolating the topmost layer from the
remainder of the neural network, followed by imposing certain constraints
separately on the two parts of the network. We demonstrate that the
Layer-Peeled Model, albeit simple, inherits many characteristics of
well-trained neural networks, thereby offering an effective tool for explaining
and predicting common empirical patterns of deep learning training. First, when
working on class-balanced datasets, we prove that any solution to this model
forms a simplex equiangular tight frame, which in part explains the recently
discovered phenomenon of neural collapse \cite{papyan2020prevalence}. More
importantly, when moving to the imbalanced case, our analysis of the
Layer-Peeled Model reveals a hitherto unknown phenomenon that we term
\textit{Minority Collapse}, which fundamentally limits the performance of deep
learning models on the minority classes. In addition, we use the Layer-Peeled
Model to gain insights into how to mitigate Minority Collapse. Interestingly,
this phenomenon is first predicted by the Layer-Peeled Model before being
confirmed by our computational experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">High-Dimensional Differentially-Private EM Algorithm: Methods and Near-Optimal Statistical Guarantees. (arXiv:2104.00245v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00245">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we develop a general framework to design differentially
private expectation-maximization (EM) algorithms in high-dimensional latent
variable models, based on the noisy iterative hard-thresholding. We derive the
statistical guarantees of the proposed framework and apply it to three specific
models: Gaussian mixture, mixture of regression, and regression with missing
covariates. In each model, we establish the near-optimal rate of convergence
with differential privacy constraints, and show the proposed algorithm is
minimax rate optimal up to logarithm factors. The technical tools developed for
the high-dimensional setting are then extended to the classic low-dimensional
latent variable models, and we propose a near rate-optimal EM algorithm with
differential privacy guarantees in this setting. Simulation studies and real
data analysis are conducted to support our results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ProAI: An Efficient Embedded AI Hardware for Automotive Applications -- a Benchmark Study. (arXiv:2108.05170v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05170">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Development in the field of Single Board Computers (SBC) have been increasing
for several years. They provide a good balance between computing performance
and power consumption which is usually required for mobile platforms, like
application in vehicles for Advanced Driver Assistance Systems (ADAS) and
Autonomous Driving (AD). However, there is an ever-increasing need of more
powerful and efficient SBCs which can run power intensive Deep Neural Networks
(DNNs) in real-time and can also satisfy necessary functional safety
requirements such as Automotive Safety Integrity Level (ASIL). ProAI is being
developed by ZF mainly to run powerful and efficient applications such as
multitask DNNs and on top of that it also has the required safety certification
for AD. In this work, we compare and discuss state of the art SBC on the basis
of power intensive multitask DNN architecture called Multitask-CenterNet with
respect to performance measures such as, FPS and power efficiency. As an
automotive supercomputer, ProAI delivers an excellent combination of
performance and efficiency, managing nearly twice the number of FPS per watt
than a modern workstation laptop and almost four times compared to the Jetson
Nano. Furthermore, it was also shown that there is still power in reserve for
further and more complex tasks on the ProAI, based on the CPU and GPU
utilization during the benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks. (arXiv:2104.01002v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01002">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Jupyter notebook allows data scientists to write machine learning code
together with its documentation in cells. In this paper, we propose a new task
of code documentation generation (CDG) for computational notebooks. In contrast
to the previous CDG tasks which focus on generating documentation for single
code snippets, in a computational notebook, one documentation in a markdown
cell often corresponds to multiple code cells, and these code cells have an
inherent structure. We proposed a new model (HAConvGNN) that uses a
hierarchical attention mechanism to consider the relevant code cells and the
relevant code tokens information when generating the documentation. Tested on a
new corpus constructed from well-documented Kaggle notebooks, we show that our
model outperforms other baseline models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributed Thompson Sampling. (arXiv:2012.01789v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study a cooperative multi-agent multi-armed bandits with M agents and K
arms. The goal of the agents is to minimized the cumulative regret. We adapt a
traditional Thompson Sampling algoirthm under the distributed setting. However,
with agent&#x27;s ability to communicate, we note that communication may further
reduce the upper bound of the regret for a distributed Thompson Sampling
approach. To further improve the performance of distributed Thompson Sampling,
we propose a distributed Elimination based Thompson Sampling algorithm that
allow the agents to learn collaboratively. We analyse the algorithm under
Bernoulli reward and derived a problem dependent upper bound on the cumulative
regret.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ISCL: Interdependent Self-Cooperative Learning for Unpaired Image Denoising. (arXiv:2102.09858v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the advent of advances in self-supervised learning, paired clean-noisy
data are no longer required in deep learning-based image denoising. However,
existing blind denoising methods still require the assumption with regard to
noise characteristics, such as zero-mean noise distribution and pixel-wise
noise-signal independence; this hinders wide adaptation of the method in the
medical domain. On the other hand, unpaired learning can overcome limitations
related to the assumption on noise characteristics, which makes it more
feasible for collecting the training data in real-world scenarios. In this
paper, we propose a novel image denoising scheme, Interdependent
Self-Cooperative Learning (ISCL), that leverages unpaired learning by combining
cyclic adversarial learning with self-supervised residual learning. Unlike the
existing unpaired image denoising methods relying on matching data
distributions in different domains, the two architectures in ISCL, designed for
different tasks, complement each other and boost the learning process. To
assess the performance of the proposed method, we conducted extensive
experiments in various biomedical image degradation scenarios, such as noise
caused by physical characteristics of electron microscopy (EM) devices (film
and charging noise), and structural noise found in low-dose computer tomography
(CT). We demonstrate that the image quality of our method is superior to
conventional and current state-of-the-art deep learning-based image denoising
methods, including supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automated Security Assessment for the Internet of Things. (arXiv:2109.04029v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04029">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Internet of Things (IoT) based applications face an increasing number of
potential security risks, which need to be systematically assessed and
addressed. Expert-based manual assessment of IoT security is a predominant
approach, which is usually inefficient. To address this problem, we propose an
automated security assessment framework for IoT networks. Our framework first
leverages machine learning and natural language processing to analyze
vulnerability descriptions for predicting vulnerability metrics. The predicted
metrics are then input into a two-layered graphical security model, which
consists of an attack graph at the upper layer to present the network
connectivity and an attack tree for each node in the network at the bottom
layer to depict the vulnerability information. This security model
automatically assesses the security of the IoT network by capturing potential
attack paths. We evaluate the viability of our approach using a
proof-of-concept smart building system model which contains a variety of
real-world IoT devices and potential vulnerabilities. Our evaluation of the
proposed framework demonstrates its effectiveness in terms of automatically
predicting the vulnerability metrics of new vulnerabilities with more than 90%
accuracy, on average, and identifying the most vulnerable attack paths within
an IoT network. The produced assessment results can serve as a guideline for
cybersecurity professionals to take further actions and mitigate risks in a
timely manner.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DROP: Deep relocating option policy for optimal ride-hailing vehicle repositioning. (arXiv:2109.04149v1 [cs.MA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04149">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In a ride-hailing system, an optimal relocation of vacant vehicles can
significantly reduce fleet idling time and balance the supply-demand
distribution, enhancing system efficiency and promoting driver satisfaction and
retention. Model-free deep reinforcement learning (DRL) has been shown to
dynamically learn the relocating policy by actively interacting with the
intrinsic dynamics in large-scale ride-hailing systems. However, the issues of
sparse reward signals and unbalanced demand and supply distribution place
critical barriers in developing effective DRL models. Conventional exploration
strategy (e.g., the $\epsilon$-greedy) may barely work under such an
environment because of dithering in low-demand regions distant from
high-revenue regions. This study proposes the deep relocating option policy
(DROP) that supervises vehicle agents to escape from oversupply areas and
effectively relocate to potentially underserved areas. We propose to learn the
Laplacian embedding of a time-expanded relocation graph, as an approximation
representation of the system relocation policy. The embedding generates
task-agnostic signals, which in combination with task-dependent signals,
constitute the pseudo-reward function for generating DROPs. We present a
hierarchical learning framework that trains a high-level relocation policy and
a set of low-level DROPs. The effectiveness of our approach is demonstrated
using a custom-built high-fidelity simulator with real-world trip record data.
We report that DROP significantly improves baseline models with 15.7% more
hourly revenue and can effectively resolve the dithering issue in low-demand
areas.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Compositional Affinity Propagation: When Clusters Have Compositional Structure. (arXiv:2109.04160v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider a new kind of clustering problem in which clusters need not be
independent of each other, but rather can have compositional relationships with
other clusters (e.g., an image set consists of rectangles, circles, as well as
combinations of rectangles and circles). This task is motivated by recent work
in few-shot learning on compositional embedding models that structure the
embedding space to distinguish the label sets, not just the individual labels,
assigned to the examples. To tackle this clustering problem, we propose a new
algorithm called Compositional Affinity Propagation (CAP). In contrast to
standard Affinity Propagation as well as other algorithms for multi-view and
hierarchical clustering, CAP can deduce compositionality among clusters
automatically. We show promising results, compared to several existing
clustering algorithms, on the MultiMNIST, OmniGlot, and LibriSpeech datasets.
Our work has applications to multi-object image recognition and speaker
diarization with simultaneous speech from multiple speakers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks. (arXiv:2106.14997v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.14997">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the approximation rates of shallow neural networks with respect
to the variation norm. Upper bounds on these rates have been established for
sigmoidal and ReLU activation functions, but it has remained an important open
problem whether these rates are sharp. In this article, we provide a solution
to this problem by proving sharp lower bounds on the approximation rates for
shallow neural networks, which are obtained by lower bounding the $L^2$-metric
entropy of the convex hull of the neural network basis functions. In addition,
our methods also give sharp lower bounds on the Kolmogorov $n$-widths of this
convex hull, which show that the variation spaces corresponding to shallow
neural networks cannot be efficiently approximated by linear methods. These
lower bounds apply to both sigmoidal activation functions with bounded
variation and to activation functions which are a power of the ReLU. Our
results also quantify how much stronger the Barron spectral norm is than the
variation norm and, combined with previous results, give the asymptotics of the
$L^\infty$-metric entropy up to logarithmic factors in the case of the ReLU
activation function.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matrix Completion of World Trade. (arXiv:2109.03930v1 [econ.GN])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03930">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This work applies Matrix Completion (MC) -- a class of machine-learning
methods commonly used in the context of recommendation systems -- to analyse
economic complexity. MC is applied to reconstruct the Revealed Comparative
Advantage (RCA) matrix, whose elements express the relative advantage of
countries in given classes of products, as evidenced by yearly trade flows. A
high-accuracy binary classifier is derived from the application of MC, with the
aim of discriminating between elements of the RCA matrix that are,
respectively, higher or lower than one. We introduce a novel Matrix cOmpletion
iNdex of Economic complexitY (MONEY) based on MC, which is related to the
predictability of countries&#x27; RCA (the lower the predictability, the higher the
complexity). Differently from previously-developed indices of economic
complexity, the MONEY index takes into account the various singular vectors of
the matrix reconstructed by MC, whereas other indices are based only on one/two
eigenvectors of a suitable symmetric matrix, derived from the RCA matrix.
Finally, MC is compared with a state-of-the-art economic complexity index
(GENEPY). We show that the false positive rate per country of a binary
classifier constructed starting from the average entry-wise output of MC can be
used as a proxy of GENEPY.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IMG2SMI: Translating Molecular Structure Images to Simplified Molecular-input Line-entry System. (arXiv:2109.04202v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Like many scientific fields, new chemistry literature has grown at a
staggering pace, with thousands of papers released every month. A large portion
of chemistry literature focuses on new molecules and reactions between
molecules. Most vital information is conveyed through 2-D images of molecules,
representing the underlying molecules or reactions described. In order to
ensure reproducible and machine-readable molecule representations, text-based
molecule descriptors like SMILES and SELFIES were created. These text-based
molecule representations provide molecule generation but are unfortunately
rarely present in published literature. In the absence of molecule descriptors,
the generation of molecule descriptors from the 2-D images present in the
literature is necessary to understand chemistry literature at scale. Successful
methods such as Optical Structure Recognition Application (OSRA), and
ChemSchematicResolver are able to extract the locations of molecules structures
in chemistry papers and infer molecular descriptions and reactions. While
effective, existing systems expect chemists to correct outputs, making them
unsuitable for unsupervised large-scale data mining. Leveraging the task
formulation of image captioning introduced by DECIMER, we introduce IMG2SMI, a
model which leverages Deep Residual Networks for image feature extraction and
an encoder-decoder Transformer layers for molecule description generation.
Unlike previous Neural Network-based systems, IMG2SMI builds around the task of
molecule description generation, which enables IMG2SMI to outperform OSRA-based
systems by 163% in molecule similarity prediction as measured by the molecular
MACCS Fingerprint Tanimoto Similarity. Additionally, to facilitate further
research on this task, we release a new molecule prediction dataset. including
81 million molecules for molecule description generation</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Fully Automated Segmentation of Rat Cardiac MRI by Leveraging Deep Learning Frameworks. (arXiv:2109.04188v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04188">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Automated segmentation of human cardiac magnetic resonance datasets has been
steadily improving during recent years. However, these methods are not directly
applicable in preclinical context due to limited datasets and lower image
resolution. Successful application of deep architectures for rat cardiac
segmentation, although of critical importance for preclinical evaluation of
cardiac function, has to our knowledge not yet been reported. We developed
segmentation models that expand on the standard U-Net architecture and
evaluated separate models for systole and diastole phases, 2MSA, and one model
for all timepoints, 1MSA. Furthermore, we calibrated model outputs using a
Gaussian Process (GP)-based prior to improve phase selection. Resulting models
approach human performance in terms of left ventricular segmentation quality
and ejection fraction (EF) estimation in both 1MSA and 2MSA settings
(S{\o}rensen-Dice score 0.91 +/- 0.072 and 0.93 +/- 0.032, respectively). 2MSA
achieved a mean absolute difference between estimated and reference EF of 3.5
+/- 2.5 %, while 1MSA resulted in 4.1 +/- 3.0 %. Applying Gaussian Processes to
1MSA allows to automate the selection of systole and diastole phases. Combined
with a novel cardiac phase selection strategy, our work presents an important
first step towards a fully automated segmentation pipeline in the context of
rat cardiac analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepEMO: Deep Learning for Speech Emotion Recognition. (arXiv:2109.04081v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We proposed the industry level deep learning approach for speech emotion
recognition task. In industry, carefully proposed deep transfer learning
technology shows real results due to mostly low amount of training data
availability, machine training cost, and specialized learning on dedicated AI
tasks. The proposed speech recognition framework, called DeepEMO, consists of
two main pipelines such that preprocessing to extract efficient main features
and deep transfer learning model to train and recognize. Main source code is in
https://github.com/enkhtogtokh/deepemo repository</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07705">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While large language models a la BERT are used ubiquitously in NLP,
pretraining them is considered a luxury that only a few well-funded industry
labs can afford. How can one train such models with a more modest budget? We
present a recipe for pretraining a masked language model in 24 hours using a
single low-end deep learning server. We demonstrate that through a combination
of software optimizations, design choices, and hyperparameter tuning, it is
possible to produce models that are competitive with BERT-base on GLUE tasks at
a fraction of the original pretraining cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Neural Code Intelligence Through Program Simplification. (arXiv:2106.03353v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A wide range of code intelligence (CI) tools, powered by deep neural
networks, have been developed recently to improve programming productivity and
perform program analysis. To reliably use such tools, developers often need to
reason about the behavior of the underlying models and the factors that affect
them. This is especially challenging for tools backed by deep neural networks.
Various methods have tried to reduce this opacity in the vein of
&quot;transparent/interpretable-AI&quot;. However, these approaches are often specific to
a particular set of network architectures, even requiring access to the
network&#x27;s parameters. This makes them difficult to use for the average
programmer, which hinders the reliable adoption of neural CI systems. In this
paper, we propose a simple, model-agnostic approach to identify critical input
features for models in CI systems, by drawing on software debugging research,
specifically delta debugging. Our approach, SIVAND, uses simplification
techniques that reduce the size of input programs of a CI model while
preserving the predictions of the model. We show that this approach yields
remarkably small outputs and is broadly applicable across many model
architectures and problem domains. We find that the models in our experiments
often rely heavily on just a few syntactic features in input programs. We
believe that SIVAND&#x27;s extracted features may help understand neural CI systems&#x27;
predictions and learned behavior.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Reinforcement Learning for Equal Risk Pricing and Hedging under Dynamic Expectile Risk Measures. (arXiv:2109.04001v1 [q-fin.PR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04001">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recently equal risk pricing, a framework for fair derivative pricing, was
extended to consider dynamic risk measures. However, all current
implementations either employ a static risk measure that violates time
consistency, or are based on traditional dynamic programming solution schemes
that are impracticable in problems with a large number of underlying assets
(due to the curse of dimensionality) or with incomplete asset dynamics
information. In this paper, we extend for the first time a famous off-policy
deterministic actor-critic deep reinforcement learning (ACRL) algorithm to the
problem of solving a risk averse Markov decision process that models risk using
a time consistent recursive expectile risk measure. This new ACRL algorithm
allows us to identify high quality time consistent hedging policies (and equal
risk prices) for options, such as basket options, that cannot be handled using
traditional methods, or in context where only historical trajectories of the
underlying assets are available. Our numerical experiments, which involve both
a simple vanilla option and a more exotic basket option, confirm that the new
ACRL algorithm can produce 1) in simple environments, nearly optimal hedging
policies, and highly accurate prices, simultaneously for a range of maturities
2) in complex environments, good quality policies and prices using reasonable
amount of computing resources; and 3) overall, hedging strategies that actually
outperform the strategies produced using static risk measures when the risk is
evaluated at later points of time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mining Points of Interest via Address Embeddings: An Unsupervised Approach. (arXiv:2109.04467v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04467">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Digital maps are commonly used across the globe for exploring places that
users are interested in, commonly referred to as points of interest (PoI). In
online food delivery platforms, PoIs could represent any major private
compounds where customers could order from such as hospitals, residential
complexes, office complexes, educational institutes and hostels. In this work,
we propose an end-to-end unsupervised system design for obtaining polygon
representations of PoIs (PoI polygons) from address locations and address
texts. We preprocess the address texts using locality names and generate
embeddings for the address texts using a deep learning-based architecture, viz.
RoBERTa, trained on our internal address dataset. The PoI candidates are
identified by jointly clustering the anonymised customer phone GPS locations
(obtained during address onboarding) and the embeddings of the address texts.
The final list of PoI polygons is obtained from these PoI candidates using
novel post-processing steps. This algorithm identified 74.8 % more PoIs than
those obtained using the Mummidi-Krumm baseline algorithm run on our internal
dataset. The proposed algorithm achieves a median area precision of 98 %, a
median area recall of 8 %, and a median F-score of 0.15. In order to improve
the recall of the algorithmic polygons, we post-process them using building
footprint polygons from the OpenStreetMap (OSM) database. The post-processing
algorithm involves reshaping the algorithmic polygon using intersecting
polygons and closed private roads from the OSM database, and accounting for
intersection with public roads on the OSM database. We achieve a median area
recall of 70 %, a median area precision of 69 %, and a median F-score of 0.69
on these post-processed polygons.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NTS-NOTEARS: Learning Nonparametric Temporal DAGs With Time-Series Data and Prior Knowledge. (arXiv:2109.04286v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04286">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a score-based DAG structure learning method for time-series data
that captures linear, nonlinear, lagged and instantaneous relations among
variables while ensuring acyclicity throughout the entire graph. The proposed
method extends nonparametric NOTEARS, a recent continuous optimization approach
for learning nonparametric instantaneous DAGs. The proposed method is faster
than constraint-based methods using nonlinear conditional independence tests.
We also promote the use of optimization constraints to incorporate prior
knowledge into the structure learning process. A broad set of experiments with
simulated data demonstrates that the proposed method discovers better DAG
structures than several recent comparison methods. We also evaluate the
proposed method on complex real-world data acquired from NHL ice hockey games
containing a mixture of continuous and discrete variables. The code is
available at https://github.com/xiangyu-sun-789/NTS-NOTEARS/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08803">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We develop a novel approach for confidently accelerating inference in the
large and expensive multilayer Transformers that are now ubiquitous in natural
language processing (NLP). Amortized or approximate computational methods
increase efficiency, but can come with unpredictable performance costs. In this
work, we present CATs -- Confident Adaptive Transformers -- in which we
simultaneously increase computational efficiency, while guaranteeing a
specifiable degree of consistency with the original model with high confidence.
Our method trains additional prediction heads on top of intermediate layers,
and dynamically decides when to stop allocating computational effort to each
input using a meta consistency classifier. To calibrate our early prediction
stopping rule, we formulate a unique extension of conformal prediction. We
demonstrate the effectiveness of this approach on four classification and
regression tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Multi-Layer Perceptron. (arXiv:2108.10097v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10097">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph neural networks (GNNs) have recently achieved state-of-the-art
performance in many graph-based applications. Despite the high expressive
power, they typically need to perform an expensive recursive neighborhood
expansion in multiple training epochs and face a scalability issue. Moreover,
most of them are inflexible since they are restricted to fixed-hop
neighborhoods and insensitive to actual receptive field demands for different
nodes. We circumvent these limitations by introducing a scalable and flexible
Graph Attention Multilayer Perceptron (GAMLP). With the separation of the
non-linear transformation and feature propagation, GAMLP significantly improves
the scalability and efficiency by performing the propagation procedure in a
pre-compute manner. With three principled receptive field attention, each node
in GAMLP is flexible and adaptive in leveraging the propagated features over
the different sizes of reception field. We conduct extensive evaluations on the
three large open graph benchmarks (e.g., ogbn-papers100M, ogbn-products and
ogbn-mag), demonstrating that GAMLP not only achieves the state-of-art
performance, but also additionally provide high scalability and efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Order preserving hierarchical agglomerative clustering. (arXiv:2004.12488v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12488">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Partial orders and directed acyclic graphs are commonly recurring data
structures that arise naturally in numerous domains and applications and are
used to represent ordered relations between entities in the domains. Examples
are task dependencies in a project plan, transaction order in distributed
ledgers and execution sequences of tasks in computer programs, just to mention
a few. We study the problem of order preserving hierarchical clustering of this
kind of ordered data. That is, if we have $a &lt; b$ in the original data and
denote their respective clusters by $[a]$ and $[b]$, then we shall have $[a] &lt;
[b]$ in the produced clustering. The clustering is similarity based and uses
standard linkage functions, such as single- and complete linkage, and is an
extension of classical hierarchical clustering.

To achieve this, we define the output from running classical hierarchical
clustering on strictly ordered data to be partial dendrograms; sub-trees of
classical dendrograms with several connected components. We then construct an
embedding of partial dendrograms over a set into the family of ultrametrics
over the same set. An optimal hierarchical clustering is defined as the partial
dendrogram corresponding to the ultrametric closest to the original
dissimilarity measure, measured in the p-norm. Thus, the method is a
combination of classical hierarchical clustering and ultrametric fitting.

A reference implementation is employed for experiments on both synthetic
random data and real world data from a database of machine parts. When compared
to existing methods, the experiments show that our method excels both in
cluster quality and order preservation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Regression Framework. (arXiv:2109.04447v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04447">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Geographic Information Systems (GIS) and related technologies have generated
substantial interest among statisticians with regard to scalable methodologies
for analyzing large spatial datasets. A variety of scalable spatial process
models have been proposed that can be easily embedded within a hierarchical
modeling framework to carry out Bayesian inference. While the focus of
statistical research has mostly been directed toward innovative and more
complex model development, relatively limited attention has been accorded to
approaches for easily implementable scalable hierarchical models for the
practicing scientist or spatial analyst. This article discusses how
point-referenced spatial process models can be cast as a conjugate Bayesian
linear regression that can rapidly deliver inference on spatial processes. The
approach allows exact sampling directly (avoids iterative algorithms such as
Markov chain Monte Carlo) from the joint posterior distribution of regression
parameters, the latent process and the predictive random variables, and can be
easily implemented on statistical programming environments such as R.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive Binary-Ternary Quantization. (arXiv:1909.12205v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.12205">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural network models are resource hungry. It is difficult to deploy such
deep networks on devices with limited resources, like smart wearables,
cellphones, drones, and autonomous vehicles. Low bit quantization such as
binary and ternary quantization is a common approach to alleviate this resource
requirements. Ternary quantization provides a more flexible model and
outperforms binary quantization in terms of accuracy, however doubles the
memory footprint and increases the computational cost. Contrary to these
approaches, mixed quantized models allow a trade-off between accuracy and
memory footprint. In such models, quantization depth is often chosen manually,
or is tuned using a separate optimization routine. The latter requires training
a quantized network multiple times. Here, we propose an adaptive combination of
binary and ternary quantization, namely Smart Quantization (SQ), in which the
quantization depth is modified directly via a regularization function, so that
the model is trained only once. Our experimental results show that the proposed
method adapts quantization depth successfully while keeping the model accuracy
high on MNIST and CIFAR10 benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Proactive and AoI-aware Failure Recovery for Stateful NFV-enabled Zero-Touch 6G Networks: Model-Free DRL Approach. (arXiv:2103.03817v3 [eess.SP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a Zero-Touch, deep reinforcement learning
(DRL)-based Proactive Failure Recovery framework called ZT-PFR for stateful
network function virtualization (NFV)-enabled networks. To this end, we
formulate a resource-efficient optimization problem minimizing the network cost
function including resource cost and wrong decision penalty. As a solution, we
propose state-of-the-art DRL-based methods such as soft-actor-critic (SAC) and
proximal-policy-optimization (PPO). In addition, to train and test our DRL
agents, we propose a novel impending failure model. Moreover, to keep network
status information at an acceptable freshness level for appropriate
decision-making, we apply the concept of age of information to strike a balance
between the event and scheduling-based monitoring. Several key systems and DRL
algorithm design insights for ZT-PFR are drawn from our analysis and simulation
results. For example, we use a hybrid neural network, consisting long
short-term memory layers in the DRL agents</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Dependencies in Distributed Cloud Applications to Identify and Localize Anomalies. (arXiv:2103.05245v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.05245">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Operation and maintenance of large distributed cloud applications can quickly
become unmanageably complex, putting human operators under immense stress when
problems occur. Utilizing machine learning for identification and localization
of anomalies in such systems supports human experts and enables fast
mitigation. However, due to the various inter-dependencies of system
components, anomalies do not only affect their origin but propagate through the
distributed system. Taking this into account, we present Arvalus and its
variant D-Arvalus, a neural graph transformation method that models system
components as nodes and their dependencies and placement as edges to improve
the identification and localization of anomalies. Given a series of metric
KPIs, our method predicts the most likely system state - either normal or an
anomaly class - and performs localization when an anomaly is detected. During
our experiments, we simulate a distributed cloud application deployment and
synthetically inject anomalies. The evaluation shows the generally good
prediction performance of Arvalus and reveals the advantage of D-Arvalus which
incorporates information about system component dependencies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we introduce back-training, an alternative to self-training for
unsupervised domain adaptation (UDA) from source to target domain. While
self-training generates synthetic training data where natural inputs are
aligned with noisy outputs, back-training results in natural outputs aligned
with noisy inputs. This significantly reduces the gap between the target domain
and synthetic data distribution, and reduces model overfitting to the source
domain. We run UDA experiments on question generation and passage retrieval
from the \textit{Natural Questions} domain to machine learning and biomedical
domains. We find that back-training vastly outperforms self-training by a mean
improvement of 7.8 BLEU-4 points on generation, and 17.6\% top-20 retrieval
accuracy across both domains. We further propose consistency filters to remove
low-quality synthetic data before training. We also release a new
domain-adaptation dataset- \textit{MLQuestions} containing 35K unaligned
questions, 50K unaligned passages, and 3K aligned question-passage pairs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data. (arXiv:2109.04260v1 [cs.MM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04260">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>With the vigorous development of multimedia equipment and applications,
efficient retrieval of large-scale multi-modal data has become a trendy
research topic. Thereinto, hashing has become a prevalent choice due to its
retrieval efficiency and low storage cost. Although multi-modal hashing has
drawn lots of attention in recent years, there still remain some problems. The
first point is that existing methods are mainly designed in batch mode and not
able to efficiently handle streaming multi-modal data. The second point is that
all existing online multi-modal hashing methods fail to effectively handle
unseen new classes which come continuously with streaming data chunks. In this
paper, we propose a new model, termed Online enhAnced SemantIc haShing (OASIS).
We design novel semantic-enhanced representation for data, which could help
handle the new coming classes, and thereby construct the enhanced semantic
objective function. An efficient and effective discrete online optimization
algorithm is further proposed for OASIS. Extensive experiments show that our
method can exceed the state-of-the-art models. For good reproducibility and
benefiting the community, our code and data are already available in
supplementary material and will be made publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">From Weakly Supervised Learning to Biquality Learning: an Introduction. (arXiv:2012.09632v3 [cs.LG] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09632">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The field of Weakly Supervised Learning (WSL) has recently seen a surge of
popularity, with numerous papers addressing different types of &quot;supervision
deficiencies&quot;. In WSL use cases, a variety of situations exists where the
collected &quot;information&quot; is imperfect. The paradigm of WSL attempts to list and
cover these problems with associated solutions. In this paper, we review the
research progress on WSL with the aim to make it as a brief introduction to
this field. We present the three axis of WSL cube and an overview of most of
all the elements of their facets. We propose three measurable quantities that
acts as coordinates in the previously defined cube namely: Quality,
Adaptability and Quantity of information. Thus we suggest that Biquality
Learning framework can be defined as a plan of the WSL cube and propose to
re-discover previously unrelated patches in WSL literature as a unified
Biquality Learning literature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep controlled learning of MDP policies with an application to lost-sales inventory control. (arXiv:2011.15122v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.15122">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent literature established that neural networks can represent good
policies across a range of stochastic dynamic models in supply chain and
logistics. We incorporate variance reduction techniques in a newly proposed
algorithm, to overcome limitations of the model-free algorithms typically
employed to learn such neural network policies. For the classical lost sales
inventory model, the algorithm learns neural network policies that are superior
to those learned using model-free algorithms, while outperforming the best
heuristic benchmarks by an order of magnitude. The algorithm is an interesting
candidate to apply to other stochastic dynamic problems in supply chain and
logistics, because the ideas in its development are generic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dynamic Modeling of Hand-Object Interactions via Tactile Sensing. (arXiv:2109.04378v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tactile sensing is critical for humans to perform everyday tasks. While
significant progress has been made in analyzing object grasping from vision, it
remains unclear how we can utilize tactile sensing to reason about and model
the dynamics of hand-object interactions. In this work, we employ a
high-resolution tactile glove to perform four different interactive activities
on a diversified set of objects. We build our model on a cross-modal learning
framework and generate the labels using a visual processing pipeline to
supervise the tactile model, which can then be used on its own during the test
time. The tactile model aims to predict the 3d locations of both the hand and
the object purely from the touch data by combining a predictive model and a
contrastive learning module. This framework can reason about the interaction
patterns from the tactile data, hallucinate the changes in the environment,
estimate the uncertainty of the prediction, and generalize to unseen objects.
We also provide detailed ablation studies regarding different system designs as
well as visualizations of the predicted trajectories. This work takes a step on
dynamics modeling in hand-object interactions from dense tactile sensing, which
opens the door for future applications in activity learning, human-computer
interactions, and imitation learning for robotics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A divide-and-conquer algorithm for quantum state preparation. (arXiv:2008.01511v2 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01511">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Advantages in several fields of research and industry are expected with the
rise of quantum computers. However, the computational cost to load classical
data in quantum computers can impose restrictions on possible quantum speedups.
Known algorithms to create arbitrary quantum states require quantum circuits
with depth O(N) to load an N-dimensional vector. Here, we show that it is
possible to load an N-dimensional vector with a quantum circuit with
polylogarithmic depth and entangled information in ancillary qubits. Results
show that we can efficiently load data in quantum devices using a
divide-and-conquer strategy to exchange computational time for space. We
demonstrate a proof of concept on a real quantum device and present two
applications for quantum machine learning. We expect that this new loading
strategy allows the quantum speedup of tasks that require to load a significant
volume of information to quantum devices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross DQN: Cross Deep Q Network for Ads Allocation in Feed. (arXiv:2109.04353v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>E-commerce platforms usually display a mixed list of ads and organic items in
feed. One key problem is to allocate the limited slots in the feed to maximize
the overall revenue as well as improve user experience, which requires a good
model for user preference. Instead of modeling the influence of individual
items on user behaviors, the arrangement signal models the influence of the
arrangement of items and may lead to a better allocation strategy. However,
most of previous strategies fail to model such a signal and therefore result in
suboptimal performance. To this end, we propose Cross Deep Q Network (Cross
DQN) to extract the arrangement signal by crossing the embeddings of different
items and processing the crossed sequence in the feed. Our model results in
higher revenue and better user experience than state-of-the-art baselines in
offline experiments. Moreover, our model demonstrates a significant improvement
in the online A/B test and has been fully deployed on Meituan feed to serve
more than 300 millions of customers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantum Machine Learning for Finance. (arXiv:2109.04298v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04298">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum computers are expected to surpass the computational capabilities of
classical computers during this decade, and achieve disruptive impact on
numerous industry sectors, particularly finance. In fact, finance is estimated
to be the first industry sector to benefit from Quantum Computing not only in
the medium and long terms, but even in the short term. This review paper
presents the state of the art of quantum algorithms for financial applications,
with particular focus to those use cases that can be solved via Machine
Learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MutualGraphNet: A novel model for motor imagery classification. (arXiv:2109.04361v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04361">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motor imagery classification is of great significance to humans with mobility
impairments, and how to extract and utilize the effective features from motor
imagery electroencephalogram(EEG) channels has always been the focus of
attention. There are many different methods for the motor imagery
classification, but the limited understanding on human brain requires more
effective methods for extracting the features of EEG data. Graph neural
networks(GNNs) have demonstrated its effectiveness in classifying graph
structures; and the use of GNN provides new possibilities for brain structure
connection feature extraction. In this paper we propose a novel graph neural
network based on the mutual information of the raw EEG channels called
MutualGraphNet. We use the mutual information as the adjacency matrix combined
with the spatial temporal graph convolution network(ST-GCN) could extract the
transition rules of the motor imagery electroencephalogram(EEG) channels data
more effectively. Experiments are conducted on motor imagery EEG data set and
we compare our model with the current state-of-the-art approaches and the
results suggest that MutualGraphNet is robust enough to learn the interpretable
features and outperforms the current state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching. (arXiv:2109.04307v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04307">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Inverse Reinforcement Learning (IRL) is attractive in scenarios where reward
engineering can be tedious. However, prior IRL algorithms use on-policy
transitions, which require intensive sampling from the current policy for
stable and optimal performance. This limits IRL applications in the real world,
where environment interactions can become highly expensive. To tackle this
problem, we present Off-Policy Inverse Reinforcement Learning (OPIRL), which
(1) adopts off-policy data distribution instead of on-policy and enables
significant reduction of the number of interactions with the environment, (2)
learns a stationary reward function that is transferable with high
generalization capabilities on changing dynamics, and (3) leverages
mode-covering behavior for faster convergence. We demonstrate that our method
is considerably more sample efficient and generalizes to novel environments
through the experiments. Our method achieves better or comparable results on
policy performance baselines with significantly fewer interactions.
Furthermore, we empirically show that the recovered reward function generalizes
to different tasks where prior arts are prone to fail.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Energy Attack: On Transferring Adversarial Examples. (arXiv:2109.04300v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04300">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work we propose Energy Attack, a transfer-based black-box
$L_\infty$-adversarial attack. The attack is parameter-free and does not
require gradient approximation. In particular, we first obtain white-box
adversarial perturbations of a surrogate model and divide these perturbations
into small patches. Then we extract the unit component vectors and eigenvalues
of these patches with principal component analysis (PCA). Base on the
eigenvalues, we can model the energy distribution of adversarial perturbations.
We then perform black-box attacks by sampling from the perturbation patches
according to their energy distribution, and tiling the sampled patches to form
a full-size adversarial perturbation. This can be done without the available
access to victim models. Extensive experiments well demonstrate that the
proposed Energy Attack achieves state-of-the-art performance in black-box
attacks on various models and several datasets. Moreover, the extracted
distribution is able to transfer among different model architectures and
different datasets, and is therefore intrinsic to vision architectures.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-Driven Robust Optimization using Unsupervised Deep Learning. (arXiv:2011.09769v3 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.09769">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robust optimization has been established as a leading methodology to approach
decision problems under uncertainty. To derive a robust optimization model, a
central ingredient is to identify a suitable model for uncertainty, which is
called the uncertainty set. An ongoing challenge in the recent literature is to
derive uncertainty sets from given historical data that result in solutions
that are robust regarding future scenarios. In this paper we use an
unsupervised deep learning method to learn and extract hidden structures from
data, leading to non-convex uncertainty sets and better robust solutions. We
prove that most of the classical uncertainty classes are special cases of our
derived sets and that optimizing over them is strongly NP-hard. Nevertheless,
we show that the trained neural networks can be integrated into a robust
optimization model by formulating the adversarial problem as a convex quadratic
mixed-integer program. This allows us to derive robust solutions through an
iterative scenario generation process. In our computational experiments, we
compare this approach to a similar approach using kernel-based support vector
clustering. We find that uncertainty sets derived by the unsupervised deep
learning method find a better description of data and lead to robust solutions
that outperform the comparison method both with respect to objective value and
feasibility.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04404">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Similarity measures are a vital tool for understanding how language models
represent and process language. Standard representational similarity measures
such as cosine similarity and Euclidean distance have been successfully used in
static word embedding models to understand how words cluster in semantic space.
Recently, these measures have been applied to embeddings from contextualized
models such as BERT and GPT-2. In this work, we call into question the
informativity of such measures for contextualized language models. We find that
a small number of rogue dimensions, often just 1-3, dominate these measures.
Moreover, we find a striking mismatch between the dimensions that dominate
similarity measures and those which are important to the behavior of the model.
We show that simple postprocessing techniques such as standardization are able
to correct for rogue dimensions and reveal underlying representational quality.
We argue that accounting for rogue dimensions is essential for any
similarity-based analysis of contextual language models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dubhe: Towards Data Unbiasedness with Homomorphic Encryption in Federated Learning Client Selection. (arXiv:2109.04253v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04253">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning (FL) is a distributed machine learning paradigm that
allows clients to collaboratively train a model over their own local data. FL
promises the privacy of clients and its security can be strengthened by
cryptographic methods such as additively homomorphic encryption (HE). However,
the efficiency of FL could seriously suffer from the statistical heterogeneity
in both the data distribution discrepancy among clients and the global
distribution skewness. We mathematically demonstrate the cause of performance
degradation in FL and examine the performance of FL over various datasets. To
tackle the statistical heterogeneity problem, we propose a pluggable
system-level client selection method named Dubhe, which allows clients to
proactively participate in training, meanwhile preserving their privacy with
the assistance of HE. Experimental results show that Dubhe is comparable with
the optimal greedy method on the classification accuracy, with negligible
encryption and communication overhead.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">fGOT: Graph Distances based on Filters and Optimal Transport. (arXiv:2109.04442v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04442">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph comparison deals with identifying similarities and dissimilarities
between graphs. A major obstacle is the unknown alignment of graphs, as well as
the lack of accurate and inexpensive comparison metrics. In this work we
introduce the filter graph distance. It is an optimal transport based distance
which drives graph comparison through the probability distribution of filtered
graph signals. This creates a highly flexible distance, capable of prioritising
different spectral information in observed graphs, offering a wide range of
choices for a comparison metric. We tackle the problem of graph alignment by
computing graph permutations that minimise our new filter distances, which
implicitly solves the graph comparison problem. We then propose a new
approximate cost function that circumvents many computational difficulties
inherent to graph comparison and permits the exploitation of fast algorithms
such as mirror gradient descent, without grossly sacrificing the performance.
We finally propose a novel algorithm derived from a stochastic version of
mirror gradient descent, which accommodates the non-convexity of the alignment
problem, offering a good trade-off between performance accuracy and speed. The
experiments on graph alignment and classification show that the flexibility
gained through filter graph distances can have a significant impact on
performance, while the difference in speed offered by the approximation cost
makes the framework applicable in practical settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervised Linear Dimension-Reduction Methods: Review, Extensions, and Comparisons. (arXiv:2109.04244v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04244">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Principal component analysis (PCA) is a well-known linear dimension-reduction
method that has been widely used in data analysis and modeling. It is an
unsupervised learning technique that identifies a suitable linear subspace for
the input variable that contains maximal variation and preserves as much
information as possible. PCA has also been used in prediction models where the
original, high-dimensional space of predictors is reduced to a smaller, more
manageable, set before conducting regression analysis. However, this approach
does not incorporate information in the response during the dimension-reduction
stage and hence can have poor predictive performance. To address this concern,
several supervised linear dimension-reduction techniques have been proposed in
the literature. This paper reviews selected techniques, extends some of them,
and compares their performance through simulations. Two of these techniques,
partial least squares (PLS) and least-squares PCA (LSPCA), consistently
outperform the others in this study.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Hardness of PAC-learning stabilizer States with Noise. (arXiv:2102.05174v2 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05174">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the problem of learning stabilizer states with noise in the
Probably Approximately Correct (PAC) framework of Aaronson (2007) for learning
quantum states. In the noiseless setting, an algorithm for this problem was
recently given by Rocchetto (2018), but the noisy case was left open. Motivated
by approaches to noise tolerance from classical learning theory, we introduce
the Statistical Query (SQ) model for PAC-learning quantum states, and prove
that algorithms in this model are indeed resilient to common forms of noise,
including classification and depolarizing noise. We prove an exponential lower
bound on learning stabilizer states in the SQ model. Even outside the SQ model,
we prove that learning stabilizer states with noise is in general as hard as
Learning Parity with Noise (LPN) using classical examples. Our results position
the problem of learning stabilizer states as a natural quantum analogue of the
classical problem of learning parities: easy in the noiseless setting, but
seemingly intractable even with simple forms of noise.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GNisi: A graph network for reconstructing Ising models from multivariate binarized data. (arXiv:2109.04257v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04257">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Ising models are a simple generative approach to describing interacting
binary variables. They have proven useful in a number of biological settings
because they enable one to represent observed many-body correlations as the
separable consequence of many direct, pairwise statistical interactions. The
inference of Ising models from data can be computationally very challenging and
often one must be satisfied with numerical approximations or limited precision.
In this paper we present a novel method for the determination of Ising
parameters from data, called GNisi, which uses a Graph Neural network trained
on known Ising models in order to construct the parameters for unseen data. We
show that GNisi is more accurate than the existing state of the art software,
and we illustrate our method by applying GNisi to gene expression data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DAE : Discriminatory Auto-Encoder for multivariate time-series anomaly detection in air transportation. (arXiv:2109.04247v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04247">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Automatic Dependent Surveillance Broadcast protocol is one of the latest
compulsory advances in air surveillance. While it supports the tracking of the
ever-growing number of aircraft in the air, it also introduces cybersecurity
issues that must be mitigated e.g., false data injection attacks where an
attacker emits fake surveillance information. The recent data sources and tools
available to obtain flight tracking records allow the researchers to create
datasets and develop Machine Learning models capable of detecting such
anomalies in En-Route trajectories. In this context, we propose a novel
multivariate anomaly detection model called Discriminatory Auto-Encoder (DAE).
It uses the baseline of a regular LSTM-based auto-encoder but with several
decoders, each getting data of a specific flight phase (e.g. climbing, cruising
or descending) during its training.To illustrate the DAE&#x27;s efficiency, an
evaluation dataset was created using real-life anomalies as well as
realistically crafted ones, with which the DAE as well as three anomaly
detection models from the literature were evaluated. Results show that the DAE
achieves better results in both accuracy and speed of detection. The dataset,
the models implementations and the evaluation results are available in an
online repository, thereby enabling replicability and facilitating future
experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents a sparse-attention Transformer architecture for modeling
documents that contain large tables. Tables are ubiquitous on the web, and are
rich in information. However, more than 20% of relational tables on the web
have 20 or more rows (Cafarella et al., 2008), and these large tables present a
challenge for current Transformer models, which are typically limited to 512
tokens. Here we propose MATE, a novel Transformer architecture designed to
model the structure of web tables. MATE uses sparse attention in a way that
allows heads to efficiently attend to either rows or columns in a table. This
architecture scales linearly with respect to speed and memory, and can handle
documents containing more than 8000 tokens with current accelerators. MATE also
has a more appropriate inductive bias for tabular data, and sets a new
state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,
2020b), a dataset that involves large documents containing tables, we improve
the best prior result by 19 points.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03437">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transfer learning has fundamentally changed the landscape of natural language
processing (NLP) research. Many existing state-of-the-art models are first
pre-trained on a large text corpus and then fine-tuned on downstream tasks.
However, due to limited data resources from downstream tasks and the extremely
large capacity of pre-trained models, aggressive fine-tuning often causes the
adapted model to overfit the data of downstream tasks and forget the knowledge
of the pre-trained model. To address the above issue in a more principled
manner, we propose a new computational framework for robust and efficient
fine-tuning for pre-trained language models. Specifically, our proposed
framework contains two important ingredients: 1. Smoothness-inducing
regularization, which effectively manages the capacity of the model; 2. Bregman
proximal point optimization, which is a class of trust-region methods and can
prevent knowledge forgetting. Our experiments demonstrate that our proposed
method achieves the state-of-the-art performance on multiple NLP benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The UU-test for Statistical Modeling of Unimodal Data. (arXiv:2008.12537v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.12537">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deciding on the unimodality of a dataset is an important problem in data
analysis and statistical modeling. It allows to obtain knowledge about the
structure of the dataset, ie. whether data points have been generated by a
probability distribution with a single or more than one peaks. Such knowledge
is very useful for several data analysis problems, such as for deciding on the
number of clusters and determining unimodal projections. We propose a technique
called UU-test (Unimodal Uniform test) to decide on the unimodality of a
one-dimensional dataset. The method operates on the empirical cumulative
density function (ecdf) of the dataset. It attempts to build a piecewise linear
approximation of the ecdf that is unimodal and models the data sufficiently in
the sense that the data corresponding to each linear segment follows the
uniform distribution. A unique feature of this approach is that in the case of
unimodality, it also provides a statistical model of the data in the form of a
Uniform Mixture Model. We present experimental results in order to assess the
ability of the method to decide on unimodality and perform comparisons with the
well-known dip-test approach. In addition, in the case of unimodal datasets we
evaluate the Uniform Mixture Models provided by the proposed method using the
test set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimation of Corporate Greenhouse Gas Emissions via Machine Learning. (arXiv:2109.04318v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04318">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As an important step to fulfill the Paris Agreement and achieve net-zero
emissions by 2050, the European Commission adopted the most ambitious package
of climate impact measures in April 2021 to improve the flow of capital towards
sustainable activities. For these and other international measures to be
successful, reliable data is key. The ability to see the carbon footprint of
companies around the world will be critical for investors to comply with the
measures. However, with only a small portion of companies volunteering to
disclose their greenhouse gas (GHG) emissions, it is nearly impossible for
investors to align their investment strategies with the measures. By training a
machine learning model on disclosed GHG emissions, we are able to estimate the
emissions of other companies globally who do not disclose their emissions. In
this paper, we show that our model provides accurate estimates of corporate GHG
emissions to investors such that they are able to align their investments with
the regulatory measures and achieve net-zero goals.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Combining resampling and reweighting for faithful stochastic optimization. (arXiv:2105.14694v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14694">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Many machine learning and data science tasks require solving non-convex
optimization problems. When the loss function is a sum of multiple terms, a
popular method is the stochastic gradient descent. Viewed as a process for
sampling the loss function landscape, the stochastic gradient descent is known
to prefer flat minima. Though this is desired for certain optimization problems
such as in deep learning, it causes issues when the goal is to find the global
minimum, especially if the global minimum resides in a sharp valley.

Illustrated with a simple motivating example, we show that the fundamental
reason is that the difference in the Lipschitz constants of multiple terms in
the loss function causes stochastic gradient descent to experience different
variances at different minima. In order to mitigate this effect and perform
faithful optimization, we propose a combined resampling-reweighting scheme to
balance the variance at local minima and extend to general loss functions. We
explain from the numerical stability perspective how the proposed scheme is
more likely to select the true global minimum, and the local convergence
analysis perspective how it converges to a minimum faster when compared with
the vanilla stochastic gradient descent. Experiments from robust statistics and
computational chemistry are provided to demonstrate the theoretical findings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Black-box Adversarial Attack Strategy with Adjustable Sparsity and Generalizability for Deep Image Classifiers. (arXiv:2004.13002v3 [cs.CR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13002">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Constructing adversarial perturbations for deep neural networks is an
important direction of research. Crafting image-dependent adversarial
perturbations using white-box feedback has hitherto been the norm for such
adversarial attacks. However, black-box attacks are much more practical for
real-world applications. Universal perturbations applicable across multiple
images are gaining popularity due to their innate generalizability. There have
also been efforts to restrict the perturbations to a few pixels in the image.
This helps to retain visual similarity with the original images making such
attacks hard to detect. This paper marks an important step which combines all
these directions of research. We propose the DEceit algorithm for constructing
effective universal pixel-restricted perturbations using only black-box
feedback from the target network. We conduct empirical investigations using the
ImageNet validation set on the state-of-the-art deep neural classifiers by
varying the number of pixels to be perturbed from a meagre 10 pixels to as high
as all pixels in the image. We find that perturbing only about 10% of the
pixels in an image using DEceit achieves a commendable and highly transferable
Fooling Rate while retaining the visual quality. We further demonstrate that
DEceit can be successfully applied to image dependent attacks as well. In both
sets of experiments, we outperformed several state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EEGDnet: Fusing Non-Local and Local Self-Similarity for 1-D EEG Signal Denoising with 2-D Transformer. (arXiv:2109.04235v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04235">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Electroencephalogram (EEG) has shown a useful approach to produce a
brain-computer interface (BCI). One-dimensional (1-D) EEG signal is yet easily
disturbed by certain artifacts (a.k.a. noise) due to the high temporal
resolution. Thus, it is crucial to remove the noise in received EEG signal.
Recently, deep learning-based EEG signal denoising approaches have achieved
impressive performance compared with traditional ones. It is well known that
the characteristics of self-similarity (including non-local and local ones) of
data (e.g., natural images and time-domain signals) are widely leveraged for
denoising. However, existing deep learning-based EEG signal denoising methods
ignore either the non-local self-similarity (e.g., 1-D convolutional neural
network) or local one (e.g., fully connected network and recurrent neural
network). To address this issue, we propose a novel 1-D EEG signal denoising
network with 2-D transformer, namely EEGDnet. Specifically, we comprehensively
take into account the non-local and local self-similarity of EEG signal through
the transformer module. By fusing non-local self-similarity in self-attention
blocks and local self-similarity in feed forward blocks, the negative impact
caused by noises and outliers can be reduced significantly. Extensive
experiments show that, compared with other state-of-the-art models, EEGDnet
achieves much better performance in terms of both quantitative and qualitative
metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The challenge of reproducible ML: an empirical study on the impact of bugs. (arXiv:2109.03991v1 [cs.SE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03991">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reproducibility is a crucial requirement in scientific research. When results
of research studies and scientific papers have been found difficult or
impossible to reproduce, we face a challenge which is called reproducibility
crisis. Although the demand for reproducibility in Machine Learning (ML) is
acknowledged in the literature, a main barrier is inherent non-determinism in
ML training and inference. In this paper, we establish the fundamental factors
that cause non-determinism in ML systems. A framework, ReproduceML, is then
introduced for deterministic evaluation of ML experiments in a real, controlled
environment. ReproduceML allows researchers to investigate software
configuration effects on ML training and inference. Using ReproduceML, we run a
case study: investigation of the impact of bugs inside ML libraries on
performance of ML experiments. This study attempts to quantify the impact that
the occurrence of bugs in a popular ML framework, PyTorch, has on the
performance of trained models. To do so, a comprehensive methodology is
proposed to collect buggy versions of ML libraries and run deterministic ML
experiments using ReproduceML. Our initial finding is that there is no evidence
based on our limited dataset to show that bugs which occurred in PyTorch do
affect the performance of trained models. The proposed methodology as well as
ReproduceML can be employed for further research on non-determinism and bugs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal Reservoir Operations using Long Short-Term Memory Network. (arXiv:2109.04255v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04255">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A reliable forecast of inflows to the reservoir is a key factor in the
optimal operation of reservoirs. Real-time operation of the reservoir based on
forecasts of inflows can lead to substantial economic gains. However, the
forecast of inflow is an intricate task as it has to incorporate the impacts of
climate and hydrological changes. Therefore, the major objective of the present
work is to develop a novel approach based on long short-term memory (LSTM) for
the forecast of inflows. Real-time inflow forecast, in other words, daily
inflow at the reservoir helps in efficient operation of water resources. Also,
daily variations in the release can be monitored efficiently and the
reliability of operation is improved. This work proposes a naive anomaly
detection algorithm baseline based on LSTM. In other words, a strong baseline
to forecast flood and drought for any deep learning-based prediction model. The
practicality of the approach has been demonstrated using the observed daily
data of the past 20 years from Bhakra Dam in India. The results of the
simulations conducted herein clearly indicate the supremacy of the LSTM
approach over the traditional methods of forecasting. Although, experiments are
run on data from Bhakra Dam Reservoir in India, LSTM model, and anomaly
detection algorithm are general purpose and can be applied to any basin with
minimal changes. A distinct practical advantage of the LSTM method presented
herein is that it can adequately simulate non-stationarity and non-linearity in
the historical data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoSmart: An Efficient and Automatic Machine Learning framework for Temporal Relational Data. (arXiv:2109.04115v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal relational data, perhaps the most commonly used data type in
industrial machine learning applications, needs labor-intensive feature
engineering and data analyzing for giving precise model predictions. An
automatic machine learning framework is needed to ease the manual efforts in
fine-tuning the models so that the experts can focus more on other problems
that really need humans&#x27; engagement such as problem definition, deployment, and
business services. However, there are three main challenges for building
automatic solutions for temporal relational data: 1) how to effectively and
automatically mining useful information from the multiple tables and the
relations from them? 2) how to be self-adjustable to control the time and
memory consumption within a certain budget? and 3) how to give generic
solutions to a wide range of tasks? In this work, we propose our solution that
successfully addresses the above issues in an end-to-end automatic way. The
proposed framework, AutoSmart, is the winning solution to the KDD Cup 2019 of
the AutoML Track, which is one of the largest AutoML competition to date (860
teams with around 4,955 submissions). The framework includes automatic data
processing, table merging, feature engineering, and model tuning, with a
time\&amp;memory controller for efficiently and automatically formulating the
models. The proposed framework outperforms the baseline solution significantly
on several datasets in various domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive Federated Optimization. (arXiv:2003.00295v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.00295">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning is a distributed machine learning paradigm in which a
large number of clients coordinate with a central server to learn a model
without sharing their own training data. Standard federated optimization
methods such as Federated Averaging (FedAvg) are often difficult to tune and
exhibit unfavorable convergence behavior. In non-federated settings, adaptive
optimization methods have had notable success in combating such issues. In this
work, we propose federated versions of adaptive optimizers, including Adagrad,
Adam, and Yogi, and analyze their convergence in the presence of heterogeneous
data for general non-convex settings. Our results highlight the interplay
between client heterogeneity and communication efficiency. We also perform
extensive experiments on these methods and show that the use of adaptive
optimizers can significantly improve the performance of federated learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sharp Bounds on the Approximation Rates, Metric Entropy, and $n$-widths of Shallow Neural Networks. (arXiv:2101.12365v7 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.12365">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this article, we study approximation properties of the variation spaces
corresponding to shallow neural networks with a variety of activation
functions. We introduce two main tools for estimating the metric entropy,
approximation rates, and $n$-widths of these spaces. First, we introduce the
notion of a smoothly parameterized dictionary and give upper bounds on the
non-linear approximation rates, metric entropy and $n$-widths of their absolute
convex hull. The upper bounds depend upon the order of smoothness of the
parameterization. This result is applied to dictionaries of ridge functions
corresponding to shallow neural networks, and they improve upon existing
results in many cases. Next, we provide a method for lower bounding the metric
entropy and $n$-widths of variation spaces which contain certain classes of
ridge functions. This result gives sharp lower bounds on the
$L^2$-approximation rates, metric entropy, and $n$-widths for variation spaces
corresponding to neural networks with a range of important activation
functions, including ReLU$^k$, sigmoidal activation functions with bounded
variation, and the B-spline activation functions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04367">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, the textual adversarial attack models become increasingly popular
due to their successful in estimating the robustness of NLP models. However,
existing works have obvious deficiencies. (1) They usually consider only a
single granularity of modification strategies (e.g. word-level or
sentence-level), which is insufficient to explore the holistic textual space
for generation; (2) They need to query victim models hundreds of times to make
a successful attack, which is highly inefficient in practice. To address such
problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to
effectively generate high-quality adversarial samples with fewer queries to
victim models. Furthermore, we propose a reinforcement-learning based method to
train a multi-granularity attack agent through behavior cloning with the expert
knowledge from our MAYA algorithm to further reduce the query times.
Additionally, we also adapt the agent to attack black-box models that only
output labels without confidence scores. We conduct comprehensive experiments
to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two
different black-box attack settings and three benchmark datasets. Experimental
results show that our models achieve overall better attacking performance and
produce more fluent and grammatical adversarial samples compared to baseline
models. Besides, our adversarial attack agent significantly reduces the query
times in both attack settings. Our codes are released at
https://github.com/Yangyi-Chen/MAYA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the use of Wasserstein metric in topological clustering of distributional data. (arXiv:2109.04301v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04301">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper deals with a clustering algorithm for histogram data based on a
Self-Organizing Map (SOM) learning. It combines a dimension reduction by SOM
and the clustering of the data in a reduced space. Related to the kind of data,
a suitable dissimilarity measure between distributions is introduced: the $L_2$
Wasserstein distance. Moreover, the number of clusters is not fixed in advance
but it is automatically found according to a local data density estimation in
the original space. Applications on synthetic and real data sets corroborate
the proposed strategy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04240">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Albeit the universal representational power of pre-trained language models,
adapting them onto a specific NLP task still requires a considerably large
amount of labeled data. Effective task fine-tuning meets challenges when only a
few labeled examples are present for the task. In this paper, we aim to the
address of the problem of few shot task learning by exploiting and transferring
from a different task which admits a related but disparate label space.
Specifically, we devise a label transfer network (LTN) to transform the labels
from source task to the target task of interest for training. Both the LTN and
the model for task prediction are learned via a bi-level optimization
framework, which we term as MetaXT. MetaXT offers a principled solution to best
adapt a pre-trained language model to the target task by transferring knowledge
from the source task. Empirical evaluations on cross-task transfer settings for
four NLP tasks, from two different types of label space disparities,
demonstrate the effectiveness of MetaXT, especially when the labeled data in
the target task is limited.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">System Optimization in Synchronous Federated Training: A Survey. (arXiv:2109.03999v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03999">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The unprecedented demand for collaborative machine learning in a
privacy-preserving manner gives rise to a novel machine learning paradigm
called federated learning (FL). Given a sufficient level of privacy guarantees,
the practicality of an FL system mainly depends on its time-to-accuracy
performance during the training process. Despite bearing some resemblance with
traditional distributed training, FL has four distinct challenges that
complicate the optimization towards shorter time-to-accuracy: information
deficiency, coupling for contrasting factors, client heterogeneity, and huge
configuration space. Motivated by the need for inspiring related research, in
this paper we survey highly relevant attempts in the FL literature and organize
them by the related training phases in the standard workflow: selection,
configuration, and reporting. We also review exploratory work including
measurement studies and benchmarking tools to friendly support FL developers.
Although a few survey articles on FL already exist, our work differs from them
in terms of the focus, classification, and implications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accounting for Variations in Speech Emotion Recognition with Nonparametric Hierarchical Neural Network. (arXiv:2109.04316v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04316">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, deep-learning-based speech emotion recognition models have
outperformed classical machine learning models. Previously, neural network
designs, such as Multitask Learning, have accounted for variations in emotional
expressions due to demographic and contextual factors. However, existing models
face a few constraints: 1) they rely on a clear definition of domains (e.g.
gender, noise condition, etc.) and the availability of domain labels; 2) they
often attempt to learn domain-invariant features while emotion expressions can
be domain-specific. In the present study, we propose the Nonparametric
Hierarchical Neural Network (NHNN), a lightweight hierarchical neural network
model based on Bayesian nonparametric clustering. In comparison to Multitask
Learning approaches, the proposed model does not require domain/task labels. In
our experiments, the NHNN models generally outperform the models with similar
levels of complexity and state-of-the-art models in within-corpus and
cross-corpus tests. Through clustering analysis, we show that the NHNN models
are able to learn group-specific features and bridge the performance gap
between groups.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Experimental Study of Class Imbalance in Federated Learning. (arXiv:2109.04094v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04094">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning is a distributed machine learning paradigm that trains a
global model for prediction based on a number of local models at clients while
local data privacy is preserved. Class imbalance is believed to be one of the
factors that degrades the global model performance. However, there has been
very little research on if and how class imbalance can affect the global
performance. class imbalance in federated learning is much more complex than
that in traditional non-distributed machine learning, due to different class
imbalance situations at local clients. Class imbalance needs to be re-defined
in distributed learning environments. In this paper, first, we propose two new
metrics to define class imbalance -- the global class imbalance degree (MID)
and the local difference of class imbalance among clients (WCS). Then, we
conduct extensive experiments to analyze the impact of class imbalance on the
global performance in various scenarios based on our definition. Our results
show that a higher MID and a larger WCS degrade more the performance of the
global model. Besides, WCS is shown to slow down the convergence of the global
model by misdirecting the optimization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Iterated Vector Fields and Conservatism, with Applications to Federated Learning. (arXiv:2109.03973v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study iterated vector fields and investigate whether they are
conservative, in the sense that they are the gradient of some scalar-valued
function. We analyze the conservatism of various iterated vector fields,
including gradient vector fields associated to loss functions of generalized
linear models. We relate this study to optimization and derive novel
convergence results for federated learning algorithms. In particular, we show
that for certain classes of functions (including non-convex functions),
federated averaging is equivalent to gradient descent on a surrogate loss
function. Finally, we discuss a variety of open questions spanning topics in
geometry, dynamical systems, and optimization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Systematic Approach to Group Fairness in Automated Decision Making. (arXiv:2109.04230v1 [cs.CY])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04230">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While the field of algorithmic fairness has brought forth many ways to
measure and improve the fairness of machine learning models, these findings are
still not widely used in practice. We suspect that one reason for this is that
the field of algorithmic fairness came up with a lot of definitions of
fairness, which are difficult to navigate. The goal of this paper is to provide
data scientists with an accessible introduction to group fairness metrics and
to give some insight into the philosophical reasoning for caring about these
metrics. We will do this by considering in which sense socio-demographic groups
are compared for making a statement on fairness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommend for a Reason: Unlocking the Power of Unsupervised Aspect-Sentiment Co-Extraction. (arXiv:2109.03821v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03821">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Compliments and concerns in reviews are valuable for understanding users&#x27;
shopping interests and their opinions with respect to specific aspects of
certain items. Existing review-based recommenders favor large and complex
language encoders that can only learn latent and uninterpretable text
representations. They lack explicit user attention and item property modeling,
which however could provide valuable information beyond the ability to
recommend items. Therefore, we propose a tightly coupled two-stage approach,
including an Aspect-Sentiment Pair Extractor (ASPE) and an
Attention-Property-aware Rating Estimator (APRE). Unsupervised ASPE mines
Aspect-Sentiment pairs (AS-pairs) and APRE predicts ratings using AS-pairs as
concrete aspect-level evidence. Extensive experiments on seven real-world
Amazon Review Datasets demonstrate that ASPE can effectively extract AS-pairs
which enable APRE to deliver superior accuracy over the leading baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Memory semantization through perturbed and adversarial dreaming. (arXiv:2109.04261v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04261">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Classical theories of memory consolidation emphasize the importance of replay
in extracting semantic information from episodic memories. However, the
characteristic creative nature of dreams suggests that memory semantization may
go beyond merely replaying previous experiences. We propose that
rapid-eye-movement (REM) dreaming is essential for efficient memory
semantization by randomly combining episodic memories to create new, virtual
sensory experiences. We support this hypothesis by implementing a cortical
architecture with hierarchically organized feedforward and feedback pathways,
inspired by generative adversarial networks (GANs). Learning in our model is
organized across three different global brain states mimicking wakefulness,
non-REM (NREM) and REM sleep, optimizing different, but complementary objective
functions. We train the model in an unsupervised fashion on standard datasets
of natural images and evaluate the quality of the learned representations. Our
results suggest that adversarial dreaming during REM sleep is essential for
extracting memory contents, while perturbed dreaming during NREM sleep improves
robustness of the latent representation to noisy sensory inputs. The model
provides a new computational perspective on sleep states, memory replay and
dreams and suggests a cortical implementation of GANs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Social Media Monitoring for IoT Cyber-Threats. (arXiv:2109.04306v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04306">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid development of IoT applications and their use in various fields of
everyday life has resulted in an escalated number of different possible
cyber-threats, and has consequently raised the need of securing IoT devices.
Collecting Cyber-Threat Intelligence (e.g., zero-day vulnerabilities or
trending exploits) from various online sources and utilizing it to proactively
secure IoT systems or prepare mitigation scenarios has proven to be a promising
direction. In this work, we focus on social media monitoring and investigate
real-time Cyber-Threat Intelligence detection from the Twitter stream.
Initially, we compare and extensively evaluate six different machine-learning
based classification alternatives trained with vulnerability descriptions and
tested with real-world data from the Twitter stream to identify the
best-fitting solution. Subsequently, based on our findings, we propose a novel
social media monitoring system tailored to the IoT domain; the system allows
users to identify recent/trending vulnerabilities and exploits on IoT devices.
Finally, to aid research on the field and support the reproducibility of our
results we publicly release all annotated datasets created during this process.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ECQ$^{\text{x}}$: Explainability-Driven Quantization for Low-Bit and Sparse DNNs. (arXiv:2109.04236v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04236">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The remarkable success of deep neural networks (DNNs) in various applications
is accompanied by a significant increase in network parameters and arithmetic
operations. Such increases in memory and computational demands make deep
learning prohibitive for resource-constrained hardware platforms such as mobile
devices. Recent efforts aim to reduce these overheads, while preserving model
performance as much as possible, and include parameter reduction techniques,
parameter quantization, and lossless compression techniques.

In this chapter, we develop and describe a novel quantization paradigm for
DNNs: Our method leverages concepts of explainable AI (XAI) and concepts of
information theory: Instead of assigning weight values based on their distances
to the quantization clusters, the assignment function additionally considers
weight relevances obtained from Layer-wise Relevance Propagation (LRP) and the
information content of the clusters (entropy optimization). The ultimate goal
is to preserve the most relevant weights in quantization clusters of highest
information content.

Experimental results show that this novel Entropy-Constrained and
XAI-adjusted Quantization (ECQ$^{\text{x}}$) method generates ultra
low-precision (2-5 bit) and simultaneously sparse neural networks while
maintaining or even improving model performance. Due to reduced parameter
precision and high number of zero-elements, the rendered networks are highly
compressible in terms of file size, up to $103\times$ compared to the
full-precision unquantized DNN model. Our approach was evaluated on different
types of models and datasets (including Google Speech Commands and CIFAR-10)
and compared with previous work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Where Did You Learn That From? Surprising Effectiveness of Membership Inference Attacks Against Temporally Correlated Data in Deep Reinforcement Learning. (arXiv:2109.03975v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03975">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While significant research advances have been made in the field of deep
reinforcement learning, a major challenge to widespread industrial adoption of
deep reinforcement learning that has recently surfaced but little explored is
the potential vulnerability to privacy breaches. In particular, there have been
no concrete adversarial attack strategies in literature tailored for studying
the vulnerability of deep reinforcement learning algorithms to membership
inference attacks. To address this gap, we propose an adversarial attack
framework tailored for testing the vulnerability of deep reinforcement learning
algorithms to membership inference attacks. More specifically, we design a
series of experiments to investigate the impact of temporal correlation, which
naturally exists in reinforcement learning training data, on the probability of
information leakage. Furthermore, we study the differences in the performance
of \emph{collective} and \emph{individual} membership attacks against deep
reinforcement learning algorithms. Experimental results show that the proposed
adversarial attack framework is surprisingly effective at inferring the data
used during deep reinforcement training with an accuracy exceeding $84\%$ in
individual and $97\%$ in collective mode on two different control tasks in
OpenAI Gym, which raises serious privacy concerns in the deployment of models
resulting from deep reinforcement learning. Moreover, we show that the learning
state of a reinforcement learning algorithm significantly influences the level
of the privacy breach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DAE-PINN: A Physics-Informed Neural Network Model for Simulating Differential-Algebraic Equations with Application to Power Networks. (arXiv:2109.04304v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04304">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning-based surrogate modeling is becoming a promising approach for
learning and simulating dynamical systems. Deep-learning methods, however, find
very challenging learning stiff dynamics. In this paper, we develop DAE-PINN,
the first effective deep-learning framework for learning and simulating the
solution trajectories of nonlinear differential-algebraic equations (DAE),
which present a form of infinite stiffness and describe, for example, the
dynamics of power networks. Our DAE-PINN bases its effectiveness on the synergy
between implicit Runge-Kutta time-stepping schemes (designed specifically for
solving DAEs) and physics-informed neural networks (PINN) (deep neural networks
that we train to satisfy the dynamics of the underlying problem). Furthermore,
our framework (i) enforces the neural network to satisfy the DAEs as
(approximate) hard constraints using a penalty-based method and (ii) enables
simulating DAEs for long-time horizons. We showcase the effectiveness and
accuracy of DAE-PINN by learning and simulating the solution trajectories of a
three-bus power network.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constants of Motion: The Antidote to Chaos in Optimization and Game Dynamics. (arXiv:2109.03974v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03974">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Several recent works in online optimization and game dynamics have
established strong negative complexity results including the formal emergence
of instability and chaos even in small such settings, e.g., $2\times 2$ games.
These results motivate the following question: Which methodological tools can
guarantee the regularity of such dynamics and how can we apply them in standard
settings of interest such as discrete-time first-order optimization dynamics?
We show how proving the existence of invariant functions, i.e., constant of
motions, is a fundamental contribution in this direction and establish a
plethora of such positive results (e.g. gradient descent, multiplicative
weights update, alternating gradient descent and manifold gradient descent)
both in optimization as well as in game settings. At a technical level, for
some conservation laws we provide an explicit and concise closed form, whereas
for other ones we present non-constructive proofs using tools from dynamical
systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Provably efficient RL with Rich Observations via Latent State Decoding. (arXiv:1901.09018v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1901.09018">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the exploration problem in episodic MDPs with rich observations
generated from a small number of latent states. Under certain identifiability
assumptions, we demonstrate how to estimate a mapping from the observations to
latent states inductively through a sequence of regression and clustering steps
-- where previously decoded latent states provide labels for later regression
problems -- and use it to construct good exploration policies. We provide
finite-sample guarantees on the quality of the learned state decoding function
and exploration policies, and complement our theory with an empirical
evaluation on a class of hard exploration problems. Our method exponentially
improves over $Q$-learning with na\&quot;ive exploration, even when $Q$-learning has
cheating access to latent states.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward a Perspectivist Turn in Ground Truthing for Predictive Computing. (arXiv:2109.04270v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04270">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most Artificial Intelligence applications are based on supervised machine
learning (ML), which ultimately grounds on manually annotated data. The
annotation process is often performed in terms of a majority vote and this has
been proved to be often problematic, as highlighted by recent studies on the
evaluation of ML models. In this article we describe and advocate for a
different paradigm, which we call data perspectivism, which moves away from
traditional gold standard datasets, towards the adoption of methods that
integrate the opinions and perspectives of the human subjects involved in the
knowledge representation step of ML processes. Drawing on previous works which
inspired our proposal we describe the potential of our proposal for not only
the more subjective tasks (e.g. those related to human language) but also to
tasks commonly understood as objective (e.g. medical decision making), and
present the main advantages of adopting a perspectivist stance in ML, as well
as possible disadvantages, and various ways in which such a stance can be
implemented in practice. Finally, we share a set of recommendations and outline
a research agenda to advance the perspectivist stance in ML.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing. (arXiv:2107.12809v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Bayesian optimization (BO) is an approach to globally optimizing black-box
objective functions that are expensive to evaluate. BO-powered experimental
design has found wide application in materials science, chemistry, experimental
physics, drug development, etc. This work aims to bring attention to the
benefits of applying BO in designing experiments and to provide a BO manual,
covering both methodology and software, for the convenience of anyone who wants
to apply or learn BO. In particular, we briefly explain the BO technique,
review all the applications of BO in additive manufacturing, compare and
exemplify the features of different open BO libraries, unlock new potential
applications of BO to other types of data (e.g., preferential output). This
article is aimed at readers with some understanding of Bayesian methods, but
not necessarily with knowledge of additive manufacturing; the software
performance overview and implementation instructions are instrumental for any
experimental-design practitioner. Moreover, our review in the field of additive
manufacturing highlights the current knowledge and technological trends of BO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Active Offline Policy Selection. (arXiv:2106.10251v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10251">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper addresses the problem of policy selection in domains with abundant
logged data, but with a very restricted interaction budget. Solving this
problem would enable safe evaluation and deployment of offline reinforcement
learning policies in industry, robotics, and recommendation domains among
others. Several off-policy evaluation (OPE) techniques have been proposed to
assess the value of policies using only logged data. However, there is still a
big gap between the evaluation by OPE and the full online evaluation in the
real environment. At the same time, large amount of online interactions is
often not feasible in practice. To overcome this problem, we introduce
\emph{active offline policy selection} -- a novel sequential decision approach
that combines logged data with online interaction to identify the best policy.
This approach uses OPE estimates to warm start the online evaluation. Then, in
order to utilize the limited environment interactions wisely, it relies on a
Bayesian optimization method, with a kernel function that represents policy
similarity, to decide which policy to evaluate next. We use multiple benchmarks
with a large number of candidate policies to show that the proposed approach
improves upon state-of-the-art OPE estimates and pure online policy evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Opinion summarization has been traditionally approached with unsupervised,
weakly-supervised and few-shot learning techniques. In this work, we collect a
large dataset of summaries paired with user reviews for over 31,000 products,
enabling supervised training. However, the number of reviews per product is
large (320 on average), making summarization - and especially training a
summarizer - impractical. Moreover, the content of many reviews is not
reflected in the human-written summaries, and, thus, the summarizer trained on
random review subsets hallucinates. In order to deal with both of these
challenges, we formulate the task as jointly learning to select informative
subsets of reviews and summarizing the opinions expressed in these subsets. The
choice of the review subset is treated as a latent variable, predicted by a
small and simple selector. The subset is then fed into a more powerful
summarizer. For joint training, we use amortized variational inference and
policy gradient methods. Our experiments demonstrate the importance of
selecting informative reviews resulting in improved quality of summaries and
reduced hallucinations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Channel Coding Benchmark for Meta-Learning. (arXiv:2107.07579v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07579">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Meta-learning provides a popular and effective family of methods for
data-efficient learning of new tasks. However, several important issues in
meta-learning have proven hard to study thus far. For example, performance
degrades in real-world settings where meta-learners must learn from a wide and
potentially multi-modal distribution of training tasks; and when distribution
shift exists between meta-train and meta-test task distributions. These issues
are typically hard to study since the shape of task distributions, and shift
between them are not straightforward to measure or control in standard
benchmarks. We propose the channel coding problem as a benchmark for
meta-learning. Channel coding is an important practical application where task
distributions naturally arise, and fast adaptation to new tasks is practically
valuable. We use our MetaCC benchmark to study several aspects of
meta-learning, including the impact of task distribution breadth and shift,
which can be controlled in the coding problem. Going forward, MetaCC provides a
tool for the community to study the capabilities and limitations of
meta-learning, and to drive research on practically robust and effective
meta-learners.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Interpretable Web-based Glioblastoma Multiforme Prognosis Prediction Tool using Random Forest Model. (arXiv:2108.13039v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We propose predictive models that estimate GBM patients&#x27; health status of
one-year after treatments (Classification task), predict the long-term
prognosis of GBM patients at an individual level (Survival task). We used total
of 467 GBM patients&#x27; clinical profile consists of 13 features and two follow-up
dates. For baseline models of random forest classifier(RFC) and random survival
forest model (RSF), we introduced generalized linear model (GLM), support
vector machine (SVM) and Cox proportional hazardous model (COX), accelerated
failure time model (AFT) respectively. After preprocessing and prefixing
stratified 5-fold data set, we generated best performing models for model types
using recursive feature elimination process. Total 10, 4, and 13 features were
extracted for best performing one-year survival/progression status RFC models
and RSF model via the recursive feature elimination process. In classification
task, AUROC of best performing RFC recorded 0.6990 (for one-year survival
status classification) and 0.7076 (for one-year progression classification)
while that of second best baseline models (GLM in both cases) recorded 0.6691
and 0.6997 respectively. About survival task, the highest C-index of 0.7157 and
the lowest IBS of 0.1038 came from the best performing RSF model while that of
second best baseline models were 0.6556 and 0.1139 respectively. A simplified
linear correlation (extracted from LIME and virtual patient group analysis)
between each feature and prognosis of GBM patient were consistent with proven
medical knowledge. Our machine learning models suggest that the top three
prognostic factors for GBM patient survival were MGMT gene promoter, the extent
of resection, and age. To the best of our knowledge, this study is the very
first study introducing a interpretable and medical knowledge consistent GBM
prognosis predictive models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00922">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal sentiment analysis is a trending area of research, and the
multimodal fusion is one of its most active topic. Acknowledging humans
communicate through a variety of channels (i.e visual, acoustic, linguistic),
multimodal systems aim at integrating different unimodal representations into a
synthetic one. So far, a consequent effort has been made on developing complex
architectures allowing the fusion of these modalities. However, such systems
are mainly trained by minimising simple losses such as $L_1$ or cross-entropy.
In this work, we investigate unexplored penalties and propose a set of new
objectives that measure the dependency between modalities. We demonstrate that
our new penalties lead to a consistent improvement (up to $4.3$ on accuracy)
across a large variety of state-of-the-art models on two well-known sentiment
analysis datasets: \texttt{CMU-MOSI} and \texttt{CMU-MOSEI}. Our method not
only achieves a new SOTA on both datasets but also produces representations
that are more robust to modality drops. Finally, a by-product of our methods
includes a statistical network which can be used to interpret the high
dimensional representations learnt by the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model-predictive control and reinforcement learning in multi-energy system case studies. (arXiv:2104.09785v2 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Model-predictive-control (MPC) offers an optimal control technique to
establish and ensure that the total operation cost of multi-energy systems
remains at a minimum while fulfilling all system constraints. However, this
method presumes an adequate model of the underlying system dynamics, which is
prone to modelling errors and is not necessarily adaptive. This has an
associated initial and ongoing project-specific engineering cost. In this
paper, we present an on- and off-policy multi-objective reinforcement learning
(RL) approach, that does not assume a model a priori, benchmarking this against
a linear MPC (LMPC - to reflect current practice, though non-linear MPC
performs better) - both derived from the general optimal control problem,
highlighting their differences and similarities. In a simple multi-energy
system (MES) configuration case study, we show that a twin delayed deep
deterministic policy gradient (TD3) RL agent offers potential to match and
outperform the perfect foresight LMPC benchmark (101.5%). This while the
realistic LMPC, i.e. imperfect predictions, only achieves 98%. While in a more
complex MES system configuration, the RL agent&#x27;s performance is generally lower
(94.6%), yet still better than the realistic LMPC (88.9%). In both case
studies, the RL agents outperformed the realistic LMPC after a training period
of 2 years using quarterly interactions with the environment. We conclude that
reinforcement learning is a viable optimal control technique for multi-energy
systems given adequate constraint handling and pre-training, to avoid unsafe
interactions and long training periods, as is proposed in fundamental future
work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive importance sampling for seismic fragility curve estimation. (arXiv:2109.04323v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04323">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As part of Probabilistic Risk Assessment studies, it is necessary to study
the fragility of mechanical and civil engineered structures when subjected to
seismic loads. This risk can be measured with fragility curves, which express
the probability of failure of the structure conditionally to a seismic
intensity measure. The estimation of fragility curves relies on time-consuming
numerical simulations, so that careful experimental design is required in order
to gain the maximum information on the structure&#x27;s fragility with a limited
number of code evaluations. We propose and implement an active learning
methodology based on adaptive importance sampling in order to reduce the
variance of the training loss. The efficiency of the proposed method in terms
of bias, standard deviation and prediction interval coverage are theoretically
and numerically characterized.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COLUMBUS: Automated Discovery of New Multi-Level Features for Domain Generalization via Knowledge Corruption. (arXiv:2109.04320v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04320">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine learning models that can generalize to unseen domains are essential
when applied in real-world scenarios involving strong domain shifts. We address
the challenging domain generalization (DG) problem, where a model trained on a
set of source domains is expected to generalize well in unseen domains without
any exposure to their data. The main challenge of DG is that the features
learned from the source domains are not necessarily present in the unseen
target domains, leading to performance deterioration. We assume that learning a
richer set of features is crucial to improve the transfer to a wider set of
unknown domains. For this reason, we propose COLUMBUS, a method that enforces
new feature discovery via a targeted corruption of the most relevant input and
multi-level representations of the data. We conduct an extensive empirical
evaluation to demonstrate the effectiveness of the proposed approach which
achieves new state-of-the-art results by outperforming 18 DG algorithms on
multiple DG benchmark datasets in the DomainBed framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13061">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Structural features are important features in a geometrical graph. Although
there are some correlation analysis of features based on covariance, there is
no relevant research on structural feature correlation analysis with graph
neural networks. In this paper, we introuduce graph feature to feature
(Fea2Fea) prediction pipelines in a low dimensional space to explore some
preliminary results on structural feature correlation, which is based on graph
neural network. The results show that there exists high correlation between
some of the structural features. An irredundant feature combination with
initial node features, which is filtered by graph neural network has improved
its classification accuracy in some graph-based tasks. We compare differences
between concatenation methods on connecting embeddings between features and
show that the simplest is the best. We generalize on the synthetic geometric
graphs and certify the results on prediction difficulty between structural
features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NEAT: Neural Attention Fields for End-to-End Autonomous Driving. (arXiv:2109.04456v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04456">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Efficient reasoning about the semantic, spatial, and temporal structure of a
scene is a crucial prerequisite for autonomous driving. We present NEural
ATtention fields (NEAT), a novel representation that enables such reasoning for
end-to-end imitation learning models. NEAT is a continuous function which maps
locations in Bird&#x27;s Eye View (BEV) scene coordinates to waypoints and
semantics, using intermediate attention maps to iteratively compress
high-dimensional 2D image features into a compact representation. This allows
our model to selectively attend to relevant regions in the input while ignoring
information irrelevant to the driving task, effectively associating the images
with the BEV representation. In a new evaluation setting involving adverse
environmental conditions and challenging scenarios, NEAT outperforms several
strong baselines and achieves driving scores on par with the privileged CARLA
expert used to generate its training data. Furthermore, visualizing the
attention maps for models with NEAT intermediate representations provides
improved interpretability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physics-Guided Deep Learning for Dynamical Systems: A Survey. (arXiv:2107.01272v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01272">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Modeling complex physical dynamics is a fundamental task in science and
engineering. Traditional physics-based models are sample efficient,
interpretable but often rely on rigid assumptions. Furthermore, direct
numerical approximation is usually computationally intensive, requiring
significant computational resources and expertise. While deep learning (DL)
provides novel alternatives for efficiently recognizing complex patterns and
emulating nonlinear dynamics, its predictions do not necessarily obey the
governing laws of physical systems, nor do they generalize well across
different systems. Thus, the study of physics-guided DL emerged and has gained
great progress. Physics-guided DL aims to take the best from both physics-based
modeling and state-of-the-art DL models to better solve scientific problems. In
this paper, we provide a structured overview of existing methodologies of
integrating prior physical knowledge or physics-based modeling into DL, with a
special emphasis on learning dynamical systems. We also discuss the fundamental
challenges and emerging opportunities in the area.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SONIC: A Sparse Neural Network Inference Accelerator with Silicon Photonics for Energy-Efficient Deep Learning. (arXiv:2109.04459v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sparse neural networks can greatly facilitate the deployment of neural
networks on resource-constrained platforms as they offer compact model sizes
while retaining inference accuracy. Because of the sparsity in parameter
matrices, sparse neural networks can, in principle, be exploited in accelerator
architectures for improved energy-efficiency and latency. However, to realize
these improvements in practice, there is a need to explore sparsity-aware
hardware-software co-design. In this paper, we propose a novel silicon
photonics-based sparse neural network inference accelerator called SONIC. Our
experimental analysis shows that SONIC can achieve up to 5.8x better
performance-per-watt and 8.4x lower energy-per-bit than state-of-the-art sparse
electronic neural network accelerators; and up to 13.8x better
performance-per-watt and 27.6x lower energy-per-bit than the best known
photonic neural network accelerators.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QueryNet: An Attack Framework with Surrogates Carrying Multiple Identities. (arXiv:2105.15010v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15010">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep Neural Networks (DNNs) are acknowledged as vulnerable to adversarial
attacks, while the existing black-box attacks require extensive queries on the
victim DNN to achieve high success rates. For query-efficiency, surrogate
models of the victim are adopted as transferable attackers in consideration of
their Gradient Similarity (GS), i.e., surrogates&#x27; attack gradients are similar
to the victim&#x27;s ones to some extent. However, it is generally neglected to
exploit their similarity on outputs, namely the Prediction Similarity (PS), to
filter out inefficient queries. To jointly utilize and also optimize
surrogates&#x27; GS and PS, we develop QueryNet, an efficient attack network that
can significantly reduce queries. QueryNet crafts several transferable
Adversarial Examples (AEs) by surrogates, and then decides also by surrogates
on the most promising AE, which is then sent to query the victim. That is to
say, in QueryNet, surrogates are not only exploited as transferable attackers,
but also as transferability evaluators for AEs. The AEs are generated using
surrogates&#x27; GS and evaluated based on their PS, and therefore, the query
results could be back-propagated to optimize surrogates&#x27; parameters and also
their architectures, enhancing both the GS and the PS. QueryNet has significant
query-efficiency, i.e., reduces queries by averagely about an order of
magnitude compared to recent SOTA methods according to our comprehensive and
real-world experiments: 11 victims (including 2 commercial models) on
MNIST/CIFAR10/ImageNet, allowing only 8-bit image queries, and no access to the
victim&#x27;s training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Leveraging Code Clones and Natural Language Processing for Log Statement Prediction. (arXiv:2109.03859v1 [cs.SE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03859">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Software developers embed logging statements inside the source code as an
imperative duty in modern software development as log files are necessary for
tracking down runtime system issues and troubleshooting system management
tasks. Prior research has emphasized the importance of logging statements in
the operation and debugging of software systems. However, the current logging
process is mostly manual and ad hoc, and thus, proper placement and content of
logging statements remain as challenges. To overcome these challenges, methods
that aim to automate log placement and log content, i.e., &#x27;where, what, and how
to log&#x27;, are of high interest. Thus, we propose to accomplish the goal of this
research, that is &quot;to predict the log statements by utilizing source code
clones and natural language processing (NLP)&quot;, as these approaches provide
additional context and advantage for log prediction. We pursue the following
four research objectives: (RO1) investigate whether source code clones can be
leveraged for log statement location prediction, (RO2) propose a clone-based
approach for log statement prediction, (RO3) predict log statement&#x27;s
description with code-clone and NLP models, and (RO4) examine approaches to
automatically predict additional details of the log statement, such as its
verbosity level and variables. For this purpose, we perform an experimental
analysis on seven open-source java projects, extract their method-level code
clones, investigate their attributes, and utilize them for log location and
description prediction. Our work demonstrates the effectiveness of log-aware
clone detection for automated log location and description prediction and
outperforms the prior work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04400">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In cross-lingual text classification, it is required that task-specific
training data in high-resource source languages are available, where the task
is identical to that of a low-resource target language. However, collecting
such training data can be infeasible because of the labeling cost, task
characteristics, and privacy concerns. This paper proposes an alternative
solution that uses only task-independent word embeddings of high-resource
languages and bilingual dictionaries. First, we construct a dictionary-based
heterogeneous graph (DHG) from bilingual dictionaries. This opens the
possibility to use graph neural networks for cross-lingual transfer. The
remaining challenge is the heterogeneity of DHG because multiple languages are
considered. To address this challenge, we propose dictionary-based
heterogeneous graph neural network (DHGNet) that effectively handles the
heterogeneity of DHG by two-step aggregations, which are word-level and
language-level aggregations. Experimental results demonstrate that our method
outperforms pretrained models even though it does not access to large corpora.
Furthermore, it can perform well even though dictionaries contain many
incorrect translations. Its robustness allows the usage of a wider range of
dictionaries such as an automatically constructed dictionary and crowdsourced
dictionary, which are convenient for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Gradual (In)Compatibility of Fairness Criteria. (arXiv:2109.04399v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04399">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Impossibility results show that important fairness measures (independence,
separation, sufficiency) cannot be satisfied at the same time under reasonable
assumptions. This paper explores whether we can satisfy and/or improve these
fairness measures simultaneously to a certain degree. We introduce
information-theoretic formulations of the fairness measures and define degrees
of fairness based on these formulations. The information-theoretic formulations
suggest unexplored theoretical relations between the three fairness measures.
In the experimental part, we use the information-theoretic expressions as
regularizers to obtain fairness-regularized predictors for three standard
datasets. Our experiments show that a) fairness regularization directly
increases fairness measures, in line with existing work, and b) some fairness
regularizations indirectly increase other fairness measures, as suggested by
our theoretical findings. This establishes that it is possible to increase the
degree to which some fairness measures are satisfied at the same time -- some
fairness measures are gradually compatible.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Intrusion Prevention Policies through Optimal Stopping. (arXiv:2106.07160v7 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study automated intrusion prevention using reinforcement learning. In a
novel approach, we formulate the problem of intrusion prevention as an optimal
stopping problem. This formulation allows us insight into the structure of the
optimal policies, which turn out to be threshold based. Since the computation
of the optimal defender policy using dynamic programming is not feasible for
practical cases, we approximate the optimal policy through reinforcement
learning in a simulation environment. To define the dynamics of the simulation,
we emulate the target infrastructure and collect measurements. Our evaluations
show that the learned policies are close to optimal and that they indeed can be
expressed using thresholds.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04443">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Back-translation (BT) of target monolingual corpora is a widely used data
augmentation strategy for neural machine translation (NMT), especially for
low-resource language pairs. To improve effectiveness of the available BT data,
we introduce HintedBT -- a family of techniques which provides hints (through
tags) to the encoder and decoder. First, we propose a novel method of using
both high and low quality BT data by providing hints (as source tags on the
encoder) to the model about the quality of each source-target pair. We don&#x27;t
filter out low quality data but instead show that these hints enable the model
to learn effectively from noisy data. Second, we address the problem of
predicting whether a source token needs to be translated or transliterated to
the target language, which is common in cross-script translation tasks (i.e.,
where source and target do not share the written script). For such cases, we
propose training the model with additional hints (as target tags on the
decoder) that provide information about the operation required on the source
(translation or both translation and transliteration). We conduct experiments
and detailed analyses on standard WMT benchmarks for three cross-script
low/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our
methods compare favorably with five strong and well established baselines. We
show that using these hints, both separately and together, significantly
improves translation quality and leads to state-of-the-art performance in all
three language pairs in corresponding bilingual settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Adversarial Network: Some Analytical Perspectives. (arXiv:2104.12210v2 [q-fin.MF] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12210">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Ever since its debut, generative adversarial networks (GANs) have attracted
tremendous amount of attention. Over the past years, different variations of
GANs models have been developed and tailored to different applications in
practice. Meanwhile, some issues regarding the performance and training of GANs
have been noticed and investigated from various theoretical perspectives. This
subchapter will start from an introduction of GANs from an analytical
perspective, then move on to the training of GANs via SDE approximations and
finally discuss some applications of GANs in computing high dimensional MFGs as
well as tackling mathematical finance problems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ErfAct: Non-monotonic smooth trainable Activation Functions. (arXiv:2109.04386v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04386">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An activation function is a crucial component of a neural network that
introduces non-linearity in the network. The state-of-the-art performance of a
neural network depends on the perfect choice of an activation function. We
propose two novel non-monotonic smooth trainable activation functions, called
ErfAct-1 and ErfAct-2. Experiments suggest that the proposed functions improve
the network performance significantly compared to the widely used activations
like ReLU, Swish, and Mish. Replacing ReLU by ErfAct-1 and ErfAct-2, we have
5.21% and 5.04% improvement for top-1 accuracy on PreactResNet-34 network in
CIFAR100 dataset, 2.58% and 2.76% improvement for top-1 accuracy on
PreactResNet-34 network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean
average precision (mAP) on SSD300 model in Pascal VOC dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Optimal Classification Trees Against Adversarial Examples. (arXiv:2109.03857v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03857">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Decision trees are a popular choice of explainable model, but just like
neural networks, they suffer from adversarial examples. Existing algorithms for
fitting decision trees robust against adversarial examples are greedy
heuristics and lack approximation guarantees. In this paper we propose ROCT, a
collection of methods to train decision trees that are optimally robust against
user-specified attack models. We show that the min-max optimization problem
that arises in adversarial learning can be solved using a single minimization
formulation for decision trees with 0-1 loss. We propose such formulations in
Mixed-Integer Linear Programming and Maximum Satisfiability, which widely
available solvers can optimize. We also present a method that determines the
upper bound on adversarial accuracy for any model using bipartite matching. Our
experimental results demonstrate that the existing heuristics achieve close to
optimal scores while ROCT achieves state-of-the-art scores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Translate &amp; Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04319">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While multilingual pretrained language models (LMs) fine-tuned on a single
language have shown substantial cross-lingual task transfer capabilities, there
is still a wide performance gap in semantic parsing tasks when target language
supervision is available. In this paper, we propose a novel Translate-and-Fill
(TaF) method to produce silver training data for a multilingual semantic
parser. This method simplifies the popular Translate-Align-Project (TAP)
pipeline and consists of a sequence-to-sequence filler model that constructs a
full parse conditioned on an utterance and a view of the same parse. Our filler
is trained on English data only but can accurately complete instances in other
languages (i.e., translations of the English training utterances), in a
zero-shot fashion. Experimental results on three multilingual semantic parsing
datasets show that data augmentation with TaF reaches accuracies competitive
with similar systems which rely on traditional alignment techniques.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning the Physics of Particle Transport via Transformers. (arXiv:2109.03951v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03951">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Particle physics simulations are the cornerstone of nuclear engineering
applications. Among them radiotherapy (RT) is crucial for society, with 50% of
cancer patients receiving radiation treatments. For the most precise targeting
of tumors, next generation RT treatments aim for real-time correction during
radiation delivery, necessitating particle transport algorithms that yield
precise dose distributions in sub-second times even in highly heterogeneous
patient geometries. This is infeasible with currently available, purely physics
based simulations. In this study, we present a data-driven dose calculation
algorithm predicting the dose deposited by mono-energetic proton beams for
arbitrary energies and patient geometries. Our approach frames particle
transport as sequence modeling, where convolutional layers extract important
spatial features into tokens and the transformer self-attention mechanism
routes information between such tokens in the sequence and a beam energy token.
We train our network and evaluate prediction accuracy using computationally
expensive but accurate Monte Carlo (MC) simulations, considered the gold
standard in particle physics. Our proposed model is 33 times faster than
current clinical analytic pencil beam algorithms, improving upon their accuracy
in the most heterogeneous and challenging geometries. With a relative error of
0.34% and very high gamma pass rate of 99.59% (1%, 3 mm), it also greatly
outperforms the only published similar data-driven proton dose algorithm, even
at a finer grid resolution. Offering MC precision 400 times faster, our model
could overcome a major obstacle that has so far prohibited real-time adaptive
proton treatments and significantly increase cancer treatment efficacy. Its
potential to model physics interactions of other particles could also boost
heavy ion treatment planning procedures limited by the speed of traditional
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NU:BRIEF -- A Privacy-aware Newsletter Personalization Engine for Publishers. (arXiv:2109.03955v1 [cs.DL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03955">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Newsletters have (re-) emerged as a powerful tool for publishers to engage
with their readers directly and more effectively. Despite the diversity in
their audiences, publishers&#x27; newsletters remain largely a one-size-fits-all
offering, which is suboptimal. In this paper, we present NU:BRIEF, a web
application for publishers that enables them to personalize their newsletters
without harvesting personal data. Personalized newsletters build a habit and
become a great conversion tool for publishers, providing an alternative
readers-generated revenue model to a declining ad/clickbait-centered business
model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the estimation of discrete choice models to capture irrational customer behaviors. (arXiv:2109.03882v1 [econ.EM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03882">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Random Utility Maximization model is by far the most adopted framework to
estimate consumer choice behavior. However, behavioral economics has provided
strong empirical evidence of irrational choice behavior, such as halo effects,
that are incompatible with this framework. Models belonging to the Random
Utility Maximization family may therefore not accurately capture such
irrational behavior. Hence, more general choice models, overcoming such
limitations, have been proposed. However, the flexibility of such models comes
at the price of increased risk of overfitting. As such, estimating such models
remains a challenge. In this work, we propose an estimation method for the
recently proposed Generalized Stochastic Preference choice model, which
subsumes the family of Random Utility Maximization models and is capable of
capturing halo effects. Specifically, we show how to use partially-ranked
preferences to efficiently model rational and irrational customer types from
transaction data. Our estimation procedure is based on column generation, where
relevant customer types are efficiently extracted by expanding a tree-like data
structure containing the customer behaviors. Further, we propose a new
dominance rule among customer types whose effect is to prioritize low orders of
interactions among products. An extensive set of experiments assesses the
predictive accuracy of the proposed approach. Our results show that accounting
for irrational preferences can boost predictive accuracy by 12.5% on average,
when tested on a real-world dataset from a large chain of grocery and drug
stores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Learning for Cooperative Multi-Player Multi-Armed Bandits. (arXiv:2109.03818v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce a framework for decentralized online learning for multi-armed
bandits (MAB) with multiple cooperative players. The reward obtained by the
players in each round depends on the actions taken by all the players. It&#x27;s a
team setting, and the objective is common. Information asymmetry is what makes
the problem interesting and challenging. We consider three types of information
asymmetry: action information asymmetry when the actions of the players can&#x27;t
be observed but the rewards received are common; reward information asymmetry
when the actions of the other players are observable but rewards received are
IID from the same distribution; and when we have both action and reward
information asymmetry. For the first setting, we propose a UCB-inspired
algorithm that achieves $O(\log T)$ regret whether the rewards are IID or
Markovian. For the second section, we offer an environment such that the
algorithm given for the first setting gives linear regret. For the third
setting, we show that a variation of the &#x60;explore then commit&#x27; algorithm
achieves almost log regret.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SORNet: Spatial Object-Centric Representations for Sequential Manipulation. (arXiv:2109.03891v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03891">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sequential manipulation tasks require a robot to perceive the state of an
environment and plan a sequence of actions leading to a desired goal state,
where the ability to reason about spatial relationships among object entities
from raw sensor inputs is crucial. Prior works relying on explicit state
estimation or end-to-end learning struggle with novel objects. In this work, we
propose SORNet (Spatial Object-Centric Representation Network), which extracts
object-centric representations from RGB images conditioned on canonical views
of the objects of interest. We show that the object embeddings learned by
SORNet generalize zero-shot to unseen object entities on three spatial
reasoning tasks: spatial relationship classification, skill precondition
classification and relative direction regression, significantly outperforming
baselines. Further, we present real-world robotic experiments demonstrating the
usage of the learned object embeddings in task planning for sequential
manipulation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single Image 3D Object Estimation with Primitive Graph Networks. (arXiv:2109.04153v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04153">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reconstructing 3D object from a single image (RGB or depth) is a fundamental
problem in visual scene understanding and yet remains challenging due to its
ill-posed nature and complexity in real-world scenes. To address those
challenges, we adopt a primitive-based representation for 3D object, and
propose a two-stage graph network for primitive-based 3D object estimation,
which consists of a sequential proposal module and a graph reasoning module.
Given a 2D image, our proposal module first generates a sequence of 3D
primitives from input image with local feature attention. Then the graph
reasoning module performs joint reasoning on a primitive graph to capture the
global shape context for each primitive. Such a framework is capable of taking
into account rich geometry and semantic constraints during 3D structure
recovery, producing 3D objects with more coherent structure even under
challenging viewing conditions. We train the entire graph neural network in a
stage-wise strategy and evaluate it on three benchmarks: Pix3D, ModelNet and
NYU Depth V2. Extensive experiments show that our approach outperforms the
previous state of the arts with a considerable margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intriguing Parameters of Structural Causal Models. (arXiv:2105.12697v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.12697">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years there has been a lot of focus on adversarial attacks,
especially on deep neural networks. Here, we argue that they are more general
in nature and can easily affect a larger class of models, e.g., any
differentiable perturbed optimizers. We further show that such attacks can be
determined by the hidden confounders in a domain, thus drawing a novel
connection between such attacks and causality. Establishing this causal
perspective is characterized by the influence of the structural causal model&#x27;s
data generating process on the subsequent optimization thereby exhibiting
intriguing parameters of the former. We reveal the existence of such parameters
for three combinatorial optimization problems, namely linear assignment,
shortest path and a real world problem of energy systems. Our empirical
examination also unveils worrisome consequences of these attacks on
differentiable perturbed optimizers thereby highlighting the criticality of our
findings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Coordinate Descent Methods for DC Minimization. (arXiv:2109.04228v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Difference-of-Convex (DC) minimization, referring to the problem of
minimizing the difference of two convex functions, has been found rich
applications in statistical learning and studied extensively for decades.
However, existing methods are primarily based on multi-stage convex relaxation,
only leading to weak optimality of critical points. This paper proposes a
coordinate descent method for minimizing DC functions based on sequential
nonconvex approximation. Our approach iteratively solves a nonconvex
one-dimensional subproblem globally, and it is guaranteed to converge to a
coordinate-wise stationary point. We prove that this new optimality condition
is always stronger than the critical point condition and the directional point
condition when the objective function is weakly convex. For comparisons, we
also include a naive variant of coordinate descent methods based on sequential
convex approximation in our study. When the objective function satisfies an
additional regularity condition called \emph{sharpness}, coordinate descent
methods with an appropriate initialization converge \emph{linearly} to the
optimal solution set. Also, for many applications of interest, we show that the
nonconvex one-dimensional subproblem can be computed exactly and efficiently
using a breakpoint searching method. We present some discussions and extensions
of our proposed method. Finally, we have conducted extensive experiments on
several statistical learning tasks to show the superiority of our approach.

Keywords: Coordinate Descent, DC Minimization, DC Programming,
Difference-of-Convex Programs, Nonconvex Optimization, Sparse Optimization,
Binary Optimization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Approximation of Cooperative Heterogeneous Multi-Agent Reinforcement Learning (MARL) using Mean Field Control (MFC). (arXiv:2109.04024v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04024">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Mean field control (MFC) is an effective way to mitigate the curse of
dimensionality of cooperative multi-agent reinforcement learning (MARL)
problems. This work considers a collection of $N_{\mathrm{pop}}$ heterogeneous
agents that can be segregated into $K$ classes such that the $k$-th class
contains $N_k$ homogeneous agents. We aim to prove approximation guarantees of
the MARL problem for this heterogeneous system by its corresponding MFC
problem. We consider three scenarios where the reward and transition dynamics
of all agents are respectively taken to be functions of $(1)$ joint state and
action distributions across all classes, $(2)$ individual distributions of each
class, and $(3)$ marginal distributions of the entire population. We show that,
in these cases, the $K$-class MARL problem can be approximated by MFC with
errors given as
$e_1&#x3D;\mathcal{O}(\frac{\sqrt{|\mathcal{X}||\mathcal{U}|}}{N_{\mathrm{pop}}}\sum_{k}\sqrt{N_k})$,
$e_2&#x3D;\mathcal{O}(\sqrt{|\mathcal{X}||\mathcal{U}|}\sum_{k}\frac{1}{\sqrt{N_k}})$
and
$e_3&#x3D;\mathcal{O}\left(\sqrt{|\mathcal{X}||\mathcal{U}|}\left[\frac{A}{N_{\mathrm{pop}}}\sum_{k\in[K]}\sqrt{N_k}+\frac{B}{\sqrt{N_{\mathrm{pop}}}}\right]\right)$,
respectively, where $A, B$ are some constants and $|\mathcal{X}|,|\mathcal{U}|$
are the sizes of state and action spaces of each agent. Finally, we design a
Natural Policy Gradient (NPG) based algorithm that, in the three cases stated
above, can converge to an optimal MARL policy within $\mathcal{O}(e_j)$ error
with a sample complexity of $\mathcal{O}(e_j^{-3})$, $j\in\{1,2,3\}$,
respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spectral clustering on spherical coordinates under the degree-corrected stochastic blockmodel. (arXiv:2011.04558v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04558">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Spectral clustering is a popular method for community detection in network
graphs: starting from a matrix representation of the graph, the nodes are
clustered on a low dimensional projection obtained from a truncated spectral
decomposition of the matrix. Estimating correctly the number of communities and
the dimension of the reduced latent space is critical for good performance of
spectral clustering algorithms. Furthermore, many real-world graphs, such as
enterprise computer networks studied in cyber-security applications, often
display heterogeneous within-community degree distributions. Such heterogeneous
degree distributions are usually not well captured by standard spectral
clustering algorithms. In this article, a novel spectral clustering algorithm
is proposed for community detection under the degree-corrected stochastic
blockmodel. The proposed method is based on a transformation of the spectral
embedding to spherical coordinates, and a novel modelling assumption in the
transformed space. The method allows for simultaneous and automated selection
of the number of communities and the latent dimension for spectral embeddings
of graphs with uneven node degrees. Results show improved performance over
competing methods in representing computer networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An objective function for order preserving hierarchical clustering. (arXiv:2109.04266v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04266">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present an objective function for similarity based hierarchical clustering
of partially ordered data that preserves the partial order in the sense that if
$x \leq y$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$,
then there is an order relation $\leq&#x27;$ on the clusters for which $[x] \leq&#x27;
|y]$. The model distinguishes itself from existing methods and models for
clustering of ordered data in that the order relation and the similarity are
combined to obtain an optimal hierarchical clustering seeking to satisfy both,
and that the order relation is equipped with a pairwise level of comparability
in the range $[0,1]$. In particular, if the similarity and the order relation
are not aligned, then order preservation may have to yield in favor of
clustering. Finding an optimal solution is NP-hard, so we provide a polynomial
time approximation algorithm, with a relative performance guarantee of
$O(\log^{3/2}n)$, based on successive applications of directed sparsest cut.
The model is an extension of the Dasgupta cost function for divisive
hierarchical clustering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relating Graph Neural Networks to Structural Causal Models. (arXiv:2109.04173v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Causality can be described in terms of a structural causal model (SCM) that
carries information on the variables of interest and their mechanistic
relations. For most processes of interest the underlying SCM will only be
partially observable, thus causal inference tries to leverage any exposed
information. Graph neural networks (GNN) as universal approximators on
structured input pose a viable candidate for causal learning, suggesting a
tighter integration with SCM. To this effect we present a theoretical analysis
from first principles that establishes a novel connection between GNN and SCM
while providing an extended view on general neural-causal models. We then
establish a new model class for GNN-based causal inference that is necessary
and sufficient for causal effect identification. Our empirical illustration on
simulations and standard benchmarks validate our theoretical proofs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Reinforcement Learning with Independently Controllable Subgoals. (arXiv:2109.04150v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To successfully tackle challenging manipulation tasks, autonomous agents must
learn a diverse set of skills and how to combine them. Recently,
self-supervised agents that set their own abstract goals by exploiting the
discovered structure in the environment were shown to perform well on many
different tasks. In particular, some of them were applied to learn basic
manipulation skills in compositional multi-object environments. However, these
methods learn skills without taking the dependencies between objects into
account. Thus, the learned skills are difficult to combine in realistic
environments. We propose a novel self-supervised agent that estimates relations
between environment components and uses them to independently control different
parts of the environment state. In addition, the estimated relations between
objects can be used to decompose a complex goal into a compatible sequence of
subgoals. We show that, by using this framework, an agent can efficiently and
automatically learn manipulation tasks in multi-object environments with
different relations between objects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Cross-domain Image Understanding with Unsupervised Noise Removal. (arXiv:2109.04284v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Deep learning models usually require a large amount of labeled data to
achieve satisfactory performance. In multimedia analysis, domain adaptation
studies the problem of cross-domain knowledge transfer from a label rich source
domain to a label scarce target domain, thus potentially alleviates the
annotation requirement for deep learning models. However, we find that
contemporary domain adaptation methods for cross-domain image understanding
perform poorly when source domain is noisy. Weakly Supervised Domain Adaptation
(WSDA) studies the domain adaptation problem under the scenario where source
data can be noisy. Prior methods on WSDA remove noisy source data and align the
marginal distribution across domains without considering the fine-grained
semantic structure in the embedding space, which have the problem of class
misalignment, e.g., features of cats in the target domain might be mapped near
features of dogs in the source domain. In this paper, we propose a novel
method, termed Noise Tolerant Domain Adaptation, for WSDA. Specifically, we
adopt the cluster assumption and learn cluster discriminatively with class
prototypes in the embedding space. We propose to leverage the location
information of the data points in the embedding space and model the location
information with a Gaussian mixture model to identify noisy source data. We
then design a network which incorporates the Gaussian mixture noise model as a
sub-module for unsupervised noise removal and propose a novel cluster-level
adversarial adaptation method which aligns unlabeled target data with the less
noisy class prototypes for mapping the semantic structure across domains. We
conduct extensive experiments to evaluate the effectiveness of our method on
both general images and medical images from COVID-19 and e-commerce datasets.
The results show that our method significantly outperforms state-of-the-art
WSDA methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Cropping on Twitter: Fairness Metrics, their Limitations, and the Importance of Representation, Design, and Agency. (arXiv:2105.08667v2 [cs.CY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.08667">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Twitter uses machine learning to crop images, where crops are centered around
the part predicted to be the most salient. In fall 2020, Twitter users raised
concerns that the automated image cropping system on Twitter favored
light-skinned over dark-skinned individuals, as well as concerns that the
system favored cropping woman&#x27;s bodies instead of their heads. In order to
address these concerns, we conduct an extensive analysis using formalized group
fairness metrics. We find systematic disparities in cropping and identify
contributing factors, including the fact that the cropping based on the single
most salient point can amplify the disparities because of an effect we term
argmax bias. However, we demonstrate that formalized fairness metrics and
quantitative analysis on their own are insufficient for capturing the risk of
representational harm in automatic cropping. We suggest the removal of
saliency-based cropping in favor of a solution that better preserves user
agency. For developing a new solution that sufficiently address concerns
related to representational harm, our critique motivates a combination of
quantitative and qualitative methods that include human-centered design.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Incentivizing an Unknown Crowd. (arXiv:2109.04226v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04226">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by the common strategic activities in crowdsourcing labeling, we
study the problem of sequential eliciting information without verification
(EIWV) for workers with a heterogeneous and unknown crowd. We propose a
reinforcement learning-based approach that is effective against a wide range of
settings including potential irrationality and collusion among workers. With
the aid of a costly oracle and the inference method, our approach dynamically
decides the oracle calls and gains robustness even under the presence of
frequent collusion activities. Extensive experiments show the advantage of our
approach. Our results also present the first comprehensive experiments of EIWV
on large-scale real datasets and the first thorough study of the effects of
environmental variables.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Juvenile state hypothesis: What we can learn from lottery ticket hypothesis researches?. (arXiv:2109.03862v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The proposition of lottery ticket hypothesis revealed the relationship
between network structure and initialization parameters and the learning
potential of neural networks. The original lottery ticket hypothesis performs
pruning and weight resetting after training convergence, exposing it to the
problem of forgotten learning knowledge and potential high cost of training.
Therefore, we propose a strategy that combines the idea of neural network
structure search with a pruning algorithm to alleviate this problem. This
algorithm searches and extends the network structure on existing winning ticket
sub-network to producing new winning ticket recursively. This allows the
training and pruning process to continue without compromising performance. A
new winning ticket sub-network with deeper network structure, better
generalization ability and better test performance can be obtained in this
recursive manner. This method can solve: the difficulty of training or
performance degradation of the sub-networks after pruning, the forgetting of
the weights of the original lottery ticket hypothesis and the difficulty of
generating winning ticket sub-network when the final network structure is not
given. We validate this strategy on the MNIST and CIFAR-10 datasets. And after
relating it to similar biological phenomena and relevant lottery ticket
hypothesis studies in recent years, we will further propose a new hypothesis to
discuss which factors that can keep a network juvenile, i.e., those possible
factors that influence the learning potential or generalization performance of
a neural network during training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SensiX++: Bringing MLOPs and Multi-tenant Model Serving to Sensory Edge Devices. (arXiv:2109.03947v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03947">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present SensiX++ - a multi-tenant runtime for adaptive model execution
with integrated MLOps on edge devices, e.g., a camera, a microphone, or IoT
sensors. SensiX++ operates on two fundamental principles - highly modular
componentisation to externalise data operations with clear abstractions and
document-centric manifestation for system-wide orchestration. First, a data
coordinator manages the lifecycle of sensors and serves models with correct
data through automated transformations. Next, a resource-aware model server
executes multiple models in isolation through model abstraction, pipeline
automation and feature sharing. An adaptive scheduler then orchestrates the
best-effort executions of multiple models across heterogeneous accelerators,
balancing latency and throughput. Finally, microservices with REST APIs serve
synthesised model predictions, system statistics, and continuous deployment.
Collectively, these components enable SensiX++ to serve multiple models
efficiently with fine-grained control on edge devices while minimising data
operation redundancy, managing data and device heterogeneity, reducing resource
contention and removing manual MLOps. We benchmark SensiX++ with ten different
vision and acoustics models across various multi-tenant configurations on
different edge accelerators (Jetson AGX and Coral TPU) designed for sensory
devices. We report on the overall throughput and quantified benefits of various
automation components of SensiX++ and demonstrate its efficacy to significantly
reduce operational complexity and lower the effort to deploy, upgrade,
reconfigure and serve embedded models on edge devices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04114">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04053">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tables provide valuable knowledge that can be used to verify textual
statements. While a number of works have considered table-based fact
verification, direct alignments of tabular data with tokens in textual
statements are rarely available. Moreover, training a generalized fact
verification model requires abundant labeled training data. In this paper, we
propose a novel system to address these problems. Inspired by counterfactual
causality, our system identifies token-level salience in the statement with
probing-based salience estimation. Salience estimation allows enhanced learning
of fact verification from two perspectives. From one perspective, our system
conducts masked salient token prediction to enhance the model for alignment and
reasoning between the table and the statement. From the other perspective, our
system applies salience-aware data augmentation to generate a more diverse set
of training instances by replacing non-salient terms. Experimental results on
TabFact show the effective improvement by the proposed salience-aware learning
techniques, leading to the new SOTA performance on the benchmark. Our code is
publicly available at https://github.com/luka-group/Salience-aware-Learning .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04101">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal knowledge graph (TKG) reasoning is a crucial task that has gained
increasing research interest in recent years. Most existing methods focus on
reasoning at past timestamps to complete the missing facts, and there are only
a few works of reasoning on known TKGs to forecast future facts. Compared with
the completion task, the forecasting task is more difficult that faces two main
challenges: (1) how to effectively model the time information to handle future
timestamps? (2) how to make inductive inference to handle previously unseen
entities that emerge over time? To address these challenges, we propose the
first reinforcement learning method for forecasting. Specifically, the agent
travels on historical knowledge graph snapshots to search for the answer. Our
method defines a relative time encoding function to capture the timespan
information, and we design a novel time-shaped reward based on Dirichlet
distribution to guide the model learning. Furthermore, we propose a novel
representation method for unseen entities to improve the inductive inference
ability of the model. We evaluate our method for this link prediction task at
future timestamps. Extensive experiments on four benchmark datasets demonstrate
substantial performance improvement meanwhile with higher explainability, less
calculation, and fewer parameters when compared with existing state-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUINT: Node embedding using network hashing. (arXiv:2109.04206v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04206">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Representation learning using network embedding has received tremendous
attention due to its efficacy to solve downstream tasks. Popular embedding
methods (such as deepwalk, node2vec, LINE) are based on a neural architecture,
thus unable to scale on large networks both in terms of time and space usage.
Recently, we proposed BinSketch, a sketching technique for compressing binary
vectors to binary vectors. In this paper, we show how to extend BinSketch and
use it for network hashing. Our proposal named QUINT is built upon BinSketch,
and it embeds nodes of a sparse network onto a low-dimensional space using
simple bi-wise operations. QUINT is the first of its kind that provides
tremendous gain in terms of speed and space usage without compromising much on
the accuracy of the downstream tasks. Extensive experiments are conducted to
compare QUINT with seven state-of-the-art network embedding methods for two end
tasks - link prediction and node classification. We observe huge performance
gain for QUINT in terms of speedup (up to 7000x) and space saving (up to 800x)
due to its bit-wise nature to obtain node embedding. Moreover, QUINT is a
consistent top-performer for both the tasks among the baselines across all the
datasets. Our empirical observations are backed by rigorous theoretical
analysis to justify the effectiveness of QUINT. In particular, we prove that
QUINT retains enough structural information which can be used further to
approximate many topological properties of networks with high confidence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Versions of Gradient Temporal Difference Learning. (arXiv:2109.04033v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04033">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sutton, Szepesv\&#x27;{a}ri and Maei introduced the first gradient
temporal-difference (GTD) learning algorithms compatible with both linear
function approximation and off-policy training. The goal of this paper is (a)
to propose some variants of GTDs with extensive comparative analysis and (b) to
establish new theoretical analysis frameworks for the GTDs. These variants are
based on convex-concave saddle-point interpretations of GTDs, which effectively
unify all the GTDs into a single framework, and provide simple stability
analysis based on recent results on primal-dual gradient dynamics. Finally,
numerical comparative analysis is given to evaluate these approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Local Augmentation for Graph Neural Networks. (arXiv:2109.03856v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03856">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Data augmentation has been widely used in image data and linguistic data but
remains under-explored on graph-structured data. Existing methods focus on
augmenting the graph data from a global perspective and largely fall into two
genres: structural manipulation and adversarial training with feature noise
injection. However, the structural manipulation approach suffers information
loss issues while the adversarial training approach may downgrade the feature
quality by injecting noise. In this work, we introduce the local augmentation,
which enhances node features by its local subgraph structures. Specifically, we
model the data argumentation as a feature generation process. Given the central
node&#x27;s feature, our local augmentation approach learns the conditional
distribution of its neighbors&#x27; features and generates the neighbors&#x27; optimal
feature to boost the performance of downstream tasks. Based on the local
augmentation, we further design a novel framework: LA-GNN, which can apply to
any GNN models in a plug-and-play manner. Extensive experiments and analyses
show that local augmentation consistently yields performance improvement for
various GNN architectures across a diverse set of benchmarks. Code is available
at https://github.com/Soughing0823/LAGNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mean-Square Analysis with An Application to Optimal Dimension Dependence of Langevin Monte Carlo. (arXiv:2109.03839v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sampling algorithms based on discretizations of Stochastic Differential
Equations (SDEs) compose a rich and popular subset of MCMC methods. This work
provides a general framework for the non-asymptotic analysis of sampling error
in 2-Wasserstein distance, which also leads to a bound of mixing time. The
method applies to any consistent discretization of contractive SDEs. When
applied to Langevin Monte Carlo algorithm, it establishes
$\tilde{\mathcal{O}}\left( \frac{\sqrt{d}}{\epsilon} \right)$ mixing time,
without warm start, under the common log-smooth and log-strongly-convex
conditions, plus a growth condition on the 3rd-order derivative of the
potential of target measures at infinity. This bound improves the best
previously known $\tilde{\mathcal{O}}\left( \frac{d}{\epsilon} \right)$ result
and is optimal (in terms of order) in both dimension $d$ and accuracy tolerance
$\epsilon$ for target measures satisfying the aforementioned assumptions. Our
theoretical analysis is further validated by numerical experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PowerGym: A Reinforcement Learning Environment for Volt-Var Control in Power Distribution Systems. (arXiv:2109.03970v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03970">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce PowerGym, an open-source reinforcement learning environment for
Volt-Var control in power distribution systems. Following OpenAI Gym APIs,
PowerGym targets minimizing power loss and voltage violations under physical
networked constraints. PowerGym provides four distribution systems (13Bus,
34Bus, 123Bus, and 8500Node) based on IEEE benchmark systems and design
variants for various control difficulties. To foster generalization, PowerGym
offers a detailed customization guide for users working with their distribution
systems. As a demonstration, we examine state-of-the-art reinforcement learning
algorithms in PowerGym and validate the environment by studying controller
behaviors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stationary Density Estimation of It\^o Diffusions Using Deep Learning. (arXiv:2109.03992v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03992">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we consider the density estimation problem associated with the
stationary measure of ergodic It\^o diffusions from a discrete-time series that
approximate the solutions of the stochastic differential equations. To take an
advantage of the characterization of density function through the stationary
solution of a parabolic-type Fokker-Planck PDE, we proceed as follows. First,
we employ deep neural networks to approximate the drift and diffusion terms of
the SDE by solving appropriate supervised learning tasks. Subsequently, we
solve a steady-state Fokker-Plank equation associated with the estimated drift
and diffusion coefficients with a neural-network-based least-squares method. We
establish the convergence of the proposed scheme under appropriate mathematical
assumptions, accounting for the generalization errors induced by regressing the
drift and diffusion coefficients, and the PDE solvers. This theoretical study
relies on a recent perturbation theory of Markov chain result that shows a
linear dependence of the density estimation to the error in estimating the
drift term, and generalization error results of nonparametric regression and of
PDE regression solution obtained with neural-network models. The effectiveness
of this method is reflected by numerical simulations of a two-dimensional
Student&#x27;s t distribution and a 20-dimensional Langevin dynamics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mapping Research Topics in Software Testing: A Bibliometric Analysis. (arXiv:2109.04086v1 [cs.DL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04086">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this study, we apply co-word analysis - a text mining technique based on
the co-occurrence of terms - to map the topology of software testing research
topics, with the goal of providing current and prospective researchers with a
map, and observations about the evolution, of the software testing field. Our
analysis enables the mapping of software testing research into clusters of
connected topics, from which emerge a total of 16 high-level research themes
and a further 18 subthemes. This map also suggests topics that are growing in
importance, including topics related to web and mobile applications and
artificial intelligence. Exploration of author and country-based collaboration
patterns offers similar insight into the implicit and explicit factors that
influence collaboration and suggests emerging sources of collaboration for
future work. We make our observations - and the underlying mapping of research
topics and research collaborations - available so that researchers can gain a
deeper understanding of the topology of the software testing field, inspiration
regarding new areas and connections to explore, and collaborators who will
broaden their perspectives.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine learning modeling of family wide enzyme-substrate specificity screens. (arXiv:2109.03900v1 [q-bio.BM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Biocatalysis is a promising approach to sustainably synthesize
pharmaceuticals, complex natural products, and commodity chemicals at scale.
However, the adoption of biocatalysis is limited by our ability to select
enzymes that will catalyze their natural chemical transformation on non-natural
substrates. While machine learning and in silico directed evolution are
well-posed for this predictive modeling challenge, efforts to date have
primarily aimed to increase activity against a single known substrate, rather
than to identify enzymes capable of acting on new substrates of interest. To
address this need, we curate 6 different high-quality enzyme family screens
from the literature that each measure multiple enzymes against multiple
substrates. We compare machine learning-based compound-protein interaction
(CPI) modeling approaches from the literature used for predicting drug-target
interactions. Surprisingly, comparing these interaction-based models against
collections of independent (single task) enzyme-only or substrate-only models
reveals that current CPI approaches are incapable of learning interactions
between compounds and proteins in the current family level data regime. We
further validate this observation by demonstrating that our no-interaction
baseline can outperform CPI-based models from the literature used to guide the
discovery of kinase inhibitors. Given the high performance of non-interaction
based models, we introduce a new structure-based strategy for pooling residue
representations across a protein sequence. Altogether, this work motivates a
principled path forward in order to build and evaluate meaningful predictive
models for biocatalysis and other drug discovery applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03892">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We investigate the use of multimodal information contained in images as an
effective method for enhancing the commonsense of Transformer models for text
generation. We perform experiments using BART and T5 on concept-to-text
generation, specifically the task of generative commonsense reasoning, or
CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text
Generation. VisCTG involves captioning images representing appropriate everyday
scenarios, and using these captions to enrich and steer the generation process.
Comprehensive evaluation and analysis demonstrate that VisCTG noticeably
improves model performance while successfully addressing several issues of the
baseline generations, including poor commonsense, fluency, and specificity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While recent research on natural language inference has considerably
benefited from large annotated datasets, the amount of inference-related
knowledge (including commonsense) provided in the annotated data is still
rather limited. There have been two lines of approaches that can be used to
further address the limitation: (1) unsupervised pretraining can leverage
knowledge in much larger unstructured text data; (2) structured (often
human-curated) knowledge has started to be considered in neural-network-based
models for NLI. An immediate question is whether these two approaches
complement each other, or how to develop models that can bring together their
advantages. In this paper, we propose models that leverage structured knowledge
in different components of pre-trained models. Our results show that the
proposed models perform better than previous BERT-based state-of-the-art
models. Although our models are proposed for NLI, they can be easily extended
to other sentence or sentence-pair classification problems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bag of Tricks for Optimizing Transformer Efficiency. (arXiv:2109.04030v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04030">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Improving Transformer efficiency has become increasingly attractive recently.
A wide range of methods has been proposed, e.g., pruning, quantization, new
architectures and etc. But these methods are either sophisticated in
implementation or dependent on hardware. In this paper, we show that the
efficiency of Transformer can be improved by combining some simple and
hardware-agnostic methods, including tuning hyper-parameters, better design
choices and training strategies. On the WMT news translation tasks, we improve
the inference efficiency of a strong Transformer system by 3.80X on CPU and
2.52X on GPU. The code is publicly available at
https://github.com/Lollipop321/mini-decoder-network.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MaterialsAtlas.org: A Materials Informatics Web App Platform for Materials Discovery and Survey of State-of-the-Art. (arXiv:2109.04007v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04007">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The availability and easy access of large scale experimental and
computational materials data have enabled the emergence of accelerated
development of algorithms and models for materials property prediction,
structure prediction, and generative design of materials. However, lack of
user-friendly materials informatics web servers has severely constrained the
wide adoption of such tools in the daily practice of materials screening,
tinkering, and design space exploration by materials scientists. Herein we
first survey current materials informatics web apps and then propose and
develop MaterialsAtlas.org, a web based materials informatics toolbox for
materials discovery, which includes a variety of routinely needed tools for
exploratory materials discovery, including materials composition and structure
check (e.g. for neutrality, electronegativity balance, dynamic stability,
Pauling rules), materials property prediction (e.g. band gap, elastic moduli,
hardness, thermal conductivity), and search for hypothetical materials. These
user-friendly tools can be freely accessed at \url{www.materialsatlas.org}. We
argue that such materials informatics apps should be widely developed by the
community to speed up the materials discovery processes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Popularity Adjusted Block Models are Generalized Random Dot Product Graphs. (arXiv:2109.04010v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04010">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We connect two random graph models, the Popularity Adjusted Block Model
(PABM) and the Generalized Random Dot Product Graph (GRDPG), by demonstrating
that the PABM is a special case of the GRDPG in which communities correspond to
mutually orthogonal subspaces of latent vectors. This insight allows us to
construct new algorithms for community detection and parameter estimation for
the PABM, as well as improve an existing algorithm that relies on Sparse
Subspace Clustering. Using established asymptotic properties of Adjacency
Spectral Embedding for the GRDPG, we derive asymptotic properties of these
algorithms. In particular, we demonstrate that the absolute number of community
detection errors tends to zero as the number of graph vertices tends to
infinity. Simulation experiments illustrate these properties.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sensitive Samples Revisited: Detecting Neural Network Attacks Using Constraint Solvers. (arXiv:2109.03966v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03966">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Neural Networks are used today in numerous security- and safety-relevant
domains and are, as such, a popular target of attacks that subvert their
classification capabilities, by manipulating the network parameters. Prior work
has introduced sensitive samples -- inputs highly sensitive to parameter
changes -- to detect such manipulations, and proposed a gradient ascent-based
approach to compute them. In this paper we offer an alternative, using symbolic
constraint solvers. We model the network and a formal specification of a
sensitive sample in the language of the solver and ask for a solution. This
approach supports a rich class of queries, corresponding, for instance, to the
presence of certain types of attacks. Unlike earlier techniques, our approach
does not depend on convex search domains, or on the suitability of a starting
point for the search. We address the performance limitations of constraint
solvers by partitioning the search space for the solver, and exploring the
partitions according to a balanced schedule that still retains completeness of
the search. We demonstrate the impact of the use of solvers in terms of
functionality and search efficiency, using a case study for the detection of
Trojan attacks on Neural Networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generation, augmentation, and alignment: A pseudo-source domain based method for source-free domain adaptation. (arXiv:2109.04015v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04015">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conventional unsupervised domain adaptation (UDA) methods need to access both
labeled source samples and unlabeled target samples simultaneously to train the
model. While in some scenarios, the source samples are not available for the
target domain due to data privacy and safety. To overcome this challenge,
recently, source-free domain adaptation (SFDA) has attracted the attention of
researchers, where both a trained source model and unlabeled target samples are
given. Existing SFDA methods either adopt a pseudo-label based strategy or
generate more samples. However, these methods do not explicitly reduce the
distribution shift across domains, which is the key to a good adaptation.
Although there are no source samples available, fortunately, we find that some
target samples are very similar to the source domain and can be used to
approximate the source domain. This approximated domain is denoted as the
pseudo-source domain. In this paper, inspired by this observation, we propose a
novel method based on the pseudo-source domain. The proposed method firstly
generates and augments the pseudo-source domain, and then employs distribution
alignment with four novel losses based on pseudo-label based strategy. Among
them, a domain adversarial loss is introduced between the pseudo-source domain
the remaining target domain to reduce the distribution shift. The results on
three real-world datasets verify the effectiveness of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tom: Leveraging trend of the observed gradients for faster convergence. (arXiv:2109.03820v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The success of deep learning can be attributed to various factors such as
increase in computational power, large datasets, deep convolutional neural
networks, optimizers etc. Particularly, the choice of optimizer affects the
generalization, convergence rate, and training stability. Stochastic Gradient
Descent (SGD) is a first order iterative optimizer that updates the gradient
uniformly for all parameters. This uniform update may not be suitable across
the entire training phase. A rudimentary solution for this is to employ a
fine-tuned learning rate scheduler which decreases learning rate as a function
of iteration. To eliminate the dependency of learning rate schedulers, adaptive
gradient optimizers such as AdaGrad, AdaDelta, RMSProp, Adam employ a
parameter-wise scaling term for learning rate which is a function of the
gradient itself. We propose Tom (Trend over Momentum) optimizer, which is a
novel variant of Adam that takes into account of the trend which is observed
for the gradients in the loss landscape traversed by the neural network. In the
proposed Tom optimizer, an additional smoothing equation is introduced to
address the trend observed during the process of optimization. The smoothing
parameter introduced for the trend requires no tuning and can be used with
default values. Experimental results for classification datasets such as
CIFAR-10, CIFAR-100 and CINIC-10 image datasets show that Tom outperforms
Adagrad, Adadelta, RMSProp and Adam in terms of both accuracy and has a faster
convergence. The source code is publicly made available at
https://github.com/AnirudhMaiya/Tom</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Explanations via the Axiomatic Causal Lens. (arXiv:2109.03890v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03890">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Explaining the decisions of black-box models has been a central theme in the
study of trustworthy ML. Numerous measures have been proposed in the
literature; however, none of them have been able to adopt a provably causal
take on explainability. Building upon Halpern and Pearl&#x27;s formal definition of
a causal explanation, we derive an analogous set of axioms for the
classification setting, and use them to derive three explanation measures. Our
first measure is a natural adaptation of Chockler and Halpern&#x27;s notion of
causal responsibility, whereas the other two correspond to existing
game-theoretic influence measures. We present an axiomatic treatment for our
proposed indices, showing that they can be uniquely characterized by a set of
desirable properties. We compliment this with computational analysis, providing
probabilistic approximation schemes for all of our proposed measures. Thus, our
work is the first to formally bridge the gap between model explanations,
game-theoretic influence, and causal analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multilingual neural machine translation (MNMT) learns to translate multiple
language pairs with a single model, potentially improving both the accuracy and
the memory-efficiency of deployed models. However, the heavy data imbalance
between languages hinders the model from performing uniformly across language
pairs. In this paper, we propose a new learning objective for MNMT based on
distributionally robust optimization, which minimizes the worst-case expected
loss over the set of language pairs. We further show how to practically
optimize this objective for large translation corpora using an iterated best
response scheme, which is both effective and incurs negligible additional
computational cost compared to standard empirical risk minimization. We perform
extensive experiments on three sets of languages from two datasets and show
that our method consistently outperforms strong baseline methods in terms of
average and per-language performance under both many-to-one and one-to-many
translation settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Attacks on IoT Devices using Featureless 1D-CNN. (arXiv:2109.03989v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03989">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The generalization of deep learning has helped us, in the past, address
challenges such as malware identification and anomaly detection in the network
security domain. However, as effective as it is, scarcity of memory and
processing power makes it difficult to perform these tasks in Internet of
Things (IoT) devices. This research finds an easy way out of this bottleneck by
depreciating the need for feature engineering and subsequent processing in
machine learning techniques. In this study, we introduce a Featureless machine
learning process to perform anomaly detection. It uses unprocessed byte streams
of packets as training data. Featureless machine learning enables a low cost
and low memory time-series analysis of network traffic. It benefits from
eliminating the significant investment in subject matter experts and the time
required for feature engineering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning the hypotheses space from data through a U-curve algorithm: a statistically consistent complexity regularizer for Model Selection. (arXiv:2109.03866v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03866">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a data-driven systematic, consistent and non-exhaustive
approach to Model Selection, that is an extension of the classical agnostic PAC
learning model. In this approach, learning problems are modeled not only by a
hypothesis space $\mathcal{H}$, but also by a Learning Space
$\mathbb{L}(\mathcal{H})$, a poset of subspaces of $\mathcal{H}$, which covers
$\mathcal{H}$ and satisfies a property regarding the VC dimension of related
subspaces, that is a suitable algebraic search space for Model Selection
algorithms. Our main contributions are a data-driven general learning algorithm
to perform regularized Model Selection on $\mathbb{L}(\mathcal{H})$ and a
framework under which one can, theoretically, better estimate a target
hypothesis with a given sample size by properly modeling
$\mathbb{L}(\mathcal{H})$ and employing high computational power. A remarkable
consequence of this approach are conditions under which a non-exhaustive search
of $\mathbb{L}(\mathcal{H})$ can return an optimal solution. The results of
this paper lead to a practical property of Machine Learning, that the lack of
experimental data may be mitigated by a high computational capacity. In a
context of continuous popularization of computational power, this property may
help understand why Machine Learning has become so important, even where data
is expensive and hard to get.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AdjointNet: Constraining machine learning models with physics-based codes. (arXiv:2109.03956v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03956">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Physics-informed Machine Learning has recently become attractive for learning
physical parameters and features from simulation and observation data. However,
most existing methods do not ensure that the physics, such as balance laws
(e.g., mass, momentum, energy conservation), are constrained. Some recent works
(e.g., physics-informed neural networks) softly enforce physics constraints by
including partial differential equation (PDE)-based loss functions but need
re-discretization of the PDEs using auto-differentiation. Training these neural
nets on observational data showed that one could solve forward and inverse
problems in one shot. They evaluate the state variables and the parameters in a
PDE. This re-discretization of PDEs is not necessarily an attractive option for
domain scientists that work with physics-based codes that have been developed
for decades with sophisticated discretization techniques to solve complex
process models and advanced equations of state. This paper proposes a physics
constrained machine learning framework, AdjointNet, allowing domain scientists
to embed their physics code in neural network training workflows. This
embedding ensures that physics is constrained everywhere in the domain.
Additionally, the mathematical properties such as consistency, stability, and
convergence vital to the numerical solution of a PDE are still satisfied. We
show that the proposed AdjointNet framework can be used for parameter
estimation (and uncertainty quantification by extension) and experimental
design using active learning. The applicability of our framework is
demonstrated for four flow cases. Results show that AdjointNet-based inversion
can estimate process model parameters with reasonable accuracy. These examples
demonstrate the applicability of using existing software with no changes in
source code to perform accurate and reliable inversion of model parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Initialization for Nonnegative Matrix Factorization: a Comprehensive Review. (arXiv:2109.03874v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Non-negative matrix factorization (NMF) has become a popular method for
representing meaningful data by extracting a non-negative basis feature from an
observed non-negative data matrix. Some of the unique features of this method
in identifying hidden data put this method amongst the powerful methods in the
machine learning area. The NMF is a known non-convex optimization problem and
the initial point has a significant effect on finding an efficient local
solution. In this paper, we investigate the most popular initialization
procedures proposed for NMF so far. We describe each method and present some of
their advantages and disadvantages. Finally, some numerical results to
illustrate the performance of each algorithm are presented.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study Comparative Preference Classification (CPC) which aims at predicting
whether a preference comparison exists between two entities in a given sentence
and, if so, which entity is preferred over the other. High-quality CPC models
can significantly benefit applications such as comparative question answering
and review-based recommendations. Among the existing approaches, non-deep
learning methods suffer from inferior performances. The state-of-the-art graph
neural network-based ED-GAT (Ma et al., 2020) only considers syntactic
information while ignoring the critical semantic relations and the sentiments
to the compared entities. We proposed sentiment Analysis Enhanced COmparative
Network (SAECON) which improves CPC ac-curacy with a sentiment analyzer that
learns sentiments to individual entities via domain adaptive knowledge
transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset
present a significant improvement on the F1 scores over the best existing CPC
approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Simplified Quantum Algorithm for the Oracle Identification Problem. (arXiv:2109.03902v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03902">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In the oracle identification problem we have oracle access to bits of an
unknown string $x$ of length $n$, with the promise that it belongs to a known
set $C\subseteq\{0,1\}^n$. The goal is to identify $x$ using as few queries to
the oracle as possible. We develop a quantum query algorithm for this problem
with query complexity $O\left(\sqrt{\frac{n\log M }{\log(n/\log M)+1}}\right)$,
where $M$ is the size of $C$. This bound is already derived by Kothari in 2014,
for which we provide a more elegant simpler proof.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LSB: Local Self-Balancing MCMC in Discrete Spaces. (arXiv:2109.03867v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03867">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Markov Chain Monte Carlo (MCMC) methods are promising solutions to sample
from target distributions in high dimensions. While MCMC methods enjoy nice
theoretical properties, like guaranteed convergence and mixing to the true
target, in practice their sampling efficiency depends on the choice of the
proposal distribution and the target at hand. This work considers using machine
learning to adapt the proposal distribution to the target, in order to improve
the sampling efficiency in the purely discrete domain. Specifically, (i) it
proposes a new parametrization for a family of proposal distributions, called
locally balanced proposals, (ii) it defines an objective function based on
mutual information and (iii) it devises a learning procedure to adapt the
parameters of the proposal to the target, thus achieving fast convergence and
fast mixing. We call the resulting sampler as the Locally Self-Balancing
Sampler (LSB). We show through experimental analysis on the Ising model and
Bayesian networks that LSB is indeed able to improve the efficiency over a
state-of-the-art sampler based on locally balanced proposals, thus reducing the
number of iterations required to converge, while achieving comparable mixing
performance.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-09">2021-09-09</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03630">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It has been shown for English that discrete and soft prompting perform
strongly in few-shot learning with pretrained language models (PLMs). In this
paper, we show that discrete and soft prompting perform better than finetuning
in multilingual cases: Crosslingual transfer and in-language training of
multilingual natural language inference. For example, with 48 English training
examples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely
surpassing the majority baseline (33.33%). In contrast, discrete and soft
prompting outperform finetuning, achieving 36.43% and 38.79%. We also
demonstrate good performance of prompting with training data in multiple
languages other than English.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03587">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sarcasm employs ambivalence, where one says something positive but actually
means negative, and vice versa. Due to the sophisticated and obscure sentiment,
sarcasm brings in great challenges to sentiment analysis. In this paper, we
show up the essence of sarcastic text is that the literal sentiment (expressed
by the surface form of the text) is opposite to the deep sentiment (expressed
by the actual meaning of the text). To this end, we propose a Dual-Channel
Framework by modeling both literal and deep sentiments to recognize the
sentiment conflict. Specifically, the proposed framework is capable of
detecting the sentiment conflict between the literal and deep meanings of the
input text. Experiments on the political debates and the Twitter datasets show
that our framework achieves the best performance on sarcasm recognition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Collaborative software development is an integral part of the modern software
development life cycle, essential to the success of large-scale software
projects. When multiple developers make concurrent changes around the same
lines of code, a merge conflict may occur. Such conflicts stall pull requests
and continuous integration pipelines for hours to several days, seriously
hurting developer productivity.

In this paper, we introduce MergeBERT, a novel neural program merge framework
based on the token-level three-way differencing and a transformer encoder
model. Exploiting restricted nature of merge conflict resolutions, we
reformulate the task of generating the resolution sequence as a classification
task over a set of primitive merge patterns extracted from real-world merge
commit data.

Our model achieves 64--69% precision of merge resolution synthesis, yielding
nearly a 2x performance improvement over existing structured and neural program
merge tools. Finally, we demonstrate versatility of our model, which is able to
perform program merge in a multilingual setting with Java, JavaScript,
TypeScript, and C# programming languages, generalizing zero-shot to unseen
languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Simultaneous machine translation has recently gained traction thanks to
significant quality improvements and the advent of streaming applications.
Simultaneous translation systems need to find a trade-off between translation
quality and response time, and with this purpose multiple latency measures have
been proposed. However, latency evaluations for simultaneous translation are
estimated at the sentence level, not taking into account the sequential nature
of a streaming scenario. Indeed, these sentence-level latency measures are not
well suited for continuous stream translation resulting in figures that are not
coherent with the simultaneous translation policy of the system being assessed.
This work proposes a stream-level adaptation of the current latency measures
based on a re-segmentation approach applied to the output translation, that is
successfully evaluated on streaming conditions for a reference IWSLT task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03808">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent work on multilingual AMR-to-text generation has exclusively focused on
data augmentation strategies that utilize silver AMR. However, this assumes a
high quality of generated AMRs, potentially limiting the transferability to the
target task. In this paper, we investigate different techniques for
automatically generating AMR annotations, where we aim to study which source of
information yields better multilingual results. Our models trained on gold AMR
with silver (machine translated) sentences outperform approaches which leverage
generated silver AMR. We find that combining both complementary sources of
information further improves multilingual AMR-to-text generation. Our models
surpass the previous state of the art for German, Italian, Spanish, and Chinese
by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08663">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing neural information retrieval (IR) models have often been studied in
homogeneous and narrow settings, which has considerably limited insights into
their out-of-distribution (OOD) generalization capabilities. To address this,
and to facilitate researchers to broadly evaluate the effectiveness of their
models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous
evaluation benchmark for information retrieval. We leverage a careful selection
of 18 publicly available datasets from diverse text retrieval tasks and domains
and evaluate 10 state-of-the-art retrieval systems including lexical, sparse,
dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our
results show BM25 is a robust baseline and re-ranking and
late-interaction-based models on average achieve the best zero-shot
performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efficient but often
underperform other approaches, highlighting the considerable room for
improvement in their generalization capabilities. We hope this framework allows
us to better evaluate and understand existing retrieval systems, and
contributes to accelerating progress towards better robust and generalizable
systems in the future. BEIR is publicly available at
https://github.com/UKPLab/beir.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03731">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Policy compliance detection is the task of ensuring that a scenario conforms
to a policy (e.g. a claim is valid according to government rules or a post in
an online platform conforms to community guidelines). This task has been
previously instantiated as a form of textual entailment, which results in poor
accuracy due to the complexity of the policies. In this paper we propose to
address policy compliance detection via decomposing it into question answering,
where questions check whether the conditions stated in the policy apply to the
scenario, and an expression tree combines the answers to obtain the label.
Despite the initial upfront annotation cost, we demonstrate that this approach
results in better accuracy, especially in the cross-policy setup where the
policies during testing are unseen in training. In addition, it allows us to
use existing question answering models pre-trained on existing large datasets.
Finally, it explicitly identifies the information missing from a scenario in
case policy compliance cannot be determined. We conduct our experiments using a
recent dataset consisting of government policies, which we augment with expert
annotations and find that the cost of annotating question answering
decomposition is largely offset by improved inter-annotator agreement and
speed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02902">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT&#x27;s final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02377">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A recent variation of Transformer, Performer, scales Transformer to longer
sequences with a linear attention mechanism. However, it is not compatible with
relative position encoding, which has advantages over absolute position
encoding. In this paper, we discuss possible ways to add relative position
encoding to Performer. Based on the analysis, we propose PermuteFormer, a
Performer-based model with relative position encoding that scales linearly on
long sequences. PermuteFormer applies position-dependent transformation on
queries and keys to encode positional information into the attention module.
This transformation is carefully crafted so that the final output of
self-attention is not affected by absolute positions of tokens. PermuteFormer
introduces negligible computational overhead by design that it runs as fast as
Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long
sequences, as well as WikiText-103, a language modeling dataset. The
experiments show that PermuteFormer uniformly improves the performance of
Performer with almost no computational overhead and outperforms vanilla
Transformer on most of the tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03537">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Pre-training language models (LMs) on large-scale unlabeled text data makes
the model much easier to achieve exceptional downstream performance than their
counterparts directly trained on the downstream tasks. In this work, we study
what specific traits in the pre-training data, other than the semantics, make a
pre-trained LM superior to their counterparts trained from scratch on
downstream tasks. We propose to use artificially constructed datasets as the
pre-training data to exclude the effect of semantics, and further control what
characteristics the pre-training corpora have. By fine-tuning the pre-trained
models on GLUE benchmark, we can learn how beneficial it is to transfer the
knowledge from the model trained on the dataset possessing that specific trait.
We define and discuss three different characteristics in the artificial
dataset: 1) matching the token&#x27;s uni-gram or bi-gram distribution between
pre-training and downstream fine-tuning, 2) the presence of the explicit
dependencies among the tokens in a sequence, 3) the length of the implicit
dependencies among the tokens in a sequence. Our experiments show that the
explicit dependencies in the sequences of the pre-training data are critical to
the downstream performance. Our results also reveal that models achieve better
downstream performance when pre-trained on a dataset with a longer range of
implicit dependencies. Based on our analysis, we find that models pre-trained
with artificial datasets are prone to learn spurious correlation in downstream
tasks. Our work reveals that even if the LMs are not pre-trained on natural
language, they still gain transferability on certain human language downstream
tasks once the LMs learn to model the token dependencies in the sequences. This
result helps us understand the exceptional transferability of pre-trained LMs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03685">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For many business applications, we often seek to analyze sentiments
associated with any arbitrary aspects of commercial products, despite having a
very limited amount of labels or even without any labels at all. However,
existing aspect target sentiment classification (ATSC) models are not trainable
if annotated datasets are not available. Even with labeled data, they fall
short of reaching satisfactory performance. To address this, we propose simple
approaches that better solve ATSC with natural language prompts, enabling the
task under zero-shot cases and enhancing supervised settings, especially for
few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop
domain, our method of reformulating ATSC as an NLI task outperforms supervised
SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.
Moreover, we demonstrate that our prompts could handle implicitly stated
aspects as well: our models reach about 77% accuracy on detecting sentiments
for aspect categories (e.g., food), which do not necessarily appear within the
text, even though we trained the models only with explicitly mentioned aspect
terms (e.g., fajitas) from just 16 reviews - while the accuracy of the
no-prompt baseline is only around 65%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03659">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Relation extraction systems require large amounts of labeled examples which
are costly to annotate. In this work we reformulate relation extraction as an
entailment task, with simple, hand-made, verbalizations of relations produced
in less than 15 min per relation. The system relies on a pretrained textual
entailment engine which is run as-is (no training examples, zero-shot) or
further fine-tuned on labeled examples (few-shot or fully trained). In our
experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per
relation (17% points better than the best supervised system on the same
conditions), and only 4 points short to the state-of-the-art (which uses 20
times more training data). We also show that the performance can be improved
significantly with larger entailment models, up to 12 points in zero-shot,
allowing to report the best results to date on TACRED when fully trained. The
analysis shows that our few-shot systems are specially effective when
discriminating between relations, and that the performance difference in low
data regimes comes mainly from identifying no-relation cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03764">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Common acquisition functions for active learning use either uncertainty or
diversity sampling, aiming to select difficult and diverse data points from the
pool of unlabeled data, respectively. In this work, leveraging the best of both
worlds, we propose an acquisition function that opts for selecting
\textit{contrastive examples}, i.e. data points that are similar in the model
feature space and yet the model outputs maximally different predictive
likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a
diverse set of acquisition functions in four natural language understanding
tasks and seven datasets. Our experiments show that CAL performs consistently
better or equal than the best performing baseline across all tasks, on both
in-domain and out-of-domain data. We also conduct an extensive ablation study
of our method and we further analyze all actively acquired datasets showing
that CAL achieves a better trade-off between uncertainty and diversity compared
to other strategies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03300">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>All AI models are susceptible to learning biases in data that they are
trained on. For generative dialogue models, being trained on real human
conversations containing unbalanced gender and race/ethnicity references can
lead to models that display learned biases, which we define here broadly as any
measurable differences in the distributions of words or semantic content of
conversations based on demographic groups. We measure the strength of such
biases by producing artificial conversations between two copies of a dialogue
model, conditioning one conversational partner to state a name commonly
associated with a certain gender and/or race/ethnicity. We find that larger
capacity models tend to exhibit more gender bias and greater stereotyping of
occupations by gender. We show that several methods of tuning these dialogue
models, specifically name scrambling, controlled generation, and unlikelihood
training, are effective in reducing bias in conversation, including on a
downstream conversational task. Name scrambling is also effective in lowering
differences in token usage across conversations where partners have names
associated with different genders or races/ethnicities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03695">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Combining a pretrained language model (PLM) with textual patterns has been
shown to help in both zero- and few-shot settings. For zero-shot performance,
it makes sense to design patterns that closely resemble the text seen during
self-supervised pretraining because the model has never seen anything else.
Supervised training allows for more flexibility. If we allow for tokens outside
the PLM&#x27;s vocabulary, patterns can be adapted more flexibly to a PLM&#x27;s
idiosyncrasies. Contrasting patterns where a &quot;token&quot; can be any continuous
vector vs. those where a discrete choice between vocabulary elements has to be
made, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two
established benchmarks for lexical inference in context (LIiC) a.k.a. predicate
entailment, a challenging natural language understanding task with relatively
small training sets. In a direct comparison with discrete patterns, CONAN
consistently leads to improved performance, setting a new state of the art. Our
experiments give valuable insights into the kind of pattern that enhances a
PLM&#x27;s performance on LIiC and raise important questions regarding our
understanding of PLMs using text patterns.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03438">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the last few years, open-domain question answering (ODQA) has advanced
rapidly due to the development of deep learning techniques and the availability
of large-scale QA datasets. However, the current datasets are essentially
designed for synchronic document collections (e.g., Wikipedia). Temporal news
collections such as long-term news archives spanning several decades, are
rarely used in training the models despite they are quite valuable for our
society. In order to foster the research in the field of ODQA on such
historical collections, we present ArchivalQA, a large question answering
dataset consisting of 1,067,056 question-answer pairs which is designed for
temporal news QA. In addition, we create four subparts of our dataset based on
the question difficulty levels and the containment of temporal expressions,
which we believe could be useful for training or testing ODQA systems
characterized by different strengths and abilities. The novel QA
dataset-constructing framework that we introduce can be also applied to create
datasets over other types of collections.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03481">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Contrastive learning models have achieved great success in unsupervised
visual representation learning, which maximize the similarities between feature
representations of different views of the same image, while minimize the
similarities between feature representations of views of different images. In
text summarization, the output summary is a shorter form of the input document
and they have similar meanings. In this paper, we propose a contrastive
learning model for supervised abstractive text summarization, where we view a
document, its gold summary and its model generated summaries as different views
of the same mean representation and maximize the similarities between them
during training. We improve over a strong sequence-to-sequence text generation
model (i.e., BART) on three different summarization datasets. Human evaluation
also shows that our model achieves better faithfulness ratings compared to its
counterpart without contrastive objectives.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03322">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Traditional event extraction methods require predefined event types and their
corresponding annotations to learn event extractors. These prerequisites are
often hard to be satisfied in real-world applications. This work presents a
corpus-based open-domain event type induction method that automatically
discovers a set of event types from a given corpus. As events of the same type
could be expressed in multiple ways, we propose to represent each event type as
a cluster of 
 pairs. Specifically, our method (1)
selects salient predicates and object heads, (2) disambiguates predicate senses
using only a verb sense dictionary, and (3) obtains event types by jointly
embedding and clustering 
 pairs in a latent
spherical space. Our experiments, on three datasets from different domains,
show our method can discover salient and high-quality event types, according to
both automatic and human evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03502">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank
twice, reaD twice). The pipeline is composed of a retriever, passage reranker,
extractive reader, generative reader and a mechanism that aggregates the final
prediction from all system&#x27;s components. We demonstrate its strength across
three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,
surpassing state-of-the-art on the first two. Our analysis demonstrates that:
(i) combining extractive and generative reader yields absolute improvements up
to 5 exact match and it is at least twice as effective as the posterior
averaging ensemble of the same models with different parameters, (ii) the
extractive reader with fewer parameters can match the performance of the
generative reader on extractive QA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised learning of speech representations has been a very active
research area but most work is focused on a single domain such as read audio
books for which there exist large quantities of labeled and unlabeled data. In
this paper, we explore more general setups where the domain of the unlabeled
data for pre-training data differs from the domain of the labeled data for
fine-tuning, which in turn may differ from the test data domain. Our
experiments show that using target domain data during pre-training leads to
large performance improvements across a variety of setups. On a large-scale
competitive setup, we show that pre-training on unlabeled in-domain data
reduces the gap between models trained on in-domain and out-of-domain labeled
data by 66%-73%. This has obvious practical implications since it is much
easier to obtain unlabeled target domain data than labeled data. Moreover, we
find that pre-training on multiple domains improves generalization performance
on domains not seen during training. Code and models will be made available at
https://github.com/pytorch/fairseq.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09313">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
paper, we probe BERT specifically to understand and measure the relational
knowledge it captures. We utilize knowledge base completion tasks to probe
every layer of pre-trained as well as fine-tuned BERT (ranking, question
answering, NER). Our findings show that knowledge is not just contained in
BERT&#x27;s final layers. Intermediate layers contribute a significant amount
(17-60%) to the total knowledge found. Probing intermediate layers also reveals
how different types of knowledge emerge at varying rates. When BERT is
fine-tuned, relational knowledge is forgotten but the extent of forgetting is
impacted by the fine-tuning objective but not the size of the dataset. We found
that ranking models forget the least and retain more knowledge in their final
layer. We release our code on github to repeat the experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13805">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As it has been unveiled that pre-trained language models (PLMs) are to some
extent capable of recognizing syntactic concepts in natural language, much
effort has been made to develop a method for extracting complete (binary)
parses from PLMs without training separate parsers. We improve upon this
paradigm by proposing a novel chart-based method and an effective top-K
ensemble technique. Moreover, we demonstrate that we can broaden the scope of
application of the approach into multilingual settings. Specifically, we show
that by applying our method on multilingual PLMs, it becomes possible to induce
non-trivial parses for sentences from nine languages in an integrated and
language-agnostic manner, attaining performance superior or comparable to that
of unsupervised PCFGs. We also verify that our approach is robust to
cross-lingual transfer. Finally, we provide analyses on the inner workings of
our method. For instance, we discover universal attention heads which are
consistently sensitive to syntactic information irrespective of the input
language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03754">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal abstractive summarization (MAS) models that summarize videos
(vision modality) and their corresponding transcripts (text modality) are able
to extract the essential information from massive multimodal data on the
Internet. Recently, large-scale generative pre-trained language models (GPLMs)
have been shown to be effective in text generation tasks. However, existing MAS
models cannot leverage GPLMs&#x27; powerful generation ability. To fill this
research gap, we aim to study two research questions: 1) how to inject visual
information into GPLMs without hurting their generation ability; and 2) where
is the optimal place in GPLMs to inject the visual information? In this paper,
we present a simple yet effective method to construct vision guided (VG) GPLMs
for the MAS task using attention-based add-on layers to incorporate visual
information while maintaining their original text generation ability. Results
show that our best model significantly surpasses the prior state-of-the-art
model by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,
and our visual guidance method contributes 83.6% of the overall improvement.
Furthermore, we conduct thorough ablation studies to analyze the effectiveness
of various modality fusion methods and fusion locations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03277">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping
$\pi$ from vertices of a graph to distinct integers that minimizes
$\sum_{\{u,v\}\in E}|\pi(u) - \pi(v)|$. In that setting, vertices are often
assumed to lie on a horizontal line and edges are drawn as semicircles above
said line. For trees, various algorithms are available to solve the problem in
polynomial time in $n&#x3D;|V|$. There exist variants of the MLA in which the
arrangements are constrained. Iordanskii, and later Hochberg and Stallmann
(HS), put forward $O(n)$-time algorithms that solve the problem when
arrangements are constrained to be planar (also known as one-page book
embeddings). We also consider linear arrangements of rooted trees that are
constrained to be projective (planar embeddings where the root is not covered
by any edge). Gildea and Temperley (GT) sketched an algorithm for projective
arrangements which they claimed runs in $O(n)$ but did not provide any
justification of its cost. In contrast, Park and Levy claimed that GT&#x27;s
algorithm runs in $O(n \log d_{max})$ where $d_{max}$ is the maximum degree but
did not provide sufficient detail. Here we correct an error in HS&#x27;s algorithm
for the planar case, show its relationship with the projective case, and derive
simple algorithms for the projective and planar cases that run without a doubt
in $O(n)$ time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03571">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Research into the classification of Image with Text (IWT) troll memes has
recently become popular. Since the online community utilizes the refuge of
memes to express themselves, there is an abundance of data in the form of
memes. These memes have the potential to demean, harras, or bully targeted
individuals. Moreover, the targeted individual could fall prey to opinion
manipulation. To comprehend the use of memes in opinion manipulation, we define
three specific domains (product, political or others) which we classify into
troll or not-troll, with or without opinion manipulation. To enable this
analysis, we enhanced an existing dataset by annotating the data with our
defined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the
English language (TrollsWithOpinion dataset). We perform baseline experiments
on the annotated dataset, and our result shows that existing state-of-the-art
techniques could only reach a weighted-average F1-score of 0.37. This shows the
need for a development of a specific technique to deal with multimodal troll
memes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03772">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-party dialogue machine reading comprehension (MRC) brings tremendous
challenge since it involves multiple speakers at one dialogue, resulting in
intricate speaker information flows and noisy dialogue contexts. To alleviate
such difficulties, previous models focus on how to incorporate these
information using complex graph-based modules and additional manually labeled
data, which is usually rare in real scenarios. In this paper, we design two
labour-free self- and pseudo-self-supervised prediction tasks on speaker and
key-utterance to implicitly model the speaker information flows, and capture
salient clues in a long dialogue. Experimental results on two benchmark
datasets have justified the effectiveness of our method over competitive
baselines and current state-of-the-art models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12279">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce SelfExplain, a novel self-explaining model that explains a text
classifier&#x27;s predictions using phrase-based concepts. SelfExplain augments
existing neural classifiers by adding (1) a globally interpretable layer that
identifies the most influential concepts in the training set for a given sample
and (2) a locally interpretable layer that quantifies the contribution of each
local input concept by computing a relevance score relative to the predicted
label. Experiments across five text-classification datasets show that
SelfExplain facilitates interpretability without sacrificing performance. Most
importantly, explanations from SelfExplain show sufficiency for model
predictions and are perceived as adequate, trustworthy and understandable by
human judges compared to existing widely-used baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03792">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generative approaches have been recently shown to be effective for both
Entity Disambiguation and Entity Linking (i.e., joint mention detection and
disambiguation). However, the previously proposed autoregressive formulation
for EL suffers from i) high computational cost due to a complex (deep) decoder,
ii) non-parallelizable decoding that scales with the source sequence length,
and iii) the need for training on a large amount of data. In this work, we
propose a very efficient approach that parallelizes autoregressive linking
across all potential mentions and relies on a shallow and efficient decoder.
Moreover, we augment the generative objective with an extra discriminative
component, i.e., a correction term which lets us directly optimize the
generator&#x27;s ranking. When taken together, these techniques tackle all the above
issues: our model is &gt;70 times faster and more accurate than the previous
generative method, outperforming state-of-the-art approaches on the standard
English dataset AIDA-CoNLL. Source code available at
https://github.com/nicola-decao/efficient-autoregressive-EL</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08812">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained Transformers achieve remarkable performance when training and test
data are from the same distribution. However, in real-world scenarios, the
model often faces out-of-distribution (OOD) instances that can cause severe
semantic shift problems at inference time. Therefore, in practice, a reliable
model should identify such instances, and then either reject them during
inference or pass them over to models that handle another distribution. In this
paper, we develop an unsupervised OOD detection method, in which only the
in-distribution (ID) data are used in training. We propose to fine-tune the
Transformers with a contrastive loss, which improves the compactness of
representations, such that OOD instances can be better differentiated from ID
ones. These OOD instances can then be accurately detected using the Mahalanobis
distance in the model&#x27;s penultimate layer. We experiment with comprehensive
settings and achieve near-perfect OOD detection performance, outperforming
baselines drastically. We further investigate the rationales behind the
improvement, finding that more compact representations through margin-based
contrastive learning bring the improvement. We release our code to the
community for future research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03552">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The widespread presence of offensive language on social media motivated the
development of systems capable of recognizing such content automatically. Apart
from a few notable exceptions, most research on automatic offensive language
identification has dealt with English. To address this shortcoming, we
introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first
dataset of its kind compiled for Marathi, thus opening a new domain for
research in low-resource Indo-Aryan languages. We present results from several
machine learning experiments on this dataset, including zero-short and other
transfer learning experiments on state-of-the-art cross-lingual transformers
from existing data in Bengali, English, and Hindi.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03529">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatically translating images to texts involves image scene understanding
and language modeling. In this paper, we propose a novel model, termed
RefineCap, that refines the output vocabulary of the language decoder using
decoder-guided visual semantics, and implicitly learns the mapping between
visual tag words and images. The proposed Visual-Concept Refinement method can
allow the generator to attend to semantic details in the image, thereby
generating more semantically descriptive captions. Our model achieves superior
performance on the MS-COCO dataset in comparison with previous visual-concept
based models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03551">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)
speech enhancement, a task that aims to improve the quality of the artificial
voice from an electrolarynx device. In frame-based VC methods, time alignment
needs to be performed prior to model training, and the dynamic time warping
(DTW) algorithm is widely adopted to compute the best time alignment between
each utterance pair. The validity is based on the assumption that the same
phonemes of the speakers have similar features and can be mapped by measuring a
pre-defined distance between speech frames of the source and the target.
However, the special characteristics of the EL speech can break the assumption,
resulting in a sub-optimal DTW alignment. In this work, we propose to use lip
images for time alignment, as we assume that the lip movements of laryngectomee
remain normal compared to healthy people. We investigate two naive lip
representations and distance metrics, and experimental results demonstrate that
the proposed method can significantly outperform the audio-only alignment in
terms of objective and subjective evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03646">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Unfair stereotypical biases (e.g., gender, racial, or religious biases)
encoded in modern pretrained language models (PLMs) have negative ethical
implications for widespread adoption of state-of-the-art language technology.
To remedy for this, a wide range of debiasing techniques have recently been
introduced to remove such stereotypical biases from PLMs. Existing debiasing
methods, however, directly modify all of the PLMs parameters, which -- besides
being computationally expensive -- comes with the inherent risk of
(catastrophic) forgetting of useful language knowledge acquired in pretraining.
In this work, we propose a more sustainable modular debiasing approach based on
dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter
modules into the original PLM layers and (2) update only the adapters (i.e., we
keep the original PLM parameters frozen) via language modeling training on a
counterfactually augmented corpus. We showcase ADELE, in gender debiasing of
BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic
bias measures, renders ADELE, very effective in bias mitigation. We further
show that -- due to its modular nature -- ADELE, coupled with task adapters,
retains fairness even after large-scale downstream training. Finally, by means
of multilingual BERT, we successfully transfer ADELE, to six target languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09120">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained language models (PLM) have recently advanced graph-to-text
generation, where the input graph is linearized into a sequence and fed into
the PLM to obtain its representation. However, efficiently encoding the graph
structure in PLMs is challenging because such models were pretrained on natural
language, and modeling structured data may lead to catastrophic forgetting of
distributional knowledge. In this paper, we propose StructAdapt, an adapter
method to encode graph structure into PLMs. Contrary to prior work, StructAdapt
effectively models interactions among the nodes based on the graph
connectivity, only training graph structure-aware adapter parameters. In this
way, we incorporate task-specific knowledge while maintaining the topological
structure of the graph. We empirically show the benefits of explicitly encoding
graph structure into PLMs using StructAdapt, outperforming the state of the art
on two AMR-to-text datasets, training only 5.1% of the PLM parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms most graph-based
models. We further fine-tune DistilBERT for comparison and find that it
outperforms all state-of-the-art models. We suggest that future studies use at
least an MLP baseline to contextualize the results. We provide recommendations
for the design and training of such a baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03402">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Diverse machine translation aims at generating various target language
translations for a given source language sentence. Leveraging the linear
relationship in the sentence latent space introduced by the mixup training, we
propose a novel method, MixDiversity, to generate different translations for
the input sentence by linearly interpolating it with different sentence pairs
sampled from the training corpus when decoding. To further improve the
faithfulness and diversity of the translations, we propose two simple but
effective approaches to select diverse sentence pairs in the training corpus
and adjust the interpolation weight for each pair correspondingly. Moreover, by
controlling the interpolation weight, our method can achieve the trade-off
between faithfulness and diversity without any additional training, which is
required in most of the previous methods. Experiments on WMT&#x27;16 en-ro, WMT&#x27;14
en-de, and WMT&#x27;17 zh-en are conducted to show that our method substantially
outperforms all previous diverse machine translation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.06381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a new technique for creating monolingual and
cross-lingual meta-embeddings. Our method integrates multiple word embeddings
created from complementary techniques, textual sources, knowledge bases and
languages. Existing word vectors are projected to a common semantic space using
linear transformations and averaging. With our method the resulting
meta-embeddings maintain the dimensionality of the original embeddings without
losing information while dealing with the out-of-vocabulary problem. An
extensive empirical evaluation demonstrates the effectiveness of our technique
with respect to previous work on various intrinsic and extrinsic multilingual
evaluations, obtaining competitive results for Semantic Textual Similarity and
state-of-the-art performance for word similarity and POS tagging (English and
Spanish). The resulting cross-lingual meta-embeddings also exhibit excellent
cross-lingual transfer learning capabilities. In other words, we can leverage
pre-trained source embeddings from a resource-rich language in order to improve
the word representations for under-resourced languages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04670">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large pre-trained language models (LMs) such as GPT-3 have acquired a
surprising ability to perform zero-shot learning. For example, to classify
sentiment without any training examples, we can &quot;prompt&quot; the LM with the review
and the label description &quot;Does the user like this movie?&quot;, and ask whether the
next word is &quot;yes&quot; or &quot;no&quot;. However, the next word prediction training
objective is still misaligned with the target zero-shot learning objective. To
address this weakness, we propose meta-tuning, which directly optimizes the
zero-shot learning objective by fine-tuning pre-trained language models on a
collection of datasets. We focus on classification tasks, and construct the
meta-dataset by aggregating 43 existing datasets and annotating 441 label
descriptions in a question-answering (QA) format. When evaluated on unseen
tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA
zero-shot learning system based on natural language inference. Additionally,
increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,
and we forecast that even larger models would perform better. Therefore,
measuring zero-shot learning performance on language models out-of-the-box
might underestimate their true potential, and community-wide efforts on
aggregating datasets and unifying their formats can help build models that
answer prompts better.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03439">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at
transferring a speaking style to the synthesised speech in a target speaker&#x27;s
voice. Most previous CSST approaches rely on expensive high-quality data
carrying desired speaking style during training and require a reference
utterance to obtain speaking style descriptors as conditioning on the
generation of a new sentence. This work presents Referee, a robust
reference-free CSST approach for expressive TTS, which fully leverages
low-quality data to learn speaking styles from text. Referee is built by
cascading a text-to-style (T2S) model with a style-to-wave (S2W) model.
Phonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are
adopted as fine-grained speaking style descriptors, which are predicted from
text using the T2S model. A novel pretrain-refinement method is adopted to
learn a robust T2S model by only using readily accessible low-quality data. The
S2W model is trained with high-quality target data, which is adopted to
effectively aggregate style descriptors and generate high-fidelity speech in
the target speaker&#x27;s voice. Experimental results are presented, showing that
Referee outperforms a global-style-token (GST)-based baseline approach in CSST.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04539">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding documents from their visual snapshots is an emerging problem
that requires both advanced computer vision and NLP methods. The recent advance
in OCR enables the accurate recognition of text blocks, yet it is still
challenging to extract key information from documents due to the diversity of
their layouts. Although recent studies on pre-trained language models show the
importance of incorporating layout information on this task, the conjugation of
texts and their layouts still follows the style of BERT optimized for
understanding the 1D text. This implies there is room for further improvement
considering the 2D nature of text layouts. This paper introduces a pre-trained
language model, BERT Relying On Spatiality (BROS), which effectively utilizes
the information included in individual text blocks and their layouts.
Specifically, BROS encodes spatial information by utilizing relative positions
and learns spatial dependencies between OCR blocks with a novel area-masking
strategy. These two novel approaches lead to an efficient encoding of spatial
layout information highlighted by the robust performance of BROS under
low-resource environments. We also introduce a general-purpose parser that can
be combined with BROS to extract key information even when there is no order
information between text blocks. BROS shows its superiority on four public
benchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in
practical cases where order information of text blocks is not available.
Further experiments with a varying number of training examples demonstrate the
high training efficiency of our approach. Our code will be open to the public.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03277">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer-based models have recently become very popular for
sequence-to-sequence applications such as machine translation and speech
recognition. This work proposes a dual-decoder transformer model for
low-resource multilingual speech recognition for Indian languages. Our proposed
model consists of a Conformer [1] encoder, two parallel transformer decoders,
and a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme
recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence
along with language information. We consider phoneme recognition and language
identification as auxiliary tasks in the multi-task learning framework. We
jointly optimize the network for phoneme recognition, grapheme recognition, and
language identification tasks with Joint CTC-Attention [2] training. Our
experiments show that we can obtain a significant reduction in WER over the
baseline approaches. We also show that our dual-decoder approach obtains
significant improvement over the single decoder approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03614">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Formal query building is an important part of complex question answering over
knowledge bases. It aims to build correct executable queries for questions.
Recent methods try to rank candidate queries generated by a state-transition
strategy. However, this candidate generation strategy ignores the structure of
queries, resulting in a considerable number of noisy queries. In this paper, we
propose a new formal query building approach that consists of two stages. In
the first stage, we predict the query structure of the question and leverage
the structure to constrain the generation of the candidate queries. We propose
a novel graph generation framework to handle the structure prediction task and
design an encoder-decoder model to predict the argument of the predetermined
operation in each generative step. In the second stage, we follow the previous
methods to rank the candidate queries. The experimental results show that our
formal query building approach outperforms existing methods on complex
questions while staying competitive on simple questions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In the context of neural machine translation, data augmentation (DA)
techniques may be used for generating additional training samples when the
available parallel data are scarce. Many DA approaches aim at expanding the
support of the empirical data distribution by generating new sentence pairs
that contain infrequent words, thus making it closer to the true data
distribution of parallel sentences. In this paper, we propose to follow a
completely different approach and present a multi-task DA approach in which we
generate new sentence pairs with transformations, such as reversing the order
of the target sentence, which produce unfluent target sentences. During
training, these augmented sentences are used as auxiliary tasks in a multi-task
framework with the aim of providing new contexts where the target prefix is not
informative enough to predict the next word. This strengthens the encoder and
forces the decoder to pay more attention to the source representations of the
encoder. Experiments carried out on six low-resource translation tasks show
consistent improvements over the baseline and over DA methods aiming at
extending the support of the empirical data distribution. The systems trained
with our approach rely more on the source tokens, are more robust against
domain shift and suffer less hallucinations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03570">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents biomedical and clinical language models for Spanish by
experimenting with different pretraining choices, such as masking at word and
subword level, varying the vocabulary size and testing with domain data,
looking for better language representations. Interestingly, in the absence of
enough clinical data to train a model from scratch, we applied mixed-domain
pretraining and cross-domain transfer approaches to generate a performant
bio-clinical model suitable for real-world clinical data. We evaluated our
models on Named Entity Recognition (NER) tasks for biomedical documents and
challenging hospital discharge reports. When compared against the competitive
mBERT and BETO models, we outperform them in all NER tasks by a significant
margin. Finally, we studied the impact of the model&#x27;s vocabulary on the NER
performances by offering an interesting vocabulary-centric analysis. The
results confirm that domain-specific pretraining is fundamental to achieving
higher performances in downstream NER tasks, even within a mid-resource
scenario. To the best of our knowledge, we provide the first biomedical and
clinical transformer-based pretrained language models for Spanish, intending to
boost native Spanish NLP applications in biomedicine. Our models will be made
freely available after publication.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13134">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite significant progress has been achieved in text summarization, factual
inconsistency in generated summaries still severely limits its practical
applications. Among the key factors to ensure factual consistency, a reliable
automatic evaluation metric is the first and the most crucial one. However,
existing metrics either neglect the intrinsic cause of the factual
inconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation
with human judgments or increasing the inconvenience of usage in practice. In
light of these challenges, we propose a novel metric to evaluate the factual
consistency in text summarization via counterfactual estimation, which
formulates the causal relationship among the source document, the generated
summary, and the language prior. We remove the effect of language prior, which
can cause factual inconsistency, from the total causal effect on the generated
summary, and provides a simple yet effective way to evaluate consistency
without relying on other auxiliary tasks. We conduct a series of experiments on
three public abstractive text summarization datasets, and demonstrate the
advantages of the proposed metric in both improving the correlation with human
judgments and the convenience of usage. The source code is available at
https://github.com/xieyxclack/factual_coco.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08116">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Language use differs between domains and even within a domain, language use
changes over time. For pre-trained language models like BERT, domain adaptation
through continued pre-training has been shown to improve performance on
in-domain downstream tasks. In this article, we investigate whether temporal
adaptation can bring additional benefits. For this purpose, we introduce a
corpus of social media comments sampled over three years. It contains
unlabelled data for adaptation and evaluation on an upstream masked language
modelling task as well as labelled data for fine-tuning and evaluation on a
downstream document classification task. We find that temporality matters for
both tasks: temporal adaptation improves upstream and temporal fine-tuning
downstream task performance. Time-specific models generally perform better on
past than on future test sets, which matches evidence on the bursty usage of
topical words. However, adapting BERT to time and domain does not improve
performance on the downstream task over only adapting to domain. Token-level
analysis shows that temporal adaptation captures event-driven changes in
language use in the downstream task, but not those changes that are actually
relevant to task performance. Based on our findings, we discuss when temporal
adaptation may be more effective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08656">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent information extraction approaches have relied on training deep neural
models. However, such models can easily overfit noisy labels and suffer from
performance degradation. While it is very costly to filter noisy labels in
large learning resources, recent studies show that such labels take more
training steps to be memorized and are more frequently forgotten than clean
labels, therefore are identifiable in training. Motivated by such properties,
we propose a simple co-regularization framework for entity-centric information
extraction, which consists of several neural models with identical structures
but different parameter initialization. These models are jointly optimized with
the task-specific losses and are regularized to generate similar predictions
based on an agreement loss, which prevents overfitting on noisy labels.
Extensive experiments on two widely used but noisy benchmarks for information
extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.
We release our code to the community for future research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03383">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reproducing results in publications by distributing publicly available source
code is becoming ever more popular. Given the difficulty of reproducing machine
learning (ML) experiments, there have been significant efforts in reducing the
variance of these results. As in any science, the ability to consistently
reproduce results effectively strengthens the underlying hypothesis of the
work, and thus, should be regarded as important as the novel aspect of the
research itself. The contribution of this work is a framework that is able to
reproduce consistent results and provides a means of easily creating, training,
and evaluating natural language processing (NLP) deep learning (DL) models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01163">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The recently proposed Conformer architecture has shown state-of-the-art
performances in Automatic Speech Recognition by combining convolution with
attention to model both local and global dependencies. In this paper, we study
how to reduce the Conformer architecture complexity with a limited computing
budget, leading to a more efficient architecture design that we call Efficient
Conformer. We introduce progressive downsampling to the Conformer encoder and
propose a novel attention mechanism named grouped attention, allowing us to
reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence
length $n$, hidden dimension $d$ and group size parameter $g$. We also
experiment the use of strided multi-head self-attention as a global
downsampling operation. Our experiments are performed on the LibriSpeech
dataset with CTC and RNN-Transducer losses. We show that within the same
computing budget, the proposed architecture achieves better performances with
faster training and decoding compared to the Conformer. Our 13M parameters CTC
model achieves competitive WERs of 3.6%/9.0% without using a language model and
2.7%/6.7% with an external n-gram language model on the test-clean/test-other
sets while being 29% faster than our CTC Conformer baseline at inference and
36% faster to train.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03264">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work
addressing the generative aspects of speech pre-training, which replaces text
with discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Word embeddings are a powerful natural lan-guage processing technique, but
they are ex-tremely difficult to interpret. To enable inter-pretable NLP
models, we create vectors whereeach dimension isinherently interpretable.
Byinherently interpretable, we mean a systemwhere each dimension is associated
with somehuman-understandablehintthat can describethe meaning of that
dimension. In order tocreate more interpretable word embeddings,we transform
pretrained dense word embed-dings into sparse embeddings. These new em-beddings
are inherently interpretable: each oftheir dimensions is created from and
repre-sents a natural language word or specific gram-matical concept. We
construct these embed-dings through sparse coding, where each vec-tor in the
basis set is itself a word embedding.Therefore, each dimension of our sparse
vec-tors corresponds to a natural language word.We also show that models
trained using thesesparse embeddings can achieve good perfor-mance and are more
interpretable in practice,including through human evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03490">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>L1 French participants learned novel L2 English words over two days of
learning sessions, with half of the words presented with their orthographic
forms (Audio-Ortho) and half without (Audio only). One group heard the words
pronounced by a single talker, while another group heard them pronounced by
multiple talkers. On the third day, they completed a variety of tasks to
evaluate their learning. Our results show a robust influence of orthography,
with faster response times in both production (picture naming) and recognition
(picture mapping) tasks for words learned in the Audio-Ortho condition.
Moreover, formant analyses of the picture naming responses show that
orthographic input pulls pronunciations of English novel words towards a
non-native (French) phonological target. Words learned with their orthographic
forms were pronounced more precisely (with smaller Dispersion Scores), but were
misplaced in the vowel space (as reflected by smaller Euclidian distances with
respect to French vowels). Finally, we found only limited evidence of an effect
of talker-based acoustic variability: novel words learned with multiple talkers
showed faster responses times in the picture naming task, but only in the
Audio-only condition, which suggests that orthographic information may have
overwhelmed any advantage of talker-based acoustic variability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03415">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multimodal machine translation (MMT) systems have been shown to outperform
their text-only neural machine translation (NMT) counterparts when visual
context is available. However, recent studies have also shown that the
performance of MMT models is only marginally impacted when the associated image
is replaced with an unrelated image or noise, which suggests that the visual
context might not be exploited by the model at all. We hypothesize that this
might be caused by the nature of the commonly used evaluation benchmark, also
known as Multi30K, where the translations of image captions were prepared
without actually showing the images to human translators. In this paper, we
present a qualitative study that examines the role of datasets in stimulating
the leverage of visual modality and we propose methods to highlight the
importance of visual signals in the datasets which demonstrate improvements in
reliance of models on the source images. Our findings suggest the research on
effective MMT architectures is currently impaired by the lack of suitable
datasets and careful consideration must be taken in creation of future MMT
datasets, for which we also provide useful insights.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03564">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Using prompts to utilize language models to perform various downstream tasks,
also known as prompt-based learning or prompt-learning, has lately gained
significant success in comparison to the pre-train and fine-tune paradigm.
Nonetheless, virtually all prompt-based methods are token-level, meaning they
all utilize GPT&#x27;s left-to-right language model or BERT&#x27;s masked language model
to perform cloze-style tasks. In this paper, we attempt to accomplish several
NLP tasks in the zero-shot scenario using a BERT original pre-training task
abandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike
token-level techniques, our sentence-level prompt-based method NSP-BERT does
not need to fix the length of the prompt or the position to be predicted,
allowing it to handle tasks such as entity linking with ease. Based on the
characteristics of NSP-BERT, we offer several quick building templates for
various downstream tasks. We suggest a two-stage prompt method for word sense
disambiguation tasks in particular. Our strategies for mapping the labels
significantly enhance the model&#x27;s performance on sentence pair tasks. On the
FewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of
these tasks and comes close to the few-shot methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03334">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Building compositional explanations requires models to combine two or more
facts that, together, describe why the answer to a question is correct.
Typically, these &quot;multi-hop&quot; explanations are evaluated relative to one (or a
small number of) gold explanations. In this work, we show these evaluations
substantially underestimate model performance, both in terms of the relevance
of included facts, as well as the completeness of model-generated explanations,
because models regularly discover and produce valid explanations that are
different than gold explanations. To address this, we construct a large corpus
of 126k domain-expert (science teacher) relevance ratings that augment a corpus
of explanations to standardized science exam questions, discovering 80k
additional relevant facts not rated as gold. We build three strong models based
on different methodologies (generation, ranking, and schemas), and empirically
show that while expert-augmented ratings provide better estimates of
explanation quality, both original (gold) and expert-augmented automatic
evaluations still substantially underestimate performance by up to 36% when
compared with full manual expert judgements, with different models being
disproportionately affected. This poses a significant methodological challenge
to accurately evaluating explanations produced by compositional reasoning
models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">It is AI&#x27;s Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing question answering (QA) datasets are created mainly for the
application of having AI to be able to answer questions asked by humans. But in
educational applications, teachers and parents sometimes may not know what
questions they should ask a child that can maximize their language learning
results. With a newly released book QA dataset (FairytaleQA), which educational
experts labeled on 46 fairytale storybooks for early childhood readers, we
developed an automated QA generation model architecture for this novel
application. Our model (1) extracts candidate answers from a given storybook
passage through carefully designed heuristics based on a pedagogical framework;
(2) generates appropriate questions corresponding to each extracted answer
using a language model; and, (3) uses another QA model to rank top QA-pairs.
Automatic and human evaluations show that our model outperforms baselines. We
also demonstrate that our method can help with the scarcity issue of the
children&#x27;s book QA dataset via data augmentation on 200 unlabeled storybooks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Spoken question answering (SQA) requires fine-grained understanding of both
spoken documents and questions for the optimal answer prediction. In this
paper, we propose novel training schemes for spoken question answering with a
self-supervised training stage and a contrastive representation learning stage.
In the self-supervised stage, we propose three auxiliary self-supervised tasks,
including utterance restoration, utterance insertion, and question
discrimination, and jointly train the model to capture consistency and
coherence among speech documents without any additional data or annotations. We
then propose to learn noise-invariant utterance representations in a
contrastive objective by adopting multiple augmentation strategies, including
span deletion and span substitution. Besides, we design a Temporal-Alignment
attention to semantically align the speech-text clues in the learned common
space and benefit the SQA tasks. By this means, the training schemes can more
effectively guide the generation model to predict more proper answers.
Experimental results show that our model achieves state-of-the-art results on
three SQA benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13405">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>COLIEE is an annual competition in automatic computerized legal text
processing. Automatic legal document processing is an ambitious goal, and the
structure and semantics of the law are often far more complex than everyday
language. In this article, we survey and report our methods and experimental
results in using deep learning in legal document processing. The results show
the difficulties as well as potentials in this family of approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03487">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we take into account both social and linguistic aspects to
perform demographic analysis by processing a large amount of tweets in Basque
language. The study of demographic characteristics and social relationships are
approached by applying machine learning and modern deep-learning Natural
Language Processing (NLP) techniques, combining social sciences with automatic
text processing. More specifically, our main objective is to combine
demographic inference and social analysis in order to detect young Basque
Twitter users and to identify the communities that arise from their
relationships or shared content. This social and demographic analysis will be
entirely based on the~automatically collected tweets using NLP to convert
unstructured textual information into interpretable knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Egocentric View Hand Action Recognition by Leveraging Hand Surface and Hand Grasp Type. (arXiv:2109.03783v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03783">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce a multi-stage framework that uses mean curvature on a hand
surface and focuses on learning interaction between hand and object by
analyzing hand grasp type for hand action recognition in egocentric videos. The
proposed method does not require 3D information of objects including 6D object
poses which are difficult to annotate for learning an object&#x27;s behavior while
it interacts with hands. Instead, the framework synthesizes the mean curvature
of the hand mesh model to encode the hand surface geometry in 3D space.
Additionally, our method learns the hand grasp type which is highly correlated
with the hand action. From our experiment, we notice that using hand grasp type
and mean curvature of hand increases the performance of the hand action
recognition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CAGAN: Text-To-Image Generation with Combined Attention GANs. (arXiv:2104.12663v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12663">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generating images according to natural language descriptions is a challenging
task. Prior research has mainly focused to enhance the quality of generation by
investigating the use of spatial attention and/or textual attention thereby
neglecting the relationship between channels. In this work, we propose the
Combined Attention Generative Adversarial Network (CAGAN) to generate
photo-realistic images according to textual descriptions. The proposed CAGAN
utilises two attention models: word attention to draw different sub-regions
conditioned on related words; and squeeze-and-excitation attention to capture
non-linear interaction among channels. With spectral normalisation to stabilise
training, our proposed CAGAN improves the state of the art on the IS and FID on
the CUB dataset and the FID on the more challenging COCO dataset. Furthermore,
we demonstrate that judging a model by a single evaluation metric can be
misleading by developing an additional model adding local self-attention which
scores a higher IS, outperforming the state of the art on the CUB dataset, but
generates unrealistic images through feature repetition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03812">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Improving speed and image quality of Magnetic Resonance Imaging (MRI) via
novel reconstruction approaches remains one of the highest impact applications
for deep learning in medical imaging. The fastMRI dataset, unique in that it
contains large volumes of raw MRI data, has enabled significant advances in
accelerating MRI using deep learning-based reconstruction methods. While the
impact of the fastMRI dataset on the field of medical imaging is unquestioned,
the dataset currently lacks clinical expert pathology annotations, critical to
addressing clinically relevant reconstruction frameworks and exploring
important questions regarding rendering of specific pathology using such novel
approaches. This work introduces fastMRI+, which consists of 16154
subspecialist expert bounding box annotations and 13 study-level labels for 22
different pathology categories on the fastMRI knee dataset, and 7570
subspecialist expert bounding box annotations and 643 study-level labels for 30
different pathology categories for the fastMRI brain dataset. The fastMRI+
dataset is open access and aims to support further research and advancement of
medical imaging in MRI reconstruction and beyond.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00952">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Improved Iterative Neural Network for High-Quality Image-Domain Material Decomposition in Dual-Energy CT. (arXiv:2012.01986v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dual-energy computed tomography (DECT) has been widely used in many
applications that need material decomposition. Image-domain methods directly
decompose material images from high- and low-energy attenuation images, and
thus, are susceptible to noise and artifacts on attenuation images. The purpose
of this study is to develop an improved iterative neural network (INN) for
high-quality image-domain material decomposition in DECT, and to study its
properties. We propose a new INN architecture for DECT material decomposition.
The proposed INN architecture uses distinct cross-material convolutional neural
network (CNN) in image refining modules, and uses image decomposition physics
in image reconstruction modules. The distinct cross-material CNN refiners
incorporate distinct encoding-decoding filters and cross-material model that
captures correlations between different materials. We study the distinct
cross-material CNN refiner with patch-based reformulation and tight-frame
condition. Numerical experiments with extended cardiactorso (XCAT) phantom and
clinical data show that the proposed INN significantly improves the image
quality over several image-domain material decomposition methods, including a
conventional model-based image decomposition (MBID) method using an
edge-preserving regularizer, a recent MBID method using pre-learned
material-wise sparsifying transforms, and a noniterative deep CNN method. Our
study with patch-based reformulations reveals that learned filters of distinct
cross-material CNN refiners can approximately satisfy the tight-frame
condition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14017">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Image translation methods typically aim to manipulate a set of labeled
attributes (given as supervision at training time e.g. domain label) while
leaving the unlabeled attributes intact. Current methods achieve either: (i)
disentanglement, which exhibits low visual fidelity and can only be satisfied
where the attributes are perfectly uncorrelated. (ii) visually-plausible
translations, which are clearly not disentangled. In this work, we propose
OverLORD, a single framework for disentangling labeled and unlabeled attributes
as well as synthesizing high-fidelity images, which is composed of two stages;
(i) Disentanglement: Learning disentangled representations with latent
optimization. Differently from previous approaches, we do not rely on
adversarial training or any architectural biases. (ii) Synthesis: Training
feed-forward encoders for inferring the learned attributes and tuning the
generator in an adversarial manner to increase the perceptual quality. When the
labeled and unlabeled attributes are correlated, we model an additional
representation that accounts for the correlated attributes and improves
disentanglement. We highlight that our flexible framework covers multiple
settings as disentangling labeled attributes, pose and appearance, localized
concepts, and shape and texture. We present significantly better
disentanglement with higher translation quality and greater output diversity
than state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking the Aligned and Misaligned Features in One-stage Object Detection. (arXiv:2108.12176v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12176">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One-stage object detectors rely on a point feature to predict the detection
results. However, the point feature often lacks the information of the whole
object, thereby leading to a misalignment between the object and the point
feature. Meanwhile, the classification and regression tasks are sensitive to
different object regions, but their features are spatially aligned. Both of
these two problems hinder the detection performance. In order to solve these
two problems, we propose a simple and plug-in operator that can generate
aligned and disentangled features for each task, respectively, without breaking
the fully convolutional manner. By predicting two task-aware point sets that
are located in each sensitive region, the proposed operator can align the point
feature with the object and disentangle the two tasks from the spatial
dimension. We also reveal an interesting finding of the opposite effect of the
long-range skip connection for classification and regression. On the basis of
the Object-Aligned and Task-disentangled operator (OAT), we propose OAT-Net,
which explicitly exploits point-set features for accurate detection results.
Extensive experiments on the MS-COCO dataset show that OAT can consistently
boost different state-of-the-art one-stage detectors by $\sim$2 AP. Notably,
OAT-Net with Res2Net-101-DCN backbone achieves 53.7 AP on the COCO test-dev.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Salient Object Detection via Integrity Learning. (arXiv:2101.07663v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.07663">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Albeit current salient object detection (SOD) works have achieved fantastic
progress, they are cast into the shade when it comes to the integrity of the
predicted salient regions. We define the concept of integrity at both the micro
and macro level. Specifically, at the micro level, the model should highlight
all parts that belong to a certain salient object, while at the macro level,
the model needs to discover all salient objects from the given image scene. To
facilitate integrity learning for salient object detection, we design a novel
Integrity Cognition Network (ICON), which explores three important components
to learn strong integrity features. 1) Unlike the existing models that focus
more on feature discriminability, we introduce a diverse feature aggregation
(DFA) component to aggregate features with various receptive fields (i.e.,,
kernel shape and context) and increase the feature diversity. Such diversity is
the foundation for mining the integral salient objects. 2) Based on the DFA
features, we introduce the integrity channel enhancement (ICE) component with
the goal of enhancing feature channels that highlight the integral salient
objects at the macro level, while suppressing the other distracting ones. 3)
After extracting the enhanced features, the part-whole verification (PWV)
method is employed to determine whether the part and whole object features have
strong agreement. Such part-whole agreements can further improve the
micro-level integrity for each salient object. To demonstrate the effectiveness
of ICON, comprehensive experiments are conducted on seven challenging
benchmarks, where promising results are achieved.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09035">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Monocular 3D detection currently struggles with extremely lower detection
rates compared to LiDAR-based methods. The poor accuracy is mainly caused by
the absence of accurate location cues due to the ill-posed nature of monocular
imagery. LiDAR point clouds, which provide precise spatial measurement, can
offer beneficial information for the training of monocular methods. To make use
of LiDAR point clouds, prior works project them to form depth map labels,
subsequently training a dense depth estimator to extract explicit location
features. This indirect and complicated way introduces intermediate products,
i.e., depth map predictions, taking much computation costs as well as leading
to suboptimal performances. In this paper, we propose LPCG (LiDAR point cloud
guided monocular 3D object detection), which is a general framework for guiding
the training of monocular 3D detectors with LiDAR point clouds. Specifically,
we use LiDAR point clouds to generate pseudo labels, allowing monocular 3D
detectors to benefit from easy-collected massive unlabeled data. LPCG works
well under both supervised and unsupervised setups. Thanks to a general design,
LPCG can be plugged into any monocular 3D detector, significantly boosting the
performance. As a result, we take the first place on KITTI monocular 3D/BEV
(bird&#x27;s-eye-view) detection benchmark with a considerable margin. The code will
be made publicly available soon.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural
network by maximizing the AUC score of the model on a dataset. Most previous
works of AUC maximization focus on the perspective of optimization by designing
efficient stochastic algorithms, and studies on generalization performance of
large-scale DAM on difficult tasks are missing. In this work, we aim to make
DAM more practical for interesting real-world applications (e.g., medical image
classification). First, we propose a new margin-based min-max surrogate loss
function for the AUC score (named as AUC min-max-margin loss or simply AUC
margin loss for short). It is more robust than the commonly used AUC square
loss, while enjoying the same advantage in terms of large-scale stochastic
optimization. Second, we conduct extensive empirical studies of our DAM method
on four difficult medical image classification tasks, namely (i) classification
of chest x-ray images for identifying many threatening diseases, (ii)
classification of images of skin lesions for identifying melanoma, (iii)
classification of mammogram for breast cancer screening, and (iv)
classification of microscopic images for identifying tumor tissue. Our studies
demonstrate that the proposed DAM method improves the performance of optimizing
cross-entropy loss by a large margin, and also achieves better performance than
optimizing the existing AUC square loss on these medical image classification
tasks. Specifically, our DAM method has achieved the 1st place on Stanford
CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is
the first work that makes DAM succeed on large-scale medical image datasets. We
also conduct extensive ablation studies to demonstrate the advantages of the
new AUC margin loss over the AUC square loss on benchmark datasets. The
proposed method is implemented in our open-sourced library LibAUC
(www.libauc.org).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14483">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Object manipulation from 3D visual inputs poses many challenges on building
generalizable perception and policy models. However, 3D assets in existing
benchmarks mostly lack the diversity of 3D shapes that align with real-world
intra-class complexity in topology and geometry. Here we propose SAPIEN
Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over
diverse objects in a full-physics simulator. 3D assets in ManiSkill include
large intra-class topological and geometric variations. Tasks are carefully
chosen to cover distinct types of manipulation challenges. Latest progress in
3D vision also makes us believe that we should customize the benchmark so that
the challenge is inviting to researchers working on 3D deep learning. To this
end, we simulate a moving panoramic camera that returns ego-centric point
clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad
set of researchers interested in manipulation research. Besides supporting the
learning of policies from interactions, we also support
learning-from-demonstrations (LfD) methods, by providing a large number of
high-quality demonstrations (~36,000 successful trajectories, ~1.5M point
cloud/RGB-D frames in total). We provide baselines using 3D deep learning and
LfD algorithms. All code of our benchmark (simulator, environment, SDK, and
baselines) is open-sourced, and a challenge facing interdisciplinary
researchers will be held based on the benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Point-Based Neural Rendering with Per-View Optimization. (arXiv:2109.02369v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02369">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>There has recently been great interest in neural rendering methods. Some
approaches use 3D geometry reconstructed with Multi-View Stereo (MVS) but
cannot recover from the errors of this process, while others directly learn a
volumetric neural representation, but suffer from expensive training and
inference. We introduce a general approach that is initialized with MVS, but
allows further optimization of scene properties in the space of input views,
including depth and reprojected features, resulting in improved novel-view
synthesis. A key element of our approach is our new differentiable point-based
pipeline, based on bi-directional Elliptical Weighted Average splatting, a
probabilistic depth test and effective camera selection. We use these elements
together in our neural renderer, that outperforms all previous methods both in
quality and speed in almost all scenes we tested. Our pipeline can be applied
to multi-view harmonization and stylization in addition to novel-view
synthesis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03778">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Choroid plexuses (CP) are structures of the ventricles of the brain which
produce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo
studies have pointed towards their role in the inflammatory process in multiple
sclerosis (MS). Automatic segmentation of CP from MRI thus has high value for
studying their characteristics in large cohorts of patients. To the best of our
knowledge, the only freely available tool for CP segmentation is FreeSurfer but
its accuracy for this specific structure is poor. In this paper, we propose to
automatically segment CP from non-contrast enhanced T1-weighted MRI. To that
end, we introduce a new model called &quot;Axial-MLP&quot; based on an assembly of Axial
multi-layer perceptrons (MLPs). This is inspired by recent works which showed
that the self-attention layers of Transformers can be replaced with MLPs. This
approach is systematically compared with a standard 3D U-Net, nnU-Net,
Freesurfer and FastSurfer. For our experiments, we make use of a dataset of 141
subjects (44 controls and 97 patients with MS). We show that all the tested
deep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs
0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is
slightly less accurate. The conclusions of our paper are two-fold: 1) the
studied deep learning methods could be useful tools to study CP in large
cohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to
convolutional neural networks for such tasks, although it could benefit from
further improvements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01887">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents an automatic algorithm for the segmentation of areas
affected by an acute stroke on the non-contrast computed tomography brain
images. The proposed algorithm is designed for learning in a weakly supervised
scenario when some images are labeled accurately, and some images are labeled
inaccurately. Wrong labels appear as a result of inaccuracy made by a
radiologist in the process of manual annotation of computed tomography images.
We propose methods for solving the segmentation problem in the case of
inaccurately labeled training data. We use the U-Net neural network
architecture with several modifications. Experiments on real computed
tomography scans show that the proposed methods increase the segmentation
accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Recognizing Occluded Faces in the Wild. (arXiv:2109.03672v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03672">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Facial appearance variations due to occlusion has been one of the main
challenges for face recognition systems. To facilitate further research in this
area, it is necessary and important to have occluded face datasets collected
from real-world, as synthetically generated occluded faces cannot represent the
nature of the problem. In this paper, we present the Real World Occluded Faces
(ROF) dataset, that contains faces with both upper face occlusion, due to
sunglasses, and lower face occlusion, due to masks. We propose two evaluation
protocols for this dataset. Benchmark experiments on the dataset have shown
that no matter how powerful the deep face representation models are, their
performance degrades significantly when they are tested on real-world occluded
faces. It is observed that the performance drop is far less when the models are
tested on synthetically generated occluded faces. The ROF dataset and the
associated evaluation protocols are publicly available at the following link
https://github.com/ekremerakin/RealWorldOccludedFaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10698">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the important and tedious task in agricultural practices is the
detection of the disease on crops. It requires huge time as well as skilled
labor. This paper proposes a smart and efficient technique for detection of
crop disease which uses computer vision and machine learning techniques. The
proposed system is able to detect 20 different diseases of 5 common plants with
93% accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03575">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks have shown their profound impact on achieving human
level performance in visual saliency prediction. However, it is still unclear
how they learn the task and what it means in terms of understanding human
visual system. In this work, we develop a technique to derive explainable
saliency models from their corresponding deep neural architecture based
saliency models by applying human perception theories and the conventional
concepts of saliency. This technique helps us understand the learning pattern
of the deep network at its intermediate layers through their activation maps.
Initially, we consider two state-of-the-art deep saliency models, namely UNISAL
and MSI-Net for our interpretation. We use a set of biologically plausible
log-gabor filters for identifying and reconstructing the activation maps of
them using our explainable saliency model. The final saliency map is generated
using these reconstructed activation maps. We also build our own deep saliency
model named cross-concatenated multi-scale residual block based network
(CMRNet) for saliency prediction. Then, we evaluate and compare the performance
of the explainable models derived from UNISAL, MSI-Net and CMRNet on three
benchmark datasets with other state-of-the-art methods. Hence, we propose that
this approach of explainability can be applied to any deep visual saliency
model for interpretation which makes it a generic one.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00802">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Although supervised convolutional neural networks (CNNs) often outperform
conventional alternatives for denoising positron emission tomography (PET)
images, they require many low- and high-quality reference PET image pairs.
Herein, we propose an unsupervised 3D PET image denoising method based on an
anatomical information-guided attention mechanism. The proposed magnetic
resonance-guided deep decoder (MR-GDD) utilizes the spatial details and
semantic features of MR-guidance image more effectively by introducing
encoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and
patterns of the guidance image do not affect the denoised PET image, because
the guidance image is input to the network through an attention gate. In a
Monte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the
proposed method achieved the highest peak signal-to-noise ratio and structural
similarity (27.92 $\pm$ 0.44 dB/0.886 $\pm$ 0.007), as compared with Gaussian
filtering (26.68 $\pm$ 0.10 dB/0.807 $\pm$ 0.004), image guided filtering
(27.40 $\pm$ 0.11 dB/0.849 $\pm$ 0.003), deep image prior (DIP) (24.22 $\pm$
0.43 dB/0.737 $\pm$ 0.017), and MR-DIP (27.65 $\pm$ 0.42 dB/0.879 $\pm$ 0.007).
Furthermore, we experimentally visualized the behavior of the optimization
process, which is often unknown in unsupervised CNN-based restoration problems.
For preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical
(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates
state-of-the-art denoising performance while retaining spatial resolution and
quantitative accuracy, despite using a common network architecture for various
noisy PET images with 1/10th of the full counts. These results suggest that the
proposed MR-GDD can reduce PET scan times and PET tracer doses considerably
without impacting patients.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LiDARTouch: Monocular metric depth estimation with a few-beam LiDAR. (arXiv:2109.03569v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03569">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Vision-based depth estimation is a key feature in autonomous systems, which
often relies on a single camera or several independent ones. In such a
monocular setup, dense depth is obtained with either additional input from one
or several expensive LiDARs, e.g., with 64 beams, or camera-only methods, which
suffer from scale-ambiguity and infinite-depth problems. In this paper, we
propose a new alternative of densely estimating metric depth by combining a
monocular camera with a light-weight LiDAR, e.g., with 4 beams, typical of
today&#x27;s automotive-grade mass-produced laser scanners. Inspired by recent
self-supervised methods, we introduce a novel framework, called LiDARTouch, to
estimate dense depth maps from monocular images with the help of &#x60;&#x60;touches&#x27;&#x27; of
LiDAR, i.e., without the need for dense ground-truth depth. In our setup, the
minimal LiDAR input contributes on three different levels: as an additional
model&#x27;s input, in a self-supervised LiDAR reconstruction objective function,
and to estimate changes of pose (a key component of self-supervised depth
estimation architectures). Our LiDARTouch framework achieves new state of the
art in self-supervised depth estimation on the KITTI dataset, thus supporting
our choices of integrating the very sparse LiDAR signal with other visual
features. Moreover, we show that the use of a few-beam LiDAR alleviates scale
ambiguity and infinite-depth issues that camera-only methods suffer from. We
also demonstrate that methods from the fully-supervised depth-completion
literature can be adapted to a self-supervised regime with a minimal LiDAR
signal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Digging into Uncertainty in Self-supervised Multi-view Stereo. (arXiv:2108.12966v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12966">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised Multi-view stereo (MVS) with a pretext task of image
reconstruction has achieved significant progress recently. However, previous
methods are built upon intuitions, lacking comprehensive explanations about the
effectiveness of the pretext task in self-supervised MVS. To this end, we
propose to estimate epistemic uncertainty in self-supervised MVS, accounting
for what the model ignores. Specially, the limitations can be categorized into
two types: ambiguious supervision in foreground and invalid supervision in
background. To address these issues, we propose a novel Uncertainty reduction
Multi-view Stereo (UMVS) framework for self-supervised learning. To alleviate
ambiguous supervision in foreground, we involve extra correspondence prior with
a flow-depth consistency loss. The dense 2D correspondence of optical flows is
used to regularize the 3D stereo correspondence in MVS. To handle the invalid
supervision in background, we use Monte-Carlo Dropout to acquire the
uncertainty map and further filter the unreliable supervision signals on
invalid regions. Extensive experiments on DTU and Tank&amp;Temples benchmark show
that our U-MVS framework achieves the best performance among unsupervised MVS
methods, with competitive performance with its supervised opponents.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03484">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Face anti-spoofing is essential to prevent false facial verification by using
a photo, video, mask, or a different substitute for an authorized person&#x27;s
face. Most of the state-of-the-art presentation attack detection (PAD) systems
suffer from overfitting, where they achieve near-perfect scores on a single
dataset but fail on a different dataset with more realistic data. This problem
drives researchers to develop models that perform well under real-world
conditions. This is an especially challenging problem for frame-based
presentation attack detection systems that use convolutional neural networks
(CNN). To this end, we propose a new PAD approach, which combines pixel-wise
binary supervision with patch-based CNN. We believe that training a CNN with
face patches allows the model to distinguish spoofs without learning background
or dataset-specific traces. We tested the proposed method both on the standard
benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.
The proposed approach shows its superiority on challenging experimental setups.
Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on
inter-dataset real-world experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Suppress-and-Refine Framework for End-to-End 3D Object Detection. (arXiv:2103.10042v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10042">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D object detector based on Hough voting achieves great success and derives
many follow-up works. Despite constantly refreshing the detection accuracy,
these works suffer from handcrafted components used to eliminate redundant
boxes, and thus are non-end-to-end and time-consuming. In this work, we propose
a suppress-and-refine framework to remove these handcrafted components. To
fully utilize full-resolution information and achieve real-time speed, it
directly consumes feature points and redundant 3D proposals. Specifically, it
first suppresses noisy 3D feature points and then feeds them to 3D proposals
for the following RoI-aware refinement. With the gating mechanism to build fine
proposal features and the self-attention mechanism to model relationships, our
method can produce high-quality predictions with a small computation budget in
an end-to-end manner. To this end, we present the first fully end-to-end 3D
detector, SRDet, on the basis of VoteNet. It achieves state-of-the-art
performance on the challenging ScanNetV2 and SUN RGB-D datasets with the
fastest speed ever. Our code will be available at
https://github.com/ZJULearning/SRDet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14306">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper proposes an unsupervised anomaly detection technique for
image-based plant disease diagnosis. The construction of large and publicly
available datasets containing labeled images of healthy and diseased crop
plants led to growing interest in computer vision techniques for automatic
plant disease diagnosis. Although supervised image classifiers based on deep
learning can be a powerful tool for plant disease diagnosis, they require a
huge amount of labeled data. The data mining technique of anomaly detection
includes unsupervised approaches that do not require rare samples for training
classifiers. We propose an unsupervised anomaly detection technique for
image-based plant disease diagnosis that is based on the reconstructability of
colors; a deep encoder-decoder network trained to reconstruct the colors of
\textit{healthy} plant images should fail to reconstruct colors of symptomatic
regions. Our proposed method includes a new image-based framework for plant
disease detection that utilizes a conditional adversarial network called
pix2pix and a new anomaly score based on CIEDE2000 color difference.
Experiments with PlantVillage dataset demonstrated the superiority of our
proposed method compared to an existing anomaly detector at identifying
diseased crop images in terms of accuracy, interpretability and computational
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03615">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Robotic touch, particularly when using soft optical tactile sensors, suffers
from distortion caused by motion-dependent shear. The manner in which the
sensor contacts a stimulus is entangled with the tactile information about the
geometry of the stimulus. In this work, we propose a supervised convolutional
deep neural network model that learns to disentangle, in the latent space, the
components of sensor deformations caused by contact geometry from those due to
sliding-induced shear. The approach is validated by reconstructing unsheared
tactile images from sheared images and showing they match unsheared tactile
images collected with no sliding motion. In addition, the unsheared tactile
images give a faithful reconstruction of the contact geometry that is not
possible from the sheared data, and robust estimation of the contact pose that
can be used for servo control sliding around various 2D shapes. Finally, the
contact geometry reconstruction in conjunction with servo control sliding were
used for faithful full object reconstruction of various 2D shapes. The methods
have broad applicability to deep learning models for robots with a
shear-sensitive sense of touch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mask is All You Need: Rethinking Mask R-CNN for Dense and Arbitrary-Shaped Scene Text Detection. (arXiv:2109.03426v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03426">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Due to the large success in object detection and instance segmentation, Mask
R-CNN attracts great attention and is widely adopted as a strong baseline for
arbitrary-shaped scene text detection and spotting. However, two issues remain
to be settled. The first is dense text case, which is easy to be neglected but
quite practical. There may exist multiple instances in one proposal, which
makes it difficult for the mask head to distinguish different instances and
degrades the performance. In this work, we argue that the performance
degradation results from the learning confusion issue in the mask head. We
propose to use an MLP decoder instead of the &quot;deconv-conv&quot; decoder in the mask
head, which alleviates the issue and promotes robustness significantly. And we
propose instance-aware mask learning in which the mask head learns to predict
the shape of the whole instance rather than classify each pixel to text or
non-text. With instance-aware mask learning, the mask branch can learn
separated and compact masks. The second is that due to large variations in
scale and aspect ratio, RPN needs complicated anchor settings, making it hard
to maintain and transfer across different datasets. To settle this issue, we
propose an adaptive label assignment in which all instances especially those
with extreme aspect ratios are guaranteed to be associated with enough anchors.
Equipped with these components, the proposed method named MAYOR achieves
state-of-the-art performance on five benchmarks including DAST1500, MSRA-TD500,
ICDAR2015, CTW1500, and Total-Text.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised clothing change adaptive person ReID. (arXiv:2109.03702v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03702">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Clothing changes and lack of data labels are both crucial challenges in
person ReID. For the former challenge, people may occur multiple times at
different locations wearing different clothing. However, most of the current
person ReID research works focus on the benchmarks in which a person&#x27;s clothing
is kept the same all the time. For the last challenge, some researchers try to
make model learn information from a labeled dataset as a source to an unlabeled
dataset. Whereas purely unsupervised training is less used. In this paper, we
aim to solve both problems at the same time. We design a novel unsupervised
model, Sync-Person-Cloud ReID, to solve the unsupervised clothing change person
ReID problem. We developer a purely unsupervised clothing change person ReID
pipeline with person sync augmentation operation and same person feature
restriction. The person sync augmentation is to supply additional same person
resources. These same person&#x27;s resources can be used as part supervised input
by same person feature restriction. The extensive experiments on clothing
change ReID datasets show the out-performance of our methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Digitize-PID: Automatic Digitization of Piping and Instrumentation Diagrams. (arXiv:2109.03794v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03794">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Digitization of scanned Piping and Instrumentation diagrams(P&amp;ID), widely
used in manufacturing or mechanical industries such as oil and gas over several
decades, has become a critical bottleneck in dynamic inventory management and
creation of smart P&amp;IDs that are compatible with the latest CAD tools.
Historically, P&amp;ID sheets have been manually generated at the design stage,
before being scanned and stored as PDFs. Current digitization initiatives
involve manual processing and are consequently very time consuming, labour
intensive and error-prone.Thanks to advances in image processing, machine and
deep learning techniques there are emerging works on P&amp;ID digitization.
However, existing solutions face several challenges owing to the variation in
the scale, size and noise in the P&amp;IDs, sheer complexity and crowdedness within
drawings, domain knowledge required to interpret the drawings. This motivates
our current solution called Digitize-PID which comprises of an end-to-end
pipeline for detection of core components from P&amp;IDs like pipes, symbols and
textual information, followed by their association with each other and
eventually, the validation and correction of output data based on inherent
domain knowledge. A novel and efficient kernel-based line detection and a
two-step method for detection of complex symbols based on a fine-grained deep
recognition technique is presented in the paper. In addition, we have created
an annotated synthetic dataset, Dataset-P&amp;ID, of 500 P&amp;IDs by incorporating
different types of noise and complex symbols which is made available for public
use (currently there exists no public P&amp;ID dataset). We evaluate our proposed
method on this synthetic dataset and a real-world anonymized private dataset of
12 P&amp;ID sheets. Results show that Digitize-PID outperforms the existing
state-of-the-art for P&amp;ID digitization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Melatect: A Machine Learning Model Approach For Identifying Malignant Melanoma in Skin Growths. (arXiv:2109.03310v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03310">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Malignant melanoma is a common skin cancer that is mostly curable before
metastasis, where melanoma growths spawn in organs away from the original site.
Melanoma is the most dangerous type of skin cancer if left untreated due to the
high chance of metastasis. This paper presents Melatect, a machine learning
model that identifies potential malignant melanoma. A recursive computer image
analysis algorithm was used to create a machine learning model which is capable
of detecting likely melanoma. The comparison is performed using 20,000 raw
images of benign and malignant lesions from the International Skin Imaging
Collaboration (ISIC) archive that were augmented to 60,000 images. Tests of the
algorithm using subsets of the ISIC images suggest it accurately classifies
lesions as malignant or benign over 95% of the time with no apparent bias or
overfitting. The Melatect iOS app was later created (unpublished), in which the
machine learning model was embedded. With the app, users have the ability to
take pictures of skin lesions (moles) using the app, which are then processed
through the machine learning model, and users are notified whether their lesion
could be abnormal or not. Melatect provides a convenient way to get free advice
on lesions and track these lesions over time.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self supervised contrastive learning for digital histopathology. (arXiv:2011.13971v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13971">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Unsupervised learning has been a long-standing goal of machine learning and
is especially important for medical image analysis, where the learning can
compensate for the scarcity of labeled datasets. A promising subclass of
unsupervised learning is self-supervised learning, which aims to learn salient
features using the raw input as the learning signal. In this paper, we use a
contrastive self-supervised learning method called SimCLR that achieved
state-of-the-art results on natural-scene images and apply this method to
digital histopathology by collecting and pretraining on 57 histopathology
datasets without any labels. We find that combining multiple multi-organ
datasets with different types of staining and resolution properties improves
the quality of the learned features. Furthermore, we find using more images for
pretraining leads to a better performance in multiple downstream tasks. Linear
classifiers trained on top of the learned features show that networks
pretrained on digital histopathology datasets perform better than ImageNet
pretrained networks, boosting task performances by more than 28% in F1 scores
on average. These findings may also be useful when applying newer contrastive
techniques to histopathology data. Pretrained PyTorch models are made publicly
available at https://github.com/ozanciga/self-supervised-histopathology.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identification of Social-Media Platform of Videos through the Use of Shared Features. (arXiv:2109.03598v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03598">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Videos have become a powerful tool for spreading illegal content such as
military propaganda, revenge porn, or bullying through social networks. To
counter these illegal activities, it has become essential to try new methods to
verify the origin of videos from these platforms. However, collecting datasets
large enough to train neural networks for this task has become difficult
because of the privacy regulations that have been enacted in recent years. To
mitigate this limitation, in this work we propose two different solutions based
on transfer learning and multitask learning to determine whether a video has
been uploaded from or downloaded to a specific social platform through the use
of shared features with images trained on the same task. By transferring
features from the shallowest to the deepest levels of the network from the
image task to videos, we measure the amount of information shared between these
two tasks. Then, we introduce a model based on multitask learning, which learns
from both tasks simultaneously. The promising experimental results show, in
particular, the effectiveness of the multitask approach. According to our
knowledge, this is the first work that addresses the problem of social media
platform identification of videos through the use of shared features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Animation Transformer: Visual Correspondence via Segment Matching. (arXiv:2109.02614v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02614">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Visual correspondence is a fundamental building block on the way to building
assistive tools for hand-drawn animation. However, while a large body of work
has focused on learning visual correspondences at the pixel-level, few
approaches have emerged to learn correspondence at the level of line enclosures
(segments) that naturally occur in hand-drawn animation. Exploiting this
structure in animation has numerous benefits: it avoids the intractable memory
complexity of attending to individual pixels in high resolution images and
enables the use of real-world animation datasets that contain correspondence
information at the level of per-segment colors. To that end, we propose the
Animation Transformer (AnT) which uses a transformer-based architecture to
learn the spatial and visual relationships between segments across a sequence
of images. AnT enables practical ML-assisted colorization for professional
animation workflows and is publicly accessible as a creative tool in Cadmium.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Site Severity Assessment of COVID-19 from CT Images via Domain Adaptation. (arXiv:2109.03478v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03478">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Early and accurate severity assessment of Coronavirus disease 2019 (COVID-19)
based on computed tomography (CT) images offers a great help to the estimation
of intensive care unit event and the clinical decision of treatment planning.
To augment the labeled data and improve the generalization ability of the
classification model, it is necessary to aggregate data from multiple sites.
This task faces several challenges including class imbalance between mild and
severe infections, domain distribution discrepancy between sites, and presence
of heterogeneous features. In this paper, we propose a novel domain adaptation
(DA) method with two components to address these problems. The first component
is a stochastic class-balanced boosting sampling strategy that overcomes the
imbalanced learning problem and improves the classification performance on
poorly-predicted classes. The second component is a representation learning
that guarantees three properties: 1) domain-transferability by prototype
triplet loss, 2) discriminant by conditional maximum mean discrepancy loss, and
3) completeness by multi-view reconstruction loss. Particularly, we propose a
domain translator and align the heterogeneous data to the estimated class
prototypes (i.e., class centers) in a hyper-sphere manifold. Experiments on
cross-site severity assessment of COVID-19 from CT images show that the
proposed method can effectively tackle the imbalanced learning problem and
outperform recent DA approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Local-Global Contextual Adaptation for Fully End-to-End Bottom-Up Human Pose Estimation. (arXiv:2109.03622v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03622">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents a method of learning Local-GlObal Contextual Adaptation
for fully end-to-end and fast bottom-up human Pose estimation, dubbed as
LOGO-CAP. It is built on the conceptually simple center-offset formulation that
lacks inaccuracy for pose estimation. When revisiting the bottom-up human pose
estimation with the thought of &quot;thinking, fast and slow&quot; by D. Kahneman, we
introduce a &quot;slow keypointer&quot; to remedy the lack of sufficient accuracy of the
&quot;fast keypointer&quot;. In learning the &quot;slow keypointer&quot;, the proposed LOGO-CAP
lifts the initial &quot;fast&quot; keypoints by offset predictions to keypoint expansion
maps (KEMs) to counter their uncertainty in two modules. Firstly, the local
KEMs (e.g., 11x11) are extracted from a low-dimensional feature map. A proposed
convolutional message passing module learns to &quot;re-focus&quot; the local KEMs to the
keypoint attraction maps (KAMs) by accounting for the structured output
prediction nature of human pose estimation, which is directly supervised by the
object keypoint similarity (OKS) loss in training. Secondly, the global KEMs
are extracted, with a sufficiently large region-of-interest (e.g., 97x97), from
the keypoint heatmaps that are computed by a direct map-to-map regression.
Then, a local-global contextual adaptation module is proposed to convolve the
global KEMs using the learned KAMs as the kernels. This convolution can be
understood as the learnable offsets guided deformable and dynamic convolution
in a pose-sensitive way. The proposed method is end-to-end trainable with near
real-time inference speed, obtaining state-of-the-art performance on the COCO
keypoint benchmark for bottom-up human pose estimation. With the COCO trained
model, our LOGO-CAP also outperforms prior arts by a large margin on the
challenging OCHuman dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03329">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks have developed rapidly and have achieved outstanding
performance in several tasks, such as image classification and natural language
processing. However, recent studies have indicated that both digital and
physical adversarial examples can fool neural networks. Face-recognition
systems are used in various applications that involve security threats from
physical adversarial examples. Herein, we propose a physical adversarial attack
with the use of full-face makeup. The presence of makeup on the human face is a
reasonable possibility, which possibly increases the imperceptibility of
attacks. In our attack framework, we combine the cycle-adversarial generative
network (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to
generate adversarial makeup, and the architecture of the victimized classifier
is VGG 16. Our experimental results show that our attack can effectively
overcome manual errors in makeup application, such as color and
position-related errors. We also demonstrate that the approaches used to train
the models can influence physical attacks; the adversarial perturbations
crafted from the pre-trained model are affected by the corresponding training
data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01169">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adaptive Few-Shot Learning PoC Ultrasound COVID-19 Diagnostic System. (arXiv:2109.03793v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03793">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents a novel ultrasound imaging point-of-care (PoC) COVID-19
diagnostic system. The adaptive visual diagnostics utilize few-shot learning
(FSL) to generate encoded disease state models that are stored and classified
using a dictionary of knowns. The novel vocabulary based feature processing of
the pipeline adapts the knowledge of a pretrained deep neural network to
compress the ultrasound images into discrimative descriptions. The
computational efficiency of the FSL approach enables high diagnostic deep
learning performance in PoC settings, where training data is limited and the
annotation process is not strictly controlled. The algorithm performance is
evaluated on the open source COVID-19 POCUS Dataset to validate the system&#x27;s
ability to distinguish COVID-19, pneumonia, and healthy disease states. The
results of the empirical analyses demonstrate the appropriate efficiency and
accuracy for scalable PoC use. The code for this work will be made publicly
available on GitHub upon acceptance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Certifiable Outlier-Robust Geometric Perception: Exact Semidefinite Relaxations and Scalable Global Optimization. (arXiv:2109.03349v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03349">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose the first general and scalable framework to design certifiable
algorithms for robust geometric perception in the presence of outliers. Our
first contribution is to show that estimation using common robust costs, such
as truncated least squares (TLS), maximum consensus, Geman-McClure, Tukey&#x27;s
biweight, among others, can be reformulated as polynomial optimization problems
(POPs). By focusing on the TLS cost, our second contribution is to exploit
sparsity in the POP and propose a sparse semidefinite programming (SDP)
relaxation that is much smaller than the standard Lasserre&#x27;s hierarchy while
preserving exactness, i.e., the SDP recovers the optimizer of the nonconvex POP
with an optimality certificate. Our third contribution is to solve the SDP
relaxations at an unprecedented scale and accuracy by presenting STRIDE, a
solver that blends global descent on the convex SDP with fast local search on
the nonconvex POP. Our fourth contribution is an evaluation of the proposed
framework on six geometric perception problems including single and multiple
rotation averaging, point cloud and mesh registration, absolute pose
estimation, and category-level object pose and shape estimation. Our
experiments demonstrate that (i) our sparse SDP relaxation is exact with up to
60%-90% outliers across applications; (ii) while still being far from
real-time, STRIDE is up to 100 times faster than existing SDP solvers on
medium-scale problems, and is the only solver that can solve large-scale SDPs
with hundreds of thousands of constraints to high accuracy; (iii) STRIDE
provides a safeguard to existing fast heuristics for robust estimation (e.g.,
RANSAC or Graduated Non-Convexity), i.e., it certifies global optimality if the
heuristic estimates are optimal, or detects and allows escaping local optima
when the heuristic estimates are suboptimal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Real-World Super-Resolution via Adaptive Downsampling Models. (arXiv:2109.03444v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03444">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Most image super-resolution (SR) methods are developed on synthetic
low-resolution (LR) and high-resolution (HR) image pairs that are constructed
by a predetermined operation, e.g., bicubic downsampling. As existing methods
typically learn an inverse mapping of the specific function, they produce
blurry results when applied to real-world images whose exact formulation is
different and unknown. Therefore, several methods attempt to synthesize much
more diverse LR samples or learn a realistic downsampling model. However, due
to restrictive assumptions on the downsampling process, they are still biased
and less generalizable. This study proposes a novel method to simulate an
unknown downsampling process without imposing restrictive prior knowledge. We
propose a generalizable low-frequency loss (LFL) in the adversarial training
framework to imitate the distribution of target LR images without using any
paired examples. Furthermore, we design an adaptive data loss (ADL) for the
downsampler, which can be adaptively learned and updated from the data during
the training loops. Extensive experiments validate that our downsampling model
can facilitate existing SR methods to perform more accurate reconstructions on
various synthetic and real-world examples than the conventional approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MRI Reconstruction Using Deep Energy-Based Model. (arXiv:2109.03237v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03237">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Purpose: Although recent deep energy-based generative models (EBMs) have
shown encouraging results in many image generation tasks, how to take advantage
of the self-adversarial cogitation in deep EBMs to boost the performance of
Magnetic Resonance Imaging (MRI) reconstruction is still desired.

Methods: With the successful application of deep learning in a wide range of
MRI reconstruction, a line of emerging research involves formulating an
optimization-based reconstruction method in the space of a generative model.
Leveraging this, a novel regularization strategy is introduced in this article
which takes advantage of self-adversarial cogitation of the deep energy-based
model. More precisely, we advocate for alternative learning a more powerful
energy-based model with maximum likelihood estimation to obtain the deep
energy-based information, represented as image prior. Simultaneously, implicit
inference with Langevin dynamics is a unique property of re-construction. In
contrast to other generative models for reconstruction, the proposed method
utilizes deep energy-based information as the image prior in reconstruction to
improve the quality of image.

Results: Experiment results that imply the proposed technique can obtain
remarkable performance in terms of high reconstruction accuracy that is
competitive with state-of-the-art methods, and does not suffer from mode
collapse.

Conclusion: Algorithmically, an iterative approach was presented to
strengthen EBM training with the gradient of energy network. The robustness and
the reproducibility of the algorithm were also experimentally validated. More
importantly, the proposed reconstruction framework can be generalized for most
MRI reconstruction scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Master Face Attacks on Face Recognition Systems. (arXiv:2109.03398v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03398">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Face authentication is now widely used, especially on mobile devices, rather
than authentication using a personal identification number or an unlock
pattern, due to its convenience. It has thus become a tempting target for
attackers using a presentation attack. Traditional presentation attacks use
facial images or videos of the victim. Previous work has proven the existence
of master faces, i.e., faces that match multiple enrolled templates in face
recognition systems, and their existence extends the ability of presentation
attacks. In this paper, we perform an extensive study on latent variable
evolution (LVE), a method commonly used to generate master faces. We run an LVE
algorithm for various scenarios and with more than one database and/or face
recognition system to study the properties of the master faces and to
understand in which conditions strong master faces could be generated.
Moreover, through analysis, we hypothesize that master faces come from some
dense areas in the embedding spaces of the face recognition systems. Last but
not least, simulated presentation attacks using generated master faces
generally preserve the false-matching ability of their original digital forms,
thus demonstrating that the existence of master faces poses an actual threat.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unfolding Taylor&#x27;s Approximations for Image Restoration. (arXiv:2109.03442v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03442">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep learning provides a new avenue for image restoration, which demands a
delicate balance between fine-grained details and high-level contextualized
information during recovering the latent clear image. In practice, however,
existing methods empirically construct encapsulated end-to-end mapping networks
without deepening into the rationality, and neglect the intrinsic prior
knowledge of restoration task. To solve the above problems, inspired by
Taylor&#x27;s Approximations, we unfold Taylor&#x27;s Formula to construct a novel
framework for image restoration. We find the main part and the derivative part
of Taylor&#x27;s Approximations take the same effect as the two competing goals of
high-level contextualized information and spatial details of image restoration
respectively. Specifically, our framework consists of two steps,
correspondingly responsible for the mapping and derivative functions. The
former first learns the high-level contextualized information and the later
combines it with the degraded input to progressively recover local high-order
spatial details. Our proposed framework is orthogonal to existing methods and
thus can be easily integrated with them for further improvement, and extensive
experiments demonstrate the effectiveness and scalability of our proposed
framework.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology. (arXiv:2109.03299v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03299">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The examination of histopathology images is considered to be the gold
standard for the diagnosis and stratification of cancer patients. A key
challenge in the analysis of such images is their size, which can run into the
gigapixels and can require tedious screening by clinicians. With the recent
advances in computational medicine, automatic tools have been proposed to
assist clinicians in their everyday practice. Such tools typically process
these large images by slicing them into tiles that can then be encoded and
utilized for different clinical models. In this study, we propose a novel
generative framework that can learn powerful representations for such tiles by
learning to plausibly expand their visual field. In particular, we developed a
progressively grown generative model with the objective of visual field
expansion. Thus trained, our model learns to generate different tissue types
with fine details, while simultaneously learning powerful representations that
can be used for different clinical endpoints, all in a self-supervised way. To
evaluate the performance of our model, we conducted classification experiments
on CAMELYON17 and CRC benchmark datasets, comparing favorably to other
self-supervised and pre-trained strategies that are commonly used in digital
pathology. Our code is available at https://github.com/jcboyd/cdpath21-gan.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.01088">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Efficient spatiotemporal modeling is an important yet challenging problem for
video action recognition. Existing state-of-the-art methods exploit motion
clues to assist in short-term temporal modeling through temporal difference
over consecutive frames. However, insignificant noises will be inevitably
introduced due to the camera movement. Besides, movements of different actions
can vary greatly. In this paper, we propose a Temporal Saliency Integration
(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and
a Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to
highlight the motion-sensitive area through local-global motion modeling, where
the saliency alignment and pyramidal feature difference are conducted
successively between neighboring frames to capture motion dynamics with less
noises caused by misaligned background. CTI is designed to perform multi-scale
temporal modeling through a group of separate 1D convolutions respectively.
Meanwhile, temporal interactions across different scales are integrated with
attention mechanism. Through these two modules, long short-term temporal
relationships can be encoded efficiently by introducing limited additional
parameters. Extensive experiments are conducted on several popular benchmarks
(i.e., Something-Something V1 &amp; V2, Kinetics-400, UCF-101, and HMDB-51), which
demonstrate the effectiveness and superiority of our proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ParamCrop: Parametric Cubic Cropping for Video Contrastive Learning. (arXiv:2108.10501v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10501">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The central idea of contrastive learning is to discriminate between different
instances and force different views of the same instance to share the same
representation. To avoid trivial solutions, augmentation plays an important
role in generating different views, among which random cropping is shown to be
effective for the model to learn a strong and generalized representation.
Commonly used random crop operation keeps the difference between two views
statistically consistent along the training process. In this work, we challenge
this convention by showing that adaptively controlling the disparity between
two augmented views along the training process enhances the quality of the
learnt representation. Specifically, we present a parametric cubic cropping
operation, ParamCrop, for video contrastive learning, which automatically crops
a 3D cubic from the video by differentiable 3D affine transformations.
ParamCrop is trained simultaneously with the video backbone using an
adversarial objective and learns an optimal cropping strategy from the data.
The visualizations show that the center distance and the IoU between two
augmented views are adaptively controlled by ParamCrop and the learned change
in the disparity along the training process is beneficial to learning a strong
representation. Extensive ablation studies demonstrate the effectiveness of the
proposed ParamCrop on multiple contrastive learning frameworks and video
backbones. With ParamCrop, we improve the state-of-the-art performance on both
HMDB51 and UCF101 datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11005">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online continual learning from data streams in dynamic environments is a
critical direction in the computer vision field. However, realistic benchmarks
and fundamental studies in this line are still missing. To bridge the gap, we
present a new online continual object detection benchmark with an egocentric
video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,
an ego-centric video stream collected over nine months by a graduate student.
OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5
hours) for 105 object categories in outdoor scenes. The emergence of new object
categories in our benchmark follows a pattern similar to what a single person
might see in their day-to-day life. The dataset also captures the natural
distribution shifts as the person travels to different places. These egocentric
long-running videos provide a realistic playground for continual learning
algorithms, especially in online embodied settings. We also introduce new
evaluation metrics to evaluate the model performance and catastrophic
forgetting and provide baseline studies for online continual object detection.
We believe this benchmark will pose new exciting challenges for learning from
non-stationary data in continual learning. The OAK dataset and the associated
benchmark are released at https://oakdata.github.io/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RGB-D Salient Object Detection with Ubiquitous Target Awareness. (arXiv:2109.03425v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03425">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Conventional RGB-D salient object detection methods aim to leverage depth as
complementary information to find the salient regions in both modalities.
However, the salient object detection results heavily rely on the quality of
captured depth data which sometimes are unavailable. In this work, we make the
first attempt to solve the RGB-D salient object detection problem with a novel
depth-awareness framework. This framework only relies on RGB data in the
testing phase, utilizing captured depth data as supervision for representation
learning. To construct our framework as well as achieving accurate salient
detection results, we propose a Ubiquitous Target Awareness (UTA) network to
solve three important challenges in RGB-D SOD task: 1) a depth awareness module
to excavate depth information and to mine ambiguous regions via adaptive
depth-error weights, 2) a spatial-aware cross-modal interaction and a
channel-aware cross-level interaction, exploiting the low-level boundary cues
and amplifying high-level salient channels, and 3) a gated multi-scale
predictor module to perceive the object saliency in different contextual
scales. Besides its high performance, our proposed UTA network is depth-free
for inference and runs in real-time with 43 FPS. Experimental evidence
demonstrates that our proposed network not only surpasses the state-of-the-art
methods on five public RGB-D SOD benchmarks by a large margin, but also
verifies its extensibility on five public RGB SOD benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09391">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion, zoom, rotation, image cut and Gaussian perturbations
improves, while significantly improving the performance on clean images without
any data augmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Disentangling Alzheimer&#x27;s disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neuroimaging biomarkers that distinguish between typical brain aging and
Alzheimer&#x27;s disease (AD) are valuable for determining how much each contributes
to cognitive decline. Machine learning models can derive multi-variate brain
change patterns related to the two processes, including the SPARE-AD (Spatial
Patterns of Atrophy for Recognition of Alzheimer&#x27;s Disease) and SPARE-BA (of
Brain Aging) investigated herein. However, substantial overlap between brain
regions affected in the two processes confounds measuring them independently.
We present a methodology toward disentangling the two. T1-weighted MRI images
of 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),
or cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based
coordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were
analyzed. First, a subset of AD patients and CN adults were selected based
purely on clinical diagnoses to train SPARE-BA1 (regression of age using CN
individuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous
groups were selected based on clinical and molecular markers to train SPARE-BA2
and SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,
A+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN
group. Finally, the combined group of the AD continuum and A-/CN individuals
was used to train SPARE-BA3, with the intention to estimate brain age
regardless of AD-related brain changes. Disentangled SPARE models derived brain
patterns that were more specific to the two types of the brain changes.
Correlation between the SPARE-BA and SPARE-AD was significantly reduced.
Correlation of disentangled SPARE-AD was non-inferior to the molecular
measurements and to the number of APOE4 alleles, but was less to AD-related
psychometric test scores, suggesting contribution of advanced brain aging to
these scores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FaceCook: Face Generation Based on Linear Scaling Factors. (arXiv:2109.03492v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03492">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the excellent disentanglement properties of state-of-the-art generative
models, image editing has been the dominant approach to control the attributes
of synthesised face images. However, these edited results often suffer from
artifacts or incorrect feature rendering, especially when there is a large
discrepancy between the image to be edited and the desired feature set.
Therefore, we propose a new approach to mapping the latent vectors of the
generative model to the scaling factors through solving a set of multivariate
linear equations. The coefficients of the equations are the eigenvectors of the
weight parameters of the pre-trained model, which form the basis of a hyper
coordinate system. The qualitative and quantitative results both show that the
proposed method outperforms the baseline in terms of image diversity. In
addition, the method is much more time-efficient because you can obtain
synthesised images with desirable features directly from the latent vectors,
rather than the former process of editing randomly generated images requiring
many processing steps.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Discriminate Information for Online Action Detection: Analysis and Application. (arXiv:2109.03393v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03393">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online action detection, which aims to identify an ongoing action from a
streaming video, is an important subject in real-world applications. For this
task, previous methods use recurrent neural networks for modeling temporal
relations in an input sequence. However, these methods overlook the fact that
the input image sequence includes not only the action of interest but
background and irrelevant actions. This would induce recurrent units to
accumulate unnecessary information for encoding features on the action of
interest. To overcome this problem, we propose a novel recurrent unit, named
Information Discrimination Unit (IDU), which explicitly discriminates the
information relevancy between an ongoing action and others to decide whether to
accumulate the input information. This enables learning more discriminative
representations for identifying an ongoing action. In this paper, we further
present a new recurrent unit, called Information Integration Unit (IIU), for
action anticipation. Our IIU exploits the outputs from IDU as pseudo action
labels as well as RGB frames to learn enriched features of observed actions
effectively. In experiments on TVSeries and THUMOS-14, the proposed methods
outperform state-of-the-art methods by a significant margin in online action
detection and action anticipation. Moreover, we demonstrate the effectiveness
of the proposed units by conducting comprehensive ablation studies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaled ReLU Matters for Training Vision Transformers. (arXiv:2109.03810v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03810">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Vision transformers (ViTs) have been an alternative design paradigm to
convolutional neural networks (CNNs). However, the training of ViTs is much
harder than CNNs, as it is sensitive to the training parameters, such as
learning rate, optimizer and warmup epoch. The reasons for training difficulty
are empirically analysed in ~\cite{xiao2021early}, and the authors conjecture
that the issue lies with the \textit{patchify-stem} of ViT models and propose
that early convolutions help transformers see better. In this paper, we further
investigate this problem and extend the above conclusion: only early
convolutions do not help for stable training, but the scaled ReLU operation in
the \textit{convolutional stem} (\textit{conv-stem}) matters. We verify, both
theoretically and empirically, that scaled ReLU in \textit{conv-stem} not only
improves training stabilization, but also increases the diversity of patch
tokens, thus boosting peak performance with a large margin via adding few
parameters and flops. In addition, extensive experiments are conducted to
demonstrate that previous ViTs are far from being well trained, further showing
that ViTs have great potential to be a better substitute of CNNs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Panoptic SegFormer. (arXiv:2109.03814v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03814">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present Panoptic SegFormer, a general framework for end-to-end panoptic
segmentation with Transformers. The proposed method extends Deformable DETR
with a unified mask prediction workflow for both things and stuff, making the
panoptic segmentation pipeline concise and effective. With a ResNet-50
backbone, our method achieves 50.0\% PQ on the COCO test-dev split, surpassing
previous state-of-the-art methods by significant margins without bells and
whistles. Using a more powerful PVTv2-B5 backbone, Panoptic-SegFormer achieves
a new record of 54.1\%PQ and 54.4\% PQ on the COCO val and test-dev splits with
single scale input.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal RoI Align for Video Object Recognition. (arXiv:2109.03495v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03495">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Video object detection is challenging in the presence of appearance
deterioration in certain video frames. Therefore, it is a natural choice to
aggregate temporal information from other frames of the same video into the
current frame. However, RoI Align, as one of the most core procedures of video
detectors, still remains extracting features from a single-frame feature map
for proposals, making the extracted RoI features lack temporal information from
videos. In this work, considering the features of the same object instance are
highly similar among frames in a video, a novel Temporal RoI Align operator is
proposed to extract features from other frames feature maps for current frame
proposals by utilizing feature similarity. The proposed Temporal RoI Align
operator can extract temporal information from the entire video for proposals.
We integrate it into single-frame video detectors and other state-of-the-art
video detectors, and conduct quantitative experiments to demonstrate that the
proposed Temporal RoI Align operator can consistently and significantly boost
the performance. Besides, the proposed Temporal RoI Align can also be applied
into video instance segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03805">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Panoptic scene understanding and tracking of dynamic agents are essential for
robots and automated vehicles to navigate in urban environments. As LiDARs
provide accurate illumination-independent geometric depictions of the scene,
performing these tasks using LiDAR point clouds provides reliable predictions.
However, existing datasets lack diversity in the type of urban scenes and have
a limited number of dynamic object instances which hinders both learning of
these tasks as well as credible benchmarking of the developed methods. In this
paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that
extends our popular nuScenes dataset with point-wise groundtruth annotations
for semantic segmentation, panoptic segmentation, and panoptic tracking tasks.
To facilitate comparison, we provide several strong baselines for each of these
tasks on our proposed dataset. Moreover, we analyze the drawbacks of the
existing metrics for the panoptic tracking problem and propose a novel
instance-centric metric that addresses the concerns. We present extensive
experiments that demonstrate the utility of Panoptic nuScenes compared to
existing datasets and make the online evaluation server available at
\url{nuScenes.org}. We believe that this extension will accelerate the research
of novel methods for scene understanding of dynamic urban environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03385">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the rapid development of intelligent detection algorithms based on deep
learning, much progress has been made in automatic road defect recognition and
road marking parsing. This can effectively address the issue of an expensive
and time-consuming process for professional inspectors to review the street
manually. Towards this goal, we present RoadAtlas, a novel end-to-end
integrated system that can support 1) road defect detection, 2) road marking
parsing, 3) a web-based dashboard for presenting and inputting data by users,
and 4) a backend containing a well-structured database and developed APIs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Which and Where to Focus: A Simple yet Accurate Framework for Arbitrary-Shaped Nearby Text Detection in Scene Images. (arXiv:2109.03451v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03451">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scene text detection has drawn the close attention of researchers. Though
many methods have been proposed for horizontal and oriented texts, previous
methods may not perform well when dealing with arbitrary-shaped texts such as
curved texts. In particular, confusion problem arises in the case of nearby
text instances. In this paper, we propose a simple yet effective method for
accurate arbitrary-shaped nearby scene text detection. Firstly, a One-to-Many
Training Scheme (OMTS) is designed to eliminate confusion and enable the
proposals to learn more appropriate groundtruths in the case of nearby text
instances. Secondly, we propose a Proposal Feature Attention Module (PFAM) to
exploit more effective features for each proposal, which can better adapt to
arbitrary-shaped text instances. Finally, we propose a baseline that is based
on Faster R-CNN and outputs the curve representation directly. Equipped with
PFAM and OMTS, the detector can achieve state-of-the-art or competitive
performance on several challenging benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06664">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Similarity learning has been recognized as a crucial step for object
tracking. However, existing multiple object tracking methods only use sparse
ground truth matching as the training objective, while ignoring the majority of
the informative regions on the images. In this paper, we present Quasi-Dense
Similarity Learning, which densely samples hundreds of region proposals on a
pair of images for contrastive learning. We can directly combine this
similarity learning with existing detection methods to build Quasi-Dense
Tracking (QDTrack) without turning to displacement regression or motion priors.
We also find that the resulting distinctive feature space admits a simple
nearest neighbor search at the inference time. Despite its simplicity, QDTrack
outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking
benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external
training data. Compared to methods with similar detectors, it boosts almost 10
points of MOTA and significantly decreases the number of ID switches on BDD100K
and Waymo datasets. Our code and trained models are available at
this http URL</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FIDNet: LiDAR Point Cloud Semantic Segmentation with Fully Interpolation Decoding. (arXiv:2109.03787v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03787">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Projecting the point cloud on the 2D spherical range image transforms the
LiDAR semantic segmentation to a 2D segmentation task on the range image.
However, the LiDAR range image is still naturally different from the regular 2D
RGB image; for example, each position on the range image encodes the unique
geometry information. In this paper, we propose a new projection-based LiDAR
semantic segmentation pipeline that consists of a novel network structure and
an efficient post-processing step. In our network structure, we design a FID
(fully interpolation decoding) module that directly upsamples the
multi-resolution feature maps using bilinear interpolation. Inspired by the 3D
distance interpolation used in PointNet++, we argue this FID module is a 2D
version distance interpolation on $(\theta, \phi)$ space. As a parameter-free
decoding module, the FID largely reduces the model complexity by maintaining
good performance. Besides the network structure, we empirically find that our
model predictions have clear boundaries between different semantic classes.
This makes us rethink whether the widely used K-nearest-neighbor
post-processing is still necessary for our pipeline. Then, we realize the
many-to-one mapping causes the blurring effect that some points are mapped into
the same pixel and share the same label. Therefore, we propose to process those
occluded points by assigning the nearest predicted label to them. This NLA
(nearest label assignment) post-processing step shows a better performance than
KNN with faster inference speed in the ablation study. On the SemanticKITTI
dataset, our pipeline achieves the best performance among all projection-based
methods with $64 \times 2048$ resolution and all point-wise solutions. With a
ResNet-34 as the backbone, both the training and testing of our model can be
finished on a single RTX 2080 Ti with 11G memory. The code is released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Domain Adaptive Ensemble Learning. (arXiv:2003.07325v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.07325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The problem of generalizing deep neural networks from multiple source domains
to a target one is studied under two settings: When unlabeled target data is
available, it is a multi-source unsupervised domain adaptation (UDA) problem,
otherwise a domain generalization (DG) problem. We propose a unified framework
termed domain adaptive ensemble learning (DAEL) to address both problems. A
DAEL model is composed of a CNN feature extractor shared across domains and
multiple classifier heads each trained to specialize in a particular source
domain. Each such classifier is an expert to its own domain and a non-expert to
others. DAEL aims to learn these experts collaboratively so that when forming
an ensemble, they can leverage complementary information from each other to be
more effective for an unseen target domain. To this end, each source domain is
used in turn as a pseudo-target-domain with its own expert providing
supervisory signal to the ensemble of non-experts learned from the other
sources. For unlabeled target data under the UDA setting where real expert does
not exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive
experiments on three multi-source UDA datasets and two DG datasets show that
DAEL improves the state of the art on both problems, often by significant
margins. The code is released at
\url{https://github.com/KaiyangZhou/Dassl.pytorch}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global feature is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object&#x27;s presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method outperforms the existing
state-of-the-arts on both fine-grained and generalized datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.05953">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Encouraged by the success of contrastive learning on image classification
tasks, we propose a new self-supervised method for the structured regression
task of 3D hand pose estimation. Contrastive learning makes use of unlabeled
data for the purpose of representation learning via a loss formulation that
encourages the learned feature representations to be invariant under any image
transformation. For 3D hand pose estimation, it too is desirable to have
invariance to appearance transformation such as color jitter. However, the task
requires equivariance under affine transformations, such as rotation and
translation. To address this issue, we propose an equivariant contrastive
objective and demonstrate its effectiveness in the context of 3D hand pose
estimation. We experimentally investigate the impact of invariant and
equivariant contrastive objectives and show that learning equivariant features
leads to better representations for the task of 3D hand pose estimation.
Furthermore, we show that standard ResNets with sufficient depth, trained on
additional unlabeled data, attain improvements of up to 14.5% in PA-EPE on
FreiHAND and thus achieves state-of-the-art performance without any task
specific, specialized architectures. Code and models are available at
https://ait.ethz.ch/projects/2021/PeCLR/</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification. (arXiv:2109.03483v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03483">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Person Re-Identification (Re-Id) in occlusion scenarios is a challenging
problem because a pedestrian can be partially occluded. The use of local
information for feature extraction and matching is still necessary. Therefore,
we propose a Pose-guided inter-and intra-part relational transformer (Pirt) for
occluded person Re-Id, which builds part-aware long-term correlations by
introducing transformers. In our framework, we firstly develop a pose-guided
feature extraction module with regional grouping and mask construction for
robust feature representations. The positions of a pedestrian in the image
under surveillance scenarios are relatively fixed, hence we propose an
intra-part and inter-part relational transformer. The intra-part module creates
local relations with mask-guided features, while the inter-part relationship
builds correlations with transformers, to develop cross relationships between
part nodes. With the collaborative learning inter- and intra-part
relationships, experiments reveal that our proposed Pirt model achieves a new
state of the art on the public occluded dataset, and further extensions on
standard non-occluded person Re-Id datasets also reveal our comparable
performances.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SSEGEP: Small SEGment Emphasized Performance evaluation metric for medical image segmentation. (arXiv:2109.03435v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03435">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatic image segmentation is a critical component of medical image
analysis, and hence quantifying segmentation performance is crucial. Challenges
in medical image segmentation are mainly due to spatial variations of regions
to be segmented and imbalance in distribution of classes. Commonly used metrics
treat all detected pixels, indiscriminately. However, pixels in smaller
segments must be treated differently from pixels in larger segments, as
detection of smaller ones aid in early treatment of associated disease and are
also easier to miss. To address this, we propose a novel evaluation metric for
segmentation performance, emphasizing smaller segments, by assigning higher
weightage to smaller segment pixels. Weighted false positives are also
considered in deriving the new metric named, &quot;SSEGEP&quot;(Small SEGment Emphasized
Performance evaluation metric), (range : 0(Bad) to 1(Good)). The experiments
were performed on diverse anatomies(eye, liver, pancreas and breast) from
publicly available datasets to show applicability of the proposed metric across
different imaging techniques. Mean opinion score (MOS) and statistical
significance testing is used to quantify the relevance of proposed approach.
Across 33 fundus images, where the largest exudate is 1.41%, and the smallest
is 0.0002% of the image, the proposed metric is 30% closer to MOS, as compared
to Dice Similarity Coefficient (DSC). Statistical significance testing resulted
in promising p-value of order 10^{-18} with SSEGEP for hepatic tumor compared
to DSC. The proposed metric is found to perform better for the images having
multiple segments for a single label.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03327">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Direct numerical simulation (DNS) of turbulent flows is computationally
expensive and cannot be applied to flows with large Reynolds numbers. Large
eddy simulation (LES) is an alternative that is computationally less demanding,
but is unable to capture all of the scales of turbulent transport accurately.
Our goal in this work is to build a new data-driven methodology based on
super-resolution techniques to reconstruct DNS data from LES predictions. We
leverage the underlying physical relationships to regularize the relationships
amongst different physical variables. We also introduce a hierarchical
generative process and a reverse degradation process to fully explore the
correspondence between DNS and LES data. We demonstrate the effectiveness of
our method through a single-snapshot experiment and a cross-time experiment.
The results confirm that our method can better reconstruct high-resolution DNS
data over space and over time in terms of pixel-wise reconstruction error and
structural similarity. Visual comparisons show that our method performs much
better in capturing fine-level flow dynamics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Level Set Binocular Stereo with Occlusions. (arXiv:2109.03464v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03464">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Localizing stereo boundaries and predicting nearby disparities are difficult
because stereo boundaries induce occluded regions where matching cues are
absent. Most modern computer vision algorithms treat occlusions secondarily
(e.g., via left-right consistency checks after matching) or rely on high-level
cues to improve nearby disparities (e.g., via deep networks and large training
sets). They ignore the geometry of stereo occlusions, which dictates that the
spatial extent of occlusion must equal the amplitude of the disparity jump that
causes it. This paper introduces an energy and level-set optimizer that
improves boundaries by encoding occlusion geometry. Our model applies to
two-layer, figure-ground scenes, and it can be implemented cooperatively using
messages that pass predominantly between parents and children in an undecimated
hierarchy of multi-scale image patches. In a small collection of figure-ground
scenes curated from Middlebury and Falling Things stereo datasets, our model
provides more accurate boundaries than previous occlusion-handling stereo
techniques. This suggests new directions for creating cooperative stereo
systems that incorporate occlusion cues in a human-like manner.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recalibrating the KITTI Dataset Camera Setup for Improved Odometry Accuracy. (arXiv:2109.03462v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03462">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Over the last decade, one of the most relevant public datasets for evaluating
odometry accuracy is the KITTI dataset. Beside the quality and rich sensor
setup, its success is also due to the online evaluation tool, which enables
researchers to benchmark and compare algorithms. The results are evaluated on
the test subset solely, without any knowledge about the ground truth, yielding
unbiased, overfit free and therefore relevant validation for robot localization
based on cameras, 3D laser or combination of both. However, as any sensor
setup, it requires prior calibration and rectified stereo images are provided,
introducing dependence on the default calibration parameters. Given that, a
natural question arises if a better set of calibration parameters can be found
that would yield higher odometry accuracy. In this paper, we propose a new
approach for one shot calibration of the KITTI dataset multiple camera setup.
The approach yields better calibration parameters, both in the sense of lower
calibration reprojection errors and lower visual odometry error. We conducted
experiments where we show for three different odometry algorithms, namely
SOFT2, ORB-SLAM2 and VISO2, that odometry accuracy is significantly improved
with the proposed calibration parameters. Moreover, our odometry, SOFT2, in
conjunction with the proposed calibration method achieved the highest accuracy
on the official KITTI scoreboard with 0.53% translational and 0.0009 deg/m
rotational error, outperforming even 3D laser-based methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03292">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite having been studied to a great extent, the task of conditional
generation of sequences of frames, or videos, remains extremely challenging. It
is a common belief that a key step towards solving this task resides in
modelling accurately both spatial and temporal information in video signals. A
promising direction to do so has been to learn latent variable models that
predict the future in latent space and project back to pixels, as suggested in
recent literature. Following this line of work and building on top of a family
of models introduced in prior work, Neural ODE, we investigate an approach that
models time-continuous dynamics over a continuous latent space with a
differential equation with respect to time. The intuition behind this approach
is that these trajectories in latent space could then be extrapolated to
generate video frames beyond the time steps for which the model is trained. We
show that our approach yields promising results in the task of future frame
prediction on the Moving MNIST dataset with 1 and 2 digits.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Branch Deep Radial Basis Function Networks for Facial Emotion Recognition. (arXiv:2109.03336v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03336">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Emotion recognition (ER) from facial images is one of the landmark tasks in
affective computing with major developments in the last decade. Initial efforts
on ER relied on handcrafted features that were used to characterize facial
images and then feed to standard predictive models. Recent methodologies
comprise end-to-end trainable deep learning methods that simultaneously learn
both, features and predictive model. Perhaps the most successful models are
based on convolutional neural networks (CNNs). While these models have excelled
at this task, they still fail at capturing local patterns that could emerge in
the learning process. We hypothesize these patterns could be captured by
variants based on locally weighted learning. Specifically, in this paper we
propose a CNN based architecture enhanced with multiple branches formed by
radial basis function (RBF) units that aims at exploiting local information at
the final stage of the learning process. Intuitively, these RBF units capture
local patterns shared by similar instances using an intermediate
representation, then the outputs of the RBFs are feed to a softmax layer that
exploits this information to improve the predictive performance of the model.
This feature could be particularly advantageous in ER as cultural / ethnicity
differences may be identified by the local units. We evaluate the proposed
method in several ER datasets and show the proposed methodology achieves
state-of-the-art in some of them, even when we adopt a pre-trained VGG-Face
model as backbone. We show it is the incorporation of local information what
makes the proposed model competitive.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks. (arXiv:2109.03513v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03513">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantization has been proven to be a vital method for improving the inference
efficiency of deep neural networks (DNNs). However, it is still challenging to
strike a good balance between accuracy and efficiency while quantizing DNN
weights or activation values from high-precision formats to their quantized
counterparts. We propose a new method called elastic significant bit
quantization (ESB) that controls the number of significant bits of quantized
values to obtain better inference accuracy with fewer resources. We design a
unified mathematical formula to constrain the quantized values of the ESB with
a flexible number of significant bits. We also introduce a distribution
difference aligner (DDA) to quantitatively align the distributions between the
full-precision weight or activation values and quantized values. Consequently,
ESB is suitable for various bell-shaped distributions of weights and activation
of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer
significant bits of quantized values, ESB can reduce the multiplication
complexity. We implement ESB as an accelerator and quantitatively evaluate its
efficiency on FPGAs. Extensive experimental results illustrate that ESB
quantization consistently outperforms state-of-the-art methods and achieves
average accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet,
ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can
achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx
ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators
on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65x,
11x, and 26x, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matching in the Dark: A Dataset for Matching Image Pairs of Low-light Scenes. (arXiv:2109.03585v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03585">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper considers matching images of low-light scenes, aiming to widen the
frontier of SfM and visual SLAM applications. Recent image sensors can record
the brightness of scenes with more than eight-bit precision, available in their
RAW-format image. We are interested in making full use of such high-precision
information to match extremely low-light scene images that conventional methods
cannot handle. For extreme low-light scenes, even if some of their brightness
information exists in the RAW format images&#x27; low bits, the standard raw image
processing on cameras fails to utilize them properly. As was recently shown by
Chen et al., CNNs can learn to produce images with a natural appearance from
such RAW-format images. To consider if and how well we can utilize such
information stored in RAW-format images for image matching, we have created a
new dataset named MID (matching in the dark). Using it, we experimentally
evaluated combinations of eight image-enhancing methods and eleven image
matching methods consisting of classical/neural local descriptors and
classical/neural initial point-matching methods. The results show the advantage
of using the RAW-format images and the strengths and weaknesses of the above
component methods. They also imply there is room for further research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">YouRefIt: Embodied Reference Understanding with Language and Gesture. (arXiv:2109.03413v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the understanding of embodied reference: One agent uses both
language and gesture to refer to an object to another agent in a shared
physical environment. Of note, this new visual task requires understanding
multimodal cues with perspective-taking to identify which object is being
referred to. To tackle this problem, we introduce YouRefIt, a new crowd-sourced
dataset of embodied reference collected in various physical scenes; the dataset
contains 4,195 unique reference clips in 432 indoor scenes. To the best of our
knowledge, this is the first embodied reference dataset that allows us to study
referring expressions in daily physical scenes to understand referential
behavior, human communication, and human-robot interaction. We further devise
two benchmarks for image-based and video-based embodied reference
understanding. Comprehensive baselines and extensive experiments provide the
very first result of machine perception on how the referring expressions and
gestures affect the embodied reference understanding. Our results provide
essential evidence that gestural cues are as critical as language cues in
understanding the embodied reference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Capturing the objects of vision with neural networks. (arXiv:2109.03351v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human visual perception carves a scene at its physical joints, decomposing
the world into objects, which are selectively attended, tracked, and predicted
as we engage our surroundings. Object representations emancipate perception
from the sensory input, enabling us to keep in mind that which is out of sight
and to use perceptual content as a basis for action and symbolic cognition.
Human behavioral studies have documented how object representations emerge
through grouping, amodal completion, proto-objects, and object files. Deep
neural network (DNN) models of visual object recognition, by contrast, remain
largely tethered to the sensory input, despite achieving human-level
performance at labeling objects. Here, we review related work in both fields
and examine how these fields can help each other. The cognitive literature
provides a starting point for the development of new experimental tasks that
reveal mechanisms of human object perception and serve as benchmarks driving
development of deep neural network models that will put the object into object
recognition.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03551">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)
speech enhancement, a task that aims to improve the quality of the artificial
voice from an electrolarynx device. In frame-based VC methods, time alignment
needs to be performed prior to model training, and the dynamic time warping
(DTW) algorithm is widely adopted to compute the best time alignment between
each utterance pair. The validity is based on the assumption that the same
phonemes of the speakers have similar features and can be mapped by measuring a
pre-defined distance between speech frames of the source and the target.
However, the special characteristics of the EL speech can break the assumption,
resulting in a sub-optimal DTW alignment. In this work, we propose to use lip
images for time alignment, as we assume that the lip movements of laryngectomee
remain normal compared to healthy people. We investigate two naive lip
representations and distance metrics, and experimental results demonstrate that
the proposed method can significantly outperform the audio-only alignment in
terms of objective and subjective evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GTT-Net: Learned Generalized Trajectory Triangulation. (arXiv:2109.03408v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03408">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present GTT-Net, a supervised learning framework for the reconstruction of
sparse dynamic 3D geometry. We build on a graph-theoretic formulation of the
generalized trajectory triangulation problem, where non-concurrent multi-view
imaging geometry is known but global image sequencing is not provided. GTT-Net
learns pairwise affinities modeling the spatio-temporal relationships among our
input observations and leverages them to determine 3D geometry estimates.
Experiments reconstructing 3D motion-capture sequences show GTT-Net outperforms
the state of the art in terms of accuracy and robustness. Within the context of
articulated motion reconstruction, our proposed architecture is 1) able to
learn and enforce semantic 3D motion priors for shared training and test
domains, while being 2) able to generalize its performance across different
training and test domains. Moreover, GTT-Net provides a computationally
streamlined framework for trajectory triangulation with applications to
multi-instance reconstruction and event segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Chord Recognition- Music and Audio Information Retrieval. (arXiv:2105.07019v2 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07019">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Music Information Retrieval (MIR) is a collaborative scientific study that
help to build innovative information research themes, novel frameworks, and
developing connected delivery mechanisms in addition to making the world&#x27;s
massive collection of music open for everyone. Modern rock music proved to be
difficult to estimate tempo and chord recognition did not work. All of the
findings indicate that modern rock and metal music can be analysed, despite its
complexity, but that further research is needed in this area to make it useful.
Using a neural network has been one of the simplest ways of dealing with it.
The pitch class profile vector is used in the neural network method. Because
the vector only contains 12 elements of semi-tone values, it is enough for
chord recognition. Of course, there are other ways of achieving this work, most
of them depend on pitch class profiling to transform the chord into a type that
can be recognised, but the recognition process is time-consuming centred on
extremely complicated and memory-intensive methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08663">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing neural information retrieval (IR) models have often been studied in
homogeneous and narrow settings, which has considerably limited insights into
their out-of-distribution (OOD) generalization capabilities. To address this,
and to facilitate researchers to broadly evaluate the effectiveness of their
models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous
evaluation benchmark for information retrieval. We leverage a careful selection
of 18 publicly available datasets from diverse text retrieval tasks and domains
and evaluate 10 state-of-the-art retrieval systems including lexical, sparse,
dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our
results show BM25 is a robust baseline and re-ranking and
late-interaction-based models on average achieve the best zero-shot
performances, however, at high computational costs. In contrast, dense and
sparse-retrieval models are computationally more efficient but often
underperform other approaches, highlighting the considerable room for
improvement in their generalization capabilities. We hope this framework allows
us to better evaluate and understand existing retrieval systems, and
contributes to accelerating progress towards better robust and generalizable
systems in the future. BEIR is publicly available at
https://github.com/UKPLab/beir.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04170">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms most graph-based
models. We further fine-tune DistilBERT for comparison and find that it
outperforms all state-of-the-art models. We suggest that future studies use at
least an MLP baseline to contextualize the results. We provide recommendations
for the design and training of such a baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions. (arXiv:2109.03540v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03540">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In light of the emergence of deep reinforcement learning (DRL) in recommender
systems research and several fruitful results in recent years, this survey aims
to provide a timely and comprehensive overview of the recent trends of deep
reinforcement learning in recommender systems. We start with the motivation of
applying DRL in recommender systems. Then, we provide a taxonomy of current
DRL-based recommender systems and a summary of existing methods. We discuss
emerging topics and open issues, and provide our perspective on advancing the
domain. This survey serves as introductory material for readers from academia
and industry into the topic and identifies notable opportunities for further
research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03502">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank
twice, reaD twice). The pipeline is composed of a retriever, passage reranker,
extractive reader, generative reader and a mechanism that aggregates the final
prediction from all system&#x27;s components. We demonstrate its strength across
three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,
surpassing state-of-the-art on the first two. Our analysis demonstrates that:
(i) combining extractive and generative reader yields absolute improvements up
to 5 exact match and it is at least twice as effective as the posterior
averaging ensemble of the same models with different parameters, (ii) the
extractive reader with fewer parameters can match the performance of the
generative reader on extractive QA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tracing Affordance and Item Adoption on Music Streaming Platforms. (arXiv:2109.03538v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03538">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Popular music streaming platforms offer users a diverse network of content
exploration through a triad of affordances: organic, algorithmic and editorial
access modes. Whilst offering great potential for discovery, such platform
developments also pose the modern user with daily adoption decisions on two
fronts: platform affordance adoption and the adoption of recommendations
therein. Following a carefully constrained set of Deezer users over a 2-year
observation period, our work explores factors driving user behaviour in the
broad sense, by differentiating users on the basis of their temporal daily
usage, adoption of the main platform affordances, and the ways in which they
react to them, especially in terms of recommendation adoption. Diverging from a
perspective common in studies on the effects of recommendation, we assume and
confirm that users exhibit very diverse behaviours in using and adopting the
platform affordances. The resulting complex and quite heterogeneous picture
demonstrates that there is no blanket answer for adoption practices of both
recommendation features and recommendations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AppQ: Warm-starting App Recommendation Based on View Graphs. (arXiv:2109.03798v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03798">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current app ranking and recommendation systems are mainly based on
user-generated information, e.g., number of downloads and ratings. However, new
apps often have few (or even no) user feedback, suffering from the classic
cold-start problem. How to quickly identify and then recommend new apps of high
quality is a challenging issue. Here, a fundamental requirement is the
capability to accurately measure an app&#x27;s quality based on its inborn features,
rather than user-generated features. Since users obtain first-hand experience
of an app by interacting with its views, we speculate that the inborn features
are largely related to the visual quality of individual views in an app and the
ways the views switch to one another. In this work, we propose AppQ, a novel
app quality grading and recommendation system that extracts inborn features of
apps based on app source code. In particular, AppQ works in parallel to perform
code analysis to extract app-level features as well as dynamic analysis to
capture view-level layout hierarchy and the switching among views. Each app is
then expressed as an attributed view graph, which is converted into a vector
and fed to classifiers for recognizing its quality classes. Our evaluation with
an app dataset from Google Play reports that AppQ achieves the best performance
with accuracy of 85.0\%. This shows a lot of promise to warm-start app grading
and recommendation systems with AppQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge Distillation (KD), which transfers the knowledge of a well-trained
large model (teacher) to a small model (student), has become an important area
of research for practical deployment of recommender systems. Recently, Relaxed
Ranking Distillation (RRD) has shown that distilling the ranking information in
the recommendation list significantly improves the performance. However, the
method still has limitations in that 1) it does not fully utilize the
prediction errors of the student model, which makes the training not fully
efficient, and 2) it only distills the user-side ranking information, which
provides an insufficient view under the sparse implicit feedback. This paper
presents Dual Correction strategy for Distillation (DCD), which transfers the
ranking information from the teacher model to the student model in a more
efficient manner. Most importantly, DCD uses the discrepancy between the
teacher model and the student model predictions to decide which knowledge to be
distilled. By doing so, DCD essentially provides the learning guidance tailored
to &quot;correcting&quot; what the student model has failed to accurately predict. This
process is applied for transferring the ranking information from the user-side
as well as the item-side to address sparse implicit user feedback. Our
experiments show that the proposed method outperforms the state-of-the-art
baselines, and ablation studies validate the effectiveness of each component.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scaling-up Disentanglement for Image Translation. (arXiv:2103.14017v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14017">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image translation methods typically aim to manipulate a set of labeled
attributes (given as supervision at training time e.g. domain label) while
leaving the unlabeled attributes intact. Current methods achieve either: (i)
disentanglement, which exhibits low visual fidelity and can only be satisfied
where the attributes are perfectly uncorrelated. (ii) visually-plausible
translations, which are clearly not disentangled. In this work, we propose
OverLORD, a single framework for disentangling labeled and unlabeled attributes
as well as synthesizing high-fidelity images, which is composed of two stages;
(i) Disentanglement: Learning disentangled representations with latent
optimization. Differently from previous approaches, we do not rely on
adversarial training or any architectural biases. (ii) Synthesis: Training
feed-forward encoders for inferring the learned attributes and tuning the
generator in an adversarial manner to increase the perceptual quality. When the
labeled and unlabeled attributes are correlated, we model an additional
representation that accounts for the correlated attributes and improves
disentanglement. We highlight that our flexible framework covers multiple
settings as disentangling labeled attributes, pose and appearance, localized
concepts, and shape and texture. We present significantly better
disentanglement with higher translation quality and greater output diversity
than state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02377">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A recent variation of Transformer, Performer, scales Transformer to longer
sequences with a linear attention mechanism. However, it is not compatible with
relative position encoding, which has advantages over absolute position
encoding. In this paper, we discuss possible ways to add relative position
encoding to Performer. Based on the analysis, we propose PermuteFormer, a
Performer-based model with relative position encoding that scales linearly on
long sequences. PermuteFormer applies position-dependent transformation on
queries and keys to encode positional information into the attention module.
This transformation is carefully crafted so that the final output of
self-attention is not affected by absolute positions of tokens. PermuteFormer
introduces negligible computational overhead by design that it runs as fast as
Performer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long
sequences, as well as WikiText-103, a language modeling dataset. The
experiments show that PermuteFormer uniformly improves the performance of
Performer with almost no computational overhead and outperforms vanilla
Transformer on most of the tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting Intraoperative Hypoxemia with Joint Sequence Autoencoder Networks. (arXiv:2104.14756v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14756">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present an end-to-end model using streaming physiological time series to
accurately predict near-term risk for hypoxemia, a rare, but life-threatening
condition known to cause serious patient harm during surgery. Our proposed
model makes inference on both hypoxemia outcomes and future input sequences,
enabled by a joint sequence autoencoder that simultaneously optimizes a
discriminative decoder for label prediction, and two auxiliary decoders trained
for data reconstruction and forecast, which seamlessly learns future-indicative
latent representation. All decoders share a memory-based encoder that helps
capture the global dynamics of patient data. In a large surgical cohort of
73,536 surgeries at a major academic medical center, our model outperforms all
baselines and gives a large performance gain over the state-of-the-art
hypoxemia prediction system. With a high sensitivity cutoff at 80%, it presents
99.36% precision in predicting hypoxemia and 86.81% precision in predicting the
much more severe and rare hypoxemic condition, persistent hypoxemia. With
exceptionally low rate of false alarms, our proposed model is promising in
improving clinical decision making and easing burden on the health system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Self-supervised learning of speech representations has been a very active
research area but most work is focused on a single domain such as read audio
books for which there exist large quantities of labeled and unlabeled data. In
this paper, we explore more general setups where the domain of the unlabeled
data for pre-training data differs from the domain of the labeled data for
fine-tuning, which in turn may differ from the test data domain. Our
experiments show that using target domain data during pre-training leads to
large performance improvements across a variety of setups. On a large-scale
competitive setup, we show that pre-training on unlabeled in-domain data
reduces the gap between models trained on in-domain and out-of-domain labeled
data by 66%-73%. This has obvious practical implications since it is much
easier to obtain unlabeled target domain data than labeled data. Moreover, we
find that pre-training on multiple domains improves generalization performance
on domains not seen during training. Code and models will be made available at
https://github.com/pytorch/fairseq.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03805">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Panoptic scene understanding and tracking of dynamic agents are essential for
robots and automated vehicles to navigate in urban environments. As LiDARs
provide accurate illumination-independent geometric depictions of the scene,
performing these tasks using LiDAR point clouds provides reliable predictions.
However, existing datasets lack diversity in the type of urban scenes and have
a limited number of dynamic object instances which hinders both learning of
these tasks as well as credible benchmarking of the developed methods. In this
paper, we introduce the large-scale Panoptic nuScenes benchmark dataset that
extends our popular nuScenes dataset with point-wise groundtruth annotations
for semantic segmentation, panoptic segmentation, and panoptic tracking tasks.
To facilitate comparison, we provide several strong baselines for each of these
tasks on our proposed dataset. Moreover, we analyze the drawbacks of the
existing metrics for the panoptic tracking problem and propose a novel
instance-centric metric that addresses the concerns. We present extensive
experiments that demonstrate the utility of Panoptic nuScenes compared to
existing datasets and make the online evaluation server available at
\url{nuScenes.org}. We believe that this extension will accelerate the research
of novel methods for scene understanding of dynamic urban environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AppQ: Warm-starting App Recommendation Based on View Graphs. (arXiv:2109.03798v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03798">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current app ranking and recommendation systems are mainly based on
user-generated information, e.g., number of downloads and ratings. However, new
apps often have few (or even no) user feedback, suffering from the classic
cold-start problem. How to quickly identify and then recommend new apps of high
quality is a challenging issue. Here, a fundamental requirement is the
capability to accurately measure an app&#x27;s quality based on its inborn features,
rather than user-generated features. Since users obtain first-hand experience
of an app by interacting with its views, we speculate that the inborn features
are largely related to the visual quality of individual views in an app and the
ways the views switch to one another. In this work, we propose AppQ, a novel
app quality grading and recommendation system that extracts inborn features of
apps based on app source code. In particular, AppQ works in parallel to perform
code analysis to extract app-level features as well as dynamic analysis to
capture view-level layout hierarchy and the switching among views. Each app is
then expressed as an attributed view graph, which is converted into a vector
and fed to classifiers for recognizing its quality classes. Our evaluation with
an app dataset from Google Play reports that AppQ achieves the best performance
with accuracy of 85.0\%. This shows a lot of promise to warm-start app grading
and recommendation systems with AppQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Discrete-Valued Latent Preference Matrix Estimation with Graph Side Information. (arXiv:2003.07040v2 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.07040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Incorporating graph side information into recommender systems has been widely
used to better predict ratings, but relatively few works have focused on
theoretical guarantees. Ahn et al. (2018) firstly characterized the optimal
sample complexity in the presence of graph side information, but the results
are limited due to strict, unrealistic assumptions made on the unknown latent
preference matrix and the structure of user clusters. In this work, we propose
a new model in which 1) the unknown latent preference matrix can have any
discrete values, and 2) users can be clustered into multiple clusters, thereby
relaxing the assumptions made in prior work. Under this new model, we fully
characterize the optimal sample complexity and develop a
computationally-efficient algorithm that matches the optimal sample complexity.
Our algorithm is robust to model errors and outperforms the existing algorithms
in terms of prediction performance on both synthetic and real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Interpretable and Efficient Heterogeneous Graph Convolutional Network. (arXiv:2005.13183v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.13183">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph Convolutional Network (GCN) has achieved extraordinary success in
learning effective task-specific representations of nodes in graphs. However,
regarding Heterogeneous Information Network (HIN), existing HIN-oriented GCN
methods still suffer from two deficiencies: (1) they cannot flexibly explore
all possible meta-paths and extract the most useful ones for a target object,
which hinders both effectiveness and interpretability; (2) they often need to
generate intermediate meta-path based dense graphs, which leads to high
computational complexity. To address the above issues, we propose an
interpretable and efficient Heterogeneous Graph Convolutional Network (ie-HGCN)
to learn the representations of objects in HINs. It is designed as a
hierarchical aggregation architecture, i.e., object-level aggregation first,
followed by type-level aggregation. The novel architecture can automatically
extract useful meta-paths for each object from all possible meta-paths (within
a length limit), which brings good model interpretability. It can also reduce
the computational cost by avoiding intermediate HIN transformation and
neighborhood attention. We provide theoretical analysis about the proposed
ie-HGCN in terms of evaluating the usefulness of all possible meta-paths, its
connection to the spectral graph convolution on HINs, and its quasi-linear time
complexity. Extensive experiments on three real network datasets demonstrate
the superiority of ie-HGCN over the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quasi-Dense Similarity Learning for Multiple Object Tracking. (arXiv:2006.06664v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06664">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Similarity learning has been recognized as a crucial step for object
tracking. However, existing multiple object tracking methods only use sparse
ground truth matching as the training objective, while ignoring the majority of
the informative regions on the images. In this paper, we present Quasi-Dense
Similarity Learning, which densely samples hundreds of region proposals on a
pair of images for contrastive learning. We can directly combine this
similarity learning with existing detection methods to build Quasi-Dense
Tracking (QDTrack) without turning to displacement regression or motion priors.
We also find that the resulting distinctive feature space admits a simple
nearest neighbor search at the inference time. Despite its simplicity, QDTrack
outperforms all existing methods on MOT, BDD100K, Waymo, and TAO tracking
benchmarks. It achieves 68.7 MOTA at 20.3 FPS on MOT17 without using external
training data. Compared to methods with similar detectors, it boosts almost 10
points of MOTA and significantly decreases the number of ID switches on BDD100K
and Waymo datasets. Our code and trained models are available at
this http URL</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Priming PCA with EigenGame. (arXiv:2109.03709v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03709">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce primed-PCA (pPCA), an extension of the recently proposed
EigenGame algorithm for computing principal components in a large-scale setup.
Our algorithm first runs EigenGame to get an approximation of the principal
components, and then applies an exact PCA in the subspace they span. Since this
subspace is of small dimension in any practical use of EigenGame, this second
step is extremely cheap computationally. Nonetheless, it improves accuracy
significantly for a given computational budget across datasets. In this setup,
the purpose of EigenGame is to narrow down the search space, and prepare the
data for the second step, an exact calculation.

We show formally that pPCA improves upon EigenGame under very mild
conditions, and we provide experimental validation on both synthetic and real
large-scale datasets showing that it systematically translates to improved
performance. In our experiments we achieve improvements in convergence speed by
factors of 5-25 on the datasets of the original EigenGame paper.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New One-Point Residual-Feedback Oracle For Black-Box Learning and Control. (arXiv:2006.10820v2 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Zeroth-order optimization (ZO) algorithms have been recently used to solve
black-box or simulation-based learning and control problems, where the gradient
of the objective function cannot be easily computed but can be approximated
using the objective function values. Many existing ZO algorithms adopt
two-point feedback schemes due to their fast convergence rate compared to
one-point feedback schemes. However, two-point schemes require two evaluations
of the objective function at each iteration, which can be impractical in
applications where the data are not all available a priori, e.g., in online
optimization. In this paper, we propose a novel one-point feedback scheme that
queries the function value once at each iteration and estimates the gradient
using the residual between two consecutive points. When optimizing a
deterministic Lipschitz function, we show that the query complexity of ZO with
the proposed one-point residual feedback matches that of ZO with the existing
two-point schemes. Moreover, the query complexity of the proposed algorithm can
be improved when the objective function has Lipschitz gradient. Then, for
stochastic bandit optimization problems where only noisy objective function
values are given, we show that ZO with one-point residual feedback achieves the
same convergence rate as that of two-point scheme with uncontrollable data
samples. We demonstrate the effectiveness of the proposed one-point residual
feedback via extensive numerical experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Maximum and Leaky Maximum Propagation. (arXiv:2105.10277v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10277">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this work, we present an alternative to conventional residual connections,
which is inspired by maxout nets. This means that instead of the addition in
residual connections, our approach only propagates the maximum value or, in the
leaky formulation, propagates a percentage of both. In our evaluation, we show
on different public data sets that the presented approaches are comparable to
the residual connections and have other interesting properties, such as better
generalization with a constant batch normalization, faster learning, and also
the possibility to generalize without additional activation functions. In
addition, the proposed approaches work very well if ensembles together with
residual networks are formed.

https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p&#x3D;%2FMaximumPropagation&amp;mode&#x3D;list</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multiscale Laplacian Learning. (arXiv:2109.03718v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03718">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine learning methods have greatly changed science, engineering, finance,
business, and other fields. Despite the tremendous accomplishments of machine
learning and deep learning methods, many challenges still remain. In
particular, the performance of machine learning methods is often severely
affected in case of diverse data, usually associated with smaller data sets or
data related to areas of study where the size of the data sets is constrained
by the complexity and/or high cost of experiments. Moreover, data with limited
labeled samples is a challenge to most learning approaches. In this paper, the
aforementioned challenges are addressed by integrating graph-based frameworks,
multiscale structure, modified and adapted optimization procedures and
semi-supervised techniques. This results in two innovative multiscale Laplacian
learning (MLL) approaches for machine learning tasks, such as data
classification, and for tackling diverse data, data with limited samples and
smaller data sets. The first approach, called multikernel manifold learning
(MML), integrates manifold learning with multikernel information and solves a
regularization problem consisting of a loss function and a warped kernel
regularizer using multiscale graph Laplacians. The second approach, called the
multiscale MBO (MMBO) method, introduces multiscale Laplacians to a
modification of the famous classical Merriman-Bence-Osher (MBO) scheme, and
makes use of fast solvers for finding the approximations to the extremal
eigenvectors of the graph Laplacian. We demonstrate the performance of our
methods experimentally on a variety of data sets, such as biological, text and
image data, and compare them favorably to existing approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Word embeddings are a powerful natural lan-guage processing technique, but
they are ex-tremely difficult to interpret. To enable inter-pretable NLP
models, we create vectors whereeach dimension isinherently interpretable.
Byinherently interpretable, we mean a systemwhere each dimension is associated
with somehuman-understandablehintthat can describethe meaning of that
dimension. In order tocreate more interpretable word embeddings,we transform
pretrained dense word embed-dings into sparse embeddings. These new em-beddings
are inherently interpretable: each oftheir dimensions is created from and
repre-sents a natural language word or specific gram-matical concept. We
construct these embed-dings through sparse coding, where each vec-tor in the
basis set is itself a word embedding.Therefore, each dimension of our sparse
vec-tors corresponds to a natural language word.We also show that models
trained using thesesparse embeddings can achieve good perfor-mance and are more
interpretable in practice,including through human evaluations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03764">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Common acquisition functions for active learning use either uncertainty or
diversity sampling, aiming to select difficult and diverse data points from the
pool of unlabeled data, respectively. In this work, leveraging the best of both
worlds, we propose an acquisition function that opts for selecting
\textit{contrastive examples}, i.e. data points that are similar in the model
feature space and yet the model outputs maximally different predictive
likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a
diverse set of acquisition functions in four natural language understanding
tasks and seven datasets. Our experiments show that CAL performs consistently
better or equal than the best performing baseline across all tasks, on both
in-domain and out-of-domain data. We also conduct an extensive ablation study
of our method and we further analyze all actively acquired datasets showing
that CAL achieves a better trade-off between uncertainty and diversity compared
to other strategies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Concurrent Alternating Least Squares for multiple simultaneous Canonical Polyadic Decompositions. (arXiv:2010.04678v2 [cs.MS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.04678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Tensor decompositions, such as CANDECOMP/PARAFAC (CP), are widely used in a
variety of applications, such as chemometrics, signal processing, and machine
learning. A broadly used method for computing such decompositions relies on the
Alternating Least Squares (ALS) algorithm. When the number of components is
small, regardless of its implementation, ALS exhibits low arithmetic intensity,
which severely hinders its performance and makes GPU offloading ineffective. We
observe that, in practice, experts often have to compute multiple
decompositions of the same tensor, each with a small number of components
(typically fewer than 20), to ultimately find the best ones to use for the
application at hand. In this paper, we illustrate how multiple decompositions
of the same tensor can be fused together at the algorithmic level to increase
the arithmetic intensity. Therefore, it becomes possible to make efficient use
of GPUs for further speedups; at the same time the technique is compatible with
many enhancements typically used in ALS, such as line search, extrapolation,
and non-negativity constraints. We introduce the Concurrent ALS algorithm and
library, which offers an interface to Matlab, and a mechanism to effectively
deal with the issue that decompositions complete at different times.
Experimental results on artificial and real datasets demonstrate a shorter time
to completion due to increased arithmetic intensity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedGraphNN: A Federated Learning System and Benchmark for Graph Neural Networks. (arXiv:2104.07145v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07145">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Graph Neural Network (GNN) research is rapidly growing thanks to the capacity
of GNNs in learning distributed representations from graph-structured data.
However, centralizing a massive amount of real-world graph data for GNN
training is prohibitive due to privacy concerns, regulation restrictions, and
commercial competitions. Federated learning (FL), a trending distributed
learning paradigm, provides possibilities to solve this challenge while
preserving data privacy. Despite recent advances in vision and language
domains, there is no suitable platform for the FL of GNNs. To this end, we
introduce FedGraphNN, an open FL benchmark system that can facilitate research
on federated GNNs. FedGraphNN is built on a unified formulation of graph FL and
contains a wide range of datasets from different domains, popular GNN models,
and FL algorithms, with secure and efficient system support. Particularly for
the datasets, we collect, preprocess, and partition 36 datasets from 7 domains,
including both publicly available ones and specifically obtained ones such as
hERG and Tencent. Our empirical analysis showcases the utility of our benchmark
system, while exposing significant challenges in graph FL: federated GNNs
perform worse in most datasets with a non-IID split than centralized GNNs; the
GNN model that attains the best result in the centralized setting may not
maintain its advantage in the FL setting. These results imply that more
research efforts are needed to unravel the mystery behind federated GNNs.
Moreover, our system performance analysis demonstrates that the FedGraphNN
system is computationally efficient and secure to large-scale graphs datasets.
We maintain the source code at https://github.com/FedML-AI/FedGraphNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The use of Generative Adversarial Networks to characterise new physics in multi-lepton final states at the LHC. (arXiv:2105.14933v2 [hep-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14933">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Semi-supervision in Machine Learning can be used in searches for new physics
where the signal plus background regions are not labelled. This strongly
reduces model dependency in the search for signals Beyond the Standard Model.
This approach displays the drawback in that over-fitting can give rise to fake
signals. Tossing toy Monte Carlo (MC) events can be used to estimate the
corresponding trials factor through a frequentist inference. However, MC events
that are based on full detector simulations are resource intensive. Generative
Adversarial Networks (GANs) can be used to mimic MC generators. GANs are
powerful generative models, but often suffer from training instability. We
henceforth show a review of GANs. We advocate the use of Wasserstein GAN (WGAN)
with weight clipping and WGAN with gradient penalty (WGAN-GP) where the norm of
gradient of the critic is penalized with respect to its input. Following the
emergence of multi-lepton anomalies at the LHC, we apply GANs for the
generation of di-leptons final states in association with b-quarks at the LHC.
A good agreement between the MC events and the WGAN-GP events is found for the
observables selected in the study.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformers in Vision: A Survey. (arXiv:2101.01169v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.01169">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Astounding results from Transformer models on natural language tasks have
intrigued the vision community to study their application to computer vision
problems. Among their salient benefits, Transformers enable modeling long
dependencies between input sequence elements and support parallel processing of
sequence as compared to recurrent networks e.g., Long short-term memory (LSTM).
Different from convolutional networks, Transformers require minimal inductive
biases for their design and are naturally suited as set-functions. Furthermore,
the straightforward design of Transformers allows processing multiple
modalities (e.g., images, videos, text and speech) using similar processing
blocks and demonstrates excellent scalability to very large capacity networks
and huge datasets. These strengths have led to exciting progress on a number of
vision tasks using Transformer networks. This survey aims to provide a
comprehensive overview of the Transformer models in the computer vision
discipline. We start with an introduction to fundamental concepts behind the
success of Transformers i.e., self-attention, large-scale pre-training, and
bidirectional encoding. We then cover extensive applications of transformers in
vision including popular recognition tasks (e.g., image classification, object
detection, action recognition, and segmentation), generative modeling,
multi-modal tasks (e.g., visual-question answering, visual reasoning, and
visual grounding), video processing (e.g., activity recognition, video
forecasting), low-level vision (e.g., image super-resolution, image
enhancement, and colorization) and 3D analysis (e.g., point cloud
classification and segmentation). We compare the respective advantages and
limitations of popular techniques both in terms of architectural design and
their experimental value. Finally, we provide an analysis on open research
directions and possible future works.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Highly Scalable and Provably Accurate Classification in Poincare Balls. (arXiv:2109.03781v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03781">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many high-dimensional and large-volume data sets of practical relevance have
hierarchical structures induced by trees, graphs or time series. Such data sets
are hard to process in Euclidean spaces and one often seeks low-dimensional
embeddings in other space forms to perform required learning tasks. For
hierarchical data, the space of choice is a hyperbolic space since it
guarantees low-distortion embeddings for tree-like structures. Unfortunately,
the geometry of hyperbolic spaces has properties not encountered in Euclidean
spaces that pose challenges when trying to rigorously analyze algorithmic
solutions. Here, for the first time, we establish a unified framework for
learning scalable and simple hyperbolic linear classifiers with provable
performance guarantees. The gist of our approach is to focus on Poincar\&#x27;e ball
models and formulate the classification problems using tangent space
formalisms. Our results include a new hyperbolic and second-order perceptron
algorithm as well as an efficient and highly accurate convex optimization setup
for hyperbolic support vector machine classifiers. All algorithms provably
converge and are highly scalable as they have complexities comparable to those
of their Euclidean counterparts. Their performance accuracies on synthetic data
sets comprising millions of points, as well as on complex real-world data sets
such as single-cell RNA-seq expression measurements, CIFAR10, Fashion-MNIST and
mini-ImageNet.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Privacy Enhancing Machine Learning via Removal of Unwanted Dependencies. (arXiv:2007.15710v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.15710">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid rise of IoT and Big Data has facilitated copious data driven
applications to enhance our quality of life. However, the omnipresent and
all-encompassing nature of the data collection can generate privacy concerns.
Hence, there is a strong need to develop techniques that ensure the data serve
only the intended purposes, giving users control over the information they
share. To this end, this paper studies new variants of supervised and
adversarial learning methods, which remove the sensitive information in the
data before they are sent out for a particular application. The explored
methods optimize privacy preserving feature mappings and predictive models
simultaneously in an end-to-end fashion. Additionally, the models are built
with an emphasis on placing little computational burden on the user side so
that the data can be desensitized on device in a cheap manner. Experimental
results on mobile sensing and face datasets demonstrate that our models can
successfully maintain the utility performances of predictive models while
causing sensitive predictions to perform poorly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedZKT: Zero-Shot Knowledge Transfer towards Heterogeneous On-Device Models in Federated Learning. (arXiv:2109.03775v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03775">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning enables distributed devices to collaboratively learn a
shared prediction model without centralizing on-device training data. Most of
the current algorithms require comparable individual efforts to train on-device
models with the same structure and size, impeding participation from
resource-constrained devices. Given the widespread yet heterogeneous devices
nowadays, this paper proposes a new framework supporting federated learning
across heterogeneous on-device models via Zero-shot Knowledge Transfer, named
by FedZKT. Specifically, FedZKT allows participating devices to independently
determine their on-device models. To transfer knowledge across on-device
models, FedZKT develops a zero-shot distillation approach contrary to certain
prior research based on a public dataset or a pre-trained data generator. To
utmostly reduce on-device workload, the resource-intensive distillation task is
assigned to the server, which constructs a generator to adversarially train
with the ensemble of the received heterogeneous on-device models. The distilled
central knowledge will then be sent back in the form of the corresponding
on-device model parameters, which can be easily absorbed at the device side.
Experimental studies demonstrate the effectiveness and the robustness of FedZKT
towards heterogeneous on-device models and challenging federated learning
scenarios, such as non-iid data distribution and straggler effects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Continuous Safety Verification of Neural Networks. (arXiv:2010.05689v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05689">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deploying deep neural networks (DNNs) as core functions in autonomous driving
creates unique verification and validation challenges. In particular, the
continuous engineering paradigm of gradually perfecting a DNN-based perception
can make the previously established result of safety verification no longer
valid. This can occur either due to the newly encountered examples (i.e., input
domain enlargement) inside the Operational Design Domain or due to the
subsequent parameter fine-tuning activities of a DNN. This paper considers
approaches to transfer results established in the previous DNN safety
verification problem to the modified problem setting. By considering the reuse
of state abstractions, network abstractions, and Lipschitz constants, we
develop several sufficient conditions that only require formally analyzing a
small part of the DNN in the new problem. The overall concept is evaluated in a
$1/10$-scaled vehicle that equips a DNN controller to determine the visual
waypoint from the perceived image.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Adversarial Robustness via Attention and Adversarial Logit Pairing. (arXiv:1908.11435v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1908.11435">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Though deep neural networks have achieved the state of the art performance in
visual classification, recent studies have shown that they are all vulnerable
to the attack of adversarial examples. In this paper, we develop improved
techniques for defending against adversarial examples. First, we propose an
enhanced defense technique denoted Attention and Adversarial Logit
Pairing(AT+ALP), which encourages both attention map and logit for the pairs of
examples to be similar. When being applied to clean examples and their
adversarial counterparts, AT+ALP improves accuracy on adversarial examples over
adversarial training. We show that AT+ALP can effectively increase the average
activations of adversarial examples in the key area and demonstrate that it
focuses on discriminate features to improve the robustness of the model.
Finally, we conduct extensive experiments using a wide range of datasets and
the experiment results show that our AT+ALP achieves the state of the art
defense performance. For example, on 17 Flower Category Database, under strong
200-iteration PGD gray-box and black-box attacks where prior art has 34% and
39% accuracy, our method achieves 50% and 51%. Compared with previous work, our
work is evaluated under highly challenging PGD attack: the maximum perturbation
$\epsilon \in \{0.25,0.5\}$ i.e. $L_\infty \in \{0.25,0.5\}$ with 10 to 200
attack iterations. To the best of our knowledge, such a strong attack has not
been previously explored on a wide range of datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Communicate Using Counterfactual Reasoning. (arXiv:2006.07200v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning to communicate in order to share state information is an active
problem in the area of multi-agent reinforcement learning (MARL). The credit
assignment problem, the non-stationarity of the communication environment and
the creation of influenceable agents are major challenges within this research
field which need to be overcome in order to learn a valid communication
protocol. This paper introduces the novel multi-agent counterfactual
communication learning (MACC) method which adapts counterfactual reasoning in
order to overcome the credit assignment problem for communicating agents.
Secondly, the non-stationarity of the communication environment while learning
the communication Q-function is overcome by creating the communication
Q-function using the action policy of the other agents and the Q-function of
the action environment. Additionally, a social loss function is introduced in
order to create influenceable agents which is required to learn a valid
communication protocol. Our experiments show that MACC is able to outperform
the state-of-the-art baselines in four different scenarios in the Particle
environment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning. (arXiv:2001.09684v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.09684">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep Reinforcement Learning (DRL) has numerous applications in the real world
thanks to its outstanding ability in quickly adapting to the surrounding
environments. Despite its great advantages, DRL is susceptible to adversarial
attacks, which precludes its use in real-life critical systems and applications
(e.g., smart grids, traffic controls, and autonomous vehicles) unless its
vulnerabilities are addressed and mitigated. Thus, this paper provides a
comprehensive survey that discusses emerging attacks in DRL-based systems and
the potential countermeasures to defend against these attacks. We first cover
some fundamental backgrounds about DRL and present emerging adversarial attacks
on machine learning techniques. We then investigate more details of the
vulnerabilities that the adversary can exploit to attack DRL along with the
state-of-the-art countermeasures to prevent such attacks. Finally, we highlight
open issues and research challenges for developing solutions to deal with
attacks for DRL-based intelligent systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A robust approach for deep neural networks in presence of label noise: relabelling and filtering instances during training. (arXiv:2109.03748v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03748">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep learning has outperformed other machine learning algorithms in a variety
of tasks, and as a result, it has become more and more popular and used.
However, as other machine learning algorithms, deep learning, and convolutional
neural networks (CNNs) in particular, perform worse when the data sets present
label noise. Therefore, it is important to develop algorithms that help the
training of deep networks and their generalization to noise-free test sets. In
this paper, we propose a robust training strategy against label noise, called
RAFNI, that can be used with any CNN. This algorithm filters and relabels
instances of the training set based on the predictions and their probabilities
made by the backbone neural network during the training process. That way, this
algorithm improves the generalization ability of the CNN on its own. RAFNI
consists of three mechanisms: two mechanisms that filter instances and one
mechanism that relabels instances. In addition, it does not suppose that the
noise rate is known nor does it need to be estimated. We evaluated our
algorithm using different data sets of several sizes and characteristics. We
also compared it with state-of-the-art models using the CIFAR10 and CIFAR100
benchmarks under different types and rates of label noise and found that RAFNI
achieves better results in most cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13805">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As it has been unveiled that pre-trained language models (PLMs) are to some
extent capable of recognizing syntactic concepts in natural language, much
effort has been made to develop a method for extracting complete (binary)
parses from PLMs without training separate parsers. We improve upon this
paradigm by proposing a novel chart-based method and an effective top-K
ensemble technique. Moreover, we demonstrate that we can broaden the scope of
application of the approach into multilingual settings. Specifically, we show
that by applying our method on multilingual PLMs, it becomes possible to induce
non-trivial parses for sentences from nine languages in an integrated and
language-agnostic manner, attaining performance superior or comparable to that
of unsupervised PCFGs. We also verify that our approach is robust to
cross-lingual transfer. Finally, we provide analyses on the inner workings of
our method. For instance, we discover universal attention heads which are
consistently sensitive to syntactic information irrespective of the input
language.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parkinson&#x27;s Disease Diagnosis based on Gait Cycle Analysis Through an Interpretable Interval Type-2 Neuro-Fuzzy System. (arXiv:2109.02442v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02442">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, an interpretable classifier using an interval type-2 fuzzy
neural network for detecting patients suffering from Parkinson&#x27;s Disease (PD)
based on analyzing the gait cycle is presented. The proposed method utilizes
clinical features extracted from the vertical Ground Reaction Force (vGRF),
measured by 16 wearable sensors placed in the soles of subjects&#x27; shoes and
learns interpretable fuzzy rules. Therefore, experts can verify the decision
made by the proposed method based on investigating the firing strength of
interpretable fuzzy rules. Moreover, experts can utilize the extracted fuzzy
rules for patient diagnosing or adjust them based on their knowledge. To
improve the robustness of the proposed method against uncertainty and noisy
sensor measurements, Interval Type-2 Fuzzy Logic is applied. To learn fuzzy
rules, two paradigms are proposed: 1- A batch learning approach based on
clustering available samples is applied to extract initial fuzzy rules, 2- A
complementary online learning is proposed to improve the rule base encountering
new labeled samples. The performance of the method is evaluated for classifying
patients and healthy subjects in different conditions including the presence of
noise or observing new instances. Moreover, the performance of the model is
compared to some previous supervised and unsupervised machine learning
approaches. The final Accuracy, Precision, Recall, and F1 Score of the proposed
method are 88.74%, 89.41%, 95.10%, and 92.16%. Finally, the extracted fuzzy
sets for each feature are reported.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification. (arXiv:2012.03173v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Deep AUC Maximization (DAM) is a new paradigm for learning a deep neural
network by maximizing the AUC score of the model on a dataset. Most previous
works of AUC maximization focus on the perspective of optimization by designing
efficient stochastic algorithms, and studies on generalization performance of
large-scale DAM on difficult tasks are missing. In this work, we aim to make
DAM more practical for interesting real-world applications (e.g., medical image
classification). First, we propose a new margin-based min-max surrogate loss
function for the AUC score (named as AUC min-max-margin loss or simply AUC
margin loss for short). It is more robust than the commonly used AUC square
loss, while enjoying the same advantage in terms of large-scale stochastic
optimization. Second, we conduct extensive empirical studies of our DAM method
on four difficult medical image classification tasks, namely (i) classification
of chest x-ray images for identifying many threatening diseases, (ii)
classification of images of skin lesions for identifying melanoma, (iii)
classification of mammogram for breast cancer screening, and (iv)
classification of microscopic images for identifying tumor tissue. Our studies
demonstrate that the proposed DAM method improves the performance of optimizing
cross-entropy loss by a large margin, and also achieves better performance than
optimizing the existing AUC square loss on these medical image classification
tasks. Specifically, our DAM method has achieved the 1st place on Stanford
CheXpert competition on Aug. 31, 2020. To the best of our knowledge, this is
the first work that makes DAM succeed on large-scale medical image datasets. We
also conduct extensive ablation studies to demonstrate the advantages of the
new AUC margin loss over the AUC square loss on benchmark datasets. The
proposed method is implemented in our open-sourced library LibAUC
(www.libauc.org).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting False Data Injection Attacks in Smart Grids with Modeling Errors: A Deep Transfer Learning Based Approach. (arXiv:2104.06307v3 [eess.SP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06307">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most traditional false data injection attack (FDIA) detection approaches rely
on a key assumption, i.e., the power system can be accurately modeled. However,
the transmission line parameters are dynamic and cannot be accurately known
during operation and thus the involved modeling errors should not be neglected.
In this paper, an illustrative case has revealed that modeling errors in
transmission lines significantly weaken the detection effectiveness of
conventional FDIA approaches. To tackle this issue, we propose an FDIA
detection mechanism from the perspective of transfer learning. Specifically,
the simulated power system is treated as a source domain, which provides
abundant simulated normal and attack data. The real world&#x27;s running system
whose transmission line parameters are unknown is taken as a target domain
where sufficient real normal data are collected for tracking the latest system
states online. The designed transfer strategy that aims at making full use of
data in hand is divided into two optimization stages. In the first stage, a
deep neural network (DNN) is built by simultaneously optimizing several
well-designed objective terms with both simulated data and real data, and then
it is fine-tuned via real data in the second stage. Several case studies on the
IEEE 14-bus and 118-bus systems verify the effectiveness of the proposed
mechanism.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GitTables: A Large-Scale Corpus of Relational Tables. (arXiv:2106.07258v2 [cs.DB] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07258">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The practical success of deep learning has sparked interest in improving
relational table tasks, like data search, with models trained on large table
corpora. Existing corpora primarily contain tables extracted from HTML pages,
limiting the capability to represent offline database tables. To train and
evaluate high-capacity models for applications beyond the Web, we need
additional resources with tables that resemble relational database tables.

Here we introduce GitTables, a corpus of currently 1.7M relational tables
extracted from GitHub. Our continuing curation aims at growing the corpus to at
least 20M tables. We annotate table columns in GitTables with more than 2K
different semantic types from Schema.org and DBpedia. Our column annotations
consist of semantic types, hierarchical relations, range types and
descriptions.

The corpus is available at https://gittables.github.io. Our analysis of
GitTables shows that its structure, content, and topical coverage differ
significantly from existing table corpora. We evaluate our annotation pipeline
on hand-labeled tables from the T2Dv2 benchmark and find that our approach
provides results on par with human annotations. We demonstrate a use case of
GitTables by training a semantic type detection model on it and obtain high
prediction accuracy. We also show that the same model trained on tables from
theWeb generalizes poorly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image-based Plant Disease Diagnosis with Unsupervised Anomaly Detection Based on Reconstructability of Colors. (arXiv:2011.14306v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14306">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes an unsupervised anomaly detection technique for
image-based plant disease diagnosis. The construction of large and publicly
available datasets containing labeled images of healthy and diseased crop
plants led to growing interest in computer vision techniques for automatic
plant disease diagnosis. Although supervised image classifiers based on deep
learning can be a powerful tool for plant disease diagnosis, they require a
huge amount of labeled data. The data mining technique of anomaly detection
includes unsupervised approaches that do not require rare samples for training
classifiers. We propose an unsupervised anomaly detection technique for
image-based plant disease diagnosis that is based on the reconstructability of
colors; a deep encoder-decoder network trained to reconstruct the colors of
\textit{healthy} plant images should fail to reconstruct colors of symptomatic
regions. Our proposed method includes a new image-based framework for plant
disease detection that utilizes a conditional adversarial network called
pix2pix and a new anomaly score based on CIEDE2000 color difference.
Experiments with PlantVillage dataset demonstrated the superiority of our
proposed method compared to an existing anomaly detector at identifying
diseased crop images in terms of accuracy, interpretability and computational
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Emergent Unfairness in Algorithmic Fairness-Accuracy Trade-Off Research. (arXiv:2102.01203v3 [cs.CY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.01203">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Across machine learning (ML) sub-disciplines, researchers make explicit
mathematical assumptions in order to facilitate proof-writing. We note that,
specifically in the area of fairness-accuracy trade-off optimization
scholarship, similar attention is not paid to the normative assumptions that
ground this approach. Such assumptions presume that 1) accuracy and fairness
are in inherent opposition to one another, 2) strict notions of mathematical
equality can adequately model fairness, 3) it is possible to measure the
accuracy and fairness of decisions independent from historical context, and 4)
collecting more data on marginalized individuals is a reasonable solution to
mitigate the effects of the trade-off. We argue that such assumptions, which
are often left implicit and unexamined, lead to inconsistent conclusions: While
the intended goal of this work may be to improve the fairness of machine
learning models, these unexamined, implicit assumptions can in fact result in
emergent unfairness. We conclude by suggesting a concrete path forward toward a
potential resolution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">tvopt: A Python Framework for Time-Varying Optimization. (arXiv:2011.07119v2 [cs.MS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.07119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper introduces tvopt, a Python framework for prototyping and
benchmarking time-varying (or online) optimization algorithms. The paper first
describes the theoretical approach that informed the development of tvopt. Then
it discusses the different components of the framework and their use for
modeling and solving time-varying optimization problems. In particular, tvopt
provides functionalities for defining both centralized and distributed online
problems, and a collection of built-in algorithms to solve them, for example
gradient-based methods, ADMM and other splitting methods. Moreover, the
framework implements prediction strategies to improve the accuracy of the
online solvers. The paper then proposes some numerical results on a benchmark
problem and discusses their implementation using tvopt. The code for tvopt is
available at https://github.com/nicola-bastianello/tvopt.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Can Increased Randomness in Stochastic Gradient Descent Improve Generalization?. (arXiv:2108.09507v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09507">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Recent works report that increasing the learning rate or decreasing the
minibatch size in stochastic gradient descent (SGD) can improve test set
performance. We argue this is expected under some conditions in models with a
loss function with multiple local minima. Our main contribution is an
approximate but analytical approach inspired by methods in Physics to study the
role of the SGD learning rate and batch size in generalization. We characterize
test set performance under a shift between the training and test data
distributions for loss functions with multiple minima. The shift can simply be
due to sampling, and is therefore typically present in practical applications.
We show that the resulting shift in local minima worsens test performance by
picking up curvature, implying that generalization improves by selecting wide
and/or little-shifted local minima. We then specialize to SGD, and study its
test performance under stationarity. Because obtaining the exact stationary
distribution of SGD is intractable, we derive a Fokker-Planck approximation of
SGD and obtain its stationary distribution instead. This process shows that the
learning rate divided by the minibatch size plays a role analogous to
temperature in statistical mechanics, and implies that SGD, including its
stationary distribution, is largely invariant to changes in learning rate or
batch size that leave its temperature constant. We show that increasing SGD
temperature encourages the selection of local minima with lower curvature, and
can enable better generalization. We provide experiments on CIFAR10
demonstrating the temperature invariance of SGD, improvement of the test loss
as SGD temperature increases, and quantifying the impact of sampling versus
domain shift in driving this effect. Finally, we present synthetic experiments
showing how our theory applies in a simplified loss with two local minima.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Desiderata for Representation Learning: A Causal Perspective. (arXiv:2109.03795v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03795">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Representation learning constructs low-dimensional representations to
summarize essential features of high-dimensional data. This learning problem is
often approached by describing various desiderata associated with learned
representations; e.g., that they be non-spurious, efficient, or disentangled.
It can be challenging, however, to turn these intuitive desiderata into formal
criteria that can be measured and enhanced based on observed data. In this
paper, we take a causal perspective on representation learning, formalizing
non-spuriousness and efficiency (in supervised representation learning) and
disentanglement (in unsupervised representation learning) using counterfactual
quantities and observable consequences of causal assertions. This yields
computable metrics that can be used to assess the degree to which
representations satisfy the desiderata of interest and learn non-spurious and
disentangled representations from single observational datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Algorithm Matters for the Performance of Neural Network Potential. (arXiv:2109.03769v1 [physics.chem-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03769">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>One hidden yet important issue for developing neural network potentials
(NNPs) is the choice of training algorithm. Here we compare the performance of
two popular training algorithms, the adaptive moment estimation algorithm
(Adam) and the extended Kalman filter algorithm (EKF), using the
Behler-Parrinello neural network (BPNN) and two publicly accessible datasets of
liquid water. It is found that NNPs trained with EKF are more transferable and
less sensitive to the value of the learning rate, as compared to Adam. In both
cases, error metrics of the test set do not always serve as a good indicator
for the actual performance of NNPs. Instead, we show that their performance
correlates well with a Fisher information based similarity measure.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Wanderlust: Online Continual Object Detection in the Real World. (arXiv:2108.11005v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11005">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Online continual learning from data streams in dynamic environments is a
critical direction in the computer vision field. However, realistic benchmarks
and fundamental studies in this line are still missing. To bridge the gap, we
present a new online continual object detection benchmark with an egocentric
video dataset, Objects Around Krishna (OAK). OAK adopts the KrishnaCAM videos,
an ego-centric video stream collected over nine months by a graduate student.
OAK provides exhaustive bounding box annotations of 80 video snippets (~17.5
hours) for 105 object categories in outdoor scenes. The emergence of new object
categories in our benchmark follows a pattern similar to what a single person
might see in their day-to-day life. The dataset also captures the natural
distribution shifts as the person travels to different places. These egocentric
long-running videos provide a realistic playground for continual learning
algorithms, especially in online embodied settings. We also introduce new
evaluation metrics to evaluate the model performance and catastrophic
forgetting and provide baseline studies for online continual object detection.
We believe this benchmark will pose new exciting challenges for learning from
non-stationary data in continual learning. The OAK dataset and the associated
benchmark are released at https://oakdata.github.io/.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anatomical-Guided Attention Enhances Unsupervised PET Image Denoising Performance. (arXiv:2109.00802v2 [physics.med-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00802">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Although supervised convolutional neural networks (CNNs) often outperform
conventional alternatives for denoising positron emission tomography (PET)
images, they require many low- and high-quality reference PET image pairs.
Herein, we propose an unsupervised 3D PET image denoising method based on an
anatomical information-guided attention mechanism. The proposed magnetic
resonance-guided deep decoder (MR-GDD) utilizes the spatial details and
semantic features of MR-guidance image more effectively by introducing
encoder-decoder and deep decoder subnetworks. Moreover, the specific shapes and
patterns of the guidance image do not affect the denoised PET image, because
the guidance image is input to the network through an attention gate. In a
Monte Carlo simulation of [$^{18}$F]fluoro-2-deoxy-D-glucose (FDG), the
proposed method achieved the highest peak signal-to-noise ratio and structural
similarity (27.92 $\pm$ 0.44 dB/0.886 $\pm$ 0.007), as compared with Gaussian
filtering (26.68 $\pm$ 0.10 dB/0.807 $\pm$ 0.004), image guided filtering
(27.40 $\pm$ 0.11 dB/0.849 $\pm$ 0.003), deep image prior (DIP) (24.22 $\pm$
0.43 dB/0.737 $\pm$ 0.017), and MR-DIP (27.65 $\pm$ 0.42 dB/0.879 $\pm$ 0.007).
Furthermore, we experimentally visualized the behavior of the optimization
process, which is often unknown in unsupervised CNN-based restoration problems.
For preclinical (using [$^{18}$F]FDG and [$^{11}$C]raclopride) and clinical
(using [$^{18}$F]florbetapir) studies, the proposed method demonstrates
state-of-the-art denoising performance while retaining spatial resolution and
quantitative accuracy, despite using a common network architecture for various
noisy PET images with 1/10th of the full counts. These results suggest that the
proposed MR-GDD can reduce PET scan times and PET tracer doses considerably
without impacting patients.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Fundamental Trade-offs in Learning Invariant Representations. (arXiv:2109.03386v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03386">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many applications of representation learning, such as privacy-preservation,
algorithmic fairness and domain adaptation, desire explicit control over
semantic information being discarded. This goal is often formulated as
satisfying two potentially competing objectives: maximizing utility for
predicting a target attribute while simultaneously being independent or
invariant with respect to a known semantic attribute. In this paper, we
\emph{identify and determine} two fundamental trade-offs between utility and
semantic dependence induced by the statistical dependencies between the data
and its corresponding target and semantic attributes. We derive closed-form
solutions for the global optima of the underlying optimization problems under
mild assumptions, which in turn yields closed formulae for the exact
trade-offs. We also derive empirical estimates of the trade-offs and show their
convergence to the corresponding population counterparts. Finally, we
numerically quantify the trade-offs on representative problems and compare to
the solutions achieved by baseline representation learning algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deriving Explanation of Deep Visual Saliency Models. (arXiv:2109.03575v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03575">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks have shown their profound impact on achieving human
level performance in visual saliency prediction. However, it is still unclear
how they learn the task and what it means in terms of understanding human
visual system. In this work, we develop a technique to derive explainable
saliency models from their corresponding deep neural architecture based
saliency models by applying human perception theories and the conventional
concepts of saliency. This technique helps us understand the learning pattern
of the deep network at its intermediate layers through their activation maps.
Initially, we consider two state-of-the-art deep saliency models, namely UNISAL
and MSI-Net for our interpretation. We use a set of biologically plausible
log-gabor filters for identifying and reconstructing the activation maps of
them using our explainable saliency model. The final saliency map is generated
using these reconstructed activation maps. We also build our own deep saliency
model named cross-concatenated multi-scale residual block based network
(CMRNet) for saliency prediction. Then, we evaluate and compare the performance
of the explainable models derived from UNISAL, MSI-Net and CMRNet on three
benchmark datasets with other state-of-the-art methods. Hence, we propose that
this approach of explainability can be applied to any deep visual saliency
model for interpretation which makes it a generic one.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">StreaMRAK a Streaming Multi-Resolution Adaptive Kernel Algorithm. (arXiv:2108.10411v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10411">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Kernel ridge regression (KRR) is a popular scheme for non-linear
non-parametric learning. However, existing implementations of KRR require that
all the data is stored in the main memory, which severely limits the use of KRR
in contexts where data size far exceeds the memory size. Such applications are
increasingly common in data mining, bioinformatics, and control. A powerful
paradigm for computing on data sets that are too large for memory is the
streaming model of computation, where we process one data sample at a time,
discarding each sample before moving on to the next one. In this paper, we
propose StreaMRAK - a streaming version of KRR. StreaMRAK improves on existing
KRR schemes by dividing the problem into several levels of resolution, which
allows continual refinement to the predictions. The algorithm reduces the
memory requirement by continuously and efficiently integrating new samples into
the training model. With a novel sub-sampling scheme, StreaMRAK reduces memory
and computational complexities by creating a sketch of the original data, where
the sub-sampling density is adapted to the bandwidth of the kernel and the
local dimensionality of the data. We present a showcase study on two synthetic
problems and the prediction of the trajectory of a double pendulum. The results
show that the proposed algorithm is fast and accurate.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">C-MinHash: Rigorously Reducing $K$ Permutations to Two. (arXiv:2109.03337v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03337">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Minwise hashing (MinHash) is an important and practical algorithm for
generating random hashes to approximate the Jaccard (resemblance) similarity in
massive binary (0/1) data. The basic theory of MinHash requires applying
hundreds or even thousands of independent random permutations to each data
vector in the dataset, in order to obtain reliable results for (e.g.,) building
large-scale learning models or approximate near neighbor search in massive
data. In this paper, we propose {\bf Circulant MinHash (C-MinHash)} and provide
the surprising theoretical results that we just need \textbf{two} independent
random permutations. For C-MinHash, we first conduct an initial permutation on
the data vector, then we use a second permutation to generate hash values.
Basically, the second permutation is re-used $K$ times via circulant shifting
to produce $K$ hashes. Unlike classical MinHash, these $K$ hashes are obviously
correlated, but we are able to provide rigorous proofs that we still obtain an
unbiased estimate of the Jaccard similarity and the theoretical variance is
uniformly smaller than that of the classical MinHash with $K$ independent
permutations. The theoretical proofs of C-MinHash require some non-trivial
efforts. Numerical experiments are conducted to justify the theory and
demonstrate the effectiveness of C-MinHash.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">YAHPO Gym -- Design Criteria and a new Multifidelity Benchmark for Hyperparameter Optimization. (arXiv:2109.03670v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03670">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>When developing and analyzing new hyperparameter optimization (HPO) methods,
it is vital to empirically evaluate and compare them on well-curated benchmark
suites. In this work, we list desirable properties and requirements for such
benchmarks and propose a new set of challenging and relevant multifidelity HPO
benchmark problems motivated by these requirements. For this, we revisit the
concept of surrogate-based benchmarks and empirically compare them to more
widely-used tabular benchmarks, showing that the latter ones may induce bias in
performance estimation and ranking of HPO methods. We present a new
surrogate-based benchmark suite for multifidelity HPO methods consisting of 9
benchmark collections that constitute over 700 multifidelity HPO problems in
total. All our benchmarks also allow for querying of multiple optimization
targets, enabling the benchmarking of multi-objective HPO. We examine and
compare our benchmark suite with respect to the defined requirements and show
that our benchmarks provide viable additions to existing suites.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Plant Disease Detection Using Image Processing and Machine Learning. (arXiv:2106.10698v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10698">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the important and tedious task in agricultural practices is the
detection of the disease on crops. It requires huge time as well as skilled
labor. This paper proposes a smart and efficient technique for detection of
crop disease which uses computer vision and machine learning techniques. The
proposed system is able to detect 20 different diseases of 5 common plants with
93% accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-World Adversarial Examples involving Makeup Application. (arXiv:2109.03329v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03329">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks have developed rapidly and have achieved outstanding
performance in several tasks, such as image classification and natural language
processing. However, recent studies have indicated that both digital and
physical adversarial examples can fool neural networks. Face-recognition
systems are used in various applications that involve security threats from
physical adversarial examples. Herein, we propose a physical adversarial attack
with the use of full-face makeup. The presence of makeup on the human face is a
reasonable possibility, which possibly increases the imperceptibility of
attacks. In our attack framework, we combine the cycle-adversarial generative
network (cycle-GAN) and a victimized classifier. The Cycle-GAN is used to
generate adversarial makeup, and the architecture of the victimized classifier
is VGG 16. Our experimental results show that our attack can effectively
overcome manual errors in makeup application, such as color and
position-related errors. We also demonstrate that the approaches used to train
the models can influence physical attacks; the adversarial perturbations
crafted from the pre-trained model are affected by the corresponding training
data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DMN4: Few-shot Learning via Discriminative Mutual Nearest Neighbor Neural Network. (arXiv:2103.08160v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Few-shot learning (FSL) aims to classify images under low-data regimes, where
the conventional pooled global feature is likely to lose useful local
characteristics. Recent work has achieved promising performances by using deep
descriptors. They generally take all deep descriptors from neural networks into
consideration while ignoring that some of them are useless in classification
due to their limited receptive field, e.g., task-irrelevant descriptors could
be misleading and multiple aggregative descriptors from background clutter
could even overwhelm the object&#x27;s presence. In this paper, we argue that a
Mutual Nearest Neighbor (MNN) relation should be established to explicitly
select the query descriptors that are most relevant to each task and discard
less relevant ones from aggregative clutters in FSL. Specifically, we propose
Discriminative Mutual Nearest Neighbor Neural Network (DMN4) for FSL. Extensive
experiments demonstrate that our method outperforms the existing
state-of-the-arts on both fine-grained and generalized datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning. (arXiv:2105.14074v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In robotic domains, learning and planning are complicated by continuous state
spaces, continuous action spaces, and long task horizons. In this work, we
address these challenges with Neuro-Symbolic Relational Transition Models
(NSRTs), a novel class of models that are data-efficient to learn, compatible
with powerful robotic planning methods, and generalizable over objects. NSRTs
have both symbolic and neural components, enabling a bilevel planning scheme
where symbolic AI planning in an outer loop guides continuous planning with
neural models in an inner loop. Experiments in four robotic planning domains
show that NSRTs can be learned after only tens or hundreds of training
episodes, and then used for fast planning in new tasks that require up to 60
actions and involve many more objects than were seen during training. Video:
https://tinyurl.com/chitnis-nsrts</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hiding Among the Clones: A Simple and Nearly Optimal Analysis of Privacy Amplification by Shuffling. (arXiv:2012.12803v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.12803">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent work of Erlingsson, Feldman, Mironov, Raghunathan, Talwar, and
Thakurta [EFMRTT19] demonstrates that random shuffling amplifies differential
privacy guarantees of locally randomized data. Such amplification implies
substantially stronger privacy guarantees for systems in which data is
contributed anonymously [BEMMRLRKTS17] and has lead to significant interest in
the shuffle model of privacy [CSUZZ19; EFMRTT19].

We show that random shuffling of $n$ data records that are input to
$\varepsilon_0$-differentially private local randomizers results in an
$(O((1-e^{-\varepsilon_0})\sqrt{\frac{e^{\varepsilon_0}\log(1/\delta)}{n}}),
\delta)$-differentially private algorithm. This significantly improves over
previous work and achieves the asymptotically optimal dependence in
$\varepsilon_0$. Our result is based on a new approach that is simpler than
previous work and extends to approximate differential privacy with nearly the
same guarantees. Importantly, our work also yields an algorithm for deriving
tighter bounds on the resulting $\varepsilon$ and $\delta$ as well as R\&#x27;enyi
differential privacy guarantees. We show numerically that our algorithm gets to
within a small constant factor of the optimal bound. As a direct corollary of
our analysis we derive a simple and nearly optimal algorithm for frequency
estimation in the shuffle model of privacy. We also observe that our result
implies the first asymptotically optimal privacy analysis of noisy stochastic
gradient descent that applies to sampling without replacement.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AutoDebias: Learning to Debias for Recommendation. (arXiv:2105.04170v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04170">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recommender systems rely on user behavior data like ratings and clicks to
build personalization model. However, the collected data is observational
rather than experimental, causing various biases in the data which
significantly affect the learned model. Most existing work for recommendation
debiasing, such as the inverse propensity scoring and imputation approaches,
focuses on one or two specific biases, lacking the universal capacity that can
account for mixed or even unknown biases in the data. Towards this research
gap, we first analyze the origin of biases from the perspective of \textit{risk
discrepancy} that represents the difference between the expectation empirical
risk and the true risk. Remarkably, we derive a general learning framework that
well summarizes most existing debiasing strategies by specifying some
parameters of the general framework. This provides a valuable opportunity to
develop a universal solution for debiasing, e.g., by learning the debiasing
parameters from data. However, the training data lacks important signal of how
the data is biased and what the unbiased data looks like. To move this idea
forward, we propose \textit{AotoDebias} that leverages another (small) set of
uniform data to optimize the debiasing parameters by solving the bi-level
optimization problem with meta-learning. Through theoretical analyses, we
derive the generalization bound for AutoDebias and prove its ability to acquire
the appropriate debiasing strategy. Extensive experiments on two real datasets
and a simulated dataset demonstrated effectiveness of AutoDebias. The code is
available at \url{https://github.com/DongHande/AutoDebias}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Residual Model Learning for Microrobot Control. (arXiv:2104.00631v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00631">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A majority of microrobots are constructed using compliant materials that are
difficult to model analytically, limiting the utility of traditional
model-based controllers. Challenges in data collection on microrobots and large
errors between simulated models and real robots make current model-based
learning and sim-to-real transfer methods difficult to apply. We propose a
novel framework residual model learning (RML) that leverages approximate models
to substantially reduce the sample complexity associated with learning an
accurate robot model. We show that using RML, we can learn a model of the
Harvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively
collected interaction data. The learned model is accurate enough to be
leveraged as &quot;proxy-simulator&quot; for learning walking and turning behaviors using
model-free reinforcement learning algorithms. RML provides a general framework
for learning from extremely small amounts of interaction data, and our
experiments with HAMR clearly demonstrate that RML substantially outperforms
existing techniques.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-explaining variational posterior distributions for Gaussian Process models. (arXiv:2109.03708v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03708">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Bayesian methods have become a popular way to incorporate prior knowledge and
a notion of uncertainty into machine learning models. At the same time, the
complexity of modern machine learning makes it challenging to comprehend a
model&#x27;s reasoning process, let alone express specific prior assumptions in a
rigorous manner. While primarily interested in the former issue, recent
developments intransparent machine learning could also broaden the range of
prior information that we can provide to complex Bayesian models. Inspired by
the idea of self-explaining models, we introduce a corresponding concept for
variational GaussianProcesses. On the one hand, our contribution improves
transparency for these types of models. More importantly though, our proposed
self-explaining variational posterior distribution allows to incorporate both
general prior knowledge about a target function as a whole and prior knowledge
about the contribution of individual features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03777">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph neural networks have triggered a resurgence of graph-based text
classification. We show that already a simple MLP baseline achieves comparable
performance on benchmark datasets, questioning the importance of synthetic
graph structures. When considering an inductive scenario, i. e., when adding
new documents to a corpus, a simple MLP even outperforms most graph-based
models. We further fine-tune DistilBERT for comparison and find that it
outperforms all state-of-the-art models. We suggest that future studies use at
least an MLP baseline to contextualize the results. We provide recommendations
for the design and training of such a baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Neural Networks: Methods, Applications, and Opportunities. (arXiv:2108.10733v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the last decade or so, we have witnessed deep learning reinvigorating the
machine learning field. It has solved many problems in the domains of computer
vision, speech recognition, natural language processing, and various other
tasks with state-of-the-art performance. The data is generally represented in
the Euclidean space in these domains. Various other domains conform to
non-Euclidean space, for which graph is an ideal representation. Graphs are
suitable for representing the dependencies and interrelationships between
various entities. Traditionally, handcrafted features for graphs are incapable
of providing the necessary inference for various tasks from this complex data
representation. Recently, there is an emergence of employing various advances
in deep learning to graph data-based tasks. This article provides a
comprehensive survey of graph neural networks (GNNs) in each learning setting:
supervised, unsupervised, semi-supervised, and self-supervised learning.
Taxonomy of each graph based learning setting is provided with logical
divisions of methods falling in the given learning setting. The approaches for
each learning task are analyzed from both theoretical as well as empirical
standpoints. Further, we provide general architecture guidelines for building
GNNs. Various applications and benchmark datasets are also provided, along with
open challenges still plaguing the general applicability of GNNs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00952">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Axial multi-layer perceptron architecture for automatic segmentation of choroid plexus in multiple sclerosis. (arXiv:2109.03778v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03778">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Choroid plexuses (CP) are structures of the ventricles of the brain which
produce most of the cerebrospinal fluid (CSF). Several postmortem and in vivo
studies have pointed towards their role in the inflammatory process in multiple
sclerosis (MS). Automatic segmentation of CP from MRI thus has high value for
studying their characteristics in large cohorts of patients. To the best of our
knowledge, the only freely available tool for CP segmentation is FreeSurfer but
its accuracy for this specific structure is poor. In this paper, we propose to
automatically segment CP from non-contrast enhanced T1-weighted MRI. To that
end, we introduce a new model called &quot;Axial-MLP&quot; based on an assembly of Axial
multi-layer perceptrons (MLPs). This is inspired by recent works which showed
that the self-attention layers of Transformers can be replaced with MLPs. This
approach is systematically compared with a standard 3D U-Net, nnU-Net,
Freesurfer and FastSurfer. For our experiments, we make use of a dataset of 141
subjects (44 controls and 97 patients with MS). We show that all the tested
deep learning (DL) methods outperform FreeSurfer (Dice around 0.7 for DL vs
0.33 for FreeSurfer). Axial-MLP is competitive with U-Nets even though it is
slightly less accurate. The conclusions of our paper are two-fold: 1) the
studied deep learning methods could be useful tools to study CP in large
cohorts of MS patients; 2)~Axial-MLP is a potentially viable alternative to
convolutional neural networks for such tasks, although it could benefit from
further improvements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Event-Driven Knowledge Graph Completion in Digital Factories. (arXiv:2109.03655v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03655">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Smart factories are equipped with machines that can sense their manufacturing
environments, interact with each other, and control production processes.
Smooth operation of such factories requires that the machines and engineering
personnel that conduct their monitoring and diagnostics share a detailed common
industrial knowledge about the factory, e.g., in the form of knowledge graphs.
Creation and maintenance of such knowledge is expensive and requires
automation. In this work we show how machine learning that is specifically
tailored towards industrial applications can help in knowledge graph
completion. In particular, we show how knowledge completion can benefit from
event logs that are common in smart factories. We evaluate this on the
knowledge graph from a real world-inspired smart factory with encouraging
results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A comparison of combined data assimilation and machine learning methods for offline and online model error correction. (arXiv:2107.11114v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11114">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies have shown that it is possible to combine machine learning
methods with data assimilation to reconstruct a dynamical system using only
sparse and noisy observations of that system. The same approach can be used to
correct the error of a knowledge-based model. The resulting surrogate model is
hybrid, with a statistical part supplementing a physical part. In practice, the
correction can be added as an integrated term (i.e. in the model resolvent) or
directly inside the tendencies of the physical model. The resolvent correction
is easy to implement. The tendency correction is more technical, in particular
it requires the adjoint of the physical model, but also more flexible. We use
the two-scale Lorenz model to compare the two methods. The accuracy in
long-range forecast experiments is somewhat similar between the surrogate
models using the resolvent correction and the tendency correction. By contrast,
the surrogate models using the tendency correction significantly outperform the
surrogate models using the resolvent correction in data assimilation
experiments. Finally, we show that the tendency correction opens the
possibility to make online model error correction, i.e. improving the model
progressively as new observations become available. The resulting algorithm can
be seen as a new formulation of weak-constraint 4D-Var. We compare online and
offline learning using the same framework with the two-scale Lorenz system, and
show that with online learning, it is possible to extract all the information
from sparse and noisy observations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DRLD-SP: A Deep Reinforcement Learning-based Dynamic Service Placement in Edge-Enabled Internet of Vehicles. (arXiv:2106.06291v2 [cs.NI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.06291">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The growth of 5G and edge computing has enabled the emergence of Internet of
Vehicles. It supports different types of services with different resource and
service requirements. However, limited resources at the edge, high mobility of
vehicles, increasing demand, and dynamicity in service request-types have made
service placement a challenging task. A typical static placement solution is
not effective as it does not consider the traffic mobility and service
dynamics. Handling dynamics in IoV for service placement is an important and
challenging problem which is the primary focus of our work in this paper. We
propose a Deep Reinforcement Learning-based Dynamic Service Placement (DRLD-SP)
framework with the objective of minimizing the maximum edge resource usage and
service delay while considering the vehicle&#x27;s mobility, varying demand, and
dynamics in the requests for different types of services. We use SUMO and
MATLAB to carry out simulation experiments. The experimental results show that
the proposed DRLD-SP approach is effective and outperforms other static and
dynamic placement approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Well Generative Adversarial Networks Learn Distributions. (arXiv:1811.03179v4 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1811.03179">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper studies the rates of convergence for learning distributions
implicitly with the adversarial framework and Generative Adversarial Networks
(GANs), which subsume Wasserstein, Sobolev, MMD GAN, and Generalized/Simulated
Method of Moments (GMM/SMM) as special cases. We study a wide range of
parametric and nonparametric target distributions under a host of objective
evaluation metrics. We investigate how to obtain valid statistical guarantees
for GANs through the lens of regularization. On the nonparametric end, we
derive the optimal minimax rates for distribution estimation under the
adversarial framework. On the parametric end, we establish a theory for general
neural network classes (including deep leaky ReLU networks) that characterizes
the interplay on the choice of generator and discriminator pair. We discover
and isolate a new notion of regularization, called the
generator-discriminator-pair regularization, that sheds light on the advantage
of GANs compared to classical parametric and nonparametric approaches for
explicit distribution estimation. We develop novel oracle inequalities as the
main technical tools for analyzing GANs, which are of independent interest.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Memorizing without overfitting: Bias, variance, and interpolation in over-parameterized models. (arXiv:2010.13933v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.13933">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The bias-variance trade-off is a central concept in supervised learning. In
classical statistics, increasing the complexity of a model (e.g., number of
parameters) reduces bias but also increases variance. Until recently, it was
commonly believed that optimal performance is achieved at intermediate model
complexities which strike a balance between bias and variance. Modern Deep
Learning methods flout this dogma, achieving state-of-the-art performance using
&quot;over-parameterized models&quot; where the number of fit parameters is large enough
to perfectly fit the training data. As a result, understanding bias and
variance in over-parameterized models has emerged as a fundamental problem in
machine learning. Here, we use methods from statistical physics to derive
analytic expressions for bias and variance in two minimal models of
over-parameterization (linear regression and two-layer neural networks with
nonlinear data distributions), allowing us to disentangle properties stemming
from the model architecture and random sampling of data. In both models,
increasing the number of fit parameters leads to a phase transition where the
training error goes to zero and the test error diverges as a result of the
variance (while the bias remains finite). Beyond this threshold in the
interpolation regime, the training error remains zero while the test error
decreases. We also show that in contrast with classical intuition,
over-parameterized models can overfit even in the absence of noise and exhibit
bias even if the student and teacher models match. We synthesize these results
to construct a holistic understanding of generalization error and the
bias-variance trade-off in over-parameterized models and relate our results to
random matrix theory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Approximation Power of Two-Layer Networks of Random ReLUs. (arXiv:2102.02336v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02336">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper considers the following question: how well can depth-two ReLU
networks with randomly initialized bottom-level weights represent smooth
functions? We give near-matching upper- and lower-bounds for
$L_2$-approximation in terms of the Lipschitz constant, the desired accuracy,
and the dimension of the problem, as well as similar results in terms of
Sobolev norms. Our positive results employ tools from harmonic analysis and
ridgelet representation theory, while our lower-bounds are based on (robust
versions of) dimensionality arguments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepAltTrip: Top-k Alternative Itineraries for Trip Recommendation. (arXiv:2109.03535v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03535">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Trip itinerary recommendation finds an ordered sequence of Points-of-Interest
(POIs) from a large number of candidate POIs in a city. In this paper, we
propose a deep learning-based framework, called DeepAltTrip, that learns to
recommend top-k alternative itineraries for given source and destination POIs.
These alternative itineraries would be not only popular given the historical
routes adopted by past users but also dissimilar (or diverse) to each other.
The DeepAltTrip consists of two major components: (i) Itinerary Net (ITRNet)
which estimates the likelihood of POIs on an itinerary by using graph
autoencoders and two (forward and backward) LSTMs; and (ii) a route generation
procedure to generate k diverse itineraries passing through relevant POIs
obtained using ITRNet. For the route generation step, we propose a novel
sampling algorithm that can seamlessly handle a wide variety of user-defined
constraints. To the best of our knowledge, this is the first work that learns
from historical trips to provide a set of alternative itineraries to the users.
Extensive experiments conducted on eight popular real-world datasets show the
effectiveness and efficacy of our approach over state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the space of coefficients of a Feed Forward Neural Network. (arXiv:2109.03362v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We define and establish the conditions for &#x60;equivalent neural networks&#x27; -
neural networks with different weights, biases, and threshold functions that
result in the same associated function. We prove that given a neural network
$\mathcal{N}$ with piece-wise linear activation, the space of coefficients
describing all equivalent neural networks is given by a semialgebraic set. This
result is obtained by studying different representations of a given piece-wise
linear function using the Tarski-Seidenberg theorem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph-MVP: Multi-View Prototypical Contrastive Learning for Multiplex Graphs. (arXiv:2109.03560v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03560">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Contrastive Learning (CL) is one of the most popular self-supervised learning
frameworks for graph representation learning, which trains a Graph Neural
Network (GNN) by discriminating positive and negative node pairs. However,
there are two challenges for CL on graphs. On the one hand, traditional CL
methods will unavoidably introduce semantic errors since they will treat some
semantically similar nodes as negative pairs. On the other hand, most of the
existing CL methods ignore the multiplexity nature of the real-world graphs,
where nodes are connected by various relations and each relation represents a
view of the graph. To address these challenges, we propose a novel Graph
Multi-View Prototypical (Graph-MVP) framework to extract node embeddings on
multiplex graphs. Firstly, we introduce a Graph Prototypical Contrastive
Learning (Graph-PCL) framework to capture both node-level and semantic-level
information for each view of multiplex graphs. Graph-PCL captures the
node-level information by a simple yet effective data transformation technique.
It captures the semantic-level information by an Expectation-Maximization (EM)
algorithm, which alternatively performs clustering over node embeddings and
parameter updating for GNN. Next, we introduce Graph-MVP based on Graph-PCL to
jointly model different views of the multiplex graphs. Our key insight behind
Graph-MVP is that different view-specific embeddings of the same node should
have similar underlying semantic, based on which we propose two versions of
Graph-MVP: Graph-MVP_hard and Graph-MVP_soft to align embeddings across views.
Finally, we evaluate the proposed Graph-PCL and Graph-MVP on a variety of
real-world datasets and downstream tasks. The experimental results demonstrate
the effectiveness of the proposed Graph-PCL and Graph-MVP frameworks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effective and interpretable dispatching rules for dynamic job shops via guided empirical learning. (arXiv:2109.03323v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03323">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The emergence of Industry 4.0 is making production systems more flexible and
also more dynamic. In these settings, schedules often need to be adapted in
real-time by dispatching rules. Although substantial progress was made until
the &#x27;90s, the performance of these rules is still rather limited. The machine
learning literature is developing a variety of methods to improve them, but the
resulting rules are difficult to interpret and do not generalise well for a
wide range of settings. This paper is the first major attempt at combining
machine learning with domain problem reasoning for scheduling. The idea
consists of using the insights obtained with the latter to guide the empirical
search of the former. Our hypothesis is that this guided empirical learning
process should result in dispatching rules that are effective and interpretable
and which generalise well to different instance classes. We test our approach
in the classical dynamic job shop scheduling problem minimising tardiness,
which is one of the most well-studied scheduling problems. Nonetheless, results
suggest that our approach was able to find new state-of-the-art rules, which
significantly outperform the existing literature in the vast majority of
settings, from loose to tight due dates and from low utilisation conditions to
congested shops. Overall, the average improvement is 19%. Moreover, the rules
are compact, interpretable, and generalise well to extreme, unseen scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CyGIL: A Cyber Gym for Training Autonomous Agents over Emulated Network Systems. (arXiv:2109.03331v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03331">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Given the success of reinforcement learning (RL) in various domains, it is
promising to explore the application of its methods to the development of
intelligent and autonomous cyber agents. Enabling this development requires a
representative RL training environment. To that end, this work presents CyGIL:
an experimental testbed of an emulated RL training environment for network
cyber operations. CyGIL uses a stateless environment architecture and
incorporates the MITRE ATT&amp;CK framework to establish a high fidelity training
environment, while presenting a sufficiently abstracted interface to enable RL
training. Its comprehensive action space and flexible game design allow the
agent training to focus on particular advanced persistent threat (APT)
profiles, and to incorporate a broad range of potential threats and
vulnerabilities. By striking a balance between fidelity and simplicity, it aims
to leverage state of the art RL algorithms for application to real-world cyber
defence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.09313">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
paper, we probe BERT specifically to understand and measure the relational
knowledge it captures. We utilize knowledge base completion tasks to probe
every layer of pre-trained as well as fine-tuned BERT (ranking, question
answering, NER). Our findings show that knowledge is not just contained in
BERT&#x27;s final layers. Intermediate layers contribute a significant amount
(17-60%) to the total knowledge found. Probing intermediate layers also reveals
how different types of knowledge emerge at varying rates. When BERT is
fine-tuned, relational knowledge is forgotten but the extent of forgetting is
impacted by the fine-tuning objective but not the size of the dataset. We found
that ranking models forget the least and retain more knowledge in their final
layer. We release our code on github to repeat the experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Class-conditioned Domain Generalization via Wasserstein Distributional Robust Optimization. (arXiv:2109.03676v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03676">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Given multiple source domains, domain generalization aims at learning a
universal model that performs well on any unseen but related target domain. In
this work, we focus on the domain generalization scenario where domain shifts
occur among class-conditional distributions of different domains. Existing
approaches are not sufficiently robust when the variation of conditional
distributions given the same class is large. In this work, we extend the
concept of distributional robust optimization to solve the class-conditional
domain generalization problem. Our approach optimizes the worst-case
performance of a classifier over class-conditional distributions within a
Wasserstein ball centered around the barycenter of the source conditional
distributions. We also propose an iterative algorithm for learning the optimal
radius of the Wasserstein balls automatically. Experiments show that the
proposed framework has better performance on unseen target domain than
approaches without domain generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Entangled Datasets for Quantum Machine Learning. (arXiv:2109.03400v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03400">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>High-quality, large-scale datasets have played a crucial role in the
development and success of classical machine learning. Quantum Machine Learning
(QML) is a new field that aims to use quantum computers for data analysis, with
the hope of obtaining a quantum advantage of some sort. While most proposed QML
architectures are benchmarked using classical datasets, there is still doubt
whether QML on classical datasets will achieve such an advantage. In this work,
we argue that one should instead employ quantum datasets composed of quantum
states. For this purpose, we introduce the NTangled dataset composed of quantum
states with different amounts and types of multipartite entanglement. We first
show how a quantum neural network can be trained to generate the states in the
NTangled dataset. Then, we use the NTangled dataset to benchmark QML models for
supervised learning classification tasks. We also consider an alternative
entanglement-based dataset, which is scalable and is composed of states
prepared by quantum circuits with different depths. As a byproduct of our
results, we introduce a novel method for generating multipartite entangled
states, providing a use-case of quantum neural networks for quantum
entanglement theory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Review of Sound Source Localization with Deep Learning Methods. (arXiv:2109.03465v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03465">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This article is a review on deep learning methods for single and multiple
sound source localization. We are particularly interested in sound source
localization in indoor/domestic environment, where reverberation and diffuse
noise are present. We provide an exhaustive topography of the neural-based
localization literature in this context, organized according to several
aspects: the neural network architecture, the type of input features, the
output strategy (classification or regression), the types of data used for
model training and evaluation, and the model training strategy. This way, an
interested reader can easily comprehend the vast panorama of the deep
learning-based sound source localization methods. Tables summarizing the
literature review are provided at the end of the review for a quick search of
methods with a given set of target characteristics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations. (arXiv:2107.14483v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14483">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Object manipulation from 3D visual inputs poses many challenges on building
generalizable perception and policy models. However, 3D assets in existing
benchmarks mostly lack the diversity of 3D shapes that align with real-world
intra-class complexity in topology and geometry. Here we propose SAPIEN
Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over
diverse objects in a full-physics simulator. 3D assets in ManiSkill include
large intra-class topological and geometric variations. Tasks are carefully
chosen to cover distinct types of manipulation challenges. Latest progress in
3D vision also makes us believe that we should customize the benchmark so that
the challenge is inviting to researchers working on 3D deep learning. To this
end, we simulate a moving panoramic camera that returns ego-centric point
clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad
set of researchers interested in manipulation research. Besides supporting the
learning of policies from interactions, we also support
learning-from-demonstrations (LfD) methods, by providing a large number of
high-quality demonstrations (~36,000 successful trajectories, ~1.5M point
cloud/RGB-D frames in total). We provide baselines using 3D deep learning and
LfD algorithms. All code of our benchmark (simulator, environment, SDK, and
baselines) is open-sourced, and a challenge facing interdisciplinary
researchers will be held based on the benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Knowledge Distillation (KD), which transfers the knowledge of a well-trained
large model (teacher) to a small model (student), has become an important area
of research for practical deployment of recommender systems. Recently, Relaxed
Ranking Distillation (RRD) has shown that distilling the ranking information in
the recommendation list significantly improves the performance. However, the
method still has limitations in that 1) it does not fully utilize the
prediction errors of the student model, which makes the training not fully
efficient, and 2) it only distills the user-side ranking information, which
provides an insufficient view under the sparse implicit feedback. This paper
presents Dual Correction strategy for Distillation (DCD), which transfers the
ranking information from the teacher model to the student model in a more
efficient manner. Most importantly, DCD uses the discrepancy between the
teacher model and the student model predictions to decide which knowledge to be
distilled. By doing so, DCD essentially provides the learning guidance tailored
to &quot;correcting&quot; what the student model has failed to accurately predict. This
process is applied for transferring the ranking information from the user-side
as well as the item-side to address sparse implicit user feedback. Our
experiments show that the proposed method outperforms the state-of-the-art
baselines, and ablation studies validate the effectiveness of each component.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EMA: Auditing Data Removal from Trained Models. (arXiv:2109.03675v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03675">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Data auditing is a process to verify whether certain data have been removed
from a trained model. A recently proposed method (Liu et al. 20) uses
Kolmogorov-Smirnov (KS) distance for such data auditing. However, it fails
under certain practical conditions. In this paper, we propose a new method
called Ensembled Membership Auditing (EMA) for auditing data removal to
overcome these limitations. We compare both methods using benchmark datasets
(MNIST and SVHN) and Chest X-ray datasets with multi-layer perceptrons (MLP)
and convolutional neural networks (CNN). Our experiments show that EMA is
robust under various conditions, including the failure cases of the previously
proposed method. Our code is available at: https://github.com/Hazelsuko07/EMA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Conservative Policy Construction Using Variational Autoencoders for Logged Data with Missing Values. (arXiv:2109.03747v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In high-stakes applications of data-driven decision making like healthcare,
it is of paramount importance to learn a policy that maximizes the reward while
avoiding potentially dangerous actions when there is uncertainty. There are two
main challenges usually associated with this problem. Firstly, learning through
online exploration is not possible due to the critical nature of such
applications. Therefore, we need to resort to observational datasets with no
counterfactuals. Secondly, such datasets are usually imperfect, additionally
cursed with missing values in the attributes of features. In this paper, we
consider the problem of constructing personalized policies using logged data
when there are missing values in the attributes of features in both training
and test data. The goal is to recommend an action (treatment) when $\Xt$, a
degraded version of $\Xb$ with missing values, is observed. We consider three
strategies for dealing with missingness. In particular, we introduce the
\textit{conservative strategy} where the policy is designed to safely handle
the uncertainty due to missingness. In order to implement this strategy we need
to estimate posterior distribution $p(\Xb|\Xt)$, we use variational autoencoder
to achieve this. In particular, our method is based on partial variational
autoencoders (PVAE) which are designed to capture the underlying structure of
features with missing values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributed Reinforcement Learning for Age of Information Minimization in Real-Time IoT Systems. (arXiv:2104.01527v2 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01527">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, the problem of minimizing the weighted sum of age of
information (AoI) and total energy consumption of Internet of Things (IoT)
devices is studied. In the considered model, each IoT device monitors a
physical process that follows nonlinear dynamics. As the dynamics of the
physical process vary over time, each device must find an optimal sampling
frequency to sample the real-time dynamics of the physical system and send
sampled information to a base station (BS). Due to limited wireless resources,
the BS can only select a subset of devices to transmit their sampled
information. Thus, edge devices must cooperatively sample their monitored
dynamics based on the local observations and the BS must collect the sampled
information from the devices immediately, hence avoiding the additional time
and energy used for sampling and information transmission. To this end, it is
necessary to jointly optimize the sampling policy of each device and the device
selection scheme of the BS so as to accurately monitor the dynamics of the
physical process using minimum energy. This problem is formulated as an
optimization problem whose goal is to minimize the weighted sum of AoI cost and
energy consumption. To solve this problem, we propose a novel distributed
reinforcement learning (RL) approach for the sampling policy optimization. The
proposed algorithm enables edge devices to cooperatively find the global
optimal sampling policy using their own local observations. Given the sampling
policy, the device selection scheme can be optimized thus minimizing the
weighted sum of AoI and energy consumption of all devices. Simulations with
real data of PM 2.5 pollution show that the proposed algorithm can reduce the
sum of AoI by up to 17.8% and 33.9% and the total energy consumption by up to
13.2% and 35.1%, compared to a conventional deep Q network method and a uniform
sampling policy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Selection&#x27;s Disparate Impact in Real-World Deep Learning Applications. (arXiv:2104.00606v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.00606">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Algorithmic fairness has emphasized the role of biased data in automated
decision outcomes. Recently, there has been a shift in attention to sources of
bias that implicate fairness in other stages in the ML pipeline. We contend
that one source of such bias, human preferences in model selection, remains
under-explored in terms of its role in disparate impact across demographic
groups. Using a deep learning model trained on real-world medical imaging data,
we verify our claim empirically and argue that choice of metric for model
comparison, especially those that do not take variability into account, can
significantly bias model selection outcomes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DexRay: A Simple, yet Effective Deep Learning Approach to Android Malware Detection based on Image Representation of Bytecode. (arXiv:2109.03326v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03326">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Computer vision has witnessed several advances in recent years, with
unprecedented performance provided by deep representation learning research.
Image formats thus appear attractive to other fields such as malware detection,
where deep learning on images alleviates the need for comprehensively
hand-crafted features generalising to different malware variants. We postulate
that this research direction could become the next frontier in Android malware
detection, and therefore requires a clear roadmap to ensure that new approaches
indeed bring novel contributions. We contribute with a first building block by
developing and assessing a baseline pipeline for image-based malware detection
with straightforward steps. We propose DexRay, which converts the bytecode of
the app DEX files into grey-scale &quot;vector&quot; images and feeds them to a
1-dimensional Convolutional Neural Network model. We view DexRay as
foundational due to the exceedingly basic nature of the design choices,
allowing to infer what could be a minimal performance that can be obtained with
image-based learning in malware detection. The performance of DexRay evaluated
on over 158k apps demonstrates that, while simple, our approach is effective
with a high detection rate(F1-score&#x3D; 0.96). Finally, we investigate the impact
of time decay and image-resizing on the performance of DexRay and assess its
resilience to obfuscation. This work-in-progress paper contributes to the
domain of Deep Learning based Malware detection by providing a sound, simple,
yet effective approach (with available artefacts) that can be the basis to
scope the many profound questions that will need to be investigated to fully
develop this domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Learning Beyond the Star: Local D2D Model Consensus with Global Cluster Sampling. (arXiv:2109.03350v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03350">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning has emerged as a popular technique for distributing model
training across the network edge. Its learning architecture is conventionally a
star topology between the devices and a central server. In this paper, we
propose two timescale hybrid federated learning (TT-HF), which migrates to a
more distributed topology via device-to-device (D2D) communications. In TT-HF,
local model training occurs at devices via successive gradient iterations, and
the synchronization process occurs at two timescales: (i) macro-scale, where
global aggregations are carried out via device-server interactions, and (ii)
micro-scale, where local aggregations are carried out via D2D cooperative
consensus formation in different device clusters. Our theoretical analysis
reveals how device, cluster, and network-level parameters affect the
convergence of TT-HF, and leads to a set of conditions under which a
convergence rate of O(1/t) is guaranteed. Experimental results demonstrate the
improvements in convergence and utilization that can be obtained by TT-HF over
state-of-the-art federated learning baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ADER:Adapting between Exploration and Robustness for Actor-Critic Methods. (arXiv:2109.03443v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03443">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Combining off-policy reinforcement learning methods with function
approximators such as neural networks has been found to lead to overestimation
of the value function and sub-optimal solutions. Improvement such as TD3 has
been proposed to address this issue. However, we surprisingly find that its
performance lags behind the vanilla actor-critic methods (such as DDPG) in some
primitive environments. In this paper, we show that the failure of some cases
can be attributed to insufficient exploration. We reveal the culprit of
insufficient exploration in TD3, and propose a novel algorithm toward this
problem that ADapts between Exploration and Robustness, namely ADER. To enhance
the exploration ability while eliminating the overestimation bias, we introduce
a dynamic penalty term in value estimation calculated from estimated
uncertainty, which takes into account different compositions of the uncertainty
in different learning stages. Experiments in several challenging environments
demonstrate the supremacy of the proposed method in continuous control tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fixed Support Tree-Sliced Wasserstein Barycenter. (arXiv:2109.03431v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03431">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The Wasserstein barycenter has been widely studied in various fields,
including natural language processing, and computer vision. However, it
requires a high computational cost to solve the Wasserstein barycenter problem
because the computation of the Wasserstein distance requires a quadratic time
with respect to the number of supports. By contrast, the Wasserstein distance
on a tree, called the tree-Wasserstein distance, can be computed in linear time
and allows for the fast comparison of a large number of distributions. In this
study, we propose a barycenter under the tree-Wasserstein distance, called the
fixed support tree-Wasserstein barycenter (FS-TWB) and its extension, called
the fixed support tree-sliced Wasserstein barycenter (FS-TSWB). More
specifically, we first show that the FS-TWB and FS-TSWB problems are convex
optimization problems and can be solved by using the projected subgradient
descent. Moreover, we propose a more efficient algorithm to compute the
subgradient and objective function value by using the properties of
tree-Wasserstein barycenter problems. Through real-world experiments, we show
that, by using the proposed algorithm, the FS-TWB and FS-TSWB can be solved two
orders of magnitude faster than the original Wasserstein barycenter.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Zero-sum Stochastic Games with Posterior Sampling. (arXiv:2109.03396v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03396">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose Posterior Sampling Reinforcement Learning for
Zero-sum Stochastic Games (PSRL-ZSG), the first online learning algorithm that
achieves Bayesian regret bound of $O(HS\sqrt{AT})$ in the infinite-horizon
zero-sum stochastic games with average-reward criterion. Here $H$ is an upper
bound on the span of the bias function, $S$ is the number of states, $A$ is the
number of joint actions and $T$ is the horizon. We consider the online setting
where the opponent can not be controlled and can take any arbitrary
time-adaptive history-dependent strategy. This improves the best existing
regret bound of $O(\sqrt[3]{DS^2AT^2})$ by Wei et. al., 2017 under the same
assumption and matches the theoretical lower bound in $A$ and $T$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Can Noise on Qubits Be Learned in Quantum Neural Network? A Case Study on QuantumFlow. (arXiv:2109.03430v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03430">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the noisy intermediate-scale quantum (NISQ) era, one of the key questions
is how to deal with the high noise level existing in physical quantum bits
(qubits). Quantum error correction is promising but requires an extensive
number (e.g., over 1,000) of physical qubits to create one &quot;perfect&quot; qubit,
exceeding the capacity of the existing quantum computers. This paper aims to
tackle the noise issue from another angle: instead of creating perfect qubits
for general quantum algorithms, we investigate the potential to mitigate the
noise issue for dedicate algorithms. Specifically, this paper targets quantum
neural network (QNN), and proposes to learn the errors in the training phase,
so that the identified QNN model can be resilient to noise. As a result, the
implementation of QNN needs no or a small number of additional physical qubits,
which is more realistic for the near-term quantum computers. To achieve this
goal, an application-specific compiler is essential: on the one hand, the error
cannot be learned if the mapping from logical qubits to physical qubits exists
randomness; on the other hand, the compiler needs to be efficient so that the
lengthy training procedure can be completed in a reasonable time. In this
paper, we utilize the recent QNN framework, QuantumFlow, as a case study.
Experimental results show that the proposed approach can optimize QNN models
for different errors in qubits, achieving up to 28% accuracy improvement
compared with the model obtained by the error-agnostic training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Non-Negative Matrix Co-Factorisation Approach for Noisy Neonatal Chest Sound Separation. (arXiv:2109.03275v1 [eess.AS])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03275">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Obtaining high-quality heart and lung sounds enables clinicians to accurately
assess a newborn&#x27;s cardio-respiratory health and provide timely care. However,
noisy chest sound recordings are common, hindering timely and accurate
assessment. A new Non-negative Matrix Co-Factorisation-based approach is
proposed to separate noisy chest sound recordings into heart, lung, and noise
components to address this problem. This method is achieved through training
with 20 high-quality heart and lung sounds, in parallel with separating the
sounds of the noisy recording. The method was tested on 68 10-second noisy
recordings containing both heart and lung sounds and compared to the current
state of the art Non-negative Matrix Factorisation methods. Results show
significant improvements in heart and lung sound quality scores respectively,
and improved accuracy of 3.6bpm and 1.2bpm in heart and breathing rate
estimation respectively, when compared to existing methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Deep AUC Maximization for Heterogeneous Data with a Constant Communication Complexity. (arXiv:2102.04635v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.04635">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep AUC (area under the ROC curve) Maximization (DAM) has attracted much
attention recently due to its great potential for imbalanced data
classification. However, the research on Federated Deep AUC Maximization (FDAM)
is still limited. Compared with standard federated learning (FL) approaches
that focus on decomposable minimization objectives, FDAM is more complicated
due to its minimization objective is non-decomposable over individual examples.
In this paper, we propose improved FDAM algorithms for heterogeneous data by
solving the popular non-convex strongly-concave min-max formulation of DAM in a
distributed fashion, which can also be applied to a class of non-convex
strongly-concave min-max problems. A striking result of this paper is that the
communication complexity of the proposed algorithm is a constant independent of
the number of machines and also independent of the accuracy level, which
improves an existing result by orders of magnitude. The experiments have
demonstrated the effectiveness of our FDAM algorithm on benchmark datasets, and
on medical chest X-ray images from different organizations. Our experiment
shows that the performance of FDAM using data from multiple hospitals can
improve the AUC score on testing data from a single hospital for detecting
life-threatening diseases based on chest radiographs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Bottom-up method Towards the Automatic and Objective Monitoring of Smoking Behavior In-the-wild using Wrist-mounted Inertial Sensors. (arXiv:2109.03475v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03475">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The consumption of tobacco has reached global epidemic proportions and is
characterized as the leading cause of death and illness. Among the different
ways of consuming tobacco (e.g., smokeless, cigars), smoking cigarettes is the
most widespread. In this paper, we present a two-step, bottom-up algorithm
towards the automatic and objective monitoring of cigarette-based, smoking
behavior during the day, using the 3D acceleration and orientation velocity
measurements from a commercial smartwatch. In the first step, our algorithm
performs the detection of individual smoking gestures (i.e., puffs) using an
artificial neural network with both convolutional and recurrent layers. In the
second step, we make use of the detected puff density to achieve the temporal
localization of smoking sessions that occur throughout the day. In the
experimental section we provide extended evaluation regarding each step of the
proposed algorithm, using our publicly available, realistic Smoking Event
Detection (SED) and Free-living Smoking Event Detection (SED-FL) datasets
recorded under semi-controlled and free-living conditions, respectively. In
particular, leave-one-subject-out (LOSO) experiments reveal an F1-score of
0.863 for the detection of puffs and an F1-score/Jaccard index equal to
0.878/0.604 towards the temporal localization of smoking sessions during the
day. Finally, to gain further insight, we also compare the puff detection part
of our algorithm with a similar approach found in the recent literature.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Spoken question answering (SQA) requires fine-grained understanding of both
spoken documents and questions for the optimal answer prediction. In this
paper, we propose novel training schemes for spoken question answering with a
self-supervised training stage and a contrastive representation learning stage.
In the self-supervised stage, we propose three auxiliary self-supervised tasks,
including utterance restoration, utterance insertion, and question
discrimination, and jointly train the model to capture consistency and
coherence among speech documents without any additional data or annotations. We
then propose to learn noise-invariant utterance representations in a
contrastive objective by adopting multiple augmentation strategies, including
span deletion and span substitution. Besides, we design a Temporal-Alignment
attention to semantically align the speech-text clues in the learned common
space and benefit the SQA tasks. By this means, the training schemes can more
effectively guide the generation model to predict more proper answers.
Experimental results show that our model achieves state-of-the-art results on
three SQA benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CRNNTL: convolutional recurrent neural network and transfer learning for QSAR modelling. (arXiv:2109.03309v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03309">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this study, we propose the convolutional recurrent neural network and
transfer learning (CRNNTL) for QSAR modelling. The method was inspired by the
applications of polyphonic sound detection and electrocardiogram
classification. Our strategy takes advantages of both convolutional and
recurrent neural networks for feature extraction, as well as the data
augmentation method. Herein, CRNNTL is evaluated on 20 benchmark datasets in
comparison with baseline methods. In addition, one isomers based dataset is
used to elucidate its ability for both local and global feature extraction.
Then, knowledge transfer performance of CRNNTL is tested, especially for small
biological activity datasets. Finally, different latent representations from
other type of AEs were used for versatility study of our model. The results
show the effectiveness of CRNNTL using different latent representation.
Moreover, efficient knowledge transfer is achieved to overcome data scarcity
considering binding site similarity between different targets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconstructing High-resolution Turbulent Flows Using Physics-Guided Neural Networks. (arXiv:2109.03327v1 [physics.flu-dyn])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03327">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Direct numerical simulation (DNS) of turbulent flows is computationally
expensive and cannot be applied to flows with large Reynolds numbers. Large
eddy simulation (LES) is an alternative that is computationally less demanding,
but is unable to capture all of the scales of turbulent transport accurately.
Our goal in this work is to build a new data-driven methodology based on
super-resolution techniques to reconstruct DNS data from LES predictions. We
leverage the underlying physical relationships to regularize the relationships
amongst different physical variables. We also introduce a hierarchical
generative process and a reverse degradation process to fully explore the
correspondence between DNS and LES data. We demonstrate the effectiveness of
our method through a single-snapshot experiment and a cross-time experiment.
The results confirm that our method can better reconstruct high-resolution DNS
data over space and over time in terms of pixel-wise reconstruction error and
structural similarity. Visual comparisons show that our method performs much
better in capturing fine-level flow dynamics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Built-in Elastic Transformations for Improved Robustness. (arXiv:2107.09391v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09391">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We focus on building robustness in the convolutions of neural visual
classifiers, especially against natural perturbations like elastic
deformations, occlusions and Gaussian noise. Existing CNNs show outstanding
performance on clean images, but fail to tackle naturally occurring
perturbations. In this paper, we start from elastic perturbations, which
approximate (local) view-point changes of the object. We present
elastically-augmented convolutions (EAConv) by parameterizing filters as a
combination of fixed elastically-perturbed bases functions and trainable
weights for the purpose of integrating unseen viewpoints in the CNN. We show on
CIFAR-10 and STL-10 datasets that the general robustness of our method on
unseen occlusion, zoom, rotation, image cut and Gaussian perturbations
improves, while significantly improving the performance on clean images without
any data augmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Disentangling Alzheimer&#x27;s disease neurodegeneration from typical brain aging using machine learning. (arXiv:2109.03723v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neuroimaging biomarkers that distinguish between typical brain aging and
Alzheimer&#x27;s disease (AD) are valuable for determining how much each contributes
to cognitive decline. Machine learning models can derive multi-variate brain
change patterns related to the two processes, including the SPARE-AD (Spatial
Patterns of Atrophy for Recognition of Alzheimer&#x27;s Disease) and SPARE-BA (of
Brain Aging) investigated herein. However, substantial overlap between brain
regions affected in the two processes confounds measuring them independently.
We present a methodology toward disentangling the two. T1-weighted MRI images
of 4,054 participants (48-95 years) with AD, mild cognitive impairment (MCI),
or cognitively normal (CN) diagnoses from the iSTAGING (Imaging-based
coordinate SysTem for AGIng and NeurodeGenerative diseases) consortium were
analyzed. First, a subset of AD patients and CN adults were selected based
purely on clinical diagnoses to train SPARE-BA1 (regression of age using CN
individuals) and SPARE-AD1 (classification of CN versus AD). Second, analogous
groups were selected based on clinical and molecular markers to train SPARE-BA2
and SPARE-AD2: amyloid-positive (A+) AD continuum group (consisting of A+AD,
A+MCI, and A+ and tau-positive CN individuals) and amyloid-negative (A-) CN
group. Finally, the combined group of the AD continuum and A-/CN individuals
was used to train SPARE-BA3, with the intention to estimate brain age
regardless of AD-related brain changes. Disentangled SPARE models derived brain
patterns that were more specific to the two types of the brain changes.
Correlation between the SPARE-BA and SPARE-AD was significantly reduced.
Correlation of disentangled SPARE-AD was non-inferior to the molecular
measurements and to the number of APOE4 alleles, but was less to AD-related
psychometric test scores, suggesting contribution of advanced brain aging to
these scores.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Edge Learning with Misaligned Over-The-Air Computation. (arXiv:2102.13604v3 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.13604">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Over-the-air computation (OAC) is a promising technique to realize fast model
aggregation in the uplink of federated edge learning. OAC, however, hinges on
accurate channel-gain precoding and strict synchronization among the edge
devices, which are challenging in practice. As such, how to design the maximum
likelihood (ML) estimator in the presence of residual channel-gain mismatch and
asynchronies is an open problem. To fill this gap, this paper formulates the
problem of misaligned OAC for federated edge learning and puts forth a whitened
matched filtering and sampling scheme to obtain oversampled, but independent,
samples from the misaligned and overlapped signals. Given the whitened samples,
a sum-product ML estimator and an aligned-sample estimator are devised to
estimate the arithmetic sum of the transmitted symbols. In particular, the
computational complexity of our sum-product ML estimator is linear in the
packet length and hence is significantly lower than the conventional ML
estimator. Extensive simulations on the test accuracy versus the average
received energy per symbol to noise power spectral density ratio (EsN0) yield
two main results: 1) In the low EsN0 regime, the aligned-sample estimator can
achieve superior test accuracy provided that the phase misalignment is
non-severe. In contrast, the ML estimator does not work well due to the error
propagation and noise enhancement in the estimation process. 2) In the high
EsN0 regime, the ML estimator attains the optimal learning performance
regardless of the severity of phase misalignment. On the other hand, the
aligned-sample estimator suffers from a test-accuracy loss caused by phase
misalignment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">fastMRI+: Clinical Pathology Annotations for Knee and Brain Fully Sampled Multi-Coil MRI Data. (arXiv:2109.03812v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03812">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Improving speed and image quality of Magnetic Resonance Imaging (MRI) via
novel reconstruction approaches remains one of the highest impact applications
for deep learning in medical imaging. The fastMRI dataset, unique in that it
contains large volumes of raw MRI data, has enabled significant advances in
accelerating MRI using deep learning-based reconstruction methods. While the
impact of the fastMRI dataset on the field of medical imaging is unquestioned,
the dataset currently lacks clinical expert pathology annotations, critical to
addressing clinically relevant reconstruction frameworks and exploring
important questions regarding rendering of specific pathology using such novel
approaches. This work introduces fastMRI+, which consists of 16154
subspecialist expert bounding box annotations and 13 study-level labels for 22
different pathology categories on the fastMRI knee dataset, and 7570
subspecialist expert bounding box annotations and 643 study-level labels for 30
different pathology categories for the fastMRI brain dataset. The fastMRI+
dataset is open access and aims to support further research and advancement of
medical imaging in MRI reconstruction and beyond.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Diagnostics-Guided Explanation Generation. (arXiv:2109.03756v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03756">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Explanations shed light on a machine learning model&#x27;s rationales and can aid
in identifying deficiencies in its reasoning process. Explanation generation
models are typically trained in a supervised way given human explanations. When
such annotations are not available, explanations are often selected as those
portions of the input that maximise a downstream task&#x27;s performance, which
corresponds to optimising an explanation&#x27;s Faithfulness to a given model.
Faithfulness is one of several so-called diagnostic properties, which prior
work has identified as useful for gauging the quality of an explanation without
requiring annotations. Other diagnostic properties are Data Consistency, which
measures how similar explanations are for similar input instances, and
Confidence Indication, which shows whether the explanation reflects the
confidence of the model. In this work, we show how to directly optimise for
these diagnostic properties when training a model to generate sentence-level
explanations, which markedly improves explanation quality, agreement with human
rationales, and downstream task performance on three complex reasoning tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Machine Learning Techniques for Auto Labeling of Video, Audio, and Text Data. (arXiv:2109.03784v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Machine learning has been utilized to perform tasks in many different domains
such as classification, object detection, image segmentation and natural
language analysis. Data labeling has always been one of the most important
tasks in machine learning. However, labeling large amounts of data increases
the monetary cost in machine learning. As a result, researchers started to
focus on reducing data annotation and labeling costs. Transfer learning was
designed and widely used as an efficient approach that can reasonably reduce
the negative impact of limited data, which in turn, reduces the data
preparation cost. Even transferring previous knowledge from a source domain
reduces the amount of data needed in a target domain. However, large amounts of
annotated data are still demanded to build robust models and improve the
prediction accuracy of the model. Therefore, researchers started to pay more
attention on auto annotation and labeling. In this survey paper, we provide a
review of previous techniques that focuses on optimized data annotation and
labeling for video, audio, and text data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RoadAtlas: Intelligent Platform for Automated Road Defect Detection and Asset Management. (arXiv:2109.03385v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03385">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With the rapid development of intelligent detection algorithms based on deep
learning, much progress has been made in automatic road defect recognition and
road marking parsing. This can effectively address the issue of an expensive
and time-consuming process for professional inspectors to review the street
manually. Towards this goal, we present RoadAtlas, a novel end-to-end
integrated system that can support 1) road defect detection, 2) road marking
parsing, 3) a web-based dashboard for presenting and inputting data by users,
and 4) a backend containing a well-structured database and developed APIs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03264">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Speech pre-training has primarily demonstrated efficacy on classification
tasks, while its capability of generating novel speech, similar to how GPT-2
can generate coherent paragraphs, has barely been explored. Generative Spoken
Language Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work
addressing the generative aspects of speech pre-training, which replaces text
with discovered phone-like units for language modeling and shows the ability to
generate meaningful novel sentences. Unfortunately, despite eliminating the
need of text, the units used in GSLM discard most of the prosodic information.
Hence, GSLM fails to leverage prosody for better comprehension, and does not
generate expressive speech. In this work, we present a prosody-aware generative
spoken language model (pGSLM). It is composed of a multi-stream transformer
language model (MS-TLM) of speech, represented as discovered unit and prosodic
feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to
waveforms. We devise a series of metrics for prosody modeling and generation,
and re-use metrics from GSLM for content modeling. Experimental results show
that the pGSLM can utilize prosody to improve both prosody and content
modeling, and also generate natural, meaningful, and coherent speech given a
spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03685">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For many business applications, we often seek to analyze sentiments
associated with any arbitrary aspects of commercial products, despite having a
very limited amount of labels or even without any labels at all. However,
existing aspect target sentiment classification (ATSC) models are not trainable
if annotated datasets are not available. Even with labeled data, they fall
short of reaching satisfactory performance. To address this, we propose simple
approaches that better solve ATSC with natural language prompts, enabling the
task under zero-shot cases and enhancing supervised settings, especially for
few-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop
domain, our method of reformulating ATSC as an NLI task outperforms supervised
SOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.
Moreover, we demonstrate that our prompts could handle implicitly stated
aspects as well: our models reach about 77% accuracy on detecting sentiments
for aspect categories (e.g., food), which do not necessarily appear within the
text, even though we trained the models only with explicitly mentioned aspect
terms (e.g., fajitas) from just 16 reviews - while the accuracy of the
no-prompt baseline is only around 65%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AWGAN: Empowering High-Dimensional Discriminator Output for Generative Adversarial Networks. (arXiv:2109.03378v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Empirically multidimensional discriminator (critic) output can be
advantageous, while a solid explanation for it has not been discussed. In this
paper, (i) we rigorously prove that high-dimensional critic output has
advantage on distinguishing real and fake distributions; (ii) we also introduce
an square-root velocity transformation (SRVT) block which further magnifies
this advantage. The proof is based on our proposed maximal p-centrality
discrepancy which is bounded above by p-Wasserstein distance and perfectly fits
the Wasserstein GAN framework with high-dimensional critic output n. We have
also showed when n &#x3D; 1, the proposed discrepancy is equivalent to 1-Wasserstein
distance. The SRVT block is applied to break the symmetric structure of
high-dimensional critic output and improve the generalization capability of the
discriminator network. In terms of implementation, the proposed framework does
not require additional hyper-parameter tuning, which largely facilitates its
usage. Experiments on image generation tasks show performance improvement on
benchmark datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Attribute-Aligned Strategy for Learning Speech Representation. (arXiv:2106.02810v2 [eess.AS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02810">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Advancement in speech technology has brought convenience to our life.
However, the concern is on the rise as speech signal contains multiple personal
attributes, which would lead to either sensitive information leakage or bias
toward decision. In this work, we propose an attribute-aligned learning
strategy to derive speech representation that can flexibly address these issues
by attribute-selection mechanism. Specifically, we propose a
layered-representation variational autoencoder (LR-VAE), which factorizes
speech representation into attribute-sensitive nodes, to derive an
identity-free representation for speech emotion recognition (SER), and an
emotionless representation for speaker verification (SV). Our proposed method
achieves competitive performances on identity-free SER and a better performance
on emotionless SV, comparing to the current state-of-the-art method of using
adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,
our proposed learning strategy reduces the model and training process needed to
achieve multiple privacy-preserving tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Forward and Inverse models in HCI:Physical simulation and deep learning for inferring 3D finger pose. (arXiv:2109.03366v1 [cs.HC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03366">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We outline the role of forward and inverse modelling approaches in the design
of human--computer interaction systems. Causal, forward models tend to be
easier to specify and simulate, but HCI requires solutions of the inverse
problem. We infer finger 3D position $(x,y,z)$ and pose (pitch and yaw) on a
mobile device using capacitive sensors which can sense the finger up to 5cm
above the screen. We use machine learning to develop data-driven models to
infer position, pose and sensor readings, based on training data from: 1. data
generated by robots, 2. data from electrostatic simulators 3. human-generated
data. Machine learned emulation is used to accelerate the electrostatic
simulation performance by a factor of millions. We combine a Conditional
Variational Autoencoder with domain expertise/models experimentally collected
data. We compare forward and inverse model approaches to direct inference of
finger pose. The combination gives the most accurate reported results on
inferring 3D position and pose with a capacitive sensor on a mobile device.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Simple Video Generation using Neural ODEs. (arXiv:2109.03292v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03292">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Despite having been studied to a great extent, the task of conditional
generation of sequences of frames, or videos, remains extremely challenging. It
is a common belief that a key step towards solving this task resides in
modelling accurately both spatial and temporal information in video signals. A
promising direction to do so has been to learn latent variable models that
predict the future in latent space and project back to pixels, as suggested in
recent literature. Following this line of work and building on top of a family
of models introduced in prior work, Neural ODE, we investigate an approach that
models time-continuous dynamics over a continuous latent space with a
differential equation with respect to time. The intuition behind this approach
is that these trajectories in latent space could then be extrapolated to
generate video frames beyond the time steps for which the model is trained. We
show that our approach yields promising results in the task of future frame
prediction on the Moving MNIST dataset with 1 and 2 digits.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How do I update my model? On the resilience of Predictive Process Monitoring models to change. (arXiv:2109.03501v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03501">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing well investigated Predictive Process Monitoring techniques typically
construct a predictive model based on past process executions, and then use it
to predict the future of new ongoing cases, without the possibility of updating
it with new cases when they complete their execution. This can make Predictive
Process Monitoring too rigid to deal with the variability of processes working
in real environments that continuously evolve and/or exhibit new variant
behaviours over time. As a solution to this problem, we evaluate the use of
three different strategies that allow the periodic rediscovery or incremental
construction of the predictive model so as to exploit new available data. The
evaluation focuses on the performance of the new learned predictive models, in
terms of accuracy and time, against the original one, and uses a number of real
and synthetic datasets with and without explicit Concept Drift. The results
provide an evidence of the potential of incremental learning algorithms for
predicting process monitoring in real environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncertainty Quantification and Experimental Design for large-scale linear Inverse Problems under Gaussian Process Priors. (arXiv:2109.03457v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03457">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the use of Gaussian process (GP) priors for solving inverse
problems in a Bayesian framework. As is well known, the computational
complexity of GPs scales cubically in the number of datapoints. We here show
that in the context of inverse problems involving integral operators, one faces
additional difficulties that hinder inversion on large grids. Furthermore, in
that context, covariance matrices can become too large to be stored. By
leveraging results about sequential disintegrations of Gaussian measures, we
are able to introduce an implicit representation of posterior covariance
matrices that reduces the memory footprint by only storing low rank
intermediate matrices, while allowing individual elements to be accessed
on-the-fly without needing to build full posterior covariance matrices.
Moreover, it allows for fast sequential inclusion of new observations. These
features are crucial when considering sequential experimental design tasks. We
demonstrate our approach by computing sequential data collection plans for
excursion set recovery for a gravimetric inverse problem, where the goal is to
provide fine resolution estimates of high density regions inside the Stromboli
volcano, Italy. Sequential data collection plans are computed by extending the
weighted integrated variance reduction (wIVR) criterion to inverse problems.
Our results show that this criterion is able to significantly reduce the
uncertainty on the excursion volume, reaching close to minimal levels of
residual uncertainty. Overall, our techniques allow the advantages of
probabilistic models to be brought to bear on large-scale inverse problems
arising in the natural sciences.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AgreementLearning: An End-to-End Framework for Learning with Multiple Annotators without Groundtruth. (arXiv:2109.03596v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The annotation of domain experts is important for some medical applications
where the objective groundtruth is ambiguous to define, e.g., the
rehabilitation for some chronic diseases, and the prescreening of some
musculoskeletal abnormalities without further medical examinations. However,
improper uses of the annotations may hinder developing reliable models. On one
hand, forcing the use of a single groundtruth generated from multiple
annotations is less informative for the modeling. On the other hand, feeding
the model with all the annotations without proper regularization is noisy given
existing disagreements. For such issues, we propose a novel agreement learning
framework to tackle the challenge of learning from multiple annotators without
objective groundtruth. The framework has two streams, with one stream fitting
with the multiple annotators and the other stream learning agreement
information between the annotators. In particular, the agreement learning
stream produces regularization information to the classifier stream, tuning its
decision to be better in line with the agreement between the annotators. The
proposed method can be easily plugged to existing backbones developed with
majority-voted groundtruth or multiple annotations. Thereon, experiments on two
medical datasets demonstrate improved agreement levels with annotators.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Learning for Multi-View Ultrasonic Image Fusion. (arXiv:2109.03616v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03616">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Ultrasonic imaging is being used to obtain information about the acoustic
properties of a medium by emitting waves into it and recording their
interaction using ultrasonic transducer arrays. The Delay-And-Sum (DAS)
algorithm forms images using the main path on which reflected signals travel
back to the transducers. In some applications, different insonification paths
can be considered, for instance by placing the transducers at different
locations or if strong reflectors inside the medium are known a-priori. These
different modes give rise to multiple DAS images reflecting different geometric
information about the scatterers and the challenge is to either fuse them into
one image or to directly extract higher-level information regarding the
materials of the medium, e.g., a segmentation map. Traditional image fusion
techniques typically use ad-hoc combinations of pre-defined image transforms,
pooling operations and thresholding. In this work, we propose a deep neural
network (DNN) architecture that directly maps all available data to a
segmentation map while explicitly incorporating the DAS image formation for the
different insonification paths as network layers. This enables information flow
between data pre-processing and image post-processing DNNs, trained end-to-end.
We compare our proposed method to a traditional image fusion technique using
simulated data experiments, mimicking a non-destructive testing application
with four image modes, i.e., two transducer locations and two internal
reflection boundaries. Using our approach, it is possible to obtain much more
accurate segmentation of defects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting Process Name from Network Data. (arXiv:2109.03328v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03328">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The ability to identify applications based on the network data they generate
could be a valuable tool for cyber defense. We report on a machine learning
technique capable of using netflow-like features to predict the application
that generated the traffic. In our experiments, we used ground-truth labels
obtained from host-based sensors deployed in a large enterprise environment; we
applied random forests and multilayer perceptrons to the tasks of browser vs.
non-browser identification, browser fingerprinting, and process name
prediction. For each of these tasks, we demonstrate how machine learning models
can achieve high classification accuracy using only netflow-like features as
the basis for classification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03552">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The widespread presence of offensive language on social media motivated the
development of systems capable of recognizing such content automatically. Apart
from a few notable exceptions, most research on automatic offensive language
identification has dealt with English. To address this shortcoming, we
introduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first
dataset of its kind compiled for Marathi, thus opening a new domain for
research in low-resource Indo-Aryan languages. We present results from several
machine learning experiments on this dataset, including zero-short and other
transfer learning experiments on state-of-the-art cross-lingual transformers
from existing data in Bengali, English, and Hindi.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Signal-domain representation of symbolic music for learning embedding spaces. (arXiv:2109.03454v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03454">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A key aspect of machine learning models lies in their ability to learn
efficient intermediate features. However, the input representation plays a
crucial role in this process, and polyphonic musical scores remain a
particularly complex type of information. In this paper, we introduce a novel
representation of symbolic music data, which transforms a polyphonic score into
a continuous signal. We evaluate the ability to learn meaningful features from
this representation from a musical point of view. Hence, we introduce an
evaluation method relying on principled generation of synthetic data. Finally,
to test our proposed representation we conduct an extensive benchmark against
recent polyphonic symbolic representations. We show that our signal-like
representation leads to better reconstruction and disentangled features. This
improvement is reflected in the metric properties and in the generation ability
of the space learned from our signal-like representation according to music
theory properties.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03502">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank
twice, reaD twice). The pipeline is composed of a retriever, passage reranker,
extractive reader, generative reader and a mechanism that aggregates the final
prediction from all system&#x27;s components. We demonstrate its strength across
three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,
surpassing state-of-the-art on the first two. Our analysis demonstrates that:
(i) combining extractive and generative reader yields absolute improvements up
to 5 exact match and it is at least twice as effective as the posterior
averaging ensemble of the same models with different parameters, (ii) the
extractive reader with fewer parameters can match the performance of the
generative reader on extractive QA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Convergence of Batch Asynchronous Stochastic Approximation With Applications to Reinforcement Learning. (arXiv:2109.03445v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03445">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The stochastic approximation (SA) algorithm is a widely used probabilistic
method for finding a solution to an equation of the form
$\mathbf{f}(\boldsymbol{\theta}) &#x3D; \mathbf{0}$ where $\mathbf{f} : \mathbb{R}^d
\rightarrow \mathbb{R}^d$, when only noisy measurements of $\mathbf{f}(\cdot)$
are available. In the literature to date, one can make a distinction between
&quot;synchronous&quot; updating, whereby the entire vector of the current guess
$\boldsymbol{\theta}_t$ is updated at each time, and &quot;asynchronous&quot; updating,
whereby ony one component of $\boldsymbol{\theta}_t$ is updated. In convex and
nonconvex optimization, there is also the notion of &quot;batch&quot; updating, whereby
some but not all components of $\boldsymbol{\theta}_t$ are updated at each time
$t$. In addition, there is also a distinction between using a &quot;local&quot; clock
versus a &quot;global&quot; clock. In the literature to date, convergence proofs when a
local clock is used make the assumption that the measurement noise is an i.i.d\
sequence, an assumption that does not hold in Reinforcement Learning (RL).

In this note, we provide a general theory of convergence for batch
asymchronous stochastic approximation (BASA), that works whether the updates
use a local clock or a global clock, for the case where the measurement noises
form a martingale difference sequence. This is the most general result to date
and encompasses all others.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating Expected Calibration Errors. (arXiv:2109.03480v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03480">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Uncertainty in probabilistic classifiers predictions is a key concern when
models are used to support human decision making, in broader probabilistic
pipelines or when sensitive automatic decisions have to be taken. Studies have
shown that most models are not intrinsically well calibrated, meaning that
their decision scores are not consistent with posterior probabilities. Hence
being able to calibrate these models, or enforce calibration while learning
them, has regained interest in recent literature. In this context, properly
assessing calibration is paramount to quantify new contributions tackling
calibration. However, there is room for improvement for commonly used metrics
and evaluation of calibration could benefit from deeper analyses. Thus this
paper focuses on the empirical evaluation of calibration metrics in the context
of classification. More specifically it evaluates different estimators of the
Expected Calibration Error ($ECE$), amongst which legacy estimators and some
novel ones, proposed in this paper. We build an empirical procedure to quantify
the quality of these $ECE$ estimators, and use it to decide which estimator
should be used in practice for different settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Clustering-aided Ensemble Method for Predicting Ridesourcing Demand in Chicago. (arXiv:2109.03433v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03433">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Accurately forecasting ridesourcing demand is important for effective
transportation planning and policy-making. With the rise of Artificial
Intelligence (AI), researchers have started to utilize machine learning models
to forecast travel demand, which, in many cases, can produce higher prediction
accuracy than statistical models. However, most existing machine-learning
studies used a global model to predict the demand and ignored the influence of
spatial heterogeneity (i.e., the spatial variations in the impacts of
explanatory variables). Spatial heterogeneity can drive the parameter
estimations varying over space; failing to consider the spatial variations may
limit the model&#x27;s prediction performance. To account for spatial heterogeneity,
this study proposes a Clustering-aided Ensemble Method (CEM) to forecast the
zone-to-zone (census-tract-to-census-tract) travel demand for ridesourcing
services. Specifically, we develop a clustering framework to split the
origin-destination pairs into different clusters and ensemble the
cluster-specific machine learning models for prediction. We implement and test
the proposed methodology by using the ridesourcing-trip data in Chicago. The
results show that, with a more transparent and flexible model structure, the
CEM significantly improves the prediction accuracy than the benchmark models
(i.e., global machine-learning and statistical models directly trained on all
observations). This study offers transportation researchers and practitioners a
new methodology of travel demand forecasting, especially for new travel modes
like ridesourcing and micromobility.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud. (arXiv:2109.03285v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03285">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding the predictions made by machine learning (ML) models and their
potential biases remains a challenging and labor-intensive task that depends on
the application, the dataset, and the specific model. We present Amazon
SageMaker Clarify, an explainability feature for Amazon SageMaker that launched
in December 2020, providing insights into data and ML models by identifying
biases and explaining predictions. It is deeply integrated into Amazon
SageMaker, a fully managed service that enables data scientists and developers
to build, train, and deploy ML models at any scale. Clarify supports bias
detection and feature importance computation across the ML lifecycle, during
data preparation, model evaluation, and post-deployment monitoring. We outline
the desiderata derived from customer input, the modular architecture, and the
methodology for bias and explanation computations. Further, we describe the
technical challenges encountered and the tradeoffs we had to make. For
illustration, we discuss two customer use cases. We present our deployment
results including qualitative customer feedback and a quantitative evaluation.
Finally, we summarize lessons learned, and discuss best practices for the
successful adoption of fairness and explanation tools in practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sample and Communication-Efficient Decentralized Actor-Critic Algorithms with Finite-Time Analysis. (arXiv:2109.03699v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03699">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Actor-critic (AC) algorithms have been widely adopted in decentralized
multi-agent systems to learn the optimal joint control policy. However,
existing decentralized AC algorithms either do not preserve the privacy of
agents or are not sample and communication-efficient. In this work, we develop
two decentralized AC and natural AC (NAC) algorithms that are private, and
sample and communication-efficient. In both algorithms, agents share noisy
information to preserve privacy and adopt mini-batch updates to improve sample
and communication efficiency. Particularly for decentralized NAC, we develop a
decentralized Markovian SGD algorithm with an adaptive mini-batch size to
efficiently compute the natural policy gradient. Under Markovian sampling and
linear function approximation, we prove the proposed decentralized AC and NAC
algorithms achieve the state-of-the-art sample complexities
$\mathcal{O}\big(\epsilon^{-2}\ln(\epsilon^{-1})\big)$ and
$\mathcal{O}\big(\epsilon^{-3}\ln(\epsilon^{-1})\big)$, respectively, and the
same small communication complexity
$\mathcal{O}\big(\epsilon^{-1}\ln(\epsilon^{-1})\big)$. Numerical experiments
demonstrate that the proposed algorithms achieve lower sample and communication
complexities than the existing decentralized AC algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Deep Reinforcement Learning Approach for Constrained Online Logistics Route Assignment. (arXiv:2109.03467v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03467">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As online shopping prevails and e-commerce platforms emerge, there is a
tremendous number of parcels being transported every day. Thus, it is crucial
for the logistics industry on how to assign a candidate logistics route for
each shipping parcel properly as it leaves a significant impact on the total
logistics cost optimization and business constraints satisfaction such as
transit hub capacity and delivery proportion of delivery providers. This online
route-assignment problem can be viewed as a constrained online decision-making
problem. Notably, the large amount (beyond ${10^5}$) of daily parcels, the
variability and non-Markovian characteristics of parcel information impose
difficulties on attaining (near-) optimal solution without violating
constraints excessively. In this paper, we develop a model-free DRL approach
named PPO-RA, in which Proximal Policy Optimization (PPO) is improved with
dedicated techniques to address the challenges for route assignment (RA). The
actor and critic networks use attention mechanism and parameter sharing to
accommodate each incoming parcel with varying numbers and identities of
candidate routes, without modeling non-Markovian parcel arriving dynamics since
we make assumption of i.i.d. parcel arrival. We use recorded delivery parcel
data to evaluate the performance of PPO-RA by comparing it with widely-used
baselines via simulation. The results show the capability of the proposed
approach to achieve considerable cost savings while satisfying most
constraints.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Preprocessing and Modeling of Radial Fan Data for Health State Prediction. (arXiv:2109.03468v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Monitoring critical components of systems is a crucial step towards failure
safety. Affordable sensors are available and the industry is in the process of
introducing and extending monitoring solutions to improve product quality.
Often, no expertise of how much data is required for a certain task (e.g.
monitoring) exists. Especially in vital machinery, a trend to exaggerated
sensors may be noticed, both in quality and in quantity. This often results in
an excessive generation of data, which should be transferred, processed and
stored nonetheless. In a previous case study, several sensors have been mounted
on a healthy radial fan, which was later artificially damaged. The gathered
data was used for modeling (and therefore monitoring) a healthy state. The
models were evaluated on a dataset created by using a faulty impeller. This
paper focuses on the reduction of this data through downsampling and binning.
Different models are created with linear regression and random forest
regression and the resulting difference in quality is discussed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">U-FNO -- an enhanced Fourier neural operator based-deep learning model for multiphase flow. (arXiv:2109.03697v1 [physics.geo-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03697">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Numerical simulation of multiphase flow in porous media is essential for many
geoscience applications. However, due to the multi-physics, non-linear, and
multi-scale problem nature, these simulations are very expensive at desirable
grid resolutions, and the computational cost often impedes rigorous engineering
decision-making. Machine learning methods provide faster alternatives to
traditional simulators by training neural network models with numerical
simulation data mappings. Traditional convolutional neural network (CNN)-based
models are accurate yet data-intensive and are prone to overfitting. Here we
present a new architecture, U-FNO, an enhanced Fourier neural operator for
solving the multiphase flow problem. The U-FNO is designed based on the Fourier
neural operator (FNO) that learns an integral kernel in the Fourier space.
Through a systematic comparison among a CNN benchmark and three types of FNO
variations on a CO2-water multiphase problem in the context of CO2 geological
storage, we show that the U-FNO architecture has the advantages of both
traditional CNN and original FNO, providing significantly more accurate and
efficient performance than previous architectures. The trained U-FNO provides
gas saturation and pressure buildup predictions with a 10,000 times speedup
compared to traditional numerical simulators while maintaining similar
accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Higher Order Kernel Mean Embeddings to Capture Filtrations of Stochastic Processes. (arXiv:2109.03582v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03582">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Stochastic processes are random variables with values in some space of paths.
However, reducing a stochastic process to a path-valued random variable ignores
its filtration, i.e. the flow of information carried by the process through
time. By conditioning the process on its filtration, we introduce a family of
higher order kernel mean embeddings (KMEs) that generalizes the notion of KME
and captures additional information related to the filtration. We derive
empirical estimators for the associated higher order maximum mean discrepancies
(MMDs) and prove consistency. We then construct a filtration-sensitive kernel
two-sample test able to pick up information that gets missed by the standard
MMD test. In addition, leveraging our higher order MMDs we construct a family
of universal kernels on stochastic processes that allows to solve real-world
calibration and optimal stopping problems in quantitative finance (such as the
pricing of American options) via classical kernel-based regression methods.
Finally, adapting existing tests for conditional independence to the case of
stochastic processes, we design a causal-discovery algorithm to recover the
causal graph of structural dependencies among interacting bodies solely from
observations of their multidimensional trajectories.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Computing on Functions Using Randomized Vector Representations. (arXiv:2109.03429v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03429">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vector space models for symbolic processing that encode symbols by random
vectors have been proposed in cognitive science and connectionist communities
under the names Vector Symbolic Architecture (VSA), and, synonymously,
Hyperdimensional (HD) computing. In this paper, we generalize VSAs to function
spaces by mapping continuous-valued data into a vector space such that the
inner product between the representations of any two data points represents a
similarity kernel. By analogy to VSA, we call this new function encoding and
computing framework Vector Function Architecture (VFA). In VFAs, vectors can
represent individual data points as well as elements of a function space (a
reproducing kernel Hilbert space). The algebraic vector operations, inherited
from VSA, correspond to well-defined operations in function space. Furthermore,
we study a previously proposed method for encoding continuous data, fractional
power encoding (FPE), which uses exponentiation of a random base vector to
produce randomized representations of data points and fulfills the kernel
properties for inducing a VFA. We show that the distribution from which
elements of the base vector are sampled determines the shape of the FPE kernel,
which in turn induces a VFA for computing with band-limited functions. In
particular, VFAs provide an algebraic framework for implementing large-scale
kernel machines with random features, extending Rahimi and Recht, 2007.
Finally, we demonstrate several applications of VFA models to problems in image
recognition, density estimation and nonlinear regression. Our analyses and
results suggest that VFAs constitute a powerful new framework for representing
and manipulating functions in distributed neural systems, with myriad
applications in artificial intelligence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single Plane-Wave Imaging using Physics-Based Deep Learning. (arXiv:2109.03661v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03661">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In plane-wave imaging, multiple unfocused ultrasound waves are transmitted
into a medium of interest from different angles and an image is formed from the
recorded reflections. The number of plane waves used leads to a trade-off
between frame-rate and image quality, with single-plane-wave (SPW) imaging
being the fastest possible modality with the worst image quality. Recently,
deep learning methods have been proposed to improve ultrasound imaging. One
approach is to use image-to-image networks that work on the formed image and
another is to directly learn a mapping from data to an image. Both approaches
utilize purely data-driven models and require deep, expressive network
architectures, combined with large numbers of training samples to obtain good
results. Here, we propose a data-to-image architecture that incorporates a
wave-physics-based image formation algorithm in-between deep convolutional
neural networks. To achieve this, we implement the Fourier (FK) migration
method as network layers and train the whole network end-to-end. We compare our
proposed data-to-image network with an image-to-image network in simulated data
experiments, mimicking a medical ultrasound application. Experiments show that
it is possible to obtain high-quality SPW images, almost similar to an image
formed using 75 plane waves over an angular range of $\pm$16$^\circ$. This
illustrates the great potential of combining deep neural networks with
physics-based image formation algorithms for SPW imaging.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Shuffled Patch-Wise Supervision for Presentation Attack Detection. (arXiv:2109.03484v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03484">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Face anti-spoofing is essential to prevent false facial verification by using
a photo, video, mask, or a different substitute for an authorized person&#x27;s
face. Most of the state-of-the-art presentation attack detection (PAD) systems
suffer from overfitting, where they achieve near-perfect scores on a single
dataset but fail on a different dataset with more realistic data. This problem
drives researchers to develop models that perform well under real-world
conditions. This is an especially challenging problem for frame-based
presentation attack detection systems that use convolutional neural networks
(CNN). To this end, we propose a new PAD approach, which combines pixel-wise
binary supervision with patch-based CNN. We believe that training a CNN with
face patches allows the model to distinguish spoofs without learning background
or dataset-specific traces. We tested the proposed method both on the standard
benchmark datasets -- Replay-Mobile, OULU-NPU -- and on a real-world dataset.
The proposed approach shows its superiority on challenging experimental setups.
Namely, it achieves higher performance on OULU-NPU protocol 3, 4 and on
inter-dataset real-world experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tactile Image-to-Image Disentanglement of Contact Geometry from Motion-Induced Shear. (arXiv:2109.03615v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03615">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robotic touch, particularly when using soft optical tactile sensors, suffers
from distortion caused by motion-dependent shear. The manner in which the
sensor contacts a stimulus is entangled with the tactile information about the
geometry of the stimulus. In this work, we propose a supervised convolutional
deep neural network model that learns to disentangle, in the latent space, the
components of sensor deformations caused by contact geometry from those due to
sliding-induced shear. The approach is validated by reconstructing unsheared
tactile images from sheared images and showing they match unsheared tactile
images collected with no sliding motion. In addition, the unsheared tactile
images give a faithful reconstruction of the contact geometry that is not
possible from the sheared data, and robust estimation of the contact pose that
can be used for servo control sliding around various 2D shapes. Finally, the
contact geometry reconstruction in conjunction with servo control sliding were
used for faithful full object reconstruction of various 2D shapes. The methods
have broad applicability to deep learning models for robots with a
shear-sensitive sense of touch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Power to the Relational Inductive Bias: Graph Neural Networks in Electrical Power Grids. (arXiv:2109.03604v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03604">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of graph neural networks (GNNs) to the domain of electrical
power grids has high potential impact on smart grid monitoring. Even though
there is a natural correspondence of power flow to message-passing in GNNs,
their performance on power grids is not well-understood. We argue that there is
a gap between GNN research driven by benchmarks which contain graphs that
differ from power grids in several important aspects. Additionally, inductive
learning of GNNs across multiple power grid topologies has not been explored
with real-world data. We address this gap by means of (i) defining power grid
graph datasets in inductive settings, (ii) an exploratory analysis of graph
properties, and (iii) an empirical study of the concrete learning task of state
estimation on real-world power grids. Our results show that GNNs are more
robust to noise with up to 400% lower error compared to baselines. Furthermore,
due to the unique properties of electrical grids, we do not observe the well
known over-smoothing phenomenon of GNNs and find the best performing models to
be exceptionally deep with up to 13 layers. This is in stark contrast to
existing benchmark datasets where the consensus is that 2 to 3 layer GNNs
perform best. Our results demonstrate that a key challenge in this domain is to
effectively handle long-range dependence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FaBiAN: A Fetal Brain magnetic resonance Acquisition Numerical phantom. (arXiv:2109.03624v1 [physics.med-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03624">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Accurate characterization of in utero human brain maturation is critical as
it involves complex and interconnected structural and functional processes that
may influence health later in life. Magnetic resonance imaging is a powerful
tool to investigate equivocal neurological patterns during fetal development.
However, the number of acquisitions of satisfactory quality available in this
cohort of sensitive subjects remains scarce, thus hindering the validation of
advanced image processing techniques. Numerical phantoms can mitigate these
limitations by providing a controlled environment with a known ground truth. In
this work, we present FaBiAN, an open-source Fetal Brain magnetic resonance
Acquisition Numerical phantom that simulates clinical T2-weighted fast spin
echo sequences of the fetal brain. This unique tool is based on a general,
flexible and realistic setup that includes stochastic fetal movements, thus
providing images of the fetal brain throughout maturation comparable to
clinical acquisitions. We demonstrate its value to evaluate the robustness and
optimize the accuracy of an algorithm for super-resolution fetal brain magnetic
resonance imaging from simulated motion-corrupted 2D low-resolution series as
compared to a synthetic high-resolution reference volume. We also show that the
images generated can complement clinical datasets to support data-intensive
deep learning methods for fetal brain tissue segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RepNAS: Searching for Efficient Re-parameterizing Blocks. (arXiv:2109.03508v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03508">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In the past years, significant improvements in the field of neural
architecture search(NAS) have been made. However, it is still challenging to
search for efficient networks due to the gap between the searched constraint
and real inference time exists. To search for a high-performance network with
low inference time, several previous works set a computational complexity
constraint for the search algorithm. However, many factors affect the speed of
inference(e.g., FLOPs, MACs). The correlation between a single indicator and
the latency is not strong. Currently, some re-parameterization(Rep) techniques
are proposed to convert multi-branch to single-path architecture which is
inference-friendly. Nevertheless, multi-branch architectures are still
human-defined and inefficient. In this work, we propose a new search space that
is suitable for structural re-parameterization techniques. RepNAS, a one-stage
NAS approach, is present to efficiently search the optimal diverse branch
block(ODBB) for each layer under the branch number constraint. Our experimental
results show the searched ODBB can easily surpass the manual diverse branch
block(DBB) with efficient training. Code and models will be available sooner.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding and Preparing Data of Industrial Processes for Machine Learning Applications. (arXiv:2109.03469v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03469">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Industrial applications of machine learning face unique challenges due to the
nature of raw industry data. Preprocessing and preparing raw industrial data
for machine learning applications is a demanding task that often takes more
time and work than the actual modeling process itself and poses additional
challenges. This paper addresses one of those challenges, specifically, the
challenge of missing values due to sensor unavailability at different
production units of nonlinear production lines. In cases where only a small
proportion of the data is missing, those missing values can often be imputed.
In cases of large proportions of missing data, imputing is often not feasible,
and removing observations containing missing values is often the only option.
This paper presents a technique, that allows to utilize all of the available
data without the need of removing large amounts of observations where data is
only partially available. We do not only discuss the principal idea of the
presented method, but also show different possible implementations that can be
applied depending on the data at hand. Finally, we demonstrate the application
of the presented method with data from a steel production plant.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-08">2021-09-08</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An individual&#x27;s variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03171">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent work on opinion summarization produces general summaries based on a
set of input reviews and the popularity of opinions expressed in them. In this
paper, we propose an approach that allows the generation of customized
summaries based on aspect queries (e.g., describing the location and room of a
hotel). Using a review corpus, we create a synthetic training dataset of
(review, summary) pairs enriched with aspect controllers which are induced by a
multi-instance learning model that predicts the aspects of a document at
different levels of granularity. We fine-tune a pretrained model using our
synthetic dataset and generate aspect-specific summaries by modifying the
aspect controllers. Experiments on two benchmarks show that our model
outperforms the previous state of the art and generates personalized summaries
by controlling the number of aspects discussed in them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03094">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The availability of language representations learned by large pretrained
neural network models (such as BERT and ELECTRA) has led to improvements in
many downstream Natural Language Processing tasks in recent years. Pretrained
models usually differ in pretraining objectives, architectures, and datasets
they are trained on which can affect downstream performance. In this
contribution, we fine-tuned German BERT and German ELECTRA models to identify
toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)
in Facebook data provided by the GermEval 2021 competition. We created
ensembles of these models and investigated whether and how classification
performance depends on the number of ensemble members and their composition. On
out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for
all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,
respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02995">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the most popular methods for context-aware machine translation (MT) is
to use separate encoders for the source sentence and context as multiple
sources for one target sentence. Recent work has cast doubt on whether these
models actually learn useful signals from the context or are improvements in
automatic evaluation metrics just a side-effect. We show that multi-source
transformer models improve MT over standard transformer-base models even with
empty lines provided as context, but the translation quality improves
significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is
provided. We also show that even though randomly shuffling in-domain context
can also improve over baselines, the correct context further improves
translation quality and random out-of-domain context further degrades it.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Over the years customers&#x27; expectation of getting information instantaneously
has given rise to the increased usage of channels like virtual assistants.
Typically, customers try to get their questions answered by low-touch channels
like search and virtual assistant first, before getting in touch with a live
chat agent or the phone representative. Higher usage of these low-touch systems
is a win-win for both customers and the organization since it enables
organizations to attain a low cost of service while customers get served
without delay. In this paper, we propose a two-part framework where the first
part describes methods to combine the information from different interaction
channels like call, search, and chat. We do this by summarizing (using a
stacked Bi-LSTM network) the high-touch interaction channel data such as call
and chat into short searchquery like customer intents and then creating an
organically grown intent taxonomy from interaction data (using Hierarchical
Agglomerative Clustering). The second part of the framework focuses on
extracting customer questions by analyzing interaction data sources. It
calculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It
also maps these identified questions to the output of the first part of the
framework using syntactic and semantic similarity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business&#x27;s financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03881">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities&#x27; stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities&#x27; stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13140">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep learning models have greatly improved the performance of most
artificial intelligence tasks, they are often criticized to be untrustworthy
due to the black-box problem. Consequently, many works have been proposed to
study the trustworthiness of deep learning. However, as most open datasets are
designed for evaluating the accuracy of model outputs, there is still a lack of
appropriate datasets for evaluating the inner workings of neural networks. The
lack of datasets obviously hinders the development of trustworthiness research.
Therefore, in order to systematically evaluate the factors for building
trustworthy systems, we propose a novel and well-annotated sentiment analysis
dataset to evaluate robustness and interpretability. To evaluate these factors,
our dataset contains diverse annotations about the challenging distribution of
instances, manual adversarial instances and sentiment explanations. Several
evaluation metrics are further proposed for interpretability and robustness.
Based on the dataset and metrics, we conduct comprehensive comparisons for the
trustworthiness of three typical models, and also study the relations between
accuracy, robustness and interpretability. We release this trustworthiness
evaluation dataset at \url{https://github/xyz} and hope our work can facilitate
the progress on building more trustworthy systems for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00904">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce MULTI-EURLEX, a new multilingual dataset for topic
classification of legal documents. The dataset comprises 65k European Union
(EU) laws, officially translated in 23 languages, annotated with multiple
labels from the EUROVOC taxonomy. We highlight the effect of temporal concept
drift and the importance of chronological, instead of random splits. We use the
dataset as a testbed for zero-shot cross-lingual transfer, where we exploit
annotated training documents in one language (source) to classify documents in
another language (target). We find that fine-tuning a multilingually pretrained
model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic
forgetting of multilingual knowledge and, consequently, poor zero-shot transfer
to other languages. Adaptation strategies, namely partial fine-tuning,
adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new
end-tasks, help retain multilingual knowledge from pretraining, substantially
improving zero-shot cross-lingual transfer, but their impact also depends on
the pretrained model used and the size of the label set.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03121">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Simultaneous neural machine translation(SNMT) models start emitting the
target sequence before they have processed the source sequence. The recent
adaptive policies for SNMT use monotonic attention to perform read/write
decisions based on the partial source and target sequences. The lack of
sufficient information might cause the monotonic attention to take poor
read/write decisions, which in turn negatively affects the performance of the
SNMT model. On the other hand, human translators make better read/write
decisions since they can anticipate the immediate future words using linguistic
information and domain knowledge.Motivated by human translators, in this work,
we propose a framework to aid monotonic attention with an external language
model to improve its decisions.We conduct experiments on the MuST-C
English-German and English-French speech-to-text translation tasks to show the
effectiveness of the proposed framework.The proposed SNMT method improves the
quality-latency trade-off over the state-of-the-art monotonic multihead
attention.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02403">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Aspect-level sentiment classification (ALSC) aims at identifying the
sentiment polarity of a specified aspect in a sentence. ALSC is a practical
setting in aspect-based sentiment analysis due to no opinion term labeling
needed, but it fails to interpret why a sentiment polarity is derived for the
aspect. To address this problem, recent works fine-tune pre-trained Transformer
encoders for ALSC to extract an aspect-centric dependency tree that can locate
the opinion words. However, the induced opinion words only provide an intuitive
cue far below human-level interpretability. Besides, the pre-trained encoder
tends to internalize an aspect&#x27;s intrinsic sentiment, causing sentiment bias
and thus affecting model performance. In this paper, we propose a span-based
anti-bias aspect representation learning framework. It first eliminates the
sentiment bias in the aspect embedding by adversarial learning against aspects&#x27;
prior sentiment. Then, it aligns the distilled opinion candidates with the
aspect by span-based dependency modeling to highlight the interpretable opinion
terms. Our method achieves new state-of-the-art performance on five benchmarks,
with the capability of unsupervised opinion extraction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people&#x27;s perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers-based pretrained language models achieve outstanding results in
many well-known NLU benchmarks. However, while pretraining methods are very
convenient, they are expensive in terms of time and resources. This calls for a
study of the impact of pretraining data size on the knowledge of the models. We
explore this impact on the syntactic capabilities of RoBERTa, using models
trained on incremental sizes of raw text data. First, we use syntactic
structural probes to determine whether models pretrained on more data encode a
higher amount of syntactic information. Second, we perform a targeted syntactic
evaluation to analyze the impact of pretraining data size on the syntactic
generalization performance of the models. Third, we compare the performance of
the different models on three downstream applications: part-of-speech tagging,
dependency parsing and paraphrase identification. We complement our study with
an analysis of the cost-benefit trade-off of training such models. Our
experiments show that while models pretrained on more data encode more
syntactic knowledge and perform better on downstream applications, they do not
always offer a better performance across the different syntactic phenomena and
come at a higher financial and environmental cost.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous work in slogan generation focused on utilising slogan skeletons
mined from existing slogans. While some generated slogans can be catchy, they
are often not coherent with the company&#x27;s focus or style across their marketing
communications because the skeletons are mined from other companies&#x27; slogans.
We propose a sequence-to-sequence (seq2seq) transformer model to generate
slogans from a brief company description. A naive seq2seq model fine-tuned for
slogan generation is prone to introducing false information. We use company
name delexicalisation and entity masking to alleviate this problem and improve
the generated slogans&#x27; quality and truthfulness. Furthermore, we apply
conditional training based on the first words&#x27; POS tag to generate
syntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score
of 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that
our method generates significantly more factual, diverse and catchy slogans
than strong LSTM and transformer seq2seq baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03598">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Intersectional bias is a bias caused by an overlap of multiple social factors
like gender, sexuality, race, disability, religion, etc. A recent study has
shown that word embedding models can be laden with biases against
intersectional groups like African American females, etc. The first step
towards tackling such intersectional biases is to identify them. However,
discovering biases against different intersectional groups remains a
challenging task. In this work, we present WordBias, an interactive visual tool
designed to explore biases against intersectional groups encoded in static word
embeddings. Given a pretrained static word embedding, WordBias computes the
association of each word along different groups based on race, age, etc. and
then visualizes them using a novel interactive interface. Using a case study,
we demonstrate how WordBias can help uncover biases against intersectional
groups like Black Muslim Males, Poor Females, etc. encoded in word embedding.
In addition, we also evaluate our tool using qualitative feedback from expert
interviews. The source code for this tool can be publicly accessed for
reproducibility at github.com/bhavyaghai/WordBias.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generating paragraphs of diverse contents is important in many applications.
Existing generation models produce similar contents from homogenized contexts
due to the fixed left-to-right sentence order. Our idea is permuting the
sentence orders to improve the content diversity of multi-sentence paragraph.
We propose a novel framework PermGen whose objective is to maximize the
expected log-likelihood of output paragraph distributions with respect to all
possible sentence orders. PermGen uses hierarchical positional embedding and
designs new procedures for training, decoding, and candidate ranking in the
sentence-permuted generation. Experiments on three paragraph generation
benchmarks demonstrate PermGen generates more diverse outputs with a higher
quality than existing models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07555">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>QuestEval is a reference-less metric used in text-to-text tasks, that
compares the generated summaries directly to the source text, by automatically
asking and answering questions. Its adaptation to Data-to-Text tasks is not
straightforward, as it requires multimodal Question Generation and Answering
systems on the considered tasks, which are seldom available. To this purpose,
we propose a method to build synthetic multimodal corpora enabling to train
multimodal components for a data-QuestEval metric. The resulting metric is
reference-less and multimodal; it obtains state-of-the-art correlations with
human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval&#x27;s
code and models available for reproducibility purpose, as part of the QuestEval
project.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The semantic understanding of natural dialogues composes of several parts.
Some of them, like intent classification and entity detection, have a crucial
role in deciding the next steps in handling user input. Handling each task as
an individual problem can be wasting of training resources, and also each
problem can benefit from each other. This paper tackles these problems as one.
Our new model, which combine intent and entity recognition into one system, is
achieving better metrics in both tasks with lower training requirements than
solving each task separately. We also optimize the model based on the inputs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03199">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversation disentanglement aims to separate intermingled messages into
detached sessions, which is a fundamental task in understanding multi-party
conversations. Existing work on conversation disentanglement relies heavily
upon human-annotated datasets, which are expensive to obtain in practice. In
this work, we explore to train a conversation disentanglement model without
referencing any human annotations. Our method is built upon a deep co-training
algorithm, which consists of two neural networks: a message-pair classifier and
a session classifier. The former is responsible for retrieving local relations
between two messages while the latter categorizes a message to a session by
capturing context-aware information. Both networks are initialized respectively
with pseudo data built from an unannotated corpus. During the deep co-training
process, we use the session classifier as a reinforcement learning component to
learn a session assigning policy by maximizing the local rewards given by the
message-pair classifier. For the message-pair classifier, we enrich its
training data by retrieving message pairs with high confidence from the
disentangled sessions predicted by the session classifier. Experimental results
on the large Movie Dialogue Dataset demonstrate that our proposed approach
achieves competitive performance compared to the previous supervised methods.
Further experiments show that the predicted disentangled conversations can
promote the performance on the downstream task of multi-party response
selection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Practical dialogue systems require robust methods of detecting out-of-scope
(OOS) utterances to avoid conversational breakdowns and related failure modes.
Directly training a model with labeled OOS examples yields reasonable
performance, but obtaining such data is a resource-intensive process. To tackle
this limited-data problem, previous methods focus on better modeling the
distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal
technique that augments existing data to train better OOS detectors operating
in low-data regimes. GOLD generates pseudo-labeled candidates using samples
from an auxiliary dataset and keeps only the most beneficial candidates for
training through a novel filtering mechanism. In experiments across three
target benchmarks, the top GOLD model outperforms all existing methods on all
key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median
baseline performance. We also analyze the unique properties of OOS data to
identify key factors for optimally applying our proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03009">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, large pre-trained neural language models have attained remarkable
performance on many downstream natural language processing (NLP) applications
via fine-tuning. In this paper, we target at how to further improve the token
representations on the language models. We, therefore, propose a simple yet
effective plug-and-play module, Sequential Attention Module (SAM), on the token
embeddings learned from a pre-trained language model. Our proposed SAM consists
of two main attention modules deployed sequentially: Feature-wise Attention
Module (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can
effectively identify the importance of features at each dimension and promote
the effect via dot-product on the original token embeddings for downstream NLP
applications. Meanwhile, TAM can further re-weight the features at the
token-wise level. Moreover, we propose an adaptive filter on FAM to prevent
noise impact and increase information absorption. Finally, we conduct extensive
experiments to demonstrate the advantages and properties of our proposed SAM.
We first show how SAM plays a primary role in the champion solution of two
subtasks of SemEval&#x27;21 Task 7. After that, we apply SAM on sentiment analysis
and three popular NLP tasks and demonstrate that SAM consistently outperforms
the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03175">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Differential privacy provides a formal approach to privacy of individuals.
Applications of differential privacy in various scenarios, such as protecting
users&#x27; original utterances, must satisfy certain mathematical properties. Our
contribution is a formal analysis of ADePT, a differentially private
auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising
results on downstream tasks while providing tight privacy guarantees. Our proof
reveals that ADePT is not differentially private, thus rendering the
experimental results unsubstantiated. We also quantify the impact of the error
in its private mechanism, showing that the true sensitivity is higher by at
least factor 6 in an optimistic case of a very small encoder&#x27;s dimension and
that the amount of utterances that are not privatized could easily reach 100%
of the entire dataset. Our intention is neither to criticize the authors, nor
the peer-reviewing process, but rather point out that if differential privacy
applications in NLP rely on formal guarantees, these should be outlined in full
and put under detailed scrutiny.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, &amp; Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02966">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we describe the methods we used for our submissions to the
GermEval 2021 shared task on the identification of toxic, engaging, and
fact-claiming comments. For all three subtasks we fine-tuned freely available
transformer-based models from the Huggingface model hub. We evaluated the
performance of various pre-trained models after fine-tuning on 80% of the
training data with different hyperparameters and submitted predictions of the
two best performing resulting models. We found that this approach worked best
for subtask 3, for which we achieved an F1-score of 0.736.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03127">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite advances in neural network language model, the representation
degeneration problem of embeddings is still challenging. Recent studies have
found that the learned output embeddings are degenerated into a narrow-cone
distribution which makes the similarity between each embeddings positive. They
analyzed the cause of the degeneration problem has been demonstrated as common
to most embeddings. However, we found that the degeneration problem is
especially originated from the training of embeddings of rare words. In this
study, we analyze the intrinsic mechanism of the degeneration of rare word
embeddings with respect of their gradient about the negative log-likelihood
loss function. Furthermore, we theoretically and empirically demonstrate that
the degeneration of rare word embeddings causes the degeneration of non-rare
word embeddings, and that the overall degeneration problem can be alleviated by
preventing the degeneration of rare word embeddings. Based on our analyses, we
propose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the
degeneration problem. Experimental results demonstrate the effectiveness of the
proposed method qualitatively and quantitatively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generate &amp; Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03034">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Math word problem (MWP) is a challenging and critical task in natural
language processing. Many recent studies formalize MWP as a generation task and
have adopted sequence-to-sequence models to transform problem descriptions to
mathematical expressions. However, mathematical expressions are prone to minor
mistakes while the generation objective does not explicitly handle such
mistakes. To address this limitation, we devise a new ranking task for MWP and
propose Generate &amp; Rank, a multi-task framework based on a generative
pre-trained language model. By joint training with generation and ranking, the
model learns from its own mistakes and is able to distinguish between correct
and incorrect expressions. Meanwhile, we perform tree-based disturbance
specially designed for MWP and an online update to boost the ranker. We
demonstrate the effectiveness of our proposed method on the benchmark and the
results show that our method consistently outperforms baselines in all
datasets. Particularly, in the classical Math23k, our method is 7% (78.4%
$\rightarrow$ 85.4%) higher than the state-of-the-art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Advertising is an important revenue source for many companies. However, it is
expensive to manually create advertisements that meet the needs of various
queries for massive items. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisements for different queries with various needs given the item
keywords. In this task, for many different queries there is only one general
purposed advertisement with no predefined query-advertisement pair, which would
discourage traditional End-to-End models from generating query-variant
advertisements for different queries with different needs. To deal with the
problem, we propose a query-variant advertisement text generation model that
takes keywords and associated external knowledge as input during training and
adds different queries during inference. Adding external knowledge helps the
model adapted to the information besides the item keywords during training,
which makes the transition between training and inference more smoothing when
the query is added during inference. Both automatic and human evaluation show
that our model can generate more attractive and query-focused advertisements
than the strong baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03062">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multitask deep learning has been applied to patient outcome prediction from
text, taking clinical notes as input and training deep neural networks with a
joint loss function of multiple tasks. However, the joint training scheme of
multitask learning suffers from inter-task interference, and diagnosis
prediction among the multiple tasks has the generalizability issue due to rare
diseases or unseen diagnoses. To solve these challenges, we propose a
hypernetwork-based approach that generates task-conditioned parameters and
coefficients of multitask prediction heads to learn task-specific prediction
and balance the multitask learning. We also incorporate semantic task
information to improves the generalizability of our task-conditioned multitask
model. Experiments on early and discharge notes extracted from the real-world
MIMIC database show our method can achieve better performance on multitask
patient outcome prediction than strong baselines in most cases. Besides, our
method can effectively handle the scenario with limited information and improve
zero-shot prediction on unseen diagnosis categories.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02938">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15455">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper explores augmenting monolingual data for knowledge distillation in
neural machine translation. Source language monolingual text can be
incorporated as a forward translation. Interestingly, we find the best way to
incorporate target language monolingual text is to translate it to the source
language and round-trip translate it back to the target language, resulting in
a fully synthetic corpus. We find that combining monolingual data from both
source and target languages yields better performance than a corpus twice as
large only in one language. Moreover, experiments reveal that the improvement
depends upon the provenance of the test set. If the test set was originally in
the source language (with the target side written by translators), then forward
translating source monolingual data matters. If the test set was originally in
the target language (with the source written by translators), then
incorporating target monolingual data matters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02905">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel Chain Guided Retriever-reader ({\tt CGR}) framework to
model the reasoning chain for multi-hop Science Question Answering. Our
framework is capable of performing explainable reasoning without the need of
any corpus-specific annotations, such as the ground-truth reasoning chain, or
human-annotated entity mentions. Specifically, we first generate reasoning
chains from a semantic graph constructed by Abstract Meaning Representation of
retrieved evidence facts. A \textit{Chain-aware loss}, concerning both local
and global chain information, is also designed to enable the generated chains
to serve as distant supervision signals for training the retriever, where
reinforcement learning is also adopted to maximize the utility of the reasoning
chains. Our framework allows the retriever to capture step-by-step clues of the
entire reasoning process, which is not only shown to be effective on two
challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,
but also favors explainability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present IndicBART, a multilingual, sequence-to-sequence
pre-trained model focusing on 11 Indic languages and English. Different from
existing pre-trained models, IndicBART utilizes the orthographic similarity
between Indic scripts to improve transfer learning between similar Indic
languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation
(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs
and extreme summarization for 7 languages using multilingual fine-tuning show
that IndicBART is competitive with or better than mBART50 despite containing
significantly fewer parameters. Our analyses focus on identifying the impact of
script unification (to Devanagari), corpora size as well as multilingualism on
the final performance. The IndicBART model is available under the MIT license
at https://indicnlp.ai4bharat.org/indic-bart .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Online hate speech has caught everyone&#x27;s attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity&#x27;s identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03004">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>One challenge for dialogue agents is to recognize feelings of the
conversation partner and respond accordingly. In this work, RoBERTa-GPT2 is
proposed for empathetic dialogue generation, where the pre-trained
auto-encoding RoBERTa is utilised as encoder and the pre-trained
auto-regressive GPT-2 as decoder. With the combination of the pre-trained
RoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.
To enable the empathetic ability of RoBERTa-GPT2 model, we propose a
commonsense knowledge and emotional concepts extractor, in which the
commonsensible and emotional concepts of dialogue context are extracted for the
GPT-2 decoder. The experiment results demonstrate that the empathetic dialogue
generation benefits from both pre-trained encoder-decoder architecture and
external knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Don&#x27;t Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper introduces a novel approach to learn visually grounded meaning
representations of words as low-dimensional node embeddings on an underlying
graph hierarchy. The lower level of the hierarchy models modality-specific word
representations through dedicated but communicating graphs, while the higher
level puts these representations together on a single graph to learn a
representation jointly from both modalities. The topology of each graph models
similarity relations among words, and is estimated jointly with the graph
embedding. The assumption underlying this model is that words sharing similar
meaning correspond to communities in an underlying similarity graph in a
low-dimensional space. We named this model Hierarchical Multi-Modal Similarity
Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE
to simulate human similarity judgements and concept categorization,
outperforming the state of the art.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02882">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Human-designed rules are widely used to build industry applications. However,
it is infeasible to maintain thousands of such hand-crafted rules. So it is
very important to integrate the rule knowledge into neural networks to build a
hybrid model that achieves better performance. Specifically, the human-designed
rules are formulated as Regular Expressions (REs), from which the equivalent
Minimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to
use the MDFA as an intermediate model to capture the matched RE patterns as
rule-based features for each input sentence and introduce these additional
features into neural networks. We evaluate the proposed method on the ATIS
intent classification task. The experiment results show that the proposed
method achieves the best performance compared to neural networks and four other
methods that combine REs and neural networks when the training dataset is
relatively small.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02950">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a new paradigm for paraphrase generation by
treating the task as unsupervised machine translation (UMT) based on the
assumption that there must be pairs of sentences expressing the same meaning in
a large-scale unlabeled monolingual corpus. The proposed paradigm first splits
a large unlabeled corpus into multiple clusters, and trains multiple UMT models
using pairs of these clusters. Then based on the paraphrase pairs produced by
these UMT models, a unified surrogate model can be trained to serve as the
final Seq2Seq model to generate paraphrases, which can be directly used for
test in the unsupervised setup, or be finetuned on labeled datasets in the
supervised setup. The proposed method offers merits over
machine-translation-based paraphrase generation methods, as it avoids reliance
on bilingual sentence pairs. It also allows human intervene with the model so
that more diverse paraphrases can be generated using different filtering
criteria. Extensive experiments on existing paraphrase dataset for both the
supervised and unsupervised setups demonstrate the effectiveness the proposed
paradigm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02738">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>People convey their intention and attitude through linguistic styles of the
text that they write. In this study, we investigate lexicon usages across
styles throughout two lenses: human perception and machine word importance,
since words differ in the strength of the stylistic cues that they provide. To
collect labels of human perception, we curate a new dataset, Hummingbird, on
top of benchmarking style datasets. We have crowd workers highlight the
representative words in the text that makes them think the text has the
following styles: politeness, sentiment, offensiveness, and five emotion types.
We then compare these human word labels with word importance derived from a
popular fine-tuned style classifier like BERT. Our results show that the BERT
often finds content words not relevant to the target style as important words
used in style prediction, but humans do not perceive the same way even though
for some styles (e.g., positive sentiment and joy) human- and
machine-identified words share significant overlap for some styles.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The scale, variety, and quantity of publicly-available NLP datasets has grown
rapidly as researchers propose new tasks, larger models, and novel benchmarks.
Datasets is a community library for contemporary NLP designed to support this
ecosystem. Datasets aims to standardize end-user interfaces, versioning, and
documentation, while providing a lightweight front-end that behaves similarly
for small datasets as for internet-scale corpora. The design of the library
incorporates a distributed, community-driven approach to adding datasets and
documenting usage. After a year of development, the library now includes more
than 650 unique datasets, has more than 250 contributors, and has helped
support a variety of novel cross-dataset research projects and shared tasks.
The library is available at https://github.com/huggingface/datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Commonsense reasoning benchmarks have been largely solved by fine-tuning
language models. The downside is that fine-tuning may cause models to overfit
to task-specific data and thereby forget their knowledge gained during
pre-training. Recent works only propose lightweight model updates as models may
already possess useful knowledge from past experience, but a challenge remains
in understanding what parts and to what extent models should be refined for a
given task. In this paper, we investigate what models learn from commonsense
reasoning datasets. We measure the impact of three different adaptation methods
on the generalization and accuracy of models. Our experiments with two models
show that fine-tuning performs best, by learning both the content and the
structure of the task, but suffers from overfitting and limited generalization
to novel answers. We observe that alternative adaptation methods like
prefix-tuning have comparable accuracy, but generalize better to unseen answers
and are more robust to adversarial splits.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik&#x27;s Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02734">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Inspiration moves a person to see new possibilities and transforms the way
they perceive their own potential. Inspiration has received little attention in
psychology, and has not been researched before in the NLP community. To the
best of our knowledge, this work is the first to study inspiration through
machine learning methods. We aim to automatically detect inspiring content from
social media data. To this end, we analyze social media posts to tease out what
makes a post inspiring and what topics are inspiring. We release a dataset of
5,800 inspiring and 5,800 non-inspiring English-language public post unique ids
collected from a dump of Reddit public posts made available by a third party
and use linguistic heuristics to automatically detect which social media
English-language posts are inspiring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most previous studies on information status (IS) classification and bridging
anaphora recognition assume that the gold mention or syntactic tree information
is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,
2020). In this paper, we propose an end-to-end neural approach for information
status classification. Our approach consists of a mention extraction component
and an information status assignment component. During the inference time, our
system takes a raw text as the input and generates mentions together with their
information status. On the ISNotes corpus (Markert et al., 2012), we show that
our information status assignment component achieves new state-of-the-art
results on fine-grained IS classification based on gold mentions. Furthermore,
our system performs significantly better than other baselines for both mention
extraction and fine-grained IS classification in the end-to-end setting.
Finally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,
2016) to recognize referential bridging anaphora. We find that our end-to-end
system trained on ISNotes achieves competitive results on bridging anaphora
recognition compared to the previous state-of-the-art system that relies on
syntactic information and is trained on the in-domain datasets (Yu and Poesio,
2020).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of &quot;Subjectivity&quot; and &quot;Identity Terms&quot;. (arXiv:2109.02691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
&quot;Muslim&quot; and &quot;black&quot;. Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02707">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study a new problem setting of information extraction (IE), referred to as
text-to-table, which can be viewed as an inverse problem of the well-studied
table-to-text. In text-to-table, given a text, one creates a table or several
tables expressing the main content of the text, while the model is learned from
text-table pair data. The problem setting differs from those of the existing
methods for IE. First, the extraction can be carried out from long texts to
large tables with complex structures. Second, the extraction is entirely
data-driven, and there is no need to explicitly define the schemas. As far as
we know, there has been no previous work that studies the problem. In this
work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.
We first employ a seq2seq model fine-tuned from a pre-trained language model to
perform the task. We also develop a new method within the seq2seq approach,
exploiting two additional techniques in table generation: table constraint and
table relation embeddings. We make use of four existing table-to-text datasets
in our experiments on text-to-table. Experimental results show that the vanilla
seq2seq model can outperform the baseline methods of using relation extraction
and named entity extraction. The results also show that our method can further
boost the performances of the vanilla seq2seq model. We further discuss the
main challenges of the proposed task. The code and data will be made publicly
available.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intelligent Motion Planning for a Cost-effective Object Follower Mobile Robotic System with Obstacle Avoidance. (arXiv:2109.02700v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02700">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>There are few industries which use manually controlled robots for carrying
material and this cannot be used all the time in all the places. So, it is very
tranquil to have robots which can follow a specific human by following the
unique coloured object held by that person. So, we propose a robotic system
which uses robot vision and deep learning to get the required linear and
angular velocities which are {\nu} and {\omega}, respectively. Which in turn
makes the robot to avoid obstacles when following the unique coloured object
held by the human. The novel methodology that we are proposing is accurate in
detecting the position of the unique coloured object in any kind of lighting
and tells us the horizontal pixel value where the robot is present and also
tells if the object is close to or far from the robot. Moreover, the artificial
neural networks that we have used in this problem gave us a meagre error in
linear and angular velocity prediction and the PI controller which was used to
control the linear and angular velocities, which in turn controls the position
of the robot gave us impressive results and this methodology outperforms all
other methodologies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training such inference models is challenging
due to the lack of paired (shape, program) data in most domains. A popular
approach is to pre-train a model on synthetic data and then fine-tune on real
shapes using slow, unstable reinforcement learning. In this paper, we argue
that self-training is a viable alternative for fine-tuning such models.
Self-training is a semi-supervised learning paradigm where a model assigns
pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)
pairs as the new ground truth. We show that for constructive solid geometry and
assembly-based modeling, self-training outperforms state-of-the-art
reinforcement learning approaches. Additionally, shape program inference has a
unique property that circumvents a potential downside of self-training
(incorrect pseudo-label assignment): inferred programs are executable. For a
given shape from our distribution of interest $\mathbf{x}^*$ and its predicted
program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape
$\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than
$(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution
self training (LEST). We demonstrate that self training infers shape programs
with higher shape reconstruction accuracy and converges significantly faster
than reinforcement learning approaches, and in some domains, LEST can further
improve this performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Grassmannian Graph-attentional Landmark Selection for Domain Adaptation. (arXiv:2109.02990v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Domain adaptation aims to leverage information from the source domain to
improve the classification performance in the target domain. It mainly utilizes
two schemes: sample reweighting and feature matching. While the first scheme
allocates different weights to individual samples, the second scheme matches
the feature of two domains using global structural statistics. The two schemes
are complementary with each other, which are expected to jointly work for
robust domain adaptation. Several methods combine the two schemes, but the
underlying relationship of samples is insufficiently analyzed due to the
neglect of the hierarchy of samples and the geometric properties between
samples. To better combine the advantages of the two schemes, we propose a
Grassmannian graph-attentional landmark selection (GGLS) framework for domain
adaptation. GGLS presents a landmark selection scheme using attention-induced
neighbors of the graphical structure of samples and performs distribution
adaptation and knowledge adaptation over Grassmann manifold. the former treats
the landmarks of each sample differently, and the latter avoids feature
distortion and achieves better geometric properties. Experimental results on
different real-world cross-domain visual recognition tasks demonstrate that
GGLS provides better classification accuracies compared with state-of-the-art
domain adaptation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MultiScene: A Large-scale Dataset and Benchmark for Multi-scene Recognition in Single Aerial Images. (arXiv:2104.02846v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02846">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Aerial scene recognition is a fundamental research problem in interpreting
high-resolution aerial imagery. Over the past few years, most studies focus on
classifying an image into one scene category, while in real-world scenarios, it
is more often that a single image contains multiple scenes. Therefore, in this
paper, we investigate a more practical yet underexplored task -- multi-scene
recognition in single images. To this end, we create a large-scale dataset,
called MultiScene, composed of 100,000 unconstrained high-resolution aerial
images. Considering that manually labeling such images is extremely arduous, we
resort to low-cost annotations from crowdsourcing platforms, e.g.,
OpenStreetMap (OSM). However, OSM data might suffer from incompleteness and
incorrectness, which introduce noise into image labels. To address this issue,
we visually inspect 14,000 images and correct their scene labels, yielding a
subset of cleanly-annotated images, named MultiScene-Clean. With it, we can
develop and evaluate deep networks for multi-scene recognition using clean
data. Moreover, we provide crowdsourced annotations of all images for the
purpose of studying network learning with noisy labels. We conduct experiments
with extensive baseline models on both MultiScene-Clean and MultiScene to offer
benchmarks for multi-scene recognition in single images and learning from noisy
labels for this task, respectively. To facilitate progress, we make our dataset
and trained models available on
https://gitlab.lrz.de/ai4eo/reasoning/multiscene.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DPR-CAE: Capsule Autoencoder with Dynamic Part Representation for Image Parsing. (arXiv:2104.14735v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.14735">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Parsing an image into a hierarchy of objects, parts, and relations is
important and also challenging in many computer vision tasks. This paper
proposes a simple and effective capsule autoencoder to address this issue,
called DPR-CAE. In our approach, the encoder parses the input into a set of
part capsules, including pose, intensity, and dynamic vector. The decoder
introduces a novel dynamic part representation (DPR) by combining the dynamic
vector and a shared template bank. These part representations are then
regulated by corresponding capsules to composite the final output in an
interpretable way. Besides, an extra translation-invariant module is proposed
to avoid directly learning the uncertain scene-part relationship in our
DPR-CAE, which makes the resulting method achieves a promising performance gain
on $rm$-MNIST and $rm$-Fashion-MNIST. % to model the scene-object relationship
DPR-CAE can be easily combined with the existing stacked capsule autoencoder
and experimental results show it significantly improves performance in terms of
unsupervised object classification. Our code is available in the Appendix.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training sample re-weighting is an effective approach for tackling data
biases such as imbalanced and corrupted labels. Recent methods develop
learning-based algorithms to learn sample re-weighting strategies jointly with
model training based on the frameworks of reinforcement learning and meta
learning. However, depending on additional unbiased reward data is limiting
their general applicability. Furthermore, existing learning-based sample
re-weighting methods require nested optimizations of models and weighting
parameters, which requires expensive second-order computation. This paper
addresses these two problems and presents a novel learning-based fast sample
re-weighting (FSR) method that does not require additional reward data. The
method is based on two key ideas: learning from history to build proxy reward
data and feature sharing to reduce the optimization cost. Our experiments show
the proposed method achieves competitive results compared to state of the arts
on label noise robustness and long-tailed recognition, and does so while
achieving significantly improved training efficiency. The source code is
publicly available at
https://github.com/google-research/google-research/tree/master/ieg.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Modal RGB-D Scene Recognition Across Domains. (arXiv:2103.14672v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14672">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scene recognition is one of the basic problems in computer vision research
with extensive applications in robotics. When available, depth images provide
helpful geometric cues that complement the RGB texture information and help to
identify discriminative scene image features. Depth sensing technology
developed fast in the last years and a great variety of 3D cameras have been
introduced, each with different acquisition properties. However, those
properties are often neglected when targeting big data collections, so
multi-modal images are gathered disregarding their original nature. In this
work, we put under the spotlight the existence of a possibly severe domain
shift issue within multi-modality scene recognition datasets. As a consequence,
a scene classification model trained on one camera may not generalize on data
from a different camera, only providing a low recognition performance. Starting
from the well-known SUN RGB-D dataset, we designed an experimental testbed to
study this problem and we use it to benchmark the performance of existing
methods. Finally, we introduce a novel adaptive scene recognition approach that
leverages self-supervised translation between modalities. Indeed, learning to
go from RGB to depth and vice-versa is an unsupervised procedure that can be
trained jointly on data of multiple cameras and may help to bridge the gap
among the extracted feature distributions. Our experimental results confirm the
effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Agent-Environment Network for Temporal Action Proposal Generation. (arXiv:2107.08323v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08323">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal action proposal generation is an essential and challenging task that
aims at localizing temporal intervals containing human actions in untrimmed
videos. Most of existing approaches are unable to follow the human cognitive
process of understanding the video context due to lack of attention mechanism
to express the concept of an action or an agent who performs the action or the
interaction between the agent and the environment. Based on the action
definition that a human, known as an agent, interacts with the environment and
performs an action that affects the environment, we propose a contextual
Agent-Environment Network. Our proposed contextual AEN involves (i) agent
pathway, operating at a local level to tell about which humans/agents are
acting and (ii) environment pathway operating at a global level to tell about
how the agents interact with the environment. Comprehensive evaluations on
20-action THUMOS-14 and 200-action ActivityNet-1.3 datasets with different
backbone networks, i.e C3D and SlowFast, show that our method robustly exhibits
outperformance against state-of-the-art methods regardless of the employed
backbone network.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.

In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DS-Net: Dynamic Spatiotemporal Network for Video Salient Object Detection. (arXiv:2012.04886v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04886">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As moving objects always draw more attention of human eyes, the temporal
motive information is always exploited complementarily with spatial information
to detect salient objects in videos. Although efficient tools such as optical
flow have been proposed to extract temporal motive information, it often
encounters difficulties when used for saliency detection due to the movement of
camera or the partial movement of salient objects. In this paper, we
investigate the complimentary roles of spatial and temporal information and
propose a novel dynamic spatiotemporal network (DS-Net) for more effective
fusion of spatiotemporal information. We construct a symmetric two-bypass
network to explicitly extract spatial and temporal features. A dynamic weight
generator (DWG) is designed to automatically learn the reliability of
corresponding saliency branch. And a top-down cross attentive aggregation (CAA)
procedure is designed so as to facilitate dynamic complementary aggregation of
spatiotemporal features. Finally, the features are modified by spatial
attention with the guidance of coarse saliency map and then go through decoder
part for final saliency map. Experimental results on five benchmarks VOS,
DAVIS, FBMS, SegTrack-v2, and ViSal demonstrate that the proposed method
achieves superior performance than state-of-the-art algorithms. The source code
is available at https://github.com/TJUMMG/DS-Net.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Real-time Human Action Recognition Using Locally Aggregated Kinematic-Guided Skeletonlet and Supervised Hashing-by-Analysis Model. (arXiv:2105.11312v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11312">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>3D action recognition is referred to as the classification of action
sequences which consist of 3D skeleton joints. While many research work are
devoted to 3D action recognition, it mainly suffers from three problems: highly
complicated articulation, a great amount of noise, and a low implementation
efficiency. To tackle all these problems, we propose a real-time 3D action
recognition framework by integrating the locally aggregated kinematic-guided
skeletonlet (LAKS) with a supervised hashing-by-analysis (SHA) model. We first
define the skeletonlet as a few combinations of joint offsets grouped in terms
of kinematic principle, and then represent an action sequence using LAKS, which
consists of a denoising phase and a locally aggregating phase. The denoising
phase detects the noisy action data and adjust it by replacing all the features
within it with the features of the corresponding previous frame, while the
locally aggregating phase sums the difference between an offset feature of the
skeletonlet and its cluster center together over all the offset features of the
sequence. Finally, the SHA model which combines sparse representation with a
hashing model, aiming at promoting the recognition accuracy while maintaining a
high efficiency. Experimental results on MSRAction3D, UTKinectAction3D and
Florence3DAction datasets demonstrate that the proposed method outperforms
state-of-the-art methods in both recognition accuracy and implementation
efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Transferability of Domain Adaptation Networks Through Domain Alignment Layers. (arXiv:2109.02693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning (DL) has been the primary approach used in various computer
vision tasks due to its relevant results achieved on many tasks. However, on
real-world scenarios with partially or no labeled data, DL methods are also
prone to the well-known domain shift problem. Multi-source unsupervised domain
adaptation (MSDA) aims at learning a predictor for an unlabeled domain by
assigning weak knowledge from a bag of source models. However, most works
conduct domain adaptation leveraging only the extracted features and reducing
their domain shift from the perspective of loss function designs. In this
paper, we argue that it is not sufficient to handle domain shift only based on
domain-level features, but it is also essential to align such information on
the feature space. Unlike previous works, we focus on the network design and
propose to embed Multi-Source version of DomaIn Alignment Layers (MS-DIAL) at
different levels of the predictor. These layers are designed to match the
feature distributions between different domains and can be easily applied to
various MSDA methods. To show the robustness of our approach, we conducted an
extensive experimental evaluation considering two challenging scenarios: digit
recognition and object classification. The experimental results indicated that
our approach can improve state-of-the-art MSDA methods, yielding relative gains
of up to +30.64% on their classification accuracies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Crowdsourcing Annotation: Partial Annotation with Salient Labels for Multi-Label Image Classification. (arXiv:2109.02688v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02688">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Annotated images are required for both supervised model training and
evaluation in image classification. Manually annotating images is arduous and
expensive, especially for multi-labeled images. A recent trend for conducting
such laboursome annotation tasks is through crowdsourcing, where images are
annotated by volunteers or paid workers online (e.g., workers of Amazon
Mechanical Turk) from scratch. However, the quality of crowdsourcing image
annotations cannot be guaranteed, and incompleteness and incorrectness are two
major concerns for crowdsourcing annotations. To address such concerns, we have
a rethinking of crowdsourcing annotations: Our simple hypothesis is that if the
annotators only partially annotate multi-label images with salient labels they
are confident in, there will be fewer annotation errors and annotators will
spend less time on uncertain labels. As a pleasant surprise, with the same
annotation budget, we show a multi-label image classifier supervised by images
with salient annotations can outperform models supervised by fully annotated
images. Our method contributions are 2-fold: An active learning way is proposed
to acquire salient labels for multi-label images; and a novel Adaptive
Temperature Associated Model (ATAM) specifically using partial annotations is
proposed for multi-label image classification. We conduct experiments on
practical crowdsourcing data, the Open Street Map (OSM) dataset and benchmark
dataset COCO 2014. When compared with state-of-the-art classification methods
trained on fully annotated images, the proposed ATAM can achieve higher
accuracy. The proposed idea is promising for crowdsourcing data annotation. Our
code will be publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pseudo-mask Matters in Weakly-supervised Semantic Segmentation. (arXiv:2108.12995v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12995">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most weakly supervised semantic segmentation (WSSS) methods follow the
pipeline that generates pseudo-masks initially and trains the segmentation
model with the pseudo-masks in fully supervised manner after. However, we find
some matters related to the pseudo-masks, including high quality pseudo-masks
generation from class activation maps (CAMs), and training with noisy
pseudo-mask supervision. For these matters, we propose the following designs to
push the performance to new state-of-art: (i) Coefficient of Variation
Smoothing to smooth the CAMs adaptively; (ii) Proportional Pseudo-mask
Generation to project the expanded CAMs to pseudo-mask based on a new metric
indicating the importance of each class on each location, instead of the scores
trained from binary classifiers. (iii) Pretended Under-Fitting strategy to
suppress the influence of noise in pseudo-mask; (iv) Cyclic Pseudo-mask to
boost the pseudo-masks during training of fully supervised semantic
segmentation (FSSS). Experiments based on our methods achieve new state-of-art
results on two changeling weakly supervised semantic segmentation datasets,
pushing the mIoU to 70.0% and 40.2% on PAS-CAL VOC 2012 and MS COCO 2014
respectively. Codes including segmentation framework are released at
https://github.com/Eli-YiLi/PMM</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TrouSPI-Net: Spatio-temporal attention on parallel atrous convolutions and U-GRUs for skeletal pedestrian crossing prediction. (arXiv:2109.00953v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00953">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Understanding the behaviors and intentions of pedestrians is still one of the
main challenges for vehicle autonomy, as accurate predictions of their
intentions can guarantee their safety and driving comfort of vehicles. In this
paper, we address pedestrian crossing prediction in urban traffic environments
by linking the dynamics of a pedestrian&#x27;s skeleton to a binary crossing
intention. We introduce TrouSPI-Net: a context-free, lightweight, multi-branch
predictor. TrouSPI-Net extracts spatio-temporal features for different time
resolutions by encoding pseudo-images sequences of skeletal joints&#x27; positions
and processes them with parallel attention modules and atrous convolutions. The
proposed approach is then enhanced by processing features such as relative
distances of skeletal joints, bounding box positions, or ego-vehicle speed with
U-GRUs. Using the newly proposed evaluation procedures for two large public
naturalistic data sets for studying pedestrian behavior in traffic: JAAD and
PIE, we evaluate TrouSPI-Net and analyze its performance. Experimental results
show that TrouSPI-Net achieved 0.76 F1 score on JAAD and 0.80 F1 score on PIE,
therefore outperforming current state-of-the-art while being lightweight and
context-free.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Animatable Neural Radiance Fields from Monocular RGB Videos. (arXiv:2106.13629v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13629">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present animatable neural radiance fields (animatable NeRF) for detailed
human avatar creation from monocular videos. Our approach extends neural
radiance fields (NeRF) to the dynamic scenes with human movements via
introducing explicit pose-guided deformation while learning the scene
representation network. In particular, we estimate the human pose for each
frame and learn a constant canonical space for the detailed human template,
which enables natural shape deformation from the observation space to the
canonical space under the explicit control of the pose parameters. To
compensate for inaccurate pose estimation, we introduce the pose refinement
strategy that updates the initial pose during the learning process, which not
only helps to learn more accurate human reconstruction but also accelerates the
convergence. In experiments we show that the proposed approach achieves 1)
implicit human geometry and appearance reconstruction with high-quality
details, 2) photo-realistic rendering of the human from novel views, and 3)
animation of the human with novel poses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Comparison: Quantifying Variance in Resultsfor Fine-grained Visual Categorization. (arXiv:2109.03156v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03156">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>For the task of image classification, researchers work arduously to develop
the next state-of-the-art (SOTA) model, each bench-marking their own
performance against that of their predecessors and of their peers.
Unfortunately, the metric used most frequently to describe a model&#x27;s
performance, average categorization accuracy, is often used in isolation. As
the number of classes increases, such as in fine-grained visual categorization
(FGVC), the amount of information conveyed by average accuracy alone dwindles.
While its most glaring weakness is its failure to describe the model&#x27;s
performance on a class-by-class basis, average accuracy also fails to describe
how performance may vary from one trained model of the same architecture, on
the same dataset, to another (both averaged across all categories and at the
per-class level). We first demonstrate the magnitude of these variations across
models and across class distributions based on attributes of the data,
comparing results on different visual domains and different per-class image
distributions, including long-tailed distributions and few-shot subsets. We
then analyze the impact various FGVC methods have on overall and per-class
variance. From this analysis, we both highlight the importance of reporting and
comparing methods based on information beyond overall accuracy, as well as
point out techniques that mitigate variance in FGVC results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Material Measurement Units for a Circular Economy: Foundations through a Survey. (arXiv:2103.01997v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01997">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Long-term availability of minerals and industrial materials is a necessary
condition for sustainable development as they are the constituents of any
manufacturing product. To enhance the efficiency of material management, we
define a computer-vision-enabled material measurement system and provide a
survey of works relevant to its development with particular emphasis on the
foundations. A network of such systems for wide-area material stock monitoring
is also covered. Finally, challenges and future research directions are
discussed. As the first article bridging industrial ecology and advanced
computer vision, this survey is intended to support both research communities
towards more sustainable manufacturing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LSD-StructureNet: Modeling Levels of Structural Detail in 3D Part Hierarchies. (arXiv:2108.13459v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Generative models for 3D shapes represented by hierarchies of parts can
generate realistic and diverse sets of outputs. However, existing models suffer
from the key practical limitation of modelling shapes holistically and thus
cannot perform conditional sampling, i.e. they are not able to generate
variants on individual parts of generated shapes without modifying the rest of
the shape. This is limiting for applications such as 3D CAD design that involve
adjusting created shapes at multiple levels of detail. To address this, we
introduce LSD-StructureNet, an augmentation to the StructureNet architecture
that enables re-generation of parts situated at arbitrary positions in the
hierarchies of its outputs. We achieve this by learning individual,
probabilistic conditional decoders for each hierarchy depth. We evaluate
LSD-StructureNet on the PartNet dataset, the largest dataset of 3D shapes
represented by hierarchies of parts. Our results show that contrarily to
existing methods, LSD-StructureNet can perform conditional sampling without
impacting inference speed or the realism and diversity of its outputs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09262">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Posit Number System was introduced in 2017 as a replacement for
floating-point numbers. Since then, the community has explored its application
in Neural Network related tasks and produced some unit designs which are still
far from being competitive with their floating-point counterparts. This paper
proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to
significantly reduce the complexity of posit multipliers, the most power-hungry
units within Deep Neural Network architectures. When comparing with
state-of-the-art posit multipliers, experiments show that the proposed
technique reduces the area, power, and delay of hardware multipliers up to
72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\&#x27;echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Robust and accurate nuclei centroid detection is important for the
understanding of biological structures in fluorescence microscopy images.
Existing automated nuclei localization methods face three main challenges: (1)
Most of object detection methods work only on 2D images and are difficult to
extend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes
but it is computational expensive for large microscopy volumes and they have
difficulty distinguishing different instances of objects; (3) Hand annotated
ground truth is limited for 3D microscopy volumes. To address these issues, we
present a scalable approach for nuclei centroid detection of 3D microscopy
volumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each
slice of the volume from different directions and 3D agglomerative hierarchical
clustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.
The model was trained with the synthetic microscopy data generated using
Spatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and
tested on different types of real 3D microscopy data. Extensive experimental
results demonstrate that our proposed method can accurately count and detect
the nuclei centroids in a 3D microscopy volume.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02752">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks may be challenging in real world data. Using
models as black-boxes, even with transfer learning, can result in poor
generalization or inconclusive results when it comes to small datasets or
specific applications. This tutorial covers the basic steps as well as more
recent options to improve models, in particular, but not restricted to,
supervised learning. It can be particularly useful in datasets that are not as
well-prepared as those in challenges, and also under scarce annotation and/or
small data. We describe basic procedures: as data preparation, optimization and
transfer learning, but also recent architectural choices such as use of
transformer modules, alternative convolutional layers, activation functions,
wide and deep networks, as well as training procedures including as curriculum,
contrastive and self-supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01378">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper studies the model compression problem of vision transformers.
Benefit from the self-attention module, transformer architectures have shown
extraordinary performance on many computer vision tasks. Although the network
performance is boosted, transformers are often required more computational
resources including memory usage and the inference complexity. Compared with
the existing knowledge distillation approaches, we propose to excavate useful
information from the teacher transformer through the relationship between
images and the divided patches. We then explore an efficient fine-grained
manifold distillation approach that simultaneously calculates cross-images,
cross-patch, and random-selected manifolds in teacher and student models.
Experimental results conducted on several benchmarks demonstrate the
superiority of the proposed algorithm for distilling portable transformer
models with higher performance. For example, our approach achieves 75.06% Top-1
accuracy on the ImageNet-1k dataset for training a DeiT-Tiny model, which
outperforms other ViT distillation methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03230">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we propose a self-supervised approach for tumor segmentation.
Specifically, we advocate a zero-shot setting, where models from
self-supervised learning should be directly applicable for the downstream task,
without using any manual annotations whatsoever. We make the following
contributions. First, with careful examination on existing self-supervised
learning approaches, we reveal the surprising result that, given suitable data
augmentation, models trained from scratch in fact achieve comparable
performance to those pre-trained with self-supervised learning. Second,
inspired by the fact that tumors tend to be characterized independently to the
contexts, we propose a scalable pipeline for generating synthetic tumor data,
and train a self-supervised model that minimises the generalisation gap with
the downstream task. Third, we conduct extensive ablation studies on different
downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for
liver tumor segmentation. While evaluating the model transferability for tumor
segmentation under a low-annotation regime, including an extreme case of
zero-shot segmentation, the proposed approach demonstrates state-of-the-art
performance, substantially outperforming all existing self-supervised
approaches, and opening up the usage of self-supervised learning in practical
scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Smart Traffic Monitoring System using Computer Vision and Edge Computing. (arXiv:2109.03141v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03141">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Traffic management systems capture tremendous video data and leverage
advances in video processing to detect and monitor traffic incidents. The
collected data are traditionally forwarded to the traffic management center
(TMC) for in-depth analysis and may thus exacerbate the network paths to the
TMC. To alleviate such bottlenecks, we propose to utilize edge computing by
equipping edge nodes that are close to cameras with computing resources (e.g.
cloudlets). A cloudlet, with limited computing resources as compared to TMC,
provides limited video processing capabilities. In this paper, we focus on two
common traffic monitoring tasks, congestion detection, and speed detection, and
propose a two-tier edge computing based model that takes into account of both
the limited computing capability in cloudlets and the unstable network
condition to the TMC. Our solution utilizes two algorithms for each task, one
implemented at the edge and the other one at the TMC, which are designed with
the consideration of different computing resources. While the TMC provides
strong computation power, the video quality it receives depends on the
underlying network conditions. On the other hand, the edge processes very
high-quality video but with limited computing resources. Our model captures
this trade-off. We evaluate the performance of the proposed two-tier model as
well as the traffic monitoring algorithms via test-bed experiments under
different weather as well as network conditions and show that our proposed
hybrid edge-cloud solution outperforms both the cloud-only and edge-only
solutions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CovarianceNet: Conditional Generative Model for Correct Covariance Prediction in Human Motion Prediction. (arXiv:2109.02965v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02965">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The correct characterization of uncertainty when predicting human motion is
equally important as the accuracy of this prediction. We present a new method
to correctly predict the uncertainty associated with the predicted distribution
of future trajectories. Our approach, CovariaceNet, is based on a Conditional
Generative Model with Gaussian latent variables in order to predict the
parameters of a bi-variate Gaussian distribution. The combination of
CovarianceNet with a motion prediction model results in a hybrid approach that
outputs a uni-modal distribution. We will show how some state of the art
methods in motion prediction become overconfident when predicting uncertainty,
according to our proposed metric and validated in the ETH data-set
\cite{pellegrini2009you}. CovarianceNet correctly predicts uncertainty, which
makes our method suitable for applications that use predicted distributions,
e.g., planning or decision making.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00632">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We train an object detector built from convolutional neural networks to count
interference fringes in elliptical antinode regions in frames of high-speed
video recordings of transient oscillations in Caribbean steelpan drums
illuminated by electronic speckle pattern interferometry (ESPI). The
annotations provided by our model aim to contribute to the understanding of
time-dependent behavior in such drums by tracking the development of
sympathetic vibration modes. The system is trained on a dataset of crowdsourced
human-annotated images obtained from the Zooniverse Steelpan Vibrations
Project. Due to the small number of human-annotated images and the ambiguity of
the annotation task, we also evaluate the model on a large corpus of synthetic
images whose properties have been matched to the real images by style transfer
using a Generative Adversarial Network. Applying the model to thousands of
unlabeled video frames, we measure oscillations consistent with audio
recordings of these drum strikes. One unanticipated result is that sympathetic
oscillations of higher-octave notes significantly precede the rise in sound
intensity of the corresponding second harmonic tones; the mechanism responsible
for this remains unidentified. This paper primarily concerns the development of
the predictive model; further exploration of the steelpan images and deeper
physical insights await its further application.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FDA: Feature Decomposition and Aggregation for Robust Airway Segmentation. (arXiv:2109.02920v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02920">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>3D Convolutional Neural Networks (CNNs) have been widely adopted for airway
segmentation. The performance of 3D CNNs is greatly influenced by the dataset
while the public airway datasets are mainly clean CT scans with coarse
annotation, thus difficult to be generalized to noisy CT scans (e.g. COVID-19
CT scans). In this work, we proposed a new dual-stream network to address the
variability between the clean domain and noisy domain, which utilizes the clean
CT scans and a small amount of labeled noisy CT scans for airway segmentation.
We designed two different encoders to extract the transferable clean features
and the unique noisy features separately, followed by two independent decoders.
Further on, the transferable features are refined by the channel-wise feature
recalibration and Signed Distance Map (SDM) regression. The feature
recalibration module emphasizes critical features and the SDM pays more
attention to the bronchi, which is beneficial to extracting the transferable
topological features robust to the coarse labels. Extensive experimental
results demonstrated the obvious improvement brought by our proposed method.
Compared to other state-of-the-art transfer learning methods, our method
accurately segmented more bronchi in the noisy CT scans.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal
MRI is a challenging field of research. Most of the existing literature on
survival prediction is based on Radiomic features, which does not consider
either non-biological factors or the functional neurological status of the
patient(s). Besides, the selection of an appropriate cut-off for survival and
the presence of censored data create further problems. Application of deep
learning models for OS prediction is also limited due to the lack of large
annotated publicly available datasets. In this scenario we analyse the
potential of two novel neuroimaging feature families, extracted from brain
parcellation atlases and spatial habitats, along with classical radiomic and
geometric features; to study their combined predictive power for analysing
overall survival. A cross validation strategy with grid search is proposed to
simultaneously select and evaluate the most predictive feature subset based on
its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for
univariate feature selection, followed by the prediction of patient-specific
survival functions by three multivariate parsimonious models viz. Coxnet,
Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI
data used for this research was taken from two open-access collections TCGA-GBM
and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding
survival data for each patient was downloaded from The Cancer Genome Atlas
(TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved
using RSF with the best $24$ selected features. Age was found to be the most
important biological predictor. There were $9$, $6$, $6$ and $2$ features
selected from the parcellation, habitat, radiomic and region-based feature
groups respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Collaborative Multi-Modal Learning for Unsupervised Kinship Estimation. (arXiv:2109.02804v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02804">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Kinship verification is a long-standing research challenge in computer
vision. The visual differences presented to the face have a significant effect
on the recognition capabilities of the kinship systems. We argue that
aggregating multiple visual knowledge can better describe the characteristics
of the subject for precise kinship identification. Typically, the age-invariant
features can represent more natural facial details. Such age-related
transformations are essential for face recognition due to the biological
effects of aging. However, the existing methods mainly focus on employing the
single-view image features for kinship identification, while more meaningful
visual properties such as race and age are directly ignored in the feature
learning step. To this end, we propose a novel deep collaborative multi-modal
learning (DCML) to integrate the underlying information presented in facial
properties in an adaptive manner to strengthen the facial details for effective
unsupervised kinship verification. Specifically, we construct a well-designed
adaptive feature fusion mechanism, which can jointly leverage the complementary
properties from different visual perspectives to produce composite features and
draw greater attention to the most informative components of spatial feature
maps. Particularly, an adaptive weighting strategy is developed based on a
novel attention mechanism, which can enhance the dependencies between different
properties by decreasing the information redundancy in channels in a
self-adaptive manner. To validate the effectiveness of the proposed method,
extensive experimental evaluations conducted on four widely-used datasets show
that our DCML method is always superior to some state-of-the-art kinship
verification methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional sparse coding improves on the standard sparse approximation by
incorporating a global shift-invariant model. The most efficient convolutional
sparse coding methods are based on the alternating direction method of
multipliers and the convolution theorem. The only major difference between
these methods is how they approach a convolutional least-squares fitting
subproblem. This letter presents a solution to this subproblem, which improves
the efficiency of the state-of-the-art algorithms. We also use the same
approach for developing an efficient convolutional dictionary learning method.
Furthermore, we propose a novel algorithm for convolutional sparse coding with
a constraint on the approximation error.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets. (arXiv:2109.03229v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03229">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many existing works have made great strides towards reducing racial bias in
face recognition. However, most of these methods attempt to rectify bias that
manifests in models during training instead of directly addressing a major
source of the bias, the dataset itself. Exceptions to this are
BUPT-Balancedface/RFW and Fairface, but these works assume that primarily
training on a single race or not racially balancing the dataset are inherently
disadvantageous. We demonstrate that these assumptions are not necessarily
valid. In our experiments, training on only African faces induced less bias
than training on a balanced distribution of faces and distributions skewed to
include more African faces produced more equitable models. We additionally
notice that adding more images of existing identities to a dataset in place of
adding new identities can lead to accuracy boosts across racial categories. Our
code is available at
https://github.com/j-alex-hanson/rethinking-race-face-datasets</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model&#x27;s dependence on key facial features. Second,
four of today&#x27;s state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative-Adversarial-Networks-based Ghost Recognition. (arXiv:2103.13858v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Nowadays, target recognition technique plays an important role in many
fields. However, the current target image information based methods suffer from
the influence of image quality and the time cost of image reconstruction. In
this paper, we propose a novel imaging-free target recognition method combining
ghost imaging (GI) and generative adversarial networks (GAN). Based on the
mechanism of GI, a set of random speckles sequence is employed to illuminate
target, and a bucket detector without resolution is utilized to receive echo
signal. The bucket signal sequence formed after continuous detections is
constructed into a bucket signal array, which is regarded as the sample of GAN.
Then, conditional GAN is used to map bucket signal array and target category.
In practical application, the speckles sequence in training step is employed to
illuminate target, and the bucket signal array is input GAN for recognition.
The proposed method can improve the problems caused by conventional recognition
methods that based on target image information, and provide a certain
turbulence-free ability. Extensive experiments show that the proposed method
achieves promising performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System. (arXiv:2109.03144v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03144">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Optical Character Recognition (OCR) systems have been widely used in various
of application scenarios. Designing an OCR system is still a challenging task.
In previous work, we proposed a practical ultra lightweight OCR system (PP-OCR)
to balance the accuracy against the efficiency. In order to improve the
accuracy of PP-OCR and keep high efficiency, in this paper, we propose a more
robust OCR system, i.e. PP-OCRv2. We introduce bag of tricks to train a better
text detector and a better text recognizer, which include Collaborative Mutual
Learning (CML), CopyPaste, Lightweight CPUNetwork (LCNet), Unified-Deep Mutual
Learning (U-DML) and Enhanced CTCLoss. Experiments on real data show that the
precision of PP-OCRv2 is 7% higher than PP-OCR under the same inference cost.
It is also comparable to the server models of the PP-OCR which uses ResNet
series as backbones. All of the above mentioned models are open-sourced and the
code is available in the GitHub repository PaddleOCR which is powered by
PaddlePaddle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention. (arXiv:2109.02955v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02955">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Automatically describing video, or video captioning, has been widely studied
in the multimedia field. This paper proposes a new task of sensor-augmented
egocentric-video captioning, a newly constructed dataset for it called MMAC
Captions, and a method for the newly proposed task that effectively utilizes
multi-modal data of video and motion sensors, or inertial measurement units
(IMUs). While conventional video captioning tasks have difficulty in dealing
with detailed descriptions of human activities due to the limited view of a
fixed camera, egocentric vision has greater potential to be used for generating
the finer-grained descriptions of human activities on the basis of a much
closer view. In addition, we utilize wearable-sensor data as auxiliary
information to mitigate the inherent problems in egocentric vision: motion
blur, self-occlusion, and out-of-camera-range activities. We propose a method
for effectively utilizing the sensor data in combination with the video data on
the basis of an attention mechanism that dynamically determines the modality
that requires more attention, taking the contextual information into account.
We compared the proposed sensor-fusion method with strong baselines on the MMAC
Captions dataset and found that using sensor data as supplementary information
to the egocentric-video data was beneficial, and that our proposed method
outperformed the strong baselines, demonstrating the effectiveness of the
proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors. (arXiv:2109.02993v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02993">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Significant advancements made in the generation of deepfakes have caused
security and privacy issues. Attackers can easily impersonate a person&#x27;s
identity in an image by replacing his face with the target person&#x27;s face.
Moreover, a new domain of cloning human voices using deep-learning technologies
is also emerging. Now, an attacker can generate realistic cloned voices of
humans using only a few seconds of audio of the target person. With the
emerging threat of potential harm deepfakes can cause, researchers have
proposed deepfake detection methods. However, they only focus on detecting a
single modality, i.e., either video or audio. On the other hand, to develop a
good deepfake detector that can cope with the recent advancements in deepfake
generation, we need to have a detector that can detect deepfakes of multiple
modalities, i.e., videos and audios. To build such a detector, we need a
dataset that contains video and respective audio deepfakes. We were able to
find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection
Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized
fake audios as well. We used this multimodal deepfake dataset and performed
detailed baseline experiments using state-of-the-art unimodal, ensemble-based,
and multimodal detection methods to evaluate it. We conclude through detailed
experimentation that unimodals, addressing only a single modality, video or
audio, do not perform well compared to ensemble-based methods. Whereas purely
multimodal-based baselines provide the worst performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged as one of the key ingredients to reduce
the size of neural networks for their deployments to resource-limited devices.
In order to overcome the nature of transforming continuous activations and
weights to discrete ones, recent study called Relaxed Quantization (RQ)
[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that
allows this transformation with efficient gradient-based optimization. However,
RQ with this Gumbel-Softmax relaxation still suffers from bias-variance
trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve
the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses
multi-class straight-through estimator to effectively reduce the bias and
variance, along with a new regularization technique, DropBits that replaces
dropout regularization to randomly drop the bits instead of neurons to further
reduce the bias of the multi-class straight-through estimator in SRQ. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer
using DropBits. We experimentally validate our method on various benchmark
datasets and network architectures, and also support the quantized lottery
ticket hypothesis: learning heterogeneous quantization levels outperforms the
case using the same but fixed quantization levels from scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene. (arXiv:2109.02917v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02917">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper contributes a new high-quality dataset for hand gesture
recognition in hand hygiene systems, named &quot;MFH&quot;. Generally, current datasets
are not focused on: (i) fine-grained actions; and (ii) data mismatch between
different viewpoints, which are available under realistic settings. To address
the aforementioned issues, the MFH dataset is proposed to contain a total of
731147 samples obtained by different camera views in 6 non-overlapping
locations. Additionally, each sample belongs to one of seven steps introduced
by the World Health Organization (WHO). As a minor contribution, inspired by
advances in fine-grained image recognition and distribution adaptation, this
paper recommends using the self-supervised learning method to handle these
preceding problems. The extensive experiments on the benchmarking MFH dataset
show that the introduced method yields competitive performance in both the
Accuracy and the Macro F1-score. The code and the MFH dataset are available at
https://github.com/willogy-team/hand-gesture-recognition-smc2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03075">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Knowledge distillation (KD) is an effective framework that aims to transfer
meaningful information from a large teacher to a smaller student. Generally, KD
often involves how to define and transfer knowledge. Previous KD methods often
focus on mining various forms of knowledge, for example, feature maps and
refined information. However, the knowledge is derived from the primary
supervised task and thus is highly task-specific. Motivated by the recent
success of self-supervised representation learning, we propose an auxiliary
self-supervision augmented task to guide networks to learn more meaningful
features. Therefore, we can derive soft self-supervision augmented
distributions as richer dark knowledge from this task for KD. Unlike previous
knowledge, this distribution encodes joint knowledge from supervised and
self-supervised feature learning. Beyond knowledge exploration, another crucial
aspect is how to learn and distill our proposed knowledge effectively. To fully
take advantage of hierarchical feature maps, we propose to append several
auxiliary branches at various hidden layers. Each auxiliary branch is guided to
learn self-supervision augmented task and distill this distribution from
teacher to student. Thus we call our KD method as Hierarchical Self-Supervision
Augmented Knowledge Distillation (HSSAKD). Experiments on standard image
classification show that both offline and online HSSAKD achieves
state-of-the-art performance in the field of KD. Further transfer experiments
on object detection further verify that HSSAKD can guide the network to learn
better features, which can be attributed to learn and distill an auxiliary
self-supervision augmented task effectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Progressive Self-Guided Loss for Salient Object Detection. (arXiv:2101.02412v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02412">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a simple yet effective progressive self-guided loss function to
facilitate deep learning-based salient object detection (SOD) in images. The
saliency maps produced by the most relevant works still suffer from incomplete
predictions due to the internal complexity of salient objects. Our proposed
progressive self-guided loss simulates a morphological closing operation on the
model predictions for progressively creating auxiliary training supervisions to
step-wisely guide the training process. We demonstrate that this new loss
function can guide the SOD model to highlight more complete salient objects
step-by-step and meanwhile help to uncover the spatial dependencies of the
salient object pixels in a region growing manner. Moreover, a new feature
aggregation module is proposed to capture multi-scale features and aggregate
them adaptively by a branch-wise attention mechanism. Benefiting from this
module, our SOD framework takes advantage of adaptively aggregated multi-scale
features to locate and detect salient objects effectively. Experimental results
on several benchmark datasets show that our loss function not only advances the
performance of existing SOD models without architecture modification but also
helps our proposed framework to achieve state-of-the-art performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Attentive 3D Human Pose and Shape Estimation from Videos. (arXiv:2103.14182v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14182">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the task of estimating 3D human pose and shape from videos. While
existing frame-based approaches have made significant progress, these methods
are independently applied to each image, thereby often leading to inconsistent
predictions. In this work, we present a video-based learning algorithm for 3D
human pose and shape estimation. The key insights of our method are two-fold.
First, to address the inconsistent temporal prediction issue, we exploit
temporal information in videos and propose a self-attention module that jointly
considers short-range and long-range dependencies across frames, resulting in
temporally coherent estimations. Second, we model human motion with a
forecasting module that allows the transition between adjacent frames to be
smooth. We evaluate our method on the 3DPW, MPI-INF-3DHP, and Human3.6M
datasets. Extensive experimental results show that our algorithm performs
favorably against the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End to end hyperspectral imaging system with coded compression imaging process. (arXiv:2109.02643v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02643">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hyperspectral images (HSIs) can provide rich spatial and spectral information
with extensive application prospects. Recently, several methods using
convolutional neural networks (CNNs) to reconstruct HSIs have been developed.
However, most deep learning methods fit a brute-force mapping relationship
between the compressive and standard HSIs. Thus, the learned mapping would be
invalid when the observation data deviate from the training data. To recover
the three-dimensional HSIs from two-dimensional compressive images, we present
dual-camera equipment with a physics-informed self-supervising CNN method based
on a coded aperture snapshot spectral imaging system. Our method effectively
exploits the spatial-spectral relativization from the coded spectral
information and forms a self-supervising system based on the camera quantum
effect model. The experimental results show that our method can be adapted to a
wide imaging environment with good performance. In addition, compared with most
of the network-based methods, our system does not require a dedicated dataset
for pre-training. Therefore, it has greater scenario adaptability and better
generalization ability. Meanwhile, our system can be constantly fine-tuned and
self-improved in real-life scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under fair
evaluation protocols. In this paper, we propose a new learning scheme to
enforce domain invariance in the space of the gradients of the loss function:
specifically, we introduce a regularization term that matches the domain-level
variances of gradients across training domains. Critically, our strategy, named
Fishr, exhibits close relations with the Fisher Information and the Hessian of
the loss. We show that forcing domain-level gradient covariances to be similar
during the learning procedure eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. In
particular, Fishr improves the state of the art on the DomainBed benchmark and
performs significantly better than Empirical Risk Minimization. The code is
released at https://github.com/alexrame/fishr.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning from Self-Discrepancy via Multiple Co-teaching for Cross-Domain Person Re-Identification. (arXiv:2104.02265v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02265">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Employing clustering strategy to assign unlabeled target images with pseudo
labels has become a trend for person re-identification (re-ID) algorithms in
domain adaptation. A potential limitation of these clustering-based methods is
that they always tend to introduce noisy labels, which will undoubtedly hamper
the performance of our re-ID system. To handle this limitation, an intuitive
solution is to utilize collaborative training to purify the pseudo label
quality. However, there exists a challenge that the complementarity of two
networks, which inevitably share a high similarity, becomes weakened gradually
as training process goes on; worse still, these approaches typically ignore to
consider the self-discrepancy of intra-class relations. To address this issue,
in this paper, we propose a multiple co-teaching framework for domain adaptive
person re-ID, opening up a promising direction about self-discrepancy problem
under unsupervised condition. On top of that, a mean-teaching mechanism is
leveraged to enlarge the difference and discover more complementary features.
Comprehensive experiments conducted on several large-scale datasets show that
our method achieves competitive performance compared with the
state-of-the-arts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Support Vector Machine for Handwritten Character Recognition. (arXiv:2109.03081v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Handwriting recognition has been one of the most fascinating and challenging
research areas in field of image processing and pattern recognition. It
contributes enormously to the improvement of automation process. In this paper,
a system for recognition of unconstrained handwritten Malayalam characters is
proposed. A database of 10,000 character samples of 44 basic Malayalam
characters is used in this work. A discriminate feature set of 64 local and 4
global features are used to train and test SVM classifier and achieved 92.24%
accuracy</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Unpaired Adversarial Learning for Single Image Deraining with Rain-Space Contrastive Constraints. (arXiv:2109.02973v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning-based single image deraining (SID) with unpaired information is
of immense importance, as relying on paired synthetic data often limits their
generality and scalability in real-world applications. However, we noticed that
direct employ of unpaired adversarial learning and cycle-consistency
constraints in the SID task is insufficient to learn the underlying
relationship from rainy input to clean outputs, since the domain knowledge
between rainy and rain-free images is asymmetrical. To address such limitation,
we develop an effective unpaired SID method which explores mutual properties of
the unpaired exemplars by a contrastive learning manner in a GAN framework,
named as CDR-GAN. The proposed method mainly consists of two cooperative
branches: Bidirectional Translation Branch (BTB) and Contrastive Guidance
Branch (CGB). Specifically, BTB takes full advantage of the circulatory
architecture of adversarial consistency to exploit latent feature distributions
and guide transfer ability between two domains by equipping it with
bidirectional mapping. Simultaneously, CGB implicitly constrains the embeddings
of different exemplars in rain space by encouraging the similar feature
distributions closer while pushing the dissimilar further away, in order to
better help rain removal and image restoration. During training, we explore
several loss functions to further constrain the proposed CDR-GAN. Extensive
experiments show that our method performs favorably against existing unpaired
deraining approaches on both synthetic and real-world datasets, even
outperforms several fully-supervised or semi-supervised models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">nnFormer: Interleaved Transformer for Volumetric Segmentation. (arXiv:2109.03201v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers, the default model of choices in natural language processing,
have drawn scant attention from the medical imaging community. Given the
ability to exploit long-term dependencies, transformers are promising to help
atypical convolutional neural networks (convnets) to overcome its inherent
shortcomings of spatial inductive bias. However, most of recently proposed
transformer-based segmentation approaches simply treated transformers as
assisted modules to help encode global context into convolutional
representations without investigating how to optimally combine self-attention
(i.e., the core of transformers) with convolution. To address this issue, in
this paper, we introduce nnFormer (i.e., Not-aNother transFormer), a powerful
segmentation model with an interleaved architecture based on empirical
combination of self-attention and convolution. In practice, nnFormer learns
volumetric representations from 3D local volumes. Compared to the naive
voxel-level self-attention implementation, such volume-based operations help to
reduce the computational complexity by approximate 98% and 99.5% on Synapse and
ACDC datasets, respectively. In comparison to prior-art network configurations,
nnFormer achieves tremendous improvements over previous transformer-based
methods on two commonly used datasets Synapse and ACDC. For instance, nnFormer
outperforms Swin-UNet by over 7 percents on Synapse. Even when compared to
nnUNet, currently the best performing fully-convolutional medical segmentation
network, nnFormer still provides slightly better performance on Synapse and
ACDC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Combine the Modalities of Language and Video for Temporal Moment Localization. (arXiv:2109.02925v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02925">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal moment localization aims to retrieve the best video segment matching
a moment specified by a query. The existing methods generate the visual and
semantic embeddings independently and fuse them without full consideration of
the long-term temporal relationship between them. To address these
shortcomings, we introduce a novel recurrent unit, cross-modal long short-term
memory (CM-LSTM), by mimicking the human cognitive process of localizing
temporal moments that focuses on the part of a video segment related to the
part of a query, and accumulates the contextual information across the entire
video recurrently. In addition, we devise a two-stream attention mechanism for
both attended and unattended video features by the input query to prevent
necessary visual information from being neglected. To obtain more precise
boundaries, we propose a two-stream attentive cross-modal interaction network
(TACI) that generates two 2D proposal maps obtained globally from the
integrated contextual features, which are generated by using CM-LSTM, and
locally from boundary score sequences and then combines them into a final 2D
map in an end-to-end manner. On the TML benchmark dataset,
ActivityNet-Captions, the TACI outperform state-of-the-art TML methods with R@1
of 45.50% and 27.23% for IoU@0.5 and IoU@0.7, respectively. In addition, we
show that the revised state-of-the-arts methods by replacing the original LSTM
with our CM-LSTM achieve performance gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02929">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present our solution to extract albedo of branded labels for
e-commerce products. To this end, we generate a large-scale photo-realistic
synthetic data set for albedo extraction followed by training a generative
model to translate images with diverse lighting conditions to albedo. We
performed an extensive evaluation to test the generalisation of our method to
in-the-wild images. From the experimental results, we observe that our solution
generalises well compared to the existing method both in the unseen rendered
images as well as in the wild image.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Perceptual Video Compression with Recurrent Conditional GAN. (arXiv:2109.03082v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03082">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a Perceptual Learned Video Compression (PLVC) approach
with recurrent conditional generative adversarial network. In our approach, the
recurrent auto-encoder-based generator learns to fully explore the temporal
correlation for compressing video. More importantly, we propose a recurrent
conditional discriminator, which judges raw and compressed video conditioned on
both spatial and temporal information, including the latent representation,
temporal motion and hidden states in recurrent cells. This way, in the
adversarial training, it pushes the generated video to be not only spatially
photo-realistic but also temporally consistent with groundtruth and coherent
among video frames. Therefore, the proposed PLVC model learns to compress video
towards good perceptual quality at low bit-rate. The experimental results show
that our PLVC approach outperforms the previous traditional and learned
approaches on several perceptual quality metrics. The user study further
validates the outstanding perceptual performance of PLVC in comparison with the
latest learned video compression approaches and the official HEVC test model
(HM 16.20). The codes will be released at https://github.com/RenYang-home/PLVC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Generation of Dense Non-rigid Optical Flow. (arXiv:1812.01946v5 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1812.01946">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There hardly exists any large-scale datasets with dense optical flow of
non-rigid motion from real-world imagery as of today. The reason lies mainly in
the required setup to derive ground truth optical flows: a series of images
with known camera poses along its trajectory, and an accurate 3D model from a
textured scene. Human annotation is not only too tedious for large databases,
it can simply hardly contribute to accurate optical flow. To circumvent the
need for manual annotation, we propose a framework to automatically generate
optical flow from real-world videos. The method extracts and matches objects
from video frames to compute initial constraints, and applies a deformation
over the objects of interest to obtain dense optical flow fields. We propose
several ways to augment the optical flow variations. Extensive experimental
results show that training on our automatically generated optical flow
outperforms methods that are trained on rigid synthetic data using FlowNet-S,
LiteFlowNet, PWC-Net, and RAFT. Datasets and implementation of our optical flow
generation framework are released at https://github.com/lhoangan/arap_flow</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Kinship Verification Based on Cross-Generation Feature Interaction Learning. (arXiv:2109.02809v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02809">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Kinship verification from facial images has been recognized as an emerging
yet challenging technique in many potential computer vision applications. In
this paper, we propose a novel cross-generation feature interaction learning
(CFIL) framework for robust kinship verification. Particularly, an effective
collaborative weighting strategy is constructed to explore the characteristics
of cross-generation relations by corporately extracting features of both
parents and children image pairs. Specifically, we take parents and children as
a whole to extract the expressive local and non-local features. Different from
the traditional works measuring similarity by distance, we interpolate the
similarity calculations as the interior auxiliary weights into the deep CNN
architecture to learn the whole and natural features. These similarity weights
not only involve corresponding single points but also excavate the multiple
relationships cross points, where local and non-local features are calculated
by using these two kinds of distance measurements. Importantly, instead of
separately conducting similarity computation and feature extraction, we
integrate similarity learning and feature extraction into one unified learning
process. The integrated representations deduced from local and non-local
features can comprehensively express the informative semantics embedded in
images and preserve abundant correlation knowledge from image pairs. Extensive
experiments demonstrate the efficiency and superiority of the proposed model
compared to some state-of-the-art kinship verification methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rendezvous: Attention Mechanisms for the Recognition of Surgical Action Triplets in Endoscopic Videos. (arXiv:2109.03223v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03223">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Out of all existing frameworks for surgical workflow analysis in endoscopic
videos, action triplet recognition stands out as the only one aiming to provide
truly fine-grained and comprehensive information on surgical activities. This
information, presented as  combinations, is highly
challenging to be accurately identified. Triplet components can be difficult to
recognize individually; in this task, it requires not only performing
recognition simultaneously for all three triplet components, but also correctly
establishing the data association between them. To achieve this task, we
introduce our new model, the Rendezvous (RDV), which recognizes triplets
directly from surgical videos by leveraging attention at two different levels.
We first introduce a new form of spatial attention to capture individual action
triplet components in a scene; called the Class Activation Guided Attention
Mechanism (CAGAM). This technique focuses on the recognition of verbs and
targets using activations resulting from instruments. To solve the association
problem, our RDV model adds a new form of semantic attention inspired by
Transformer networks. Using multiple heads of cross and self attentions, RDV is
able to effectively capture relationships between instruments, verbs, and
targets. We also introduce CholecT50 - a dataset of 50 endoscopic videos in
which every frame has been annotated with labels from 100 triplet classes. Our
proposed RDV model significantly improves the triplet prediction mAP by over 9%
compared to the state-of-the-art methods on this dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the few-shot learning (FSL) problem, where a model learns to
recognize new objects with extremely few labeled training data per category.
Most of previous FSL approaches resort to the meta-learning paradigm, where the
model accumulates inductive bias through learning many training tasks so as to
solve a new unseen few-shot task. In contrast, we propose a simple approach to
exploit unlabeled data accompanying the few-shot task for improving few-shot
performance. Firstly, we propose a Dependency Maximization method based on the
Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the
statistical dependency between the embedded feature of those unlabeled data and
their label predictions, together with the supervised loss over the support
set. We then use the obtained model to infer the pseudo-labels for those
unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to
evaluate the credibility of each pseudo-labeled example and select the most
faithful ones into an augmented support set to retrain the model as in the
first step. We iterate the above process until the pseudo-labels for the
unlabeled data becomes stable. Following the standard transductive and
semi-supervised FSL setting, our experiments show that the proposed method
out-performs previous state-of-the-art methods on four widely used benchmarks,
including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Statistical analysis of locally parameterized shapes. (arXiv:2109.03027v1 [stat.ME])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The alignment of shapes has been a crucial step in statistical shape
analysis, for example, in calculating mean shape, detecting locational
differences between two shape populations, and classification. Procrustes
alignment is the most commonly used method and state of the art. In this work,
we uncover that alignment might seriously affect the statistical analysis. For
example, alignment can induce false shape differences and lead to misleading
results and interpretations. We propose a novel hierarchical shape
parameterization based on local coordinate systems. The local parameterized
shapes are translation and rotation invariant. Thus, the inherent alignment
problems from the commonly used global coordinate system for shape
representation can be avoided using this parameterization. The new
parameterization is also superior for shape deformation and simulation. The
method&#x27;s power is demonstrated on the hypothesis testing of simulated data as
well as the left hippocampi of patients with Parkinson&#x27;s disease and controls.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Journalistic Guidelines Aware News Image Captioning. (arXiv:2109.02865v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02865">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The task of news article image captioning aims to generate descriptive and
informative captions for news article images. Unlike conventional image
captions that simply describe the content of the image in general terms, news
image captions follow journalistic guidelines and rely heavily on named
entities to describe the image content, often drawing context from the whole
article they are associated with. In this work, we propose a new approach to
this task, motivated by caption guidelines that journalists follow. Our
approach, Journalistic Guidelines Aware News Image Captioning (JoGANIC),
leverages the structure of captions to improve the generation quality and guide
our representation design. Experimental results, including detailed ablation
studies, on two large-scale publicly available datasets show that JoGANIC
substantially outperforms state-of-the-art methods both on caption generation
and named entity related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting. (arXiv:2109.02974v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02974">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformer, as a strong and flexible architecture for modelling long-range
relations, has been widely explored in vision tasks. However, when used in
video inpainting that requires fine-grained representation, existed method
still suffers from yielding blurry edges in detail due to the hard patch
splitting. Here we aim to tackle this problem by proposing FuseFormer, a
Transformer model designed for video inpainting via fine-grained feature fusion
based on novel Soft Split and Soft Composition operations. The soft split
divides feature map into many patches with given overlapping interval. On the
contrary, the soft composition operates by stitching different patches into a
whole feature map where pixels in overlapping regions are summed up. These two
modules are first used in tokenization before Transformer layers and
de-tokenization after Transformer layers, for effective mapping between tokens
and features. Therefore, sub-patch level information interaction is enabled for
more effective feature propagation between neighboring patches, resulting in
synthesizing vivid content for hole regions in videos. Moreover, in FuseFormer,
we elaborately insert the soft composition and soft split into the feed-forward
network, enabling the 1D linear layers to have the capability of modelling 2D
structure. And, the sub-patch level feature fusion ability is further enhanced.
In both quantitative and qualitative evaluations, our proposed FuseFormer
surpasses state-of-the-art methods. We also conduct detailed analysis to
examine its superiority.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Dietary Assessment Via Integrated Hierarchy Food Classification. (arXiv:2109.02736v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02736">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image-based dietary assessment refers to the process of determining what
someone eats and how much energy and nutrients are consumed from visual data.
Food classification is the first and most crucial step. Existing methods focus
on improving accuracy measured by the rate of correct classification based on
visual information alone, which is very challenging due to the high complexity
and inter-class similarity of foods. Further, accuracy in food classification
is conceptual as description of a food can always be improved. In this work, we
introduce a new food classification framework to improve the quality of
predictions by integrating the information from multiple domains while
maintaining the classification accuracy. We apply a multi-task network based on
a hierarchical structure that uses both visual and nutrition domain specific
information to cluster similar foods. Our method is validated on the modified
VIPER-FoodNet (VFN) food image dataset by including associated energy and
nutrient information. We achieve comparable classification accuracy with
existing methods that use visual information only, but with less error in terms
of energy and nutrient values for the wrong predictions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The study of functional brain connectivity (FC) is important for
understanding the underlying mechanisms of many psychiatric disorders. Many
recent analyses adopt graph convolutional networks, to study non-linear
interactions between functionally-correlated states. However, although patterns
of brain activation are known to be hierarchically organised in both space and
time, many methods have failed to extract powerful spatio-temporal features. To
overcome those challenges, and improve understanding of long-range functional
dynamics, we translate an approach, from the domain of skeleton-based action
recognition, designed to model interactions across space and time. We evaluate
this approach using the Human Connectome Project (HCP) dataset on sex
classification and fluid intelligence prediction. To account for subject
topographic variability of functional organisation, we modelled functional
connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.
Results show a prediction accuracy of 94.4% for sex classification (an increase
of 6.2% compared to other methods), and an improvement of correlation with
fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes
space and time separately. Results suggest that explicit encoding of
spatio-temporal dynamics of brain functional activity may improve the precision
with which behavioural and cognitive phenotypes may be predicted in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep SIMBAD: Active Landmark-based Self-localization Using Ranking -based Scene Descriptor. (arXiv:2109.02786v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02786">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Landmark-based robot self-localization has recently garnered interest as a
highly-compressive domain-invariant approach for performing visual place
recognition (VPR) across domains (e.g., time of day, weather, and season).
However, landmark-based self-localization can be an ill-posed problem for a
passive observer (e.g., manual robot control), as many viewpoints may not
provide an effective landmark view. In this study, we consider an active
self-localization task by an active observer and present a novel reinforcement
learning (RL)-based next-best-view (NBV) planner. Our contributions are as
follows. (1) SIMBAD-based VPR: We formulate the problem of landmark-based
compact scene description as SIMBAD (similarity-based pattern recognition) and
further present its deep learning extension. (2) VPR-to-NBV knowledge transfer:
We address the challenge of RL under uncertainty (i.e., active
self-localization) by transferring the state recognition ability of VPR to the
NBV. (3) NNQL-based NBV: We regard the available VPR as the experience database
by adapting nearest-neighbor approximation of Q-learning (NNQL). The result
shows an extremely compact data structure that compresses both the VPR and NBV
into a single incremental inverted index. Experiments using the public NCLT
dataset validated the effectiveness of the proposed approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid advancement in deep learning makes the differentiation of authentic
and manipulated facial images and video clips unprecedentedly harder. The
underlying technology of manipulating facial appearances through deep
generative approaches, enunciated as DeepFake that have emerged recently by
promoting a vast number of malicious face manipulation applications.
Subsequently, the need of other sort of techniques that can assess the
integrity of digital visual content is indisputable to reduce the impact of the
creations of DeepFake. A large body of research that are performed on DeepFake
creation and detection create a scope of pushing each other beyond the current
status. This study presents challenges, research trends, and directions related
to DeepFake creation and detection techniques by reviewing the notable research
in the DeepFake domain to facilitate the development of more robust approaches
that could deal with the more advance DeepFake in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Crash Report Data Analysis for Creating Scenario-Wise, Spatio-Temporal Attention Guidance to Support Computer Vision-based Perception of Fatal Crash Risks. (arXiv:2109.02710v1 [stat.AP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02710">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Reducing traffic fatalities and serious injuries is a top priority of the US
Department of Transportation. The computer vision (CV)-based crash anticipation
in the near-crash phase is receiving growing attention. The ability to perceive
fatal crash risks earlier is also critical because it will improve the
reliability of crash anticipation. Yet, annotated image data for training a
reliable AI model for the early visual perception of crash risks are not
abundant. The Fatality Analysis Reporting System contains big data of fatal
crashes. It is a reliable data source for learning the relationship between
driving scene characteristics and fatal crashes to compensate for the
limitation of CV. Therefore, this paper develops a data analytics model, named
scenario-wise, Spatio-temporal attention guidance, from fatal crash report
data, which can estimate the relevance of detected objects to fatal crashes
from their environment and context information. First, the paper identifies
five sparse variables that allow for decomposing the 5-year fatal crash dataset
to develop scenario-wise attention guidance. Then, exploratory analysis of
location- and time-related variables of the crash report data suggests reducing
fatal crashes to spatially defined groups. The group&#x27;s temporal pattern is an
indicator of the similarity of fatal crashes in the group. Hierarchical
clustering and K-means clustering merge the spatially defined groups into six
clusters according to the similarity of their temporal patterns. After that,
association rule mining discovers the statistical relationship between the
temporal information of driving scenes with crash features, for each cluster.
The paper shows how the developed attention guidance supports the design and
implementation of a preliminary CV model that can identify objects of a
possibility to involve in fatal crashes from their environment and context
information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing road pothole detection approaches can be classified as computer
vision-based or machine learning-based. The former approaches typically employ
2-D image analysis/understanding or 3-D point cloud modeling and segmentation
algorithms to detect road potholes from vision sensor data. The latter
approaches generally address road pothole detection using convolutional neural
networks (CNNs) in an end-to-end manner. However, road potholes are not
necessarily ubiquitous and it is challenging to prepare a large well-annotated
dataset for CNN training. In this regard, while computer vision-based methods
were the mainstream research trend in the past decade, machine learning-based
methods were merely discussed. Recently, we published the first stereo
vision-based road pothole detection dataset and a novel disparity
transformation algorithm, whereby the damaged and undamaged road areas can be
highly distinguished. However, there are no benchmarks currently available for
state-of-the-art (SoTA) CNNs trained using either disparity images or
transformed disparity images. Therefore, in this paper, we first discuss the
SoTA CNNs designed for semantic segmentation and evaluate their performance for
road pothole detection with extensive experiments. Additionally, inspired by
graph neural network (GNN), we propose a novel CNN layer, referred to as graph
attention layer (GAL), which can be easily deployed in any existing CNN to
optimize image feature representations for semantic segmentation. Our
experiments compare GAL-DeepLabv3+, our best-performing implementation, with
nine SoTA CNNs on three modalities of training data: RGB images, disparity
images, and transformed disparity images. The experimental results suggest that
our proposed GAL-DeepLabv3+ achieves the best overall pothole detection
accuracy on all training data modalities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Landmarks Correspondence Detection in Medical Images with an Application to Deformable Image Registration. (arXiv:2109.02722v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02722">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deformable Image Registration (DIR) can benefit from additional guidance
using corresponding landmarks in the images. However, the benefits thereof are
largely understudied, especially due to the lack of automatic detection methods
for corresponding landmarks in three-dimensional (3D) medical images. In this
work, we present a Deep Convolutional Neural Network (DCNN), called DCNN-Match,
that learns to predict landmark correspondences in 3D images in a
self-supervised manner. We explored five variants of DCNN-Match that use
different loss functions and tested DCNN-Match separately as well as in
combination with the open-source registration software Elastix to assess its
impact on a common DIR approach. We employed lower-abdominal Computed
Tomography (CT) scans from cervical cancer patients: 121 pelvic CT scan pairs
containing simulated elastic transformations and 11 pairs demonstrating
clinical deformations. Our results show significant improvement in DIR
performance when landmark correspondences predicted by DCNN-Match were used in
case of simulated as well as clinical deformations. We also observed that the
spatial distribution of the automatically identified landmarks and the
associated matching errors affect the extent of improvement in DIR. Finally,
DCNN-Match was found to generalize well to Magnetic Resonance Imaging (MRI)
scans without requiring retraining, indicating easy applicability to other
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02765">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep neural networks have achieved remarkable success in various
computer vision tasks, they often fail to generalize to new domains and subtle
variations of input images. Several defenses have been proposed to improve the
robustness against these variations. However, current defenses can only
withstand the specific attack used in training, and the models often remain
vulnerable to other input variations. Moreover, these methods often degrade
performance of the model on clean images and do not generalize to out-of-domain
samples. In this paper we present Generative Adversarial Training, an approach
to simultaneously improve the model&#x27;s generalization to the test set and
out-of-domain samples as well as its robustness to unseen adversarial attacks.
Instead of altering a low-level pre-defined aspect of images, we generate a
spectrum of low-level, mid-level and high-level changes using generative models
with a disentangled latent space. Adversarial training with these examples
enable the model to withstand a wide range of attacks by observing a variety of
input alterations during training. We show that our approach not only improves
performance of the model on clean images and out-of-domain samples but also
makes it robust against unforeseen attacks and outperforms prior work. We
validate effectiveness of our method by demonstrating results on various tasks
such as classification, segmentation and object detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CIM: Class-Irrelevant Mapping for Few-Shot Classification. (arXiv:2109.02840v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02840">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Few-shot classification (FSC) is one of the most concerned hot issues in
recent years. The general setting consists of two phases: (1) Pre-train a
feature extraction model (FEM) with base data (has large amounts of labeled
samples). (2) Use the FEM to extract the features of novel data (with few
labeled samples and totally different categories from base data), then classify
them with the to-be-designed classifier. The adaptability of pre-trained FEM to
novel data determines the accuracy of novel features, thereby affecting the
final classification performances. To this end, how to appraise the pre-trained
FEM is the most crucial focus in the FSC community. It sounds like traditional
Class Activate Mapping (CAM) based methods can achieve this by overlaying
weighted feature maps. However, due to the particularity of FSC (e.g., there is
no backpropagation when using the pre-trained FEM to extract novel features),
we cannot activate the feature map with the novel classes. To address this
challenge, we propose a simple, flexible method, dubbed as Class-Irrelevant
Mapping (CIM). Specifically, first, we introduce dictionary learning theory and
view the channels of the feature map as the bases in a dictionary. Then we
utilize the feature map to fit the feature vector of an image to achieve the
corresponding channel weights. Finally, we overlap the weighted feature map for
visualization to appraise the ability of pre-trained FEM on novel data. For
fair use of CIM in evaluating different models, we propose a new measurement
index, called Feature Localization Accuracy (FLA). In experiments, we first
compare our CIM with CAM in regular tasks and achieve outstanding performances.
Next, we use our CIM to appraise several classical FSC frameworks without
considering the classification results and discuss them.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">STRIVE: Scene Text Replacement In Videos. (arXiv:2109.02762v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose replacing scene text in videos using deep style transfer and
learned photometric transformations.Building on recent progress on still image
text replacement,we present extensions that alter text while preserving the
appearance and motion characteristics of the original video.Compared to the
problem of still image text replacement,our method addresses additional
challenges introduced by video, namely effects induced by changing lighting,
motion blur, diverse variations in camera-object pose over time,and
preservation of temporal consistency. We parse the problem into three steps.
First, the text in all frames is normalized to a frontal pose using a
spatio-temporal trans-former network. Second, the text is replaced in a single
reference frame using a state-of-art still-image text replacement method.
Finally, the new text is transferred from the reference to remaining frames
using a novel learned image transformation network that captures lighting and
blur effects in a temporally consistent manner. Results on synthetic and
challenging real videos show realistic text trans-fer, competitive quantitative
and qualitative performance,and superior inference speed relative to
alternatives. We introduce new synthetic and real-world datasets with paired
text objects. To the best of our knowledge this is the first attempt at deep
video text replacement.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications. (arXiv:2109.02740v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02740">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We address the problem of estimating the shape of a person&#x27;s head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person&#x27;s pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person&#x27;s head, using both traditional facial landmarks and scalp features
extracted from the head&#x27;s segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02749">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pano3D is a new benchmark for depth estimation from spherical panoramas. It
aims to assess performance across all depth estimation traits, the primary
direct depth estimation performance targeting precision and accuracy, and also
the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D
moves beyond typical intra-dataset evaluation to inter-dataset performance
assessment. By disentangling the capacity to generalize to unseen data into
different test splits, Pano3D represents a holistic benchmark for $360^o$ depth
estimation. We use it as a basis for an extended analysis seeking to offer
insights into classical choices for depth estimation. This results in a solid
baseline for panoramic depth that follow-up works can build upon to steer
future progress.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images. (arXiv:2109.02716v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02716">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Crop and weed monitoring is an important challenge for agriculture and food
production nowadays. Thanks to recent advances in data acquisition and
computation technologies, agriculture is evolving to a more smart and precision
farming to meet with the high yield and high quality crop production.
Classification and recognition in Unmanned Aerial Vehicles (UAV) images are
important phases for crop monitoring. Advances in deep learning models relying
on Convolutional Neural Network (CNN) have achieved high performances in image
classification in the agricultural domain. Despite the success of this
architecture, CNN still faces many challenges such as high computation cost,
the need of large labelled datasets, ... Natural language processing&#x27;s
transformer architecture can be an alternative approach to deal with CNN&#x27;s
limitations. Making use of the self-attention paradigm, Vision Transformer
(ViT) models can achieve competitive or better results without applying any
convolution operations. In this paper, we adopt the self-attention mechanism
via the ViT models for plant classification of weeds and crops: red beet,
off-type beet (green leaves), parsley and spinach. Our experiments show that
with small set of labelled training data, ViT models perform better compared to
state-of-the-art CNN-based models EfficientNet and ResNet, with a top accuracy
of 99.8\% achieved by the ViT model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GCsT: Graph Convolutional Skeleton Transformer for Action Recognition. (arXiv:2109.02860v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02860">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph convolutional networks (GCNs) achieve promising performance for
skeleton-based action recognition. However, in most GCN-based methods, the
spatial-temporal graph convolution is strictly restricted by the graph topology
while only captures the short-term temporal context, thus lacking the
flexibility of feature extraction. In this work, we present a novel
architecture, named Graph Convolutional skeleton Transformer (GCsT), which
addresses limitations in GCNs by introducing Transformer. Our GCsT employs all
the benefits of Transformer (i.e. dynamical attention and global context) while
keeps the advantages of GCNs (i.e. hierarchy and local topology structure). In
GCsT, the spatial-temporal GCN forces the capture of local dependencies while
Transformer dynamically extracts global spatial-temporal relationships.
Furthermore, the proposed GCsT shows stronger expressive capability by adding
additional information present in skeleton sequences. Incorporating the
Transformer allows that information to be introduced into the model almost
effortlessly. We validate the proposed GCsT by conducting extensive
experiments, which achieves the state-of-the-art performance on NTU RGB+D, NTU
RGB+D 120 and Northwestern-UCLA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Binaural SoundNet: Predicting Semantics, Depth and Motion with Binaural Sounds. (arXiv:2109.02763v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02763">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Humans can robustly recognize and localize objects by using visual and/or
auditory cues. While machines are able to do the same with visual data already,
less work has been done with sounds. This work develops an approach for scene
understanding purely based on binaural sounds. The considered tasks include
predicting the semantic masks of sound-making objects, the motion of
sound-making objects, and the depth map of the scene. To this aim, we propose a
novel sensor setup and record a new audio-visual dataset of street scenes with
eight professional binaural microphones and a 360-degree camera. The
co-existence of visual and audio cues is leveraged for supervision transfer. In
particular, we employ a cross-modal distillation framework that consists of
multiple vision teacher methods and a sound student method -- the student
method is trained to generate the same results as the teacher methods do. This
way, the auditory system can be trained without using human annotations. To
further boost the performance, we propose another novel auxiliary task, coined
Spatial Sound Super-Resolution, to increase the directional resolution of
sounds. We then formulate the four tasks into one end-to-end trainable
multi-tasking network aiming to boost the overall performance. Experimental
results show that 1) our method achieves good results for all four tasks, 2)
the four tasks are mutually beneficial -- training them together achieves the
best performance, 3) the number and orientation of microphones are both
important, and 4) features learned from the standard spectrogram and features
obtained by the classic signal processing pipeline are complementary for
auditory perception tasks. The data and code are released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Driven by the need to capture users&#x27; evolving interests and optimize their
long-term experiences, more and more recommender systems have started to model
recommendation as a Markov decision process and employ reinforcement learning
to address the problem. Shouldn&#x27;t research on the fairness of recommender
systems follow the same trend from static evaluation and one-shot intervention
to dynamic monitoring and non-stop control? In this paper, we portray the
recent developments in recommender systems first and then discuss how fairness
could be baked into the reinforcement learning techniques for recommendation.
Moreover, we argue that in order to make further progress in recommendation
fairness, we may want to consider multi-agent (game-theoretic) optimization,
multi-objective (Pareto) optimization, and simulation-based optimization, in
the general framework of stochastic games.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03154">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Educational recommenders have received much less attention in comparison to
e-commerce and entertainment-related recommenders, even though efficient
intelligent tutors have great potential to improve learning gains. One of the
main challenges in advancing this research direction is the scarcity of large,
publicly available datasets. In this work, we release a large, novel dataset of
learners engaging with educational videos in-the-wild. The dataset, named
Personalised Educational Engagement with Knowledge Topics PEEK, is the first
publicly available dataset of this nature. The video lectures have been
associated with Wikipedia concepts related to the material of the lecture, thus
providing a humanly intuitive taxonomy. We believe that granular learner
engagement signals in unison with rich content representations will pave the
way to building powerful personalization algorithms that will revolutionise
educational and informational recommendation systems. Towards this goal, we 1)
construct a novel dataset from a popular video lecture repository, 2) identify
a set of benchmark algorithms to model engagement, and 3) run extensive
experimentation on the PEEK dataset to demonstrate its value. Our experiments
with the dataset show promise in building powerful informational recommender
systems. The dataset and the support code is available publicly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refining BERT Embeddings for Document Hashing via Mutual Information Maximization. (arXiv:2109.02867v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02867">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing unsupervised document hashing methods are mostly established on
generative models. Due to the difficulties of capturing long dependency
structures, these methods rarely model the raw documents directly, but instead
to model the features extracted from them (e.g. bag-of-words (BOW), TFIDF). In
this paper, we propose to learn hash codes from BERT embeddings after observing
their tremendous successes on downstream tasks. As a first try, we modify
existing generative hashing models to accommodate the BERT embeddings. However,
little improvement is observed over the codes learned from the old BOW or TFIDF
features. We attribute this to the reconstruction requirement in the generative
hashing, which will enforce irrelevant information that is abundant in the BERT
embeddings also compressed into the codes. To remedy this issue, a new
unsupervised hashing paradigm is further proposed based on the mutual
information (MI) maximization principle. Specifically, the method first
constructs appropriate global and local codes from the documents and then seeks
to maximize their mutual information. Experimental results on three benchmark
datasets demonstrate that the proposed method is able to generate hash codes
that outperform existing ones learned from BOW features by a substantial
margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DeepFakes: Detecting Forged and Synthetic Media Content Using Machine Learning. (arXiv:2109.02874v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02874">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The rapid advancement in deep learning makes the differentiation of authentic
and manipulated facial images and video clips unprecedentedly harder. The
underlying technology of manipulating facial appearances through deep
generative approaches, enunciated as DeepFake that have emerged recently by
promoting a vast number of malicious face manipulation applications.
Subsequently, the need of other sort of techniques that can assess the
integrity of digital visual content is indisputable to reduce the impact of the
creations of DeepFake. A large body of research that are performed on DeepFake
creation and detection create a scope of pushing each other beyond the current
status. This study presents challenges, research trends, and directions related
to DeepFake creation and detection techniques by reviewing the notable research
in the DeepFake domain to facilitate the development of more robust approaches
that could deal with the more advance DeepFake in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse Distributed Memory using Spiking Neural Networks on Nengo. (arXiv:2109.03111v1 [cs.NE])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03111">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a Spiking Neural Network (SNN) based Sparse Distributed Memory
(SDM) implemented on the Nengo framework. We have based our work on previous
work by Furber et al, 2004, implementing SDM using N-of-M codes. As an integral
part of the SDM design, we have implemented Correlation Matrix Memory (CMM)
using SNN on Nengo. Our SNN implementation uses Leaky Integrate and Fire (LIF)
spiking neuron models on Nengo. Our objective is to understand how well
SNN-based SDMs perform in comparison to conventional SDMs. Towards this, we
have simulated both conventional and SNN-based SDM and CMM on Nengo. We observe
that SNN-based models perform similarly as the conventional ones. In order to
evaluate the performance of different SNNs, we repeated the experiment using
Adaptive-LIF, Spiking Rectified Linear Unit, and Izhikevich models and obtained
similar results. We conclude that it is indeed feasible to develop some types
of associative memories using spiking neurons whose memory capacity and other
features are similar to the performance without SNNs. Finally we have
implemented an application where MNIST images, encoded with N-of-M codes, are
associated with their labels and stored in the SNN-based SDM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02859">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>User purchasing prediction with multi-behavior information remains a
challenging problem for current recommendation systems. Various methods have
been proposed to address it via leveraging the advantages of graph neural
networks (GNNs) or multi-task learning. However, most existing works do not
take the complex dependencies among different behaviors of users into
consideration. They utilize simple and fixed schemes, like neighborhood
information aggregation or mathematical calculation of vectors, to fuse the
embeddings of different user behaviors to obtain a unified embedding to
represent a user&#x27;s behavioral patterns which will be used in downstream
recommendation tasks. To tackle the challenge, in this paper, we first propose
the concept of hyper meta-path to construct hyper meta-paths or hyper
meta-graphs to explicitly illustrate the dependencies among different behaviors
of a user. How to obtain a unified embedding for a user from hyper meta-paths
and avoid the previously mentioned limitations simultaneously is critical.
Thanks to the recent success of graph contrastive learning, we leverage it to
learn embeddings of user behavior patterns adaptively instead of assigning a
fixed scheme to understand the dependencies among different behaviors. A new
graph contrastive learning based framework is proposed by coupling with hyper
meta-paths, namely HMG-CR, which consistently and significantly outperforms all
baselines in extensive comparison experiments.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Normalizing field flows: Solving forward and inverse stochastic differential equations using physics-informed flow models. (arXiv:2108.12956v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12956">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce in this work the normalizing field flows (NFF) for learning
random fields from scattered measurements. More precisely, we construct a
bijective transformation (a normalizing flow characterizing by neural networks)
between a Gaussian random field with the Karhunen-Lo\&#x60;eve (KL) expansion
structure and the target stochastic field, where the KL expansion coefficients
and the invertible networks are trained by maximizing the sum of the
log-likelihood on scattered measurements. This NFF model can be used to solve
data-driven forward, inverse, and mixed forward/inverse stochastic partial
differential equations in a unified framework. We demonstrate the capability of
the proposed NFF model for learning Non Gaussian processes and different types
of stochastic partial differential equations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Point-Cloud Deep Learning of Porous Media for Permeability Prediction. (arXiv:2107.14038v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14038">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a novel deep learning framework for predicting permeability of
porous media from their digital images. Unlike convolutional neural networks,
instead of feeding the whole image volume as inputs to the network, we model
the boundary between solid matrix and pore spaces as point clouds and feed them
as inputs to a neural network based on the PointNet architecture. This approach
overcomes the challenge of memory restriction of graphics processing units and
its consequences on the choice of batch size, and convergence. Compared to
convolutional neural networks, the proposed deep learning methodology provides
freedom to select larger batch sizes, due to reducing significantly the size of
network inputs. Specifically, we use the classification branch of PointNet and
adjust it for a regression task. As a test case, two and three dimensional
synthetic digital rock images are considered. We investigate the effect of
different components of our neural network on its performance. We compare our
deep learning strategy with a convolutional neural network from various
perspectives, specifically for maximum possible batch size. We inspect the
generalizability of our network by predicting the permeability of real-world
rock samples as well as synthetic digital rocks that are statistically
different from the samples used during training. The network predicts the
permeability of digital rocks a few thousand times faster than a Lattice
Boltzmann solver with a high level of prediction accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Convergence Analysis of Nonconvex Distributed Stochastic Zeroth-order Coordinate Method. (arXiv:2103.12954v2 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12954">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper investigates the stochastic distributed nonconvex optimization
problem of minimizing a global cost function formed by the summation of $n$
local cost functions. We solve such a problem by involving zeroth-order (ZO)
information exchange. In this paper, we propose a ZO distributed primal-dual
coordinate method (ZODIAC) to solve the stochastic optimization problem. Agents
approximate their own local stochastic ZO oracle along with coordinates with an
adaptive smoothing parameter. We show that the proposed algorithm achieves the
convergence rate of $\mathcal{O}(\sqrt{p}/\sqrt{T})$ for general nonconvex cost
functions. We demonstrate the efficiency of proposed algorithms through a
numerical example in comparison with the existing state-of-the-art centralized
and distributed ZO algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PLAM: a Posit Logarithm-Approximate Multiplier. (arXiv:2102.09262v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09262">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Posit Number System was introduced in 2017 as a replacement for
floating-point numbers. Since then, the community has explored its application
in Neural Network related tasks and produced some unit designs which are still
far from being competitive with their floating-point counterparts. This paper
proposes a Posit Logarithm-Approximate Multiplication (PLAM) scheme to
significantly reduce the complexity of posit multipliers, the most power-hungry
units within Deep Neural Network architectures. When comparing with
state-of-the-art posit multipliers, experiments show that the proposed
technique reduces the area, power, and delay of hardware multipliers up to
72.86%, 81.79%, and 17.01%, respectively, without accuracy degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How to Design a Three-Stage Architecture for Audio-Visual Active Speaker Detection in the Wild. (arXiv:2106.03932v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Successful active speaker detection requires a three-stage pipeline: (i)
audio-visual encoding for all speakers in the clip, (ii) inter-speaker relation
modeling between a reference speaker and the background speakers within each
frame, and (iii) temporal modeling for the reference speaker. Each stage of
this pipeline plays an important role for the final performance of the created
architecture. Based on a series of controlled experiments, this work presents
several practical guidelines for audio-visual active speaker detection.
Correspondingly, we present a new architecture called ASDNet, which achieves a
new state-of-the-art on the AVA-ActiveSpeaker dataset with a mAP of 93.5%
outperforming the second best with a large margin of 4.7%. Our code and
pretrained models are publicly available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semiparametric Bayesian Networks. (arXiv:2109.03008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce semiparametric Bayesian networks that combine parametric and
nonparametric conditional probability distributions. Their aim is to
incorporate the advantages of both components: the bounded complexity of
parametric models and the flexibility of nonparametric ones. We demonstrate
that semiparametric Bayesian networks generalize two well-known types of
Bayesian networks: Gaussian Bayesian networks and kernel density estimation
Bayesian networks. For this purpose, we consider two different conditional
probability distributions required in a semiparametric Bayesian network. In
addition, we present modifications of two well-known algorithms (greedy
hill-climbing and PC) to learn the structure of a semiparametric Bayesian
network from data. To realize this, we employ a score function based on
cross-validation. In addition, using a validation dataset, we apply an
early-stopping criterion to avoid overfitting. To evaluate the applicability of
the proposed algorithm, we conduct an exhaustive experiment on synthetic data
sampled by mixing linear and nonlinear functions, multivariate normal data
sampled from Gaussian Bayesian networks, real data from the UCI repository, and
bearings degradation data. As a result of this experiment, we conclude that the
proposed algorithm accurately learns the combination of parametric and
nonparametric components, while achieving a performance comparable with those
provided by state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Interactive slice visualization for exploring machine learning models. (arXiv:2101.06986v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.06986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Machine learning models fit complex algorithms to arbitrarily large datasets.
These algorithms are well-known to be high on performance and low on
interpretability. We use interactive visualization of slices of predictor space
to address the interpretability deficit; in effect opening up the black-box of
machine learning algorithms, for the purpose of interrogating, explaining,
validating and comparing model fits. Slices are specified directly through
interaction, or using various touring algorithms designed to visit
high-occupancy sections or regions where the model fits have interesting
properties. The methods presented here are implemented in the R package
\pkg{condvis2}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What you need to know to train recurrent neural networks to make Flip Flops memories and more. (arXiv:2010.07858v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training neural networks to perform different tasks is relevant across
various disciplines that go beyond Machine Learning. In particular, Recurrent
Neural Networks (RNN) are of great interest to different scientific
communities, for example, Computational Neuroscience research and Dynamical
Systems among others. Open-source frameworks dedicated to Machine Learning such
as Tensorflow and Keras has produced significant changes in the development of
technologies that we currently use. One relevant problem that can be approached
is how to build the models for the study of dynamical systems, and how to
extract the relevant information to be able to answer the scientific questions
of interest. The purpose of the present work is to contribute to this aim by
using a temporal processing task, in this case, a 3-bit Flip Flop memory, to
show the modeling procedure in every step: from equations to the software code
using Tensorflow and Keras. The obtained networks are analyzed to describe the
dynamics and to show different visualization and analysis tools. The code
developed in this work is provided to be used as a base for model other
systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.00596">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Practically Feasible Policies for Online 3D Bin Packing. (arXiv:2108.13680v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13680">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We tackle the Online 3D Bin Packing Problem, a challenging yet practically
useful variant of the classical Bin Packing Problem. In this problem, the items
are delivered to the agent without informing the full sequence information.
Agent must directly pack these items into the target bin stably without
changing their arrival order, and no further adjustment is permitted. Online
3D-BPP can be naturally formulated as Markov Decision Process (MDP). We adopt
deep reinforcement learning, in particular, the on-policy actor-critic
framework, to solve this MDP with constrained action space. To learn a
practically feasible packing policy, we propose three critical designs. First,
we propose an online analysis of packing stability based on a novel stacking
tree. It attains a high analysis accuracy while reducing the computational
complexity from $O(N^2)$ to $O(N \log N)$, making it especially suited for RL
training. Second, we propose a decoupled packing policy learning for different
dimensions of placement which enables high-resolution spatial discretization
and hence high packing precision. Third, we introduce a reward function that
dictates the robot to place items in a far-to-near order and therefore
simplifies the collision avoidance in movement planning of the robotic arm.
Furthermore, we provide a comprehensive discussion on several key implemental
issues. The extensive evaluation demonstrates that our learned policy
outperforms the state-of-the-art methods significantly and is practically
usable for real-world applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fishr: Invariant Gradient Variances for Out-of-distribution Generalization. (arXiv:2109.02934v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning robust models that generalize well under changes in the data
distribution is critical for real-world applications. To this end, there has
been a growing surge of interest to learn simultaneously from multiple training
domains - while enforcing different types of invariance across those domains.
Yet, all existing approaches fail to show systematic benefits under fair
evaluation protocols. In this paper, we propose a new learning scheme to
enforce domain invariance in the space of the gradients of the loss function:
specifically, we introduce a regularization term that matches the domain-level
variances of gradients across training domains. Critically, our strategy, named
Fishr, exhibits close relations with the Fisher Information and the Hessian of
the loss. We show that forcing domain-level gradient covariances to be similar
during the learning procedure eventually aligns the domain-level loss
landscapes locally around the final weights. Extensive experiments demonstrate
the effectiveness of Fishr for out-of-distribution generalization. In
particular, Fishr improves the state of the art on the DomainBed benchmark and
performs significantly better than Empirical Risk Minimization. The code is
released at https://github.com/alexrame/fishr.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quick Learning Mechanism with Cross-Domain Adaptation for Intelligent Fault Diagnosis. (arXiv:2103.08889v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.08889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The fault diagnostic model trained for a laboratory case machine fails to
perform well on the industrial machines running under variable operating
conditions. For every new operating condition of such machines, a new
diagnostic model has to be trained which is a time-consuming and uneconomical
process. Therefore, we propose a quick learning mechanism that can transform
the existing diagnostic model into a new model suitable for industrial machines
operating in different conditions. The proposed method uses the Net2Net
transformation followed by a fine-tuning to cancel/minimize the maximum mean
discrepancy between the new data and the previous one. The fine-tuning of the
model requires a very less amount of labelled target samples and very few
iterations of training. Therefore, the proposed method is capable of learning
the new target data pattern quickly. The effectiveness of the proposed fault
diagnosis method has been demonstrated on the Case Western Reserve University
dataset, Intelligent Maintenance Systems bearing dataset, and Paderborn
university dataset under the wide variations of the operating conditions. It
has been validated that the diagnostic model trained on artificially damaged
fault datasets can be used to quickly train another model for a real damage
dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Learning for Exotic Option Valuation. (arXiv:2103.12551v2 [q-fin.CP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12551">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A common approach to valuing exotic options involves choosing a model and
then determining its parameters to fit the volatility surface as closely as
possible. We refer to this as the model calibration approach (MCA). A
disadvantage of MCA is that some information in the volatility surface is lost
during the calibration process and the prices of exotic options will not in
general be consistent with those of plain vanilla options. We consider an
alternative approach where the structure of the user&#x27;s preferred model is
preserved but points on the volatility are features input to a neural network.
We refer to this as the volatility feature approach (VFA) model. We conduct
experiments showing that VFA can be expected to outperform MCA for the
volatility surfaces encountered in practice. Once the upfront computational
time has been invested in developing the neural network, the valuation of
exotic options using VFA is very fast.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Meta-Semi: A Meta-learning Approach for Semi-supervised Learning. (arXiv:2007.02394v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.02394">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep learning based semi-supervised learning (SSL) algorithms have led to
promising results in recent years. However, they tend to introduce multiple
tunable hyper-parameters, making them less practical in real SSL scenarios
where the labeled data is scarce for extensive hyper-parameter search. In this
paper, we propose a novel meta-learning based SSL algorithm (Meta-Semi) that
requires tuning only one additional hyper-parameter, compared with a standard
supervised deep learning algorithm, to achieve competitive performance under
various conditions of SSL. We start by defining a meta optimization problem
that minimizes the loss on labeled data through dynamically reweighting the
loss on unlabeled samples, which are associated with soft pseudo labels during
training. As the meta problem is computationally intensive to solve directly,
we propose an efficient algorithm to dynamically obtain the approximate
solutions. We show theoretically that Meta-Semi converges to the stationary
point of the loss function on labeled data under mild conditions. Empirically,
Meta-Semi outperforms state-of-the-art SSL algorithms significantly on the
challenging semi-supervised CIFAR-100 and STL-10 tasks, and achieves
competitive performance on CIFAR-10 and SVHN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refined approachability algorithms and application to regret minimization with global costs. (arXiv:2009.03831v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03831">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Blackwell&#x27;s approachability is a framework where two players, the Decision
Maker and the Environment, play a repeated game with vector-valued payoffs. The
goal of the Decision Maker is to make the average payoff converge to a given
set called the target. When this is indeed possible, simple algorithms which
guarantee the convergence are known. This abstract tool was successfully used
for the construction of optimal strategies in various repeated games, but also
found several applications in online learning. By extending an approach
proposed by (Abernethy et al., 2011), we construct and analyze a class of
Follow the Regularized Leader algorithms (FTRL) for Blackwell&#x27;s approachability
which are able to minimize not only the Euclidean distance to the target set
(as it is often the case in the context of Blackwell&#x27;s approachability) but a
wide range of distance-like quantities. This flexibility enables us to apply
these algorithms to closely minimize the quantity of interest in various online
learning problems. In particular, for regret minimization with $\ell_p$ global
costs, we obtain the first bounds with explicit dependence in $p$ and the
dimension $d$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Personalized Glucose Level Forecasting Using Attention-based Recurrent Neural Networks. (arXiv:2106.00884v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00884">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we study the problem of blood glucose forecasting and provide
a deep personalized solution. Predicting blood glucose level in people with
diabetes has significant value because health complications of abnormal glucose
level are serious, sometimes even leading to death. Therefore, having a model
that can accurately and quickly warn patients of potential problems is
essential. To develop a better deep model for blood glucose forecasting, we
analyze the data and detect important patterns. These observations helped us to
propose a method that has several key advantages over existing methods: 1- it
learns a personalized model for each patient as well as a global model; 2- it
uses an attention mechanism and extracted time features to better learn
long-term dependencies in the data; 3- it introduces a new, robust training
procedure for time series data. We empirically show the efficacy of our model
on a real dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fast Sample Re-weighting Without Reward Data. (arXiv:2109.03216v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training sample re-weighting is an effective approach for tackling data
biases such as imbalanced and corrupted labels. Recent methods develop
learning-based algorithms to learn sample re-weighting strategies jointly with
model training based on the frameworks of reinforcement learning and meta
learning. However, depending on additional unbiased reward data is limiting
their general applicability. Furthermore, existing learning-based sample
re-weighting methods require nested optimizations of models and weighting
parameters, which requires expensive second-order computation. This paper
addresses these two problems and presents a novel learning-based fast sample
re-weighting (FSR) method that does not require additional reward data. The
method is based on two key ideas: learning from history to build proxy reward
data and feature sharing to reduce the optimization cost. Our experiments show
the proposed method achieves competitive results compared to state of the arts
on label noise robustness and long-tailed recognition, and does so while
achieving significantly improved training efficiency. The source code is
publicly available at
https://github.com/google-research/google-research/tree/master/ieg.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v3 [cs.IT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Jeffreys divergence is a renown symmetrization of the oriented
Kullback-Leibler divergence broadly used in information sciences. Since the
Jeffreys divergence between Gaussian mixture models is not available in
closed-form, various techniques with pros and cons have been proposed in the
literature to either estimate, approximate, or lower and upper bound this
divergence. In this paper, we propose a simple yet fast heuristic to
approximate the Jeffreys divergence between two univariate Gaussian mixtures
with arbitrary number of components. Our heuristic relies on converting the
mixtures into pairs of dually parameterized probability densities belonging to
an exponential family. In particular, we consider the versatile polynomial
exponential family densities, and design a divergence to measure in closed-form
the goodness of fit between a Gaussian mixture and its polynomial exponential
density approximation. This goodness-of-fit divergence is a generalization of
the Hyv\&quot;arinen divergence used to estimate models with computationally
intractable normalizers. It allows us to perform model selection by choosing
the orders of the polynomial exponential densities used to approximate the
mixtures. We demonstrate experimentally that our heuristic to approximate the
Jeffreys divergence improves by several orders of magnitude the computational
time of stochastic Monte Carlo estimations while approximating reasonably well
the Jeffreys divergence, specially when the mixtures have a very small number
of modes. Besides, our mixture-to-exponential family conversion techniques may
prove useful in other settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Novelty detection using ensembles with regularized disagreement. (arXiv:2012.05825v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05825">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Despite their excellent performance on in-distribution (ID) data, machine
learning-based prediction systems often predict out-of-distribution (OOD)
samples incorrectly while indicating high confidence. Instead, they should flag
samples that are not similar to the training data, for example, when new
classes emerge over time. Even though current OOD detection algorithms can
successfully distinguish completely different data sets, they fail to reliably
identify samples from novel classes. We develop a new ensemble-based procedure
that promotes model diversity and exploits regularization to limit disagreement
to only OOD samples, using a batch containing an unknown mixture of ID and OOD
data. We show that our procedure significantly outperforms state-of-the-art
methods, including those that have access, during training, to data that is
known to be OOD. We run extensive comparisons of our approach on a variety of
novel-class detection scenarios, on standard image data sets such as
SVHN/CIFAR-10/CIFAR-100, as well as on new disease detection on medical image
data sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Controlled Actor-Critic. (arXiv:2109.02517v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02517">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>On error of value function inevitably causes an overestimation phenomenon and
has a negative impact on the convergence of the algorithms. To mitigate the
negative effects of the approximation error, we propose Error Controlled
Actor-critic which ensures confining the approximation error in value function.
We present an analysis of how the approximation error can hinder the
optimization process of actor-critic methods.Then, we derive an upper boundary
of the approximation error of Q function approximator and find that the error
can be lowered by restricting on the KL-divergence between every two
consecutive policies when training the policy. The results of experiments on a
range of continuous control tasks demonstrate that the proposed actor-critic
algorithm apparently reduces the approximation error and significantly
outperforms other model-free RL algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Diff-ResNets for Few-shot Learning -- an ODE Perspective. (arXiv:2105.03155v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Interpreting deep neural networks from the ordinary differential equations
(ODEs) perspective has inspired many efficient and robust network
architectures. However, existing ODE based approaches ignore the relationship
among data points, which is a critical component in many problems including
few-shot learning and semi-supervised learning. In this paper, inspired by the
diffusive ODEs, we propose a novel diffusion residual network (Diff-ResNet) to
strengthen the interactions among data points. Under the structured data
assumption, it is proved that the diffusion mechanism can decrease the
distance-diameter ratio that improves the separability of inter-class points
and reduces the distance among local intra-class points. This property can be
easily adopted by the residual networks for constructing the separable
hyperplanes. The synthetic binary classification experiments demonstrate the
effectiveness of the proposed diffusion mechanism. Moreover, extensive
experiments of few-shot image classification and semi-supervised graph node
classification in various datasets validate the advantages of the proposed
Diff-ResNet over existing few-shot learning methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CD-SGD: Distributed Stochastic Gradient Descent with Compression and Delay Compensation. (arXiv:2106.10796v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10796">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Communication overhead is the key challenge for distributed training.
Gradient compression is a widely used approach to reduce communication traffic.
When combining with parallel communication mechanism method like pipeline,
gradient compression technique can greatly alleviate the impact of
communication overhead. However, there exists two problems of gradient
compression technique to be solved. Firstly, gradient compression brings in
extra computation cost, which will delay the next training iteration. Secondly,
gradient compression usually leads to the decrease of convergence accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GANSER: A Self-supervised Data Augmentation Framework for EEG-based Emotion Recognition. (arXiv:2109.03124v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03124">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The data scarcity problem in Electroencephalography (EEG) based affective
computing results into difficulty in building an effective model with high
accuracy and stability using machine learning algorithms especially deep
learning models. Data augmentation has recently achieved considerable
performance improvement for deep learning models: increased accuracy,
stability, and reduced over-fitting. In this paper, we propose a novel data
augmentation framework, namely Generative Adversarial Network-based
Self-supervised Data Augmentation (GANSER). As the first to combine adversarial
training with self-supervised learning for EEG-based emotion recognition, the
proposed framework can generate high-quality and high-diversity simulated EEG
samples. In particular, we utilize adversarial training to learn an EEG
generator and force the generated EEG signals to approximate the distribution
of real samples, ensuring the quality of augmented samples. A transformation
function is employed to mask parts of EEG signals and force the generator to
synthesize potential EEG signals based on the remaining parts, to produce a
wide variety of samples. The masking possibility during transformation is
introduced as prior knowledge to guide to extract distinguishable features for
simulated EEG signals and generalize the classifier to the augmented sample
space. Finally, extensive experiments demonstrate our proposed method can help
emotion recognition for performance gain and achieve state-of-the-art results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Experimental Quantum Generative Adversarial Networks for Image Generation. (arXiv:2010.06201v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06201">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantum machine learning is expected to be one of the first practical
applications of near-term quantum devices. Pioneer theoretical works suggest
that quantum generative adversarial networks (GANs) may exhibit a potential
exponential advantage over classical GANs, thus attracting widespread
attention. However, it remains elusive whether quantum GANs implemented on
near-term quantum devices can actually solve real-world learning tasks. Here,
we devise a flexible quantum GAN scheme to narrow this knowledge gap, which
could accomplish image generation with arbitrarily high-dimensional features,
and could also take advantage of quantum superposition to train multiple
examples in parallel. For the first time, we experimentally achieve the
learning and generation of real-world hand-written digit images on a
superconducting quantum processor. Moreover, we utilize a gray-scale bar
dataset to exhibit the competitive performance between quantum GANs and the
classical GANs based on multilayer perceptron and convolutional neural network
architectures, respectively, benchmarked by the Fr\&#x27;echet Distance score. Our
work provides guidance for developing advanced quantum generative models on
near-term quantum devices and opens up an avenue for exploring quantum
advantages in various GAN-related learning tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OdoNet: Untethered Speed Aiding for Vehicle Navigation Without Hardware Wheeled Odometer. (arXiv:2109.03091v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03091">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Odometer has been proven to significantly improve the accuracy of the Global
Navigation Satellite System / Inertial Navigation System (GNSS/INS) integrated
vehicle navigation in GNSS-challenged environments. However, the odometer is
inaccessible in many applications, especially for aftermarket devices. To apply
forward speed aiding without hardware wheeled odometer, we propose OdoNet, an
untethered one-dimensional Convolution Neural Network (CNN)-based
pseudo-odometer model learning from a single Inertial Measurement Unit (IMU),
which can act as an alternative to the wheeled odometer. Dedicated experiments
have been conducted to verify the feasibility and robustness of the OdoNet. The
results indicate that the IMU individuality, the vehicle loads, and the road
conditions have little impact on the robustness and precision of the OdoNet,
while the IMU biases and the mounting angles may notably ruin the OdoNet. Thus,
a data-cleaning procedure is added to effectively mitigate the impacts of the
IMU biases and the mounting angles. Compared to the process using only
non-holonomic constraint (NHC), after employing the pseudo-odometer, the
positioning error is reduced by around 68%, while the percentage is around 74%
for the hardware wheeled odometer. In conclusion, the proposed OdoNet can be
employed as an untethered pseudo-odometer for vehicle navigation, which can
efficiently improve the accuracy and reliability of the positioning in
GNSS-denied environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SimJEB: Simulated Jet Engine Bracket Dataset. (arXiv:2105.03534v2 [cs.CE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.03534">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper introduces the Simulated Jet Engine Bracket Dataset (SimJEB): a
new, public collection of crowdsourced mechanical brackets and accompanying
structural simulations. SimJEB is applicable to a wide range of geometry
processing tasks; the complexity of the shapes in SimJEB offer a challenge to
automated geometry cleaning and meshing, while categorical labels and
structural simulations facilitate classification and regression (i.e.
engineering surrogate modeling). In contrast to existing shape collections,
SimJEB&#x27;s models are all designed for the same engineering function and thus
have consistent structural loads and support conditions. On the other hand,
SimJEB models are more complex, diverse, and realistic than the synthetically
generated datasets commonly used in parametric surrogate model evaluation. The
designs in SimJEB were derived from submissions to the GrabCAD Jet Engine
Bracket Challenge: an open engineering design competition with over 700
hand-designed CAD entries from 320 designers representing 56 countries. Each
model has been cleaned, categorized, meshed, and simulated with finite element
analysis according to the original competition specifications. The result is a
collection of 381 diverse, high-quality and application-focused designs for
advancing geometric deep learning, engineering surrogate modeling, automated
cleaning and related geometry processing tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Auxiliary Diagnosing Coronary Stenosis Using Machine Learning. (arXiv:2007.10316v4 [q-bio.TO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2007.10316">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How to accurately classify and diagnose whether an individual has Coronary
Stenosis (CS) without invasive physical examination? This problem has not been
solved satisfactorily. To this end, the four machine learning (ML) algorithms,
i.e., Boosted Tree (BT), Decision Tree (DT), Logistic Regression (LR) and
Random Forest (RF) are employed in this paper. First, eleven features including
basic information of an individual, symptoms and results of routine physical
examination are selected, as well as one label is specified, indicating whether
an individual suffers from different severity of coronary artery stenosis or
not. On the basis of it, a sample set is constructed. Second, each of these
four ML algorithms learns from the sample set to obtain the corresponding
optimal classified results, respectively. The experimental results show that:
RF performs better than other three algorithms, and the former algorithm
classifies whether an individual has CS with an accuracy of 95.7% (&#x3D;90/94).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ConvNets for Counting: Object Detection of Transient Phenomena in Steelpan Drums. (arXiv:2102.00632v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.00632">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We train an object detector built from convolutional neural networks to count
interference fringes in elliptical antinode regions in frames of high-speed
video recordings of transient oscillations in Caribbean steelpan drums
illuminated by electronic speckle pattern interferometry (ESPI). The
annotations provided by our model aim to contribute to the understanding of
time-dependent behavior in such drums by tracking the development of
sympathetic vibration modes. The system is trained on a dataset of crowdsourced
human-annotated images obtained from the Zooniverse Steelpan Vibrations
Project. Due to the small number of human-annotated images and the ambiguity of
the annotation task, we also evaluate the model on a large corpus of synthetic
images whose properties have been matched to the real images by style transfer
using a Generative Adversarial Network. Applying the model to thousands of
unlabeled video frames, we measure oscillations consistent with audio
recordings of these drum strikes. One unanticipated result is that sympathetic
oscillations of higher-octave notes significantly precede the rise in sound
intensity of the corresponding second harmonic tones; the mechanism responsible
for this remains unidentified. This paper primarily concerns the development of
the predictive model; further exploration of the steelpan images and deeper
physical insights await its further application.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Phenotype Prediction using Long-Range Spatio-Temporal Dynamics of Functional Connectivity. (arXiv:2109.03115v1 [q-bio.NC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The study of functional brain connectivity (FC) is important for
understanding the underlying mechanisms of many psychiatric disorders. Many
recent analyses adopt graph convolutional networks, to study non-linear
interactions between functionally-correlated states. However, although patterns
of brain activation are known to be hierarchically organised in both space and
time, many methods have failed to extract powerful spatio-temporal features. To
overcome those challenges, and improve understanding of long-range functional
dynamics, we translate an approach, from the domain of skeleton-based action
recognition, designed to model interactions across space and time. We evaluate
this approach using the Human Connectome Project (HCP) dataset on sex
classification and fluid intelligence prediction. To account for subject
topographic variability of functional organisation, we modelled functional
connectomes using multi-resolution dual-regressed (subject-specific) ICA nodes.
Results show a prediction accuracy of 94.4% for sex classification (an increase
of 6.2% compared to other methods), and an improvement of correlation with
fluid intelligence of 0.325 vs 0.144, relative to a baseline model that encodes
space and time separately. Results suggest that explicit encoding of
spatio-temporal dynamics of brain functional activity may improve the precision
with which behavioural and cognitive phenotypes may be predicted in the future.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Relaxed Quantization with DropBits: Training Low-Bit Neural Networks via Bit-wise Regularization. (arXiv:1911.12990v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12990">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged as one of the key ingredients to reduce
the size of neural networks for their deployments to resource-limited devices.
In order to overcome the nature of transforming continuous activations and
weights to discrete ones, recent study called Relaxed Quantization (RQ)
[Louizos et al. 2019] successfully employ the popular Gumbel-Softmax that
allows this transformation with efficient gradient-based optimization. However,
RQ with this Gumbel-Softmax relaxation still suffers from bias-variance
trade-off depending on the temperature parameter of Gumbel-Softmax. To resolve
the issue, we propose a novel method, Semi-Relaxed Quantization (SRQ) that uses
multi-class straight-through estimator to effectively reduce the bias and
variance, along with a new regularization technique, DropBits that replaces
dropout regularization to randomly drop the bits instead of neurons to further
reduce the bias of the multi-class straight-through estimator in SRQ. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer
using DropBits. We experimentally validate our method on various benchmark
datasets and network architectures, and also support the quantized lottery
ticket hypothesis: learning heterogeneous quantization levels outperforms the
case using the same but fixed quantization levels from scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Explanations for Occluded Images. (arXiv:2103.03622v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03622">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing algorithms for explaining the output of image classifiers perform
poorly on inputs where the object of interest is partially occluded. We present
a novel, black-box algorithm for computing explanations that uses a principled
approach based on causal theory. We have implemented the method in the
DEEPCOVER tool. We obtain explanations that are much more accurate than those
generated by the existing explanation tools on images with occlusions and
observe a level of performance comparable to the state of the art when
explaining images without occlusions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backdoor Attacks Against Deep Learning Systems in the Physical World. (arXiv:2006.14580v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.14580">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Backdoor attacks embed hidden malicious behaviors into deep learning models,
which only activate and cause misclassifications on model inputs containing a
specific trigger. Existing works on backdoor attacks and defenses, however,
mostly focus on digital attacks that use digitally generated patterns as
triggers. A critical question remains unanswered: can backdoor attacks succeed
using physical objects as triggers, thus making them a credible threat against
deep learning systems in the real world? We conduct a detailed empirical study
to explore this question for facial recognition, a critical deep learning task.
Using seven physical objects as triggers, we collect a custom dataset of 3205
images of ten volunteers and use it to study the feasibility of physical
backdoor attacks under a variety of real-world conditions. Our study reveals
two key findings. First, physical backdoor attacks can be highly successful if
they are carefully configured to overcome the constraints imposed by physical
objects. In particular, the placement of successful triggers is largely
constrained by the target model&#x27;s dependence on key facial features. Second,
four of today&#x27;s state-of-the-art defenses against (digital) backdoors are
ineffective against physical backdoors, because the use of physical objects
breaks core assumptions used to construct these defenses. Our study confirms
that (physical) backdoor attacks are not a hypothetical phenomenon but rather
pose a serious real-world threat to critical classification tasks. We need new
and more robust defenses against backdoors in the physical world.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online hate speech has caught everyone&#x27;s attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity&#x27;s identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distance preserving model order reduction of graph-Laplacians and cluster analysis. (arXiv:1809.03048v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.03048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph-Laplacians and their spectral embeddings play an important role in
multiple areas of machine learning. This paper is focused on graph-Laplacian
dimension reduction for the spectral clustering of data as a primary
application. Spectral embedding provides a low-dimensional parametrization of
the data manifold which makes the subsequent task (e.g., clustering) much
easier. However, despite reducing the dimensionality of data, the overall
computational cost may still be prohibitive for large data sets due to two
factors. First, computing the partial eigendecomposition of the graph-Laplacian
typically requires a large Krylov subspace. Second, after the spectral
embedding is complete, one still has to operate with the same number of data
points. For example, clustering of the embedded data is typically performed
with various relaxations of k-means which computational cost scales poorly with
respect to the size of data set. In this work, we switch the focus from the
entire data set to a subset of graph vertices (target subset). We develop two
novel algorithms for such low-dimensional representation of the original graph
that preserves important global distances between the nodes of the target
subset. In particular, it allows to ensure that target subset clustering is
consistent with the spectral clustering of the full data set if one would
perform such. That is achieved by a properly parametrized reduced-order model
(ROM) of the graph-Laplacian that approximates accurately the diffusion
transfer function of the original graph for inputs and outputs restricted to
the target subset. Working with a small target subset reduces greatly the
required dimension of Krylov subspace and allows to exploit the conventional
algorithms (like approximations of k-means) in the regimes when they are most
robust and efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Utilizing a digital swarm intelligence platform to improve consensus among radiologists and exploring its applications. (arXiv:2107.07341v2 [cs.HC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07341">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Radiologists today play a key role in making diagnostic decisions and
labeling images for training A.I. algorithms. Low inter-reader reliability
(IRR) can be seen between experts when interpreting challenging cases. While
teams-based decisions are known to outperform individual decisions,
inter-personal biases often creep up in group interactions which limit
non-dominant participants from expressing true opinions. To overcome the dual
problems of low consensus and inter-personal bias, we explored a solution
modeled on biological swarms of bees. Two separate cohorts; three radiologists
and five radiology residents collaborated on a digital swarm platform in real
time and in a blinded fashion, grading meniscal lesions on knee MR exams. These
consensus votes were benchmarked against clinical (arthroscopy) and
radiological (senior-most radiologist) observations. The IRR of the consensus
votes was compared to the IRR of the majority and most confident votes of the
two cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm
votes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm
votes over majority vote, was observed. The 5-resident swarm had an even higher
improvement of 32% in IRR over majority vote. Swarm consensus votes also
improved specificity by up to 50%. The swarm consensus votes outperformed
individual and majority vote decisions in both the radiologists and resident
cohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating
positive effect of increased swarm size. The attending and resident swarms also
outperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a
digital swarm platform improved agreement and allows participants to express
judgement free intent, resulting in superior clinical performance and robust
A.I. training labels.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of MRI Biomarkers for Brain Cancer Survival Prediction. (arXiv:2109.02785v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Prediction of Overall Survival (OS) of brain cancer patients from multi-modal
MRI is a challenging field of research. Most of the existing literature on
survival prediction is based on Radiomic features, which does not consider
either non-biological factors or the functional neurological status of the
patient(s). Besides, the selection of an appropriate cut-off for survival and
the presence of censored data create further problems. Application of deep
learning models for OS prediction is also limited due to the lack of large
annotated publicly available datasets. In this scenario we analyse the
potential of two novel neuroimaging feature families, extracted from brain
parcellation atlases and spatial habitats, along with classical radiomic and
geometric features; to study their combined predictive power for analysing
overall survival. A cross validation strategy with grid search is proposed to
simultaneously select and evaluate the most predictive feature subset based on
its predictive power. A Cox Proportional Hazard (CoxPH) model is employed for
univariate feature selection, followed by the prediction of patient-specific
survival functions by three multivariate parsimonious models viz. Coxnet,
Random survival forests (RSF) and Survival SVM (SSVM). The brain cancer MRI
data used for this research was taken from two open-access collections TCGA-GBM
and TCGA-LGG available from The Cancer Imaging Archive (TCIA). Corresponding
survival data for each patient was downloaded from The Cancer Genome Atlas
(TCGA). A high cross validation $C-index$ score of $0.82\pm.10$ was achieved
using RSF with the best $24$ selected features. Age was found to be the most
important biological predictor. There were $9$, $6$, $6$ and $2$ features
selected from the parcellation, habitat, radiomic and region-based feature
groups respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Prescriptive Process Monitoring Under Resource Constraints: A Causal Inference Approach. (arXiv:2109.02894v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02894">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Prescriptive process monitoring is a family of techniques to optimize the
performance of a business process by triggering interventions at runtime.
Existing prescriptive process monitoring techniques assume that the number of
interventions that may be triggered is unbounded. In practice, though, specific
interventions consume resources with finite capacity. For example, in a loan
origination process, an intervention may consist of preparing an alternative
loan offer to increase the applicant&#x27;s chances of taking a loan. This
intervention requires a certain amount of time from a credit officer, and thus,
it is not possible to trigger this intervention in all cases. This paper
proposes a prescriptive process monitoring technique that triggers
interventions to optimize a cost function under fixed resource constraints. The
proposed technique relies on predictive modeling to identify cases that are
likely to lead to a negative outcome, in combination with causal inference to
estimate the effect of an intervention on the outcome of the case. These
outputs are then used to allocate resources to interventions to maximize a cost
function. A preliminary empirical evaluation suggests that the proposed
approach produces a higher net gain than a purely predictive (non-causal)
baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fruit-CoV: An Efficient Vision-based Framework for Speedy Detection and Diagnosis of SARS-CoV-2 Infections Through Recorded Cough Sounds. (arXiv:2109.03219v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03219">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>SARS-CoV-2 is colloquially known as COVID-19 that had an initial outbreak in
December 2019. The deadly virus has spread across the world, taking part in the
global pandemic disease since March 2020. In addition, a recent variant of
SARS-CoV-2 named Delta is intractably contagious and responsible for more than
four million deaths over the world. Therefore, it is vital to possess a
self-testing service of SARS-CoV-2 at home. In this study, we introduce
Fruit-CoV, a two-stage vision framework, which is capable of detecting
SARS-CoV-2 infections through recorded cough sounds. Specifically, we convert
sounds into Log-Mel Spectrograms and use the EfficientNet-V2 network to extract
its visual features in the first stage. In the second stage, we use 14
convolutional layers extracted from the large-scale Pretrained Audio Neural
Networks for audio pattern recognition (PANNs) and the Wavegram-Log-Mel-CNN to
aggregate feature representations of the Log-Mel Spectrograms. Finally, we use
the combined features to train a binary classifier. In this study, we use a
dataset provided by the AICovidVN 115M Challenge, which includes a total of
7371 recorded cough sounds collected throughout Vietnam, India, and
Switzerland. Experimental results show that our proposed model achieves an AUC
score of 92.8% and ranks the 1st place on the leaderboard of the AICovidVN
Challenge. More importantly, our proposed framework can be integrated into a
call center or a VoIP system to speed up detecting SARS-CoV-2 infections
through online/recorded cough sounds.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regularized Learning in Banach Spaces. (arXiv:2109.03159v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03159">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This article presents a different way to study the theory of regularized
learning for generalized data including representer theorems and convergence
theorems. The generalized data are composed of linear functionals and real
scalars to represent the discrete information of the local models. By the
extension of the classical machine learning, the empirical risks are computed
by the generalized data and the loss functions. According to the techniques of
regularization, the global solutions are approximated by minimizing the
regularized empirical risks over the Banach spaces. The Banach spaces are
adaptively chosen to endow the generalized input data with compactness such
that the existence and convergence of the approximate solutions are guaranteed
by the weak* topology.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Density Estimation under Besov IPM Losses. (arXiv:2004.08597v2 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.08597">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study minimax convergence rates of nonparametric density estimation in the
Huber contamination model, in which a proportion of the data comes from an
unknown outlier distribution. We provide the first results for this problem
under a large family of losses, called Besov integral probability metrics
(IPMs), that includes $\mathcal{L}^p$, Wasserstein, Kolmogorov-Smirnov, and
other common distances between probability distributions. Specifically, under a
range of smoothness assumptions on the population and outlier distributions, we
show that a re-scaled thresholding wavelet series estimator achieves minimax
optimal convergence rates under a wide variety of losses. Finally, based on
connections that have recently been shown between nonparametric density
estimation under IPM losses and generative adversarial networks (GANs), we show
that certain GAN architectures also achieve these minimax rates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Center Federated Learning. (arXiv:2005.01026v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.01026">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated learning has received great attention for its capability to train a
large-scale model in a decentralized manner without needing to access user data
directly. It helps protect the users&#x27; private data from centralized collecting.
Unlike distributed machine learning, federated learning aims to tackle non-IID
data from heterogeneous sources in various real-world applications, such as
those on smartphones. Existing federated learning approaches usually adopt a
single global model to capture the shared knowledge of all users by aggregating
their gradients, regardless of the discrepancy between their data
distributions. However, due to the diverse nature of user behaviors, assigning
users&#x27; gradients to different global models (i.e., centers) can better capture
the heterogeneity of data distributions across users. Our paper proposes a
novel multi-center aggregation mechanism for federated learning, which learns
multiple global models from the non-IID user data and simultaneously derives
the optimal matching between users and centers. We formulate the problem as a
joint optimization that can be efficiently solved by a stochastic expectation
maximization (EM) algorithm. Our experimental results on benchmark datasets
show that our method outperforms several popular federated learning methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Convolutional Neural Networks Predict Elasticity Tensors and their Bounds in Homogenization. (arXiv:2109.03020v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the present work, 3D convolutional neural networks (CNNs) are trained to
link random heterogeneous, two-phase materials of arbitrary phase fractions to
their elastic macroscale stiffness thus replacing explicit homogenization
simulations. In order to reduce the uncertainty of the true stiffness of the
synthetic composites due to unknown boundary conditions (BCs), the CNNs predict
beyond the stiffness for periodic BC the upper bound through kinematically
uniform BC, and the lower bound through stress uniform BC. This work describes
the workflow of the homogenization-CNN, from microstructure generation over the
CNN design, the operations of convolution, nonlinear activation and pooling as
well as training and validation along with backpropagation up to performance
measurements in tests. Therein the CNNs demonstrate the predictive accuracy not
only for the standard test set but also for samples of the real, two-phase
microstructure of a diamond-based coating. The CNN that covers all three
boundary types is virtually as accurate as the separate treatment in three
different nets. The CNNs of this contribution provide through stiffness bounds
an indicator of the proper RVE size for individual snapshot samples. Moreover,
they enable statistical analyses for the effective elastic stiffness on
ensembles of synthetical microstructures without costly simulations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ICCAD Special Session Paper: Quantum-Classical Hybrid Machine Learning for Image Classification. (arXiv:2109.02862v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Image classification is a major application domain for conventional deep
learning (DL). Quantum machine learning (QML) has the potential to
revolutionize image classification. In any typical DL-based image
classification, we use convolutional neural network (CNN) to extract features
from the image and multi-layer perceptron network (MLP) to create the actual
decision boundaries. On one hand, QML models can be useful in both of these
tasks. Convolution with parameterized quantum circuits (Quanvolution) can
extract rich features from the images. On the other hand, quantum neural
network (QNN) models can create complex decision boundaries. Therefore,
Quanvolution and QNN can be used to create an end-to-end QML model for image
classification. Alternatively, we can extract image features separately using
classical dimension reduction techniques such as, Principal Components Analysis
(PCA) or Convolutional Autoencoder (CAE) and use the extracted features to
train a QNN. We review two proposals on quantum-classical hybrid ML models for
image classification namely, Quanvolutional Neural Network and dimension
reduction using a classical algorithm followed by QNN. Particularly, we make a
case for trainable filters in Quanvolution and CAE-based feature extraction for
image datasets (instead of dimension reduction using linear transformations
such as, PCA). We discuss various design choices, potential opportunities, and
drawbacks of these models. We also release a Python-based framework to create
and explore these hybrid models with a variety of design choices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BioNetExplorer: Architecture-Space Exploration of Bio-Signal Processing Deep Neural Networks for Wearables. (arXiv:2109.02909v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02909">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we propose the BioNetExplorer framework to systematically
generate and explore multiple DNN architectures for bio-signal processing in
wearables. Our framework adapts key neural architecture parameters to search
for an embedded DNN with a low hardware overhead, which can be deployed in
wearable edge devices to analyse the bio-signal data and to extract the
relevant information, such as arrhythmia and seizure. Our framework also
enables hardware-aware DNN architecture search using genetic algorithms by
imposing user requirements and hardware constraints (storage, FLOPs, etc.)
during the exploration stage, thereby limiting the number of networks explored.
Moreover, BioNetExplorer can also be used to search for DNNs based on the
user-required output classes; for instance, a user might require a specific
output class due to genetic predisposition or a pre-existing heart condition.
The use of genetic algorithms reduces the exploration time, on average, by 9x,
compared to exhaustive exploration. We are successful in identifying
Pareto-optimal designs, which can reduce the storage overhead of the DNN by
~30MB for a quality loss of less than 0.5%. To enable low-cost embedded DNNs,
BioNetExplorer also employs different model compression techniques to further
reduce the storage overhead of the network by up to 53x for a quality loss of
&lt;0.2%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Understanding Model Drift in a Large Cellular Network. (arXiv:2109.03011v1 [cs.NI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03011">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Operational networks are increasingly using machine learning models for a
variety of tasks, including detecting anomalies, inferring application
performance, and forecasting demand. Accurate models are important, yet
accuracy can degrade over time due to concept drift, whereby either the
characteristics of the data change over time (data drift) or the relationship
between the features and the target predictor change over time (model drift).
Drift is important to detect because changes in properties of the underlying
data or relationships to the target prediction can require model retraining,
which can be time-consuming and expensive. Concept drift occurs in operational
networks for a variety of reasons, ranging from software upgrades to
seasonality to changes in user behavior. Yet, despite the prevalence of drift
in networks, its extent and effects on prediction accuracy have not been
extensively studied. This paper presents an initial exploration into concept
drift in a large cellular network in the United States for a major metropolitan
area in the context of demand forecasting. We find that concept drift arises
largely due to data drift, and it appears across different key performance
indicators (KPIs), models, training set sizes, and time intervals. We identify
the sources of concept drift for the particular problem of forecasting downlink
volume. Weekly and seasonal patterns introduce both high and low-frequency
model drift, while disasters and upgrades result in sudden drift due to
exogenous shocks. Regions with high population density, lower traffic volumes,
and higher speeds also tend to correlate with more concept drift. The features
that contribute most significantly to concept drift are User Equipment (UE)
downlink packets, UE uplink packets, and Real-time Transport Protocol (RTP)
total received packets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt &amp; pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt &amp; pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Certified Robustness to Programmable Transformations in LSTMs. (arXiv:2102.07818v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks for natural language processing are fragile in the face
of adversarial examples -- small input perturbations, like synonym substitution
or word duplication, which cause a neural network to change its prediction. We
present an approach to certifying the robustness of LSTMs (and extensions of
LSTMs) and training models that can be efficiently certified. Our approach can
certify robustness to intractably large perturbation spaces defined
programmatically in a language of string transformations. Our evaluation shows
that (1) our approach can train models that are more robust to combinations of
string transformations than those produced using existing techniques; (2) our
approach can show high certification accuracy of the resulting models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adversarial Parameter Defense by Multi-Step Risk Minimization. (arXiv:2109.02889v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Previous studies demonstrate DNNs&#x27; vulnerability to adversarial examples and
adversarial training can establish a defense to adversarial examples. In
addition, recent studies show that deep neural networks also exhibit
vulnerability to parameter corruptions. The vulnerability of model parameters
is of crucial value to the study of model robustness and generalization. In
this work, we introduce the concept of parameter corruption and propose to
leverage the loss change indicators for measuring the flatness of the loss
basin and the parameter robustness of neural network parameters. On such basis,
we analyze parameter corruptions and propose the multi-step adversarial
corruption algorithm. To enhance neural networks, we propose the adversarial
parameter defense algorithm that minimizes the average risk of multiple
adversarial parameter corruptions. Experimental results show that the proposed
algorithm can improve both the parameter robustness and accuracy of neural
networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scale-invariant representation of machine learning. (arXiv:2109.02914v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02914">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The success of machine learning stems from its structured data
representation. Similar data have close representation as compressed codes for
classification or emerged labels for clustering. We observe that the frequency
of the internal representation follows power laws in both supervised and
unsupervised learning. The scale-invariant distribution implies that machine
learning largely compresses frequent typical data, and at the same time,
differentiates many atypical data as outliers. In this study, we derive how the
power laws can naturally arise in machine learning. In terms of information
theory, the scale-invariant representation corresponds to a maximally uncertain
data grouping among possible representations that guarantee pre-specified
learning accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting Recursive Least Squares for Training Deep Neural Networks. (arXiv:2109.03220v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03220">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recursive least squares (RLS) algorithms were once widely used for training
small-scale neural networks, due to their fast convergence. However, previous
RLS algorithms are unsuitable for training deep neural networks (DNNs), since
they have high computational complexity and too many preconditions. In this
paper, to overcome these drawbacks, we propose three novel RLS optimization
algorithms for training feedforward neural networks, convolutional neural
networks and recurrent neural networks (including long short-term memory
networks), by using the error backpropagation and our average-approximation RLS
method, together with the equivalent gradients of the linear least squares loss
function with respect to the linear outputs of hidden layers. Compared with
previous RLS optimization algorithms, our algorithms are simple and elegant.
They can be viewed as an improved stochastic gradient descent (SGD) algorithm,
which uses the inverse autocorrelation matrix of each layer as the adaptive
learning rate. Their time and space complexities are only several times those
of SGD. They only require the loss function to be the mean squared error and
the activation function of the output layer to be invertible. In fact, our
algorithms can be also used in combination with other first-order optimization
algorithms without requiring these two preconditions. In addition, we present
two improved methods for our algorithms. Finally, we demonstrate their
effectiveness compared to the Adam algorithm on MNIST, CIFAR-10 and IMDB
datasets, and investigate the influences of their hyperparameters
experimentally.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people&#x27;s perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Safe-Critical Modular Deep Reinforcement Learning with Temporal Logic through Gaussian Processes and Control Barrier Functions. (arXiv:2109.02791v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Reinforcement learning (RL) is a promising approach and has limited success
towards real-world applications, because ensuring safe exploration or
facilitating adequate exploitation is a challenges for controlling robotic
systems with unknown models and measurement uncertainties. Such a learning
problem becomes even more intractable for complex tasks over continuous space
(state-space and action-space). In this paper, we propose a learning-based
control framework consisting of several aspects: (1) linear temporal logic
(LTL) is leveraged to facilitate complex tasks over an infinite horizons which
can be translated to a novel automaton structure; (2) we propose an innovative
reward scheme for RL-agent with the formal guarantee such that global optimal
policies maximize the probability of satisfying the LTL specifications; (3)
based on a reward shaping technique, we develop a modular policy-gradient
architecture utilizing the benefits of automaton structures to decompose
overall tasks and facilitate the performance of learned controllers; (4) by
incorporating Gaussian Processes (GPs) to estimate the uncertain dynamic
systems, we synthesize a model-based safeguard using Exponential Control
Barrier Functions (ECBFs) to address problems with high-order relative degrees.
In addition, we utilize the properties of LTL automatons and ECBFs to construct
a guiding process to further improve the efficiency of exploration. Finally, we
demonstrate the effectiveness of the framework via several robotic
environments. And we show such an ECBF-based modular deep RL algorithm achieves
near-perfect success rates and guard safety with a high probability confidence
during training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconfigurable co-processor architecture with limited numerical precision to accelerate deep convolutional neural networks. (arXiv:2109.03040v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional Neural Networks (CNNs) are widely used in deep learning
applications, e.g. visual systems, robotics etc. However, existing software
solutions are not efficient. Therefore, many hardware accelerators have been
proposed optimizing performance, power and resource utilization of the
implementation. Amongst existing solutions, Field Programmable Gate Array
(FPGA) based architecture provides better cost-energy-performance trade-offs as
well as scalability and minimizing development time. In this paper, we present
a model-independent reconfigurable co-processing architecture to accelerate
CNNs. Our architecture consists of parallel Multiply and Accumulate (MAC) units
with caching techniques and interconnection networks to exploit maximum data
parallelism. In contrast to existing solutions, we introduce limited precision
32 bit Q-format fixed point quantization for arithmetic representations and
operations. As a result, our architecture achieved significant reduction in
resource utilization with competitive accuracy. Furthermore, we developed an
assembly-type microinstructions to access the co-processing fabric to manage
layer-wise parallelism, thereby making re-use of limited resources. Finally, we
have tested our architecture up to 9x9 kernel size on Xilinx Virtex 7 FPGA,
achieving a throughput of up to 226.2 GOp/S for 3x3 kernel size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Iterative Pseudo-Labeling with Deep Feature Annotation and Confidence-Based Sampling. (arXiv:2109.02717v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02717">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks is challenging when large and annotated
datasets are unavailable. Extensive manual annotation of data samples is
time-consuming, expensive, and error-prone, notably when it needs to be done by
experts. To address this issue, increased attention has been devoted to
techniques that propagate uncertain labels (also called pseudo labels) to large
amounts of unsupervised samples and use them for training the model. However,
these techniques still need hundreds of supervised samples per class in the
training set and a validation set with extra supervised samples to tune the
model. We improve a recent iterative pseudo-labeling technique, Deep Feature
Annotation (DeepFA), by selecting the most confident unsupervised samples to
iteratively train a deep neural network. Our confidence-based sampling strategy
relies on only dozens of annotated training samples per class with no
validation set, considerably reducing user effort in data annotation. We first
ascertain the best configuration for the baseline -- a self-trained deep neural
network -- and then evaluate our confidence DeepFA for different confidence
thresholds. Experiments on six datasets show that DeepFA already outperforms
the self-trained baseline, but confidence DeepFA can considerably outperform
the original DeepFA and the baseline.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Large-Scale System Identification Using a Randomized SVD. (arXiv:2109.02703v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02703">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Learning a dynamical system from input/output data is a fundamental task in
the control design pipeline. In the partially observed setting there are two
components to identification: parameter estimation to learn the Markov
parameters, and system realization to obtain a state space model. In both
sub-problems it is implicitly assumed that standard numerical algorithms such
as the singular value decomposition (SVD) can be easily and reliably computed.
When trying to fit a high-dimensional model to data, for example in the
cyber-physical system setting, even computing an SVD is intractable. In this
work we show that an approximate matrix factorization obtained using randomized
methods can replace the standard SVD in the realization algorithm while
maintaining the non-asymptotic (in data-set size) performance and robustness
guarantees of classical methods. Numerical examples illustrate that for large
system models, this is the only method capable of producing a model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pano3D: A Holistic Benchmark and a Solid Baseline for $360^o$ Depth Estimation. (arXiv:2109.02749v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02749">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pano3D is a new benchmark for depth estimation from spherical panoramas. It
aims to assess performance across all depth estimation traits, the primary
direct depth estimation performance targeting precision and accuracy, and also
the secondary traits, boundary preservation, and smoothness. Moreover, Pano3D
moves beyond typical intra-dataset evaluation to inter-dataset performance
assessment. By disentangling the capacity to generalize to unseen data into
different test splits, Pano3D represents a holistic benchmark for $360^o$ depth
estimation. We use it as a basis for an extended analysis seeking to offer
insights into classical choices for depth estimation. This results in a solid
baseline for panoramic depth that follow-up works can build upon to steer
future progress.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Attention Layer Evolves Semantic Segmentation for Road Pothole Detection: A Benchmark and Algorithms. (arXiv:2109.02711v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Existing road pothole detection approaches can be classified as computer
vision-based or machine learning-based. The former approaches typically employ
2-D image analysis/understanding or 3-D point cloud modeling and segmentation
algorithms to detect road potholes from vision sensor data. The latter
approaches generally address road pothole detection using convolutional neural
networks (CNNs) in an end-to-end manner. However, road potholes are not
necessarily ubiquitous and it is challenging to prepare a large well-annotated
dataset for CNN training. In this regard, while computer vision-based methods
were the mainstream research trend in the past decade, machine learning-based
methods were merely discussed. Recently, we published the first stereo
vision-based road pothole detection dataset and a novel disparity
transformation algorithm, whereby the damaged and undamaged road areas can be
highly distinguished. However, there are no benchmarks currently available for
state-of-the-art (SoTA) CNNs trained using either disparity images or
transformed disparity images. Therefore, in this paper, we first discuss the
SoTA CNNs designed for semantic segmentation and evaluate their performance for
road pothole detection with extensive experiments. Additionally, inspired by
graph neural network (GNN), we propose a novel CNN layer, referred to as graph
attention layer (GAL), which can be easily deployed in any existing CNN to
optimize image feature representations for semantic segmentation. Our
experiments compare GAL-DeepLabv3+, our best-performing implementation, with
nine SoTA CNNs on three modalities of training data: RGB images, disparity
images, and transformed disparity images. The experimental results suggest that
our proposed GAL-DeepLabv3+ achieves the best overall pothole detection
accuracy on all training data modalities.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.01314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge. The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Satellite Imagery and Machine Learning to Estimate the Livelihood Impact of Electricity Access. (arXiv:2109.02890v1 [econ.GN])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02890">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In many regions of the world, sparse data on key economic outcomes inhibits
the development, targeting, and evaluation of public policy. We demonstrate how
advancements in satellite imagery and machine learning can help ameliorate
these data and inference challenges. In the context of an expansion of the
electrical grid across Uganda, we show how a combination of satellite imagery
and computer vision can be used to develop local-level livelihood measurements
appropriate for inferring the causal impact of electricity access on
livelihoods. We then show how ML-based inference techniques deliver more
reliable estimates of the causal impact of electrification than traditional
alternatives when applied to these data. We estimate that grid access improves
village-level asset wealth in rural Uganda by 0.17 standard deviations, more
than doubling the growth rate over our study period relative to untreated
areas. Our results provide country-scale evidence on the impact of a key
infrastructure investment, and provide a low-cost, generalizable approach to
future policy evaluation in data sparse environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Least-Squares ReLU Neural Network (LSNN) Method For Scalar Nonlinear Hyperbolic Conservation Law. (arXiv:2105.11627v2 [math.NA] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11627">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduced the least-squares ReLU neural network (LSNN) method for solving
the linear advection-reaction problem with discontinuous solution and showed
that the method outperforms mesh-based numerical methods in terms of the number
of degrees of freedom. This paper studies the LSNN method for scalar nonlinear
hyperbolic conservation law. The method is a discretization of an equivalent
least-squares (LS) formulation in the set of neural network functions with the
ReLU activation function. Evaluation of the LS functional is done by using
numerical integration and conservative finite volume scheme. Numerical results
of some test problems show that the method is capable of approximating the
discontinuous interface of the underlying problem automatically through the
free breaking lines of the ReLU neural network. Moreover, the method does not
exhibit the common Gibbs phenomena along the discontinuous interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02639">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Out-of-distribution (OOD) detection and lossless compression constitute two
problems that can be solved by the training of probabilistic models on a first
dataset with subsequent likelihood evaluation on a second dataset, where data
distributions differ. By defining the generalization of probabilistic models in
terms of likelihood we show that, in the case of image models, the OOD
generalization ability is dominated by local features. This motivates our
proposal of a Local Autoregressive model that exclusively models local image
features towards improving OOD performance. We apply the proposed model to OOD
detection tasks and achieve state-of-the-art unsupervised OOD detection
performance without the introduction of additional data. Additionally, we
employ our model to build a new lossless image compressor: NeLLoC (Neural Local
Lossless Compressor) and report state-of-the-art compression rates and model
size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Backpropagation and fuzzy algorithm Modelling to Resolve Blood Supply Chain Issues in the Covid-19 Pandemic. (arXiv:2109.02645v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Bloodstock shortages and its uncertain demand has become a major problem for
all countries worldwide. Therefore, this study aims to provide solution to the
issues of blood distribution during the Covid-19 Pandemic at Bengkulu,
Indonesia. The Backpropagation algorithm was used to improve the possibility of
discovering available and potential donors. Furthermore, the distances, age,
and length of donation were measured to obtain the right person to donate blood
when it needed. The Backpropagation uses three input layers to classify
eligible donors, namely age, body, weight, and bias. In addition, the system
through its query automatically counts the variables via the Fuzzy Tahani and
simultaneously access the vast database.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommendation Fairness: From Static to Dynamic. (arXiv:2109.03150v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Driven by the need to capture users&#x27; evolving interests and optimize their
long-term experiences, more and more recommender systems have started to model
recommendation as a Markov decision process and employ reinforcement learning
to address the problem. Shouldn&#x27;t research on the fairness of recommender
systems follow the same trend from static evaluation and one-shot intervention
to dynamic monitoring and non-stop control? In this paper, we portray the
recent developments in recommender systems first and then discuss how fairness
could be baked into the reinforcement learning techniques for recommendation.
Moreover, we argue that in order to make further progress in recommendation
fairness, we may want to consider multi-agent (game-theoretic) optimization,
multi-objective (Pareto) optimization, and simulation-based optimization, in
the general framework of stochastic games.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-adaptive deep neural network: Numerical approximation to functions and PDEs. (arXiv:2109.02839v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Designing an optimal deep neural network for a given task is important and
challenging in many machine learning applications. To address this issue, we
introduce a self-adaptive algorithm: the adaptive network enhancement (ANE)
method, written as loops of the form train, estimate and enhance. Starting with
a small two-layer neural network (NN), the step train is to solve the
optimization problem at the current NN; the step estimate is to compute a
posteriori estimator/indicators using the solution at the current NN; the step
enhance is to add new neurons to the current NN.

Novel network enhancement strategies based on the computed
estimator/indicators are developed in this paper to determine how many new
neurons and when a new layer should be added to the current NN. The ANE method
provides a natural process for obtaining a good initialization in training the
current NN; in addition, we introduce an advanced procedure on how to
initialize newly added neurons for a better approximation. We demonstrate that
the ANE method can automatically design a nearly minimal NN for learning
functions exhibiting sharp transitional layers as well as discontinuous
solutions of hyperbolic partial differential equations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimizing Quantum Variational Circuits with Deep Reinforcement Learning. (arXiv:2109.03188v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03188">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum Machine Learning (QML) is considered to be one of the most promising
applications of near term quantum devices. However, the optimization of quantum
machine learning models presents numerous challenges arising from the
imperfections of hardware and the fundamental obstacles in navigating an
exponentially scaling Hilbert space. In this work, we evaluate the potential of
contemporary methods in deep reinforcement learning to augment gradient based
optimization routines in quantum variational circuits. We find that
reinforcement learning augmented optimizers consistently outperform gradient
descent in noisy environments. All code and pretrained weights are available to
replicate the results or deploy the models at
https://github.com/lockwo/rl_qvc_opt.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Predictable Control. (arXiv:2109.03214v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03214">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many of the challenges facing today&#x27;s reinforcement learning (RL) algorithms,
such as robustness, generalization, transfer, and computational efficiency are
closely related to compression. Prior work has convincingly argued why
minimizing information is useful in the supervised learning setting, but
standard RL algorithms lack an explicit mechanism for compression. The RL
setting is unique because (1) its sequential nature allows an agent to use past
information to avoid looking at future observations and (2) the agent can
optimize its behavior to prefer states where decision making requires few bits.
We take advantage of these properties to propose a method (RPC) for learning
simple policies. This method brings together ideas from information
bottlenecks, model-based RL, and bits-back coding into a simple and
theoretically-justified algorithm. Our method jointly optimizes a latent-space
model and policy to be self-consistent, such that the policy avoids states
where the model is inaccurate. We demonstrate that our method achieves much
tighter compression than prior methods, achieving up to 5x higher reward than a
standard information bottleneck. We also demonstrate that our method learns
policies that are more robust and generalize better to new tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-constrained Deep Semi-Supervised Coupled Factorization Network with Enriched Prior. (arXiv:2009.03714v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.03714">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Nonnegative matrix factorization is usually powerful for learning the
&quot;shallow&quot; parts-based representation, but it clearly fails to discover deep
hierarchical information within both the basis and representation spaces. In
this paper, we technically propose a new enriched prior based Dual-constrained
Deep Semi-Supervised Coupled Factorization Network, called DS2CF-Net, for
learning the hierarchical coupled representations. To ex-tract hidden deep
features, DS2CF-Net is modeled as a deep-structure and geometrical
structure-constrained neural network. Specifically, DS2CF-Net designs a deep
coupled factorization architecture using multi-layers of linear
transformations, which coupled updates the bases and new representations in
each layer. To improve the discriminating ability of learned deep
representations and deep coefficients, our network clearly considers enriching
the supervised prior by the joint deep coefficients-regularized label
prediction, and incorporates enriched prior information as additional label and
structure constraints. The label constraint can enable the samples of the same
label to have the same coordinate in the new feature space, while the structure
constraint forces the coefficient matrices in each layer to be block-diagonal
so that the enhanced prior using the self-expressive label propagation are more
accurate. Our network also integrates the adaptive dual-graph learning to
retain the local manifold structures of both the data manifold and feature
manifold by minimizing the reconstruction errors in each layer. Extensive
experiments on several real databases demonstrate that our DS2CF-Net can obtain
state-of-the-art performance for representation learning and clustering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-dependent Label-noise Learning under a Structural Causal Model. (arXiv:2109.02986v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02986">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Label noise will degenerate the performance of deep learning algorithms
because deep neural networks easily overfit label errors. Let X and Y denote
the instance and clean label, respectively. When Y is a cause of X, according
to which many datasets have been constructed, e.g., SVHN and CIFAR, the
distributions of P(X) and P(Y|X) are entangled. This means that the
unsupervised instances are helpful to learn the classifier and thus reduce the
side effect of label noise. However, it remains elusive on how to exploit the
causal information to handle the label noise problem. In this paper, by
leveraging a structural causal model, we propose a novel generative approach
for instance-dependent label-noise learning. In particular, we show that
properly modeling the instances will contribute to the identifiability of the
label noise transition matrix and thus lead to a better classifier.
Empirically, our method outperforms all state-of-the-art methods on both
synthetic and real-world label-noise datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PEEK: A Large Dataset of Learner Engagement with Educational Videos. (arXiv:2109.03154v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03154">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Educational recommenders have received much less attention in comparison to
e-commerce and entertainment-related recommenders, even though efficient
intelligent tutors have great potential to improve learning gains. One of the
main challenges in advancing this research direction is the scarcity of large,
publicly available datasets. In this work, we release a large, novel dataset of
learners engaging with educational videos in-the-wild. The dataset, named
Personalised Educational Engagement with Knowledge Topics PEEK, is the first
publicly available dataset of this nature. The video lectures have been
associated with Wikipedia concepts related to the material of the lecture, thus
providing a humanly intuitive taxonomy. We believe that granular learner
engagement signals in unison with rich content representations will pave the
way to building powerful personalization algorithms that will revolutionise
educational and informational recommendation systems. Towards this goal, we 1)
construct a novel dataset from a popular video lecture repository, 2) identify
a set of benchmark algorithms to model engagement, and 3) run extensive
experimentation on the PEEK dataset to demonstrate its value. Our experiments
with the dataset show promise in building powerful informational recommender
systems. The dataset and the support code is available publicly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient ADMM-based Algorithms for Convolutional Sparse Coding. (arXiv:2109.02969v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02969">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Convolutional sparse coding improves on the standard sparse approximation by
incorporating a global shift-invariant model. The most efficient convolutional
sparse coding methods are based on the alternating direction method of
multipliers and the convolution theorem. The only major difference between
these methods is how they approach a convolutional least-squares fitting
subproblem. This letter presents a solution to this subproblem, which improves
the efficiency of the state-of-the-art algorithms. We also use the same
approach for developing an efficient convolutional dictionary learning method.
Furthermore, we propose a novel algorithm for convolutional sparse coding with
a constraint on the approximation error.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">COCO Denoiser: Using Co-Coercivity for Variance Reduction in Stochastic Convex Optimization. (arXiv:2109.03207v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03207">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>First-order methods for stochastic optimization have undeniable relevance, in
part due to their pivotal role in machine learning. Variance reduction for
these algorithms has become an important research topic. In contrast to common
approaches, which rarely leverage global models of the objective function, we
exploit convexity and L-smoothness to improve the noisy estimates outputted by
the stochastic gradient oracle. Our method, named COCO denoiser, is the joint
maximum likelihood estimator of multiple function gradients from their noisy
observations, subject to co-coercivity constraints between them. The resulting
estimate is the solution of a convex Quadratically Constrained Quadratic
Problem. Although this problem is expensive to solve by interior point methods,
we exploit its structure to apply an accelerated first-order algorithm, the
Fast Dual Proximal Gradient method. Besides analytically characterizing the
proposed estimator, we show empirically that increasing the number and
proximity of the queried points leads to better gradient estimates. We also
apply COCO in stochastic settings by plugging it in existing algorithms, such
as SGD, Adam or STRSAGA, outperforming their vanilla versions, even in
scenarios where our modelling assumptions are mismatched.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Bid in Contextual First Price Auctions. (arXiv:2109.03173v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we investigate the problem about how to bid in repeated
contextual first price auctions. We consider a single bidder (learner) who
repeatedly bids in the first price auctions: at each time $t$, the learner
observes a context $x_t\in \mathbb{R}^d$ and decides the bid based on
historical information and $x_t$. We assume a structured linear model of the
maximum bid of all the others $m_t &#x3D; \alpha_0\cdot x_t + z_t$, where
$\alpha_0\in \mathbb{R}^d$ is unknown to the learner and $z_t$ is randomly
sampled from a noise distribution $\mathcal{F}$ with log-concave density
function $f$. We consider both \emph{binary feedback} (the learner can only
observe whether she wins or not) and \emph{full information feedback} (the
learner can observe $m_t$) at the end of each time $t$. For binary feedback,
when the noise distribution $\mathcal{F}$ is known, we propose a bidding
algorithm, by using maximum likelihood estimation (MLE) method to achieve at
most $\widetilde{O}(\sqrt{\log(d) T})$ regret. Moreover, we generalize this
algorithm to the setting with binary feedback and the noise distribution is
unknown but belongs to a parametrized family of distributions. For the full
information feedback with \emph{unknown} noise distribution, we provide an
algorithm that achieves regret at most $\widetilde{O}(\sqrt{dT})$. Our approach
combines an estimator for log-concave density functions and then MLE method to
learn the noise distribution $\mathcal{F}$ and linear weight $\alpha_0$
simultaneously. We also provide a lower bound result such that any bidding
policy in a broad class must achieve regret at least $\Omega(\sqrt{T})$, even
when the learner receives the full information feedback and $\mathcal{F}$ is
known.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Restricted maximum-likelihood method for learning latent variance components in gene expression data with known and unknown confounders. (arXiv:2005.02921v2 [stat.ME] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02921">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Random effect models are popular statistical models for detecting and
correcting spurious sample correlations due to hidden confounders in
genome-wide gene expression data. In applications where some confounding
factors are known, estimating simultaneously the contribution of known and
latent variance components in random effect models is a challenge that has so
far relied on numerical gradient-based optimizers to maximize the likelihood
function. This is unsatisfactory because the resulting solution is poorly
characterized and the efficiency of the method may be suboptimal. Here we prove
analytically that maximum-likelihood latent variables can always be chosen
orthogonal to the known confounding factors, in other words, that
maximum-likelihood latent variables explain sample covariances not already
explained by known factors. Based on this result we propose a restricted
maximum-likelihood method which estimates the latent variables by maximizing
the likelihood on the restricted subspace orthogonal to the known confounding
factors, and show that this reduces to probabilistic PCA on that subspace. The
method then estimates the variance-covariance parameters by maximizing the
remaining terms in the likelihood function given the latent variables, using a
newly derived analytic solution for this problem. Compared to gradient-based
optimizers, our method attains greater or equal likelihood values, can be
computed using standard matrix operations, results in latent factors that don&#x27;t
overlap with any known factors, and has a runtime reduced by several orders of
magnitude. Hence the restricted maximum-likelihood method facilitates the
application of random effect modelling strategies for learning latent variance
components to much larger gene expression datasets than possible with current
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Convergence of Decentralized Adaptive Gradient Methods. (arXiv:2109.03194v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03194">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Adaptive gradient methods including Adam, AdaGrad, and their variants have
been very successful for training deep learning models, such as neural
networks. Meanwhile, given the need for distributed computing, distributed
optimization algorithms are rapidly becoming a focal point. With the growth of
computing power and the need for using machine learning models on mobile
devices, the communication cost of distributed training algorithms needs
careful consideration. In this paper, we introduce novel convergent
decentralized adaptive gradient methods and rigorously incorporate adaptive
gradient methods into decentralized training procedures. Specifically, we
propose a general algorithmic framework that can convert existing adaptive
gradient methods to their decentralized counterparts. In addition, we
thoroughly analyze the convergence behavior of the proposed algorithmic
framework and show that if a given adaptive gradient method converges, under
some specific conditions, then its decentralized counterpart is also
convergent. We illustrate the benefit of our generic decentralized framework on
a prototype method, i.e., AMSGrad, both theoretically and numerically.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning TSP Requires Rethinking Generalization. (arXiv:2006.07054v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.07054">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>End-to-end training of neural network solvers for combinatorial optimization
problems such as the Travelling Salesman Problem is intractable and inefficient
beyond a few hundreds of nodes. While state-of-the-art Machine Learning
approaches perform closely to classical solvers when trained on trivially small
sizes, they are unable to generalize the learnt policy to larger instances of
practical scales. Towards leveraging transfer learning to solve large-scale
TSPs, this paper identifies inductive biases, model architectures and learning
algorithms that promote generalization to instances larger than those seen in
training. Our controlled experiments provide the first principled investigation
into such zero-shot generalization, revealing that extrapolating beyond
training data requires rethinking the neural combinatorial optimization
pipeline, from network layers and learning paradigms to evaluation protocols.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERT based classification system for detecting rumours on Twitter. (arXiv:2109.02975v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02975">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The role of social media in opinion formation has far-reaching implications
in all spheres of society. Though social media provide platforms for expressing
news and views, it is hard to control the quality of posts due to the sheer
volumes of posts on platforms like Twitter and Facebook. Misinformation and
rumours have lasting effects on society, as they tend to influence people&#x27;s
opinions and also may motivate people to act irrationally. It is therefore very
important to detect and remove rumours from these platforms. The only way to
prevent the spread of rumours is through automatic detection and classification
of social media posts. Our focus in this paper is the Twitter social medium, as
it is relatively easy to collect data from Twitter. The majority of previous
studies used supervised learning approaches to classify rumours on Twitter.
These approaches rely on feature extraction to obtain both content and context
features from the text of tweets to distinguish rumours and non-rumours.
Manually extracting features however is time-consuming considering the volume
of tweets. We propose a novel approach to deal with this problem by utilising
sentence embedding using BERT to identify rumours on Twitter, rather than the
usual feature extraction techniques. We use sentence embedding using BERT to
represent each tweet&#x27;s sentences into a vector according to the contextual
meaning of the tweet. We classify those vectors into rumours or non-rumours by
using various supervised learning techniques. Our BERT based models improved
the accuracy by approximately 10% as compared to previous methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Complementing Handcrafted Features with Raw Waveform Using a Light-weight Auxiliary Model. (arXiv:2109.02773v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>An emerging trend in audio processing is capturing low-level speech
representations from raw waveforms. These representations have shown promising
results on a variety of tasks, such as speech recognition and speech
separation. Compared to handcrafted features, learning speech features via
backpropagation provides the model greater flexibility in how it represents
data for different tasks theoretically. However, results from empirical study
shows that, in some tasks, such as voice spoof detection, handcrafted features
are more competitive than learned features. Instead of evaluating handcrafted
features and raw waveforms independently, this paper proposes an Auxiliary
Rawnet model to complement handcrafted features with features learned from raw
waveforms. A key benefit of the approach is that it can improve accuracy at a
relatively low computational cost. The proposed Auxiliary Rawnet model is
tested using the ASVspoof 2019 dataset and the results from this dataset
indicate that a light-weight waveform encoder can potentially boost the
performance of handcrafted-features-based encoders in exchange for a small
amount of additional computational work.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Brand Label Albedo Extraction of eCommerce Products using Generative Adversarial Network. (arXiv:2109.02929v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02929">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we present our solution to extract albedo of branded labels for
e-commerce products. To this end, we generate a large-scale photo-realistic
synthetic data set for albedo extraction followed by training a generative
model to translate images with diverse lighting conditions to albedo. We
performed an extensive evaluation to test the generalisation of our method to
in-the-wild images. From the experimental results, we observe that our solution
generalises well compared to the existing method both in the unseen rendered
images as well as in the wild image.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of &quot;Subjectivity&quot; and &quot;Identity Terms&quot;. (arXiv:2109.02691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
&quot;Muslim&quot; and &quot;black&quot;. Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">gen2Out: Detecting and Ranking Generalized Anomalies. (arXiv:2109.02704v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02704">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a cloud of m-dimensional data points, how would we spot, as well as rank,
both single-point- as well as group- anomalies? We are the first to generalize
anomaly detection in two dimensions: The first dimension is that we handle both
point-anomalies, as well as group-anomalies, under a unified view -- we shall
refer to them as generalized anomalies. The second dimension is that gen2Out
not only detects, but also ranks, anomalies in suspiciousness order. Detection,
and ranking, of anomalies has numerous applications: For example, in EEG
recordings of an epileptic patient, an anomaly may indicate a seizure; in
computer network traffic data, it may signify a power failure, or a DoS/DDoS
attack. We start by setting some reasonable axioms; surprisingly, none of the
earlier methods pass all the axioms. Our main contribution is the gen2Out
algorithm, that has the following desirable properties: (a) Principled and
Sound anomaly scoring that obeys the axioms for detectors, (b) Doubly-general
in that it detects, as well as ranks generalized anomaly -- both point- and
group-anomalies, (c) Scalable, it is fast and scalable, linear on input size.
(d) Effective, experiments on real-world epileptic recordings (200GB)
demonstrate effectiveness of gen2Out as confirmed by clinicians. Experiments on
27 real-world benchmark datasets show that gen2Out detects ground truth groups,
matches or outperforms point-anomaly baseline algorithms on accuracy, with no
competition for group-anomalies and requires about 2 minutes for 1 million data
points on a stock machine.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OKSP: A Novel Deep Learning Automatic Event Detection Pipeline for Seismic Monitoringin Costa Rica. (arXiv:2109.02723v1 [physics.geo-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Small magnitude earthquakes are the most abundant but the most difficult to
locate robustly and well due to their low amplitudes and high frequencies
usually obscured by heterogeneous noise sources. They highlight crucial
information about the stress state and the spatio-temporal behavior of fault
systems during the earthquake cycle, therefore, its full characterization is
then crucial for improving earthquake hazard assessment. Modern DL algorithms
along with the increasing computational power are exploiting the continuously
growing seismological databases, allowing scientists to improve the
completeness for earthquake catalogs, systematically detecting smaller
magnitude earthquakes and reducing the errors introduced mainly by human
intervention. In this work, we introduce OKSP, a novel automatic earthquake
detection pipeline for seismic monitoring in Costa Rica. Using Kabre
supercomputer from the Costa Rica High Technology Center, we applied OKSP to
the day before and the first 5 days following the Puerto Armuelles, M6.5,
earthquake that occurred on 26 June, 2019, along the Costa Rica-Panama border
and found 1100 more earthquakes previously unidentified by the Volcanological
and Seismological Observatory of Costa Rica. From these events, a total of 23
earthquakes with magnitudes below 1.0 occurred a day to hours prior to the
mainshock, shedding light about the rupture initiation and earthquake
interaction leading to the occurrence of this productive seismic sequence. Our
observations show that for the study period, the model was 100% exhaustive and
82% precise, resulting in an F1 score of 0.90. This effort represents the very
first attempt for automatically detecting earthquakes in Costa Rica using deep
learning methods and demonstrates that, in the near future, earthquake
monitoring routines will be carried out entirely by AI algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik&#x27;s Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FastAudio: A Learnable Audio Front-End for Spoof Speech Detection. (arXiv:2109.02774v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02774">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Voice assistants, such as smart speakers, have exploded in popularity. It is
currently estimated that the smart speaker adoption rate has exceeded 35% in
the US adult population. Manufacturers have integrated speaker identification
technology, which attempts to determine the identity of the person speaking, to
provide personalized services to different members of the same family. Speaker
identification can also play an important role in controlling how the smart
speaker is used. For example, it is not critical to correctly identify the user
when playing music. However, when reading the user&#x27;s email out loud, it is
critical to correctly verify the speaker that making the request is the
authorized user. Speaker verification systems, which authenticate the speaker
identity, are therefore needed as a gatekeeper to protect against various
spoofing attacks that aim to impersonate the enrolled user. This paper compares
popular learnable front-ends which learn the representations of audio by joint
training with downstream tasks (End-to-End). We categorize the front-ends by
defining two generic architectures and then analyze the filtering stages of
both types in terms of learning constraints. We propose replacing fixed
filterbanks with a learnable layer that can better adapt to anti-spoofing
tasks. The proposed FastAudio front-end is then tested with two popular
back-ends to measure the performance on the LA track of the ASVspoof 2019
dataset. The FastAudio front-end achieves a relative improvement of 27% when
compared with fixed front-ends, outperforming all other learnable front-ends on
this task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ArGoT: A Glossary of Terms extracted from the arXiv. (arXiv:2109.02801v1 [cs.DL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We introduce ArGoT, a data set of mathematical terms extracted from the
articles hosted on the arXiv website. A term is any mathematical concept
defined in an article. Using labels in the article&#x27;s source code and examples
from other popular math websites, we mine all the terms in the arXiv data and
compile a comprehensive vocabulary of mathematical terms. Each term can be then
organized in a dependency graph by using the term&#x27;s definitions and the arXiv&#x27;s
metadata. Using both hyperbolic and standard word embeddings, we demonstrate
how this structure is reflected in the text&#x27;s vector representation and how
they capture relations of entailment in mathematical concepts. This data set is
part of an ongoing effort to align natural mathematical text with existing
Interactive Theorem Prover Libraries (ITPs) of formally verified statements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting Mood Disorder Symptoms with Remotely Collected Videos Using an Interpretable Multimodal Dynamic Attention Fusion Network. (arXiv:2109.03029v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03029">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We developed a novel, interpretable multimodal classification method to
identify symptoms of mood disorders viz. depression, anxiety and anhedonia
using audio, video and text collected from a smartphone application. We used
CNN-based unimodal encoders to learn dynamic embeddings for each modality and
then combined these through a transformer encoder. We applied these methods to
a novel dataset - collected by a smartphone application - on 3002 participants
across up to three recording sessions. Our method demonstrated better
multimodal classification performance compared to existing methods that
employed static embeddings. Lastly, we used SHapley Additive exPlanations
(SHAP) to prioritize important features in our model that could serve as
potential digital markers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Besov Function Approximation and Binary Classification on Low-Dimensional Manifolds Using Convolutional Residual Networks. (arXiv:2109.02832v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02832">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most of existing statistical theories on deep neural networks have sample
complexities cursed by the data dimension and therefore cannot well explain the
empirical success of deep learning on high-dimensional data. To bridge this
gap, we propose to exploit low-dimensional geometric structures of the real
world data sets. We establish theoretical guarantees of convolutional residual
networks (ConvResNet) in terms of function approximation and statistical
estimation for binary classification. Specifically, given the data lying on a
$d$-dimensional manifold isometrically embedded in $\mathbb{R}^D$, we prove
that if the network architecture is properly chosen, ConvResNets can (1)
approximate Besov functions on manifolds with arbitrary accuracy, and (2) learn
a classifier by minimizing the empirical logistic risk, which gives an excess
risk in the order of $n^{-\frac{s}{2s+2(s\vee d)}}$, where $s$ is a smoothness
parameter. This implies that the sample complexity depends on the intrinsic
dimension $d$, instead of the data dimension $D$. Our results demonstrate that
ConvResNets are adaptive to low-dimensional structures of data sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimizing model-agnostic Random Subspace ensembles. (arXiv:2109.03099v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a model-agnostic ensemble approach for supervised
learning. The proposed approach alternates between (1) learning an ensemble of
models using a parametric version of the Random Subspace approach, in which
feature subsets are sampled according to Bernoulli distributions, and (2)
identifying the parameters of the Bernoulli distributions that minimize the
generalization error of the ensemble model. Parameter optimization is rendered
tractable by using an importance sampling approach able to estimate the
expected model output for any given parameter set, without the need to learn
new models. While the degree of randomization is controlled by a
hyper-parameter in standard Random Subspace, it has the advantage to be
automatically tuned in our parametric version. Furthermore, model-agnostic
feature importance scores can be easily derived from the trained ensemble
model. We show the good performance of the proposed approach, both in terms of
prediction and feature ranking, on simulated and real-world datasets. We also
show that our approach can be successfully used for the reconstruction of gene
regulatory networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bringing a Ruler Into the Black Box: Uncovering Feature Impact from Individual Conditional Expectation Plots. (arXiv:2109.02724v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02724">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As machine learning systems become more ubiquitous, methods for understanding
and interpreting these models become increasingly important. In particular,
practitioners are often interested both in what features the model relies on
and how the model relies on them--the feature&#x27;s impact on model predictions.
Prior work on feature impact including partial dependence plots (PDPs) and
Individual Conditional Expectation (ICE) plots has focused on a visual
interpretation of feature impact. We propose a natural extension to ICE plots
with ICE feature impact, a model-agnostic, performance-agnostic feature impact
metric drawn out from ICE plots that can be interpreted as a close analogy to
linear regression coefficients. Additionally, we introduce an in-distribution
variant of ICE feature impact to vary the influence of out-of-distribution
points as well as heterogeneity and non-linearity measures to characterize
feature impact. Lastly, we demonstrate ICE feature impact&#x27;s utility in several
tasks using real-world data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trojan Signatures in DNN Weights. (arXiv:2109.02836v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02836">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep neural networks have been shown to be vulnerable to backdoor, or trojan,
attacks where an adversary has embedded a trigger in the network at training
time such that the model correctly classifies all standard inputs, but
generates a targeted, incorrect classification on any input which contains the
trigger. In this paper, we present the first ultra light-weight and highly
effective trojan detection method that does not require access to the
training/test data, does not involve any expensive computations, and makes no
assumptions on the nature of the trojan trigger. Our approach focuses on
analysis of the weights of the final, linear layer of the network. We
empirically demonstrate several characteristics of these weights that occur
frequently in trojaned networks, but not in benign networks. In particular, we
show that the distribution of the weights associated with the trojan target
class is clearly distinguishable from the weights associated with other
classes. Using this, we demonstrate the effectiveness of our proposed detection
method against state-of-the-art attacks across a variety of architectures,
datasets, and trigger types.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Individual Mobility Prediction via Attentive Marked Temporal Point Processes. (arXiv:2109.02715v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02715">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Individual mobility prediction is an essential task for transportation demand
management and traffic system operation. There exist a large body of works on
modeling location sequence and predicting the next location of users; however,
little attention is paid to the prediction of the next trip, which is governed
by the strong spatiotemporal dependencies between diverse attributes, including
trip start time $t$, origin $o$, and destination $d$. To fill this gap, in this
paper we propose a novel point process-based model -- Attentive Marked temporal
point processes (AMTPP) -- to model human mobility and predict the whole trip
$(t,o,d)$ in a joint manner. To encode the influence of history trips, AMTPP
employs the self-attention mechanism with a carefully designed positional
embedding to capture the daily/weekly periodicity and regularity in individual
travel behavior. Given the unique peaked nature of inter-event time in human
behavior, we use an asymmetric log-Laplace mixture distribution to precisely
model the distribution of trip start time $t$. Furthermore, an
origin-destination (OD) matrix learning block is developed to model the
relationship between every origin and destination pair. Experimental results on
two large metro trip datasets demonstrate the superior performance of AMTPP.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Refinement of Hottopixx and its Postprocessing. (arXiv:2109.02863v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02863">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Hottopixx, proposed by Bittorf et al. at NIPS 2012, is an algorithm for
solving nonnegative matrix factorization (NMF) problems under the separability
assumption. Separable NMFs have important applications, such as topic
extraction from documents and unmixing of hyperspectral images. In such
applications, the robustness of the algorithm to noise is the key to the
success. Hottopixx has been shown to be robust to noise, and its robustness can
be further enhanced through postprocessing. However, there is a drawback.
Hottopixx and its postprocessing require us to estimate the noise level
involved in the matrix we want to factorize before running, since they use it
as part of the input data. The noise-level estimation is not an easy task. In
this paper, we overcome this drawback. We present a refinement of Hottopixx and
its postprocessing that runs without prior knowledge of the noise level. We
show that the refinement has almost the same robustness to noise as the
original algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Deep Networks from Zero to Hero: avoiding pitfalls and going beyond. (arXiv:2109.02752v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02752">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep neural networks may be challenging in real world data. Using
models as black-boxes, even with transfer learning, can result in poor
generalization or inconclusive results when it comes to small datasets or
specific applications. This tutorial covers the basic steps as well as more
recent options to improve models, in particular, but not restricted to,
supervised learning. It can be particularly useful in datasets that are not as
well-prepared as those in challenges, and also under scarce annotation and/or
small data. We describe basic procedures: as data preparation, optimization and
transfer learning, but also recent architectural choices such as use of
transformer modules, alternative convolutional layers, activation functions,
wide and deep networks, as well as training procedures including as curriculum,
contrastive and self-supervised learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sequential Diagnosis Prediction with Transformer and Ontological Representation. (arXiv:2109.03069v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sequential diagnosis prediction on the Electronic Health Record (EHR) has
been proven crucial for predictive analytics in the medical domain. EHR data,
sequential records of a patient&#x27;s interactions with healthcare systems, has
numerous inherent characteristics of temporality, irregularity and data
insufficiency. Some recent works train healthcare predictive models by making
use of sequential information in EHR data, but they are vulnerable to
irregular, temporal EHR data with the states of admission/discharge from
hospital, and insufficient data. To mitigate this, we propose an end-to-end
robust transformer-based model called SETOR, which exploits neural ordinary
differential equation to handle both irregular intervals between a patient&#x27;s
visits with admitted timestamps and length of stay in each visit, to alleviate
the limitation of insufficient data by integrating medical ontology, and to
capture the dependencies between the patient&#x27;s visits by employing multi-layer
transformer blocks. Experiments conducted on two real-world healthcare datasets
show that, our sequential diagnoses prediction model SETOR not only achieves
better predictive results than previous state-of-the-art approaches,
irrespective of sufficient or insufficient training data, but also derives more
interpretable embeddings of medical codes. The experimental codes are available
at the GitHub repository (https://github.com/Xueping/SETOR).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Position-based Hash Embeddings For Scaling Graph Neural Networks. (arXiv:2109.00101v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00101">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Graph Neural Networks (GNNs) bring the power of deep representation learning
to graph and relational data and achieve state-of-the-art performance in many
applications. GNNs compute node representations by taking into account the
topology of the node&#x27;s ego-network and the features of the ego-network&#x27;s nodes.
When the nodes do not have high-quality features, GNNs learn an embedding layer
to compute node embeddings and use them as input features. However, the size of
the embedding layer is linear to the product of the number of nodes in the
graph and the dimensionality of the embedding and does not scale to big data
and graphs with hundreds of millions of nodes. To reduce the memory associated
with this embedding layer, hashing-based approaches, commonly used in
applications like NLP and recommender systems, can potentially be used.
However, a direct application of these ideas fails to exploit the fact that in
many real-world graphs, nodes that are topologically close will tend to be
related to each other (homophily) and as such their representations will be
similar.

In this work, we present approaches that take advantage of the nodes&#x27;
position in the graph to dramatically reduce the memory required, with minimal
if any degradation in the quality of the resulting GNN model. Our approaches
decompose a node&#x27;s embedding into two components: a position-specific component
and a node-specific component. The position-specific component models homophily
and the node-specific component models the node-to-node variation. Extensive
experiments using different datasets and GNN models show that our methods are
able to reduce the memory requirements by 88% to 97% while achieving, in nearly
all cases, better classification accuracy than other competing approaches,
including the full embeddings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Go Wider Instead of Deeper. (arXiv:2107.11817v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.11817">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>More transformer blocks with residual connections have recently achieved
impressive results on various tasks. To achieve better performance with fewer
trainable parameters, recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. However, weak modeling
capacity limits their performance. Contrastively, going wider by inducing more
trainable matrixes and parameters would produce a huge model requiring advanced
parallelism to train and inference.

In this paper, we propose a parameter-efficient framework, going wider
instead of deeper. Specially, following existing works, we adapt parameter
sharing to compress along depth. But, such deployment would limit the
performance. To maximize modeling capacity, we scale along model width by
replacing feed-forward network (FFN) with mixture-of-experts (MoE). Across
transformer blocks, instead of sharing normalization layers, we propose to use
individual layernorms to transform various semantic representations in a more
parameter-efficient way. To evaluate our plug-and-run framework, we design
WideNet and conduct comprehensive experiments on popular computer vision and
natural language processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by $1.5\%$ with $0.72 \times$ trainable
parameters. Using $0.46 \times$ and $0.13 \times$ parameters, our WideNet can
still surpass ViT and ViT-MoE by $0.8\%$ and $2.1\%$, respectively. On four
natural language processing datasets, WideNet outperforms ALBERT by $1.8\%$ on
average and surpass BERT using factorized embedding parameterization by $0.8\%$
with fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hyper Meta-Path Contrastive Learning for Multi-Behavior Recommendation. (arXiv:2109.02859v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02859">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>User purchasing prediction with multi-behavior information remains a
challenging problem for current recommendation systems. Various methods have
been proposed to address it via leveraging the advantages of graph neural
networks (GNNs) or multi-task learning. However, most existing works do not
take the complex dependencies among different behaviors of users into
consideration. They utilize simple and fixed schemes, like neighborhood
information aggregation or mathematical calculation of vectors, to fuse the
embeddings of different user behaviors to obtain a unified embedding to
represent a user&#x27;s behavioral patterns which will be used in downstream
recommendation tasks. To tackle the challenge, in this paper, we first propose
the concept of hyper meta-path to construct hyper meta-paths or hyper
meta-graphs to explicitly illustrate the dependencies among different behaviors
of a user. How to obtain a unified embedding for a user from hyper meta-paths
and avoid the previously mentioned limitations simultaneously is critical.
Thanks to the recent success of graph contrastive learning, we leverage it to
learn embeddings of user behavior patterns adaptively instead of assigning a
fixed scheme to understand the dependencies among different behaviors. A new
graph contrastive learning based framework is proposed by coupling with hyper
meta-paths, namely HMG-CR, which consistently and significantly outperforms all
baselines in extensive comparison experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Infer Shape Programs Using Self Training. (arXiv:2011.13045v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.13045">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Inferring programs which generate 2D and 3D shapes is important for reverse
engineering, editing, and more. Training such inference models is challenging
due to the lack of paired (shape, program) data in most domains. A popular
approach is to pre-train a model on synthetic data and then fine-tune on real
shapes using slow, unstable reinforcement learning. In this paper, we argue
that self-training is a viable alternative for fine-tuning such models.
Self-training is a semi-supervised learning paradigm where a model assigns
pseudo-labels to unlabeled data, and then retrains with (data, pseudo-label)
pairs as the new ground truth. We show that for constructive solid geometry and
assembly-based modeling, self-training outperforms state-of-the-art
reinforcement learning approaches. Additionally, shape program inference has a
unique property that circumvents a potential downside of self-training
(incorrect pseudo-label assignment): inferred programs are executable. For a
given shape from our distribution of interest $\mathbf{x}^*$ and its predicted
program $\mathbf{z}$, one can execute $\mathbf{z}$ to obtain a shape
$\mathbf{x}$ and train on $(\mathbf{z}, \mathbf{x})$ pairs, rather than
$(\mathbf{z}, \mathbf{x}^*)$ pairs. We term this procedure latent execution
self training (LEST). We demonstrate that self training infers shape programs
with higher shape reconstruction accuracy and converges significantly faster
than reinforcement learning approaches, and in some domains, LEST can further
improve this performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Representation Learning for Road Type Classification. (arXiv:2107.07791v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.07791">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a novel learning-based approach to graph representations of road
networks employing state-of-the-art graph convolutional neural networks. Our
approach is applied to realistic road networks of 17 cities from Open Street
Map. While edge features are crucial to generate descriptive graph
representations of road networks, graph convolutional networks usually rely on
node features only. We show that the highly representative edge features can
still be integrated into such networks by applying a line graph transformation.
We also propose a method for neighborhood sampling based on a topological
neighborhood composed of both local and global neighbors. We compare the
performance of learning representations using different types of neighborhood
aggregation functions in transductive and inductive tasks and in supervised and
unsupervised learning. Furthermore, we propose a novel aggregation approach,
Graph Attention Isomorphism Network, GAIN. Our results show that GAIN
outperforms state-of-the-art methods on the road type classification problem.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Early ICU Mortality Prediction and Survival Analysis for Respiratory Failure. (arXiv:2109.03048v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Respiratory failure is the one of major causes of death in critical care
unit. During the outbreak of COVID-19, critical care units experienced an
extreme shortage of mechanical ventilation because of respiratory failure
related syndromes. To help this, the early mortality risk prediction in
patients who suffer respiratory failure can provide timely support for clinical
treatment and resource management. In the study, we propose a dynamic modeling
approach for early mortality risk prediction of the respiratory failure
patients based on the first 24 hours ICU physiological data. Our proposed model
is validated on the eICU collaborate database. We achieved a high AUROC
performance (80-83%) and significantly improved AUCPR 4% on Day 5 since ICU
admission, compared to the state-of-art prediction models. In addition, we
illustrated that the survival curve includes the time-varying information for
the early ICU admission survival analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">HMSG: Heterogeneous Graph Neural Network based on Metapath Subgraph Learning. (arXiv:2109.02868v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02868">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many real-world data can be represented as heterogeneous graphs with
different types of nodes and connections. Heterogeneous graph neural network
model aims to embed nodes or subgraphs into low-dimensional vector space for
various downstream tasks such as node classification, link prediction, etc.
Although several models were proposed recently, they either only aggregate
information from the same type of neighbors, or just indiscriminately treat
homogeneous and heterogeneous neighbors in the same way. Based on these
observations, we propose a new heterogeneous graph neural network model named
HMSG to comprehensively capture structural, semantic and attribute information
from both homogeneous and heterogeneous neighbors. Specifically, we first
decompose the heterogeneous graph into multiple metapath-based homogeneous and
heterogeneous subgraphs, and each subgraph associates specific semantic and
structural information. Then message aggregation methods are applied to each
subgraph independently, so that information can be learned in a more targeted
and efficient manner. Through a type-specific attribute transformation, node
attributes can also be transferred among different types of nodes. Finally, we
fuse information from subgraphs together to get the complete representation.
Extensive experiments on several datasets for node classification, node
clustering and link prediction tasks show that HMSG achieves the best
performance in all evaluation metrics than state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Motion Artifact Reduction In Photoplethysmography For Reliable Signal Selection. (arXiv:2109.02755v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02755">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Photoplethysmography (PPG) is a non-invasive and economical technique to
extract vital signs of the human body. Although it has been widely used in
consumer and research grade wrist devices to track a user&#x27;s physiology, the PPG
signal is very sensitive to motion which can corrupt the signal&#x27;s quality.
Existing Motion Artifact (MA) reduction techniques have been developed and
evaluated using either synthetic noisy signals or signals collected during
high-intensity activities - both of which are difficult to generalize for
real-life scenarios. Therefore, it is valuable to collect realistic PPG signals
while performing Activities of Daily Living (ADL) to develop practical signal
denoising and analysis methods. In this work, we propose an automatic pseudo
clean PPG generation process for reliable PPG signal selection. For each noisy
PPG segment, the corresponding pseudo clean PPG reduces the MAs and contains
rich temporal details depicting cardiac features. Our experimental results show
that 71% of the pseudo clean PPG collected from ADL can be considered as high
quality segment where the derived MAE of heart rate and respiration rate are
1.46 BPM and 3.93 BrPM, respectively. Therefore, our proposed method can
determine the reliability of the raw noisy PPG by considering quality of the
corresponding pseudo clean PPG signal.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Zero-Shot Open Set Detection by Extending CLIP. (arXiv:2109.02748v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02748">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In a regular open set detection problem, samples of known classes (also
called closed set classes) are used to train a special classifier. In testing,
the classifier can (1) classify the test samples of known classes to their
respective classes and (2) also detect samples that do not belong to any of the
known classes (we say they belong to some unknown or open set classes). This
paper studies the problem of zero-shot open-set detection, which still performs
the same two tasks in testing but has no training except using the given known
class names. This paper proposes a novel and yet simple method (called ZO-CLIP)
to solve the problem. ZO-CLIP builds on top of the recent advances in zero-shot
classification through multi-modal representation learning. It first extends
the pre-trained multi-modal model CLIP by training a text-based image
description generator on top of CLIP. In testing, it uses the extended model to
generate some candidate unknown class names for each test sample and computes a
confidence score based on both the known class names and candidate unknown
class names for zero-shot open set detection. Experimental results on 5
benchmark datasets for open set detection confirm that ZO-CLIP outperforms the
baselines by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Machine Learning: Challenges, Limitations, and Compatibility for Audio Restoration Processes. (arXiv:2109.02692v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02692">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper machine learning networks are explored for their use in
restoring degraded and compressed speech audio. The project intent is to build
a new trained model from voice data to learn features of compression
artifacting distortion introduced by data loss from lossy compression and
resolution loss with an existing algorithm presented in SEGAN: Speech
Enhancement Generative Adversarial Network. The resulting generator from the
model was then to be used to restore degraded speech audio. This paper details
an examination of the subsequent compatibility and operational issues presented
by working with deprecated code, which obstructed the trained model from
successfully being developed. This paper further serves as an examination of
the challenges, limitations, and compatibility in the current state of machine
learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning in Emotion Recognition of Spontaneous Speech Using a Siamese Neural Network with Adaptive Sample Pair Formation. (arXiv:2109.02915v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02915">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Speech-based machine learning (ML) has been heralded as a promising solution
for tracking prosodic and spectrotemporal patterns in real-life that are
indicative of emotional changes, providing a valuable window into one&#x27;s
cognitive and mental state. Yet, the scarcity of labelled data in ambulatory
studies prevents the reliable training of ML models, which usually rely on
&quot;data-hungry&quot; distribution-based learning. Leveraging the abundance of labelled
speech data from acted emotions, this paper proposes a few-shot learning
approach for automatically recognizing emotion in spontaneous speech from a
small number of labelled samples. Few-shot learning is implemented via a metric
learning approach through a siamese neural network, which models the relative
distance between samples rather than relying on learning absolute patterns of
the corresponding distributions of each emotion. Results indicate the
feasibility of the proposed metric learning in recognizing emotions from
spontaneous speech in four datasets, even with a small amount of labelled
samples. They further demonstrate superior performance of the proposed metric
learning compared to commonly used adaptation methods, including network
fine-tuning and adversarial learning. Findings from this work provide a
foundation for the ambulatory tracking of human emotion in spontaneous speech
contributing to the real-life assessment of mental health degradation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Few-shot Learning via Dependency Maximization and Instance Discriminant Analysis. (arXiv:2109.02820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We study the few-shot learning (FSL) problem, where a model learns to
recognize new objects with extremely few labeled training data per category.
Most of previous FSL approaches resort to the meta-learning paradigm, where the
model accumulates inductive bias through learning many training tasks so as to
solve a new unseen few-shot task. In contrast, we propose a simple approach to
exploit unlabeled data accompanying the few-shot task for improving few-shot
performance. Firstly, we propose a Dependency Maximization method based on the
Hilbert-Schmidt norm of the cross-covariance operator, which maximizes the
statistical dependency between the embedded feature of those unlabeled data and
their label predictions, together with the supervised loss over the support
set. We then use the obtained model to infer the pseudo-labels for those
unlabeled data. Furthermore, we propose anInstance Discriminant Analysis to
evaluate the credibility of each pseudo-labeled example and select the most
faithful ones into an augmented support set to retrain the model as in the
first step. We iterate the above process until the pseudo-labels for the
unlabeled data becomes stable. Following the standard transductive and
semi-supervised FSL setting, our experiments show that the proposed method
out-performs previous state-of-the-art methods on four widely used benchmarks,
including mini-ImageNet, tiered-ImageNet, CUB, and CIFARFS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robustness and Generalization via Generative Adversarial Training. (arXiv:2109.02765v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02765">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While deep neural networks have achieved remarkable success in various
computer vision tasks, they often fail to generalize to new domains and subtle
variations of input images. Several defenses have been proposed to improve the
robustness against these variations. However, current defenses can only
withstand the specific attack used in training, and the models often remain
vulnerable to other input variations. Moreover, these methods often degrade
performance of the model on clean images and do not generalize to out-of-domain
samples. In this paper we present Generative Adversarial Training, an approach
to simultaneously improve the model&#x27;s generalization to the test set and
out-of-domain samples as well as its robustness to unseen adversarial attacks.
Instead of altering a low-level pre-defined aspect of images, we generate a
spectrum of low-level, mid-level and high-level changes using generative models
with a disentangled latent space. Adversarial training with these examples
enable the model to withstand a wide range of attacks by observing a variety of
input alterations during training. We show that our approach not only improves
performance of the model on clean images and out-of-domain samples but also
makes it robust against unforeseen attacks and outperforms prior work. We
validate effectiveness of our method by demonstrating results on various tasks
such as classification, segmentation and object detection.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>
  <section class="daily-content">
    <h2 class="daily-heading"><time datatime="2021-09-07">2021-09-07</time></h2>
    <ul class="sources card">
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CL updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantum Natural Language Processing on Near-Term Quantum Computers. (arXiv:2005.04147v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04147">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we describe a full-stack pipeline for natural language
processing on near-term quantum computers, aka QNLP. The language-modelling
framework we employ is that of compositional distributional semantics
(DisCoCat), which extends and complements the compositional structure of
pregroup grammars. Within this model, the grammatical reduction of a sentence
is interpreted as a diagram, encoding a specific interaction of words according
to the grammar. It is this interaction which, together with a specific choice
of word embedding, realises the meaning (or &quot;semantics&quot;) of a sentence.
Building on the formal quantum-like nature of such interactions, we present a
method for mapping DisCoCat diagrams to quantum circuits. Our methodology is
compatible both with NISQ devices and with established Quantum Machine Learning
techniques, paving the way to near-term applications of quantum technology to
natural language processing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01048">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing technologies expand BERT from different perspectives, e.g. designing
different pre-training tasks, different semantic granularities and different
model architectures. Few models consider expanding BERT from different text
formats. In this paper, we propose a heterogeneous knowledge language model
(HKLM), a unified pre-trained language model (PLM) for all forms of text,
including unstructured text, semi-structured text and well-structured text. To
capture the corresponding relations among these multi-format knowledge, our
approach uses masked language model objective to learn word knowledge, uses
triple classification objective and title matching objective to learn entity
knowledge and topic knowledge respectively. To obtain the aforementioned
multi-format text, we construct a corpus in the tourism domain and conduct
experiments on 5 tourism NLP datasets. The results show that our approach
outperforms the pre-training of plain text using only 1/4 of the data. The
code, datasets, corpus and knowledge graph will be released.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Models for Text Coherence Assessment. (arXiv:2109.02176v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02176">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Coherence is an important aspect of text quality and is crucial for ensuring
its readability. It is essential desirable for outputs from text generation
systems like summarization, question answering, machine translation, question
generation, table-to-text, etc. An automated coherence scoring model is also
helpful in essay scoring or providing writing feedback. A large body of
previous work has leveraged entity-based methods, syntactic patterns, discourse
relations, and more recently traditional deep learning architectures for text
coherence assessment. Previous work suffers from drawbacks like the inability
to handle long-range dependencies, out-of-vocabulary words, or model sequence
information. We hypothesize that coherence assessment is a cognitively complex
task that requires deeper models and can benefit from other related tasks.
Accordingly, in this paper, we propose four different Transformer-based
architectures for the task: vanilla Transformer, hierarchical Transformer,
multi-task learning-based model, and a model with fact-based input
representation. Our experiments with popular benchmark datasets across multiple
domains on four different coherence assessment tasks demonstrate that our
models achieve state-of-the-art results outperforming existing models by a good
margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;&#x3D; 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What&#x27;s in your Head? Emergent Behaviour in Multi-Task Transformer Models. (arXiv:2104.06129v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06129">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The primary paradigm for multi-task training in natural language processing
is to represent the input with a shared pre-trained language model, and add a
small, thin network (head) per task. Given an input, a target head is the head
that is selected for outputting the final prediction. In this work, we examine
the behaviour of non-target heads, that is, the output of heads when given
input that belongs to a different task than the one they were trained for. We
find that non-target heads exhibit emergent behaviour, which may either explain
the target task, or generalize beyond their original task. For example, in a
numerical reasoning task, a span extraction head extracts from the input the
arguments to a computation that results in a number generated by a target
generative head. In addition, a summarization head that is trained with a
target question answering head, outputs query-based summaries when given a
question and a context from which the answer is to be extracted. This emergent
behaviour suggests that multi-task training leads to non-trivial extrapolation
of skills, which can be harnessed for interpretability and generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01850">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As the digital news industry becomes the main channel of information
dissemination, the adverse impact of fake news is explosively magnified. The
credibility of a news report should not be considered in isolation. Rather,
previously published news articles on the similar event could be used to assess
the credibility of a news report. Inspired by this, we propose a BERT-based
multimodal unreliable news detection framework, which captures both textual and
visual information from unreliable articles utilising the contrastive learning
strategy. The contrastive learner interacts with the unreliable news classifier
to push similar credible news (or similar unreliable news) closer while moving
news articles with similar content but opposite credibility labels away from
each other in the multimodal embedding space. Experimental results on a
COVID-19 related dataset, ReCOVery, show that our model outperforms a number of
competitive baseline in unreliable news detection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v6 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer is a powerful model for text understanding. However, it is
inefficient due to its quadratic complexity to input sequence length. Although
there are many methods on Transformer acceleration, they are still either
inefficient on long sequences or not effective enough. In this paper, we
propose Fastformer, which is an efficient Transformer model based on additive
attention. In Fastformer, instead of modeling the pair-wise interactions
between tokens, we first use additive attention mechanism to model global
contexts, and then further transform each token representation based on its
interaction with global context representations. In this way, Fastformer can
achieve effective context modeling with linear complexity. Extensive
experiments on five datasets show that Fastformer is much more efficient than
many existing Transformer models and can meanwhile achieve comparable or even
better long text modeling performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">End-to-End Self-Debiasing Framework for Robust NLU Training. (arXiv:2109.02071v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02071">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Existing Natural Language Understanding (NLU) models have been shown to
incorporate dataset biases leading to strong performance on in-distribution
(ID) test sets but poor performance on out-of-distribution (OOD) ones. We
introduce a simple yet effective debiasing framework whereby the shallow
representations of the main model are used to derive a bias model and both
models are trained simultaneously. We demonstrate on three well studied NLU
tasks that despite its simplicity, our method leads to competitive OOD results.
It significantly outperforms other debiasing approaches on two tasks, while
still delivering high in-distribution performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser. (arXiv:2109.02297v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02297">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Considering the importance of building a good Visual Dialog (VD) Questioner,
many researchers study the topic under a Q-Bot-A-Bot image-guessing game
setting, where the Questioner needs to raise a series of questions to collect
information of an undisclosed image. Despite progress has been made in
Supervised Learning (SL) and Reinforcement Learning (RL), issues still exist.
Firstly, previous methods do not provide explicit and effective guidance for
Questioner to generate visually related and informative questions. Secondly,
the effect of RL is hampered by an incompetent component, i.e., the Guesser,
who makes image predictions based on the generated dialogs and assigns rewards
accordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced
Questioner (ReeQ) that generates questions under the guidance of related
entities and learns entity-based questioning strategy from human dialogs; 2) we
propose an Augmented Guesser (AugG) that is strong and is optimized for the VD
setting especially. Experimental results on the VisDial v1.0 dataset show that
our approach achieves state-of-theart performance on both image-guessing task
and question diversity. Human study further proves that our model generates
more visually related, informative and coherent questions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Putting a Spin on Language: A Quantum Interpretation of Unary Connectives for Linguistic Applications. (arXiv:2004.04128v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.04128">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Extended versions of the Lambek Calculus currently used in computational
linguistics rely on unary modalities to allow for the controlled application of
structural rules affecting word order and phrase structure. These controlled
structural operations give rise to derivational ambiguities that are missed by
the original Lambek Calculus or its pregroup simplification. Proposals for
compositional interpretation of extended Lambek Calculus in the compact closed
category of FVect and linear maps have been made, but in these proposals the
syntax-semantics mapping ignores the control modalities, effectively
restricting their role to the syntax. Our aim is to turn the modalities into
first-class citizens of the vectorial interpretation. Building on the
directional density matrix semantics, we extend the interpretation of the type
system with an extra spin density matrix space. The interpretation of proofs
then results in ambiguous derivations being tensored with orthogonal spin
states. Our method introduces a way of simultaneously representing co-existing
interpretations of ambiguous utterances, and provides a uniform framework for
the integration of lexical and derivational ambiguity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Re-entry Prediction for Online Conversations via Self-Supervised Learning. (arXiv:2109.02020v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, world business in online discussions and opinion sharing on
social media is booming. Re-entry prediction task is thus proposed to help
people keep track of the discussions which they wish to continue. Nevertheless,
existing works only focus on exploiting chatting history and context
information, and ignore the potential useful learning signals underlying
conversation data, such as conversation thread patterns and repeated engagement
of target users, which help better understand the behavior of target users in
conversations. In this paper, we propose three interesting and well-founded
auxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn
Authorship, as the self-supervised signals for re-entry prediction. These
auxiliary tasks are trained together with the main task in a multi-task manner.
Experimental results on two datasets newly collected from Twitter and Reddit
show that our method outperforms the previous state-of-the-arts with fewer
parameters and faster convergence. Extensive experiments and analysis show the
effectiveness of our proposed models and also point out some key ideas in
designing self-supervised tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nearest Neighbour Few-Shot Learning for Cross-lingual Classification. (arXiv:2109.02221v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02221">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Even though large pre-trained multilingual models (e.g. mBERT, XLM-R) have
led to significant performance gains on a wide range of cross-lingual NLP
tasks, success on many downstream tasks still relies on the availability of
sufficient annotated data. Traditional fine-tuning of pre-trained models using
only a few target samples can cause over-fitting. This can be quite limiting as
most languages in the world are under-resourced. In this work, we investigate
cross-lingual adaptation using a simple nearest neighbor few-shot (&lt;15 samples)
inference technique for classification tasks. We experiment using a total of 16
distinct languages across two NLP tasks- XNLI and PAWS-X. Our approach
consistently improves traditional fine-tuning using only a handful of labeled
samples in target locales. We also demonstrate its generalization capability
across tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncertainty-Aware Balancing for Multilingual and Multi-Domain Neural Machine Translation Training. (arXiv:2109.02284v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning multilingual and multi-domain translation model is challenging as
the heterogeneous and imbalanced data make the model converge inconsistently
over different corpora in real world. One common practice is to adjust the
share of each corpus in the training, so that the learning process is balanced
and low-resource cases can benefit from the high resource ones. However,
automatic balancing methods usually depend on the intra- and inter-dataset
characteristics, which is usually agnostic or requires human priors. In this
work, we propose an approach, MultiUAT, that dynamically adjusts the training
data usage based on the model&#x27;s uncertainty on a small set of trusted clean
data for multi-corpus machine translation. We experiments with two classes of
uncertainty measures on multilingual (16 languages with 4 settings) and
multi-domain settings (4 for in-domain and 2 for out-of-domain on
English-German translation) and demonstrate our approach MultiUAT substantially
outperforms its baselines, including both static and dynamic strategies. We
analyze the cross-domain transfer and show the deficiency of static and
similarity based methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization. (arXiv:2108.13139v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13139">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dialogue summarization has drawn much attention recently. Especially in the
customer service domain, agents could use dialogue summaries to help boost
their works by quickly knowing customer&#x27;s issues and service progress. These
applications require summaries to contain the perspective of a single speaker
and have a clear topic flow structure, while neither are available in existing
datasets. Therefore, in this paper, we introduce a novel Chinese dataset for
Customer Service Dialogue Summarization (CSDS). CSDS improves the abstractive
summaries in two aspects: (1) In addition to the overall summary for the whole
dialogue, role-oriented summaries are also provided to acquire different
speakers&#x27; viewpoints. (2) All the summaries sum up each topic separately, thus
containing the topic-level structure of the dialogue. We define tasks in CSDS
as generating the overall summary and different role-oriented summaries for a
given dialogue. Next, we compare various summarization methods on CSDS, and
experiment results show that existing methods are prone to generate redundant
and incoherent summaries. Besides, the performance becomes much worse when
analyzing the performance on role-oriented summaries and topic structures. We
hope that this study could benchmark Chinese dialogue summarization and benefit
further studies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Hierarchical Structures with Differentiable Nondeterministic Stacks. (arXiv:2109.01982v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01982">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning hierarchical structures in sequential data -- from simple
algorithmic patterns to natural language -- in a reliable, generalizable way
remains a challenging problem for neural language models. Past work has shown
that recurrent neural networks (RNNs) struggle to generalize on held-out
algorithmic or syntactic patterns without supervision or some inductive bias.
To remedy this, many papers have explored augmenting RNNs with various
differentiable stacks, by analogy with finite automata and pushdown automata.
In this paper, we present a stack RNN model based on the recently proposed
Nondeterministic Stack RNN (NS-RNN) that achieves lower cross-entropy than all
previous stack RNNs on five context-free language modeling tasks (within 0.05
nats of the information-theoretic lower bound), including a task in which the
NS-RNN previously failed to outperform a deterministic stack RNN baseline. Our
model assigns arbitrary positive weights instead of probabilities to stack
actions, and we provide an analysis of why this improves training. We also
propose a restricted version of the NS-RNN that makes it practical to use for
language modeling on natural language and present results on the Penn Treebank
corpus.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identifying emotions from text is crucial for a variety of real world tasks.
We consider the two largest now-available corpora for emotion classification:
GoEmotions, with 58k messages labelled by readers, and Vent, with 33M
writer-labelled messages. We design a benchmark and evaluate several feature
spaces and learning algorithms, including two simple yet novel models on top of
BERT that outperform previous strong baselines on GoEmotions. Through an
experiment with human participants, we also analyze the differences between how
writers express emotions and how readers perceive them. Our results suggest
that emotions expressed by writers are harder to identify than emotions that
readers perceive. We share a public web interface for researchers to explore
our models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00840">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Though language model text embeddings have revolutionized NLP research, their
ability to capture high-level semantic information, such as relations between
entities in text, is limited. In this paper, we propose a novel contrastive
learning framework that trains sentence embeddings to encode the relations in a
graph structure. Given a sentence (unstructured text) and its graph, we use
contrastive learning to impose relation-related structure on the token-level
representations of the sentence obtained with a CharacterBERT (El Boukkouri et
al.,2020) model. The resulting relation-aware sentence embeddings achieve
state-of-the-art results on the relation extraction task using only a simple
KNN classifier, thereby demonstrating the success of the proposed method.
Additional visualization by a tSNE analysis shows the effectiveness of the
learned representation space compared to baselines. Furthermore, we show that
we can learn a different space for named entity recognition, again using a
contrastive learning objective, and demonstrate how to successfully combine
both representation spaces in an entity-relation task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks. (arXiv:2109.01958v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01958">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based pre-trained language models boost the performance of
open-domain dialogue systems. Prior works leverage Transformer-based
pre-trained language models to generate texts with desired attributes in two
general approaches: (1) gradient-based methods: updating all latent
representations of pre-trained models with gradients from attribute models; (2)
weighted-decoding methods: re-ranking beam candidates from pre-trained models
with attribute functions. However, gradient-based methods lead to high
computation cost and can easily get overfitted on small training sets, while
weighted-decoding methods are inherently constrained by the low-variance
high-bias pre-trained model. In this work, we propose a novel approach to
control the generation of Transformer-based pre-trained language models: the
SideControl framework, which leverages a novel control attributes loss to
incorporate useful control signals, and is shown to perform well with very
limited training samples. We evaluate our proposed method on two benchmark
open-domain dialogue datasets, and results show that the SideControl framework
has better controllability, higher generation quality and better
sample-efficiency than existing gradient-based and weighted-decoding baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">LightTag: Text Annotation Platform. (arXiv:2109.02320v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02320">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Text annotation tools assume that their user&#x27;s goal is to create a labeled
corpus. However, users view annotation as a necessary evil on the way to
deliver business value through NLP. Thus an annotation tool should optimize for
the throughput of the global NLP process, not only the productivity of
individual annotators. LightTag is a text annotation tool designed and built on
that principle. This paper shares our design rationale, data modeling choices,
and user interface decisions then illustrates how those choices serve the full
NLP lifecycle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction. (arXiv:2109.02099v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Distantly supervised relation extraction (RE) automatically aligns
unstructured text with relation instances in a knowledge base (KB). Due to the
incompleteness of current KBs, sentences implying certain relations may be
annotated as N/A instances, which causes the so-called false negative (FN)
problem. Current RE methods usually overlook this problem, inducing improper
biases in both training and testing procedures. To address this issue, we
propose a two-stage approach. First, it finds out possible FN samples by
heuristically leveraging the memory mechanism of deep neural networks. Then, it
aligns those unlabeled data with the training data into a unified feature space
by adversarial training to assign pseudo labels and further utilize the
information contained in them. Experiments on two wildly-used benchmark
datasets demonstrate the effectiveness of our approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Interplay of Variant, Size, and Task Type in Arabic Pre-trained Language Models. (arXiv:2103.06678v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we explore the effects of language variants, data sizes, and
fine-tuning task types in Arabic pre-trained language models. To do so, we
build three pre-trained language models across three variants of Arabic: Modern
Standard Arabic (MSA), dialectal Arabic, and classical Arabic, in addition to a
fourth language model which is pre-trained on a mix of the three. We also
examine the importance of pre-training data size by building additional models
that are pre-trained on a scaled-down set of the MSA variant. We compare our
different models to each other, as well as to eight publicly available models
by fine-tuning them on five NLP tasks spanning 12 datasets. Our results suggest
that the variant proximity of pre-training data to fine-tuning data is more
important than the pre-training data size. We exploit this insight in defining
an optimized system selection model for the studied tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Linguistic entrainment is a phenomenon where people tend to mimic each other
in conversation. The core instrument to quantify entrainment is a linguistic
similarity measure between conversational partners. Most of the current
similarity measures are based on bag-of-words approaches that rely on
linguistic markers, ignoring the overall language structure and dialogue
context. To address this issue, we propose to use a neural network model to
perform the similarity measure for entrainment. Our model is context-aware, and
it further leverages a novel component to learn the shared high-level
linguistic features across dialogues. We first investigate the effectiveness of
our novel component. Then we use the model to perform similarity measure in a
corpus-based entrainment analysis. We observe promising results for both
evaluation tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An automated domain-independent text reading, interpreting and extracting approach for reviewing the scientific literature. (arXiv:2107.14638v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14638">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is presented here a machine learning-based (ML) natural language
processing (NLP) approach capable to automatically recognize and extract
categorical and numerical parameters from a corpus of articles. The approach
(named a.RIX) operates with a concomitant/interchangeable use of ML models such
as neuron networks (NNs), latent semantic analysis (LSA), naive-Bayes
classifiers (NBC), and a pattern recognition model using regular expression
(REGEX). A corpus of 7,873 scientific articles dealing with natural products
(NPs) was used to demonstrate the efficiency of the a.RIX engine. The engine
automatically extracts categorical and numerical parameters such as (i) the
plant species from which active molecules are extracted, (ii) the
microorganisms species for which active molecules can act against, and (iii)
the values of minimum inhibitory concentration (MIC) against these
microorganisms. The parameters are extracted without part-of-speech tagging
(POS) and named entity recognition (NER) approaches (i.e. without the need of
text annotation), and the models training is performed with unsupervised
approaches. In this way, a.RIX can be essentially used on articles from any
scientific field. Finally, it can potentially make obsolete the current article
reviewing process in some areas, especially those in which machine learning
models capture texts structure, text semantics, and latent knowledge.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Emotion dynamics is a framework for measuring how an individual&#x27;s emotions
change over time. It is a powerful tool for understanding how we behave and
interact with the world. In this paper, we introduce a framework to track
emotion dynamics through one&#x27;s utterances. Specifically we introduce a number
of utterance emotion dynamics (UED) metrics inspired by work in Psychology. We
use this approach to trace emotional arcs of movie characters. We analyze
thousands of such character arcs to test hypotheses that inform our broader
understanding of stories. Notably, we show that there is a tendency for
characters to use increasingly more negative words and become increasingly
emotionally discordant with each other until about 90 percent of the narrative
length. UED also has applications in behavior studies, social sciences, and
public health.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1106.0107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00430">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Medical dialogue systems (MDSs) aim to assist doctors and patients with a
range of professional medical services, i.e., diagnosis, consultation, and
treatment. However, one-stop MDS is still unexplored because: (1) no dataset
has so large-scale dialogues contains both multiple medical services and
fine-grained medical labels (i.e., intents, slots, values); (2) no model has
addressed a MDS based on multiple-service conversations in a unified framework.
In this work, we first build a Multiple-domain Multiple-service medical
dialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between
doctors and patients, covering 276 types of diseases, 2,468 medical entities,
and 3 specialties of medical services. To the best of our knowledge, it is the
only medical dialogue dataset that includes both multiple medical services and
fine-grained medical labels. Then, we formulate a one-stop MDS as a
sequence-to-sequence generation problem. We unify a MDS with causal language
modeling and conditional causal language modeling, respectively. Specifically,
we employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)
and their variants to get benchmarks on M^2-MedDialog dataset. We also propose
pseudo labeling and natural perturbation methods to expand M2-MedDialog dataset
and enhance the state-of-the-art pretrained models. We demonstrate the results
achieved by the benchmarks so far through extensive experiments on
M2-MedDialog. We release the dataset, the code, as well as the evaluation
scripts to facilitate future research in this important research direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00270">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user&#x27;s
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user&#x27;s comments
conditioned on relevant user&#x27;s reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constructive and Toxic Speech Detection for Open-domain Social Media Comments in Vietnamese. (arXiv:2103.10069v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rise of social media has led to the increasing of comments on online
forums. However, there still exists invalid comments which are not informative
for users. Moreover, those comments are also quite toxic and harmful to people.
In this paper, we create a dataset for constructive and toxic speech detection,
named UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)
with 10,000 human-annotated comments. For these tasks, we propose a system for
constructive and toxic speech detection with the state-of-the-art transfer
learning model in Vietnamese NLP as PhoBERT. With this system, we obtain
F1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,
respectively. Besides, we implement various baseline models as traditional
Machine Learning and Deep Neural Network-Based models to evaluate the dataset.
With the results, we can solve several tasks on the online discussions and
develop the framework for identifying constructiveness and toxicity of
Vietnamese social media comments automatically.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hocalarim: Mining Turkish Student Reviews. (arXiv:2109.02325v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02325">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce Hocalarim (MyProfessors), the largest student review dataset
available for the Turkish language. It consists of over 5000 professor reviews
left online by students, with different aspects of education rated on a scale
of 1 to 5 stars. We investigate the properties of the dataset and present its
statistics. We examine the impact of students&#x27; institution type on their
ratings and the correlation of students&#x27; bias to give positive or negative
feedback.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BERT might be Overkill: A Tiny but Effective Biomedical Entity Linker based on Residual Convolutional Neural Networks. (arXiv:2109.02237v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02237">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Biomedical entity linking is the task of linking entity mentions in a
biomedical document to referent entities in a knowledge base. Recently, many
BERT-based models have been introduced for the task. While these models have
achieved competitive results on many datasets, they are computationally
expensive and contain about 110M parameters. Little is known about the factors
contributing to their impressive performance and whether the
over-parameterization is needed. In this work, we shed some light on the inner
working mechanisms of these large BERT-based models. Through a set of probing
experiments, we have found that the entity linking performance only changes
slightly when the input word order is shuffled or when the attention scope is
limited to a fixed window size. From these observations, we propose an
efficient convolutional neural network with residual connections for biomedical
entity linking. Because of the sparse connectivity and weight sharing
properties, our model has a small number of parameters and is highly efficient.
On five public datasets, our model achieves comparable or even better linking
accuracy than the state-of-the-art BERT-based models while having about 60
times fewer parameters.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">STaCK: Sentence Ordering with Temporal Commonsense Knowledge. (arXiv:2109.02247v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02247">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sentence order prediction is the task of finding the correct order of
sentences in a randomly ordered document. Correctly ordering the sentences
requires an understanding of coherence with respect to the chronological
sequence of events described in the text. Document-level contextual
understanding and commonsense knowledge centered around these events are often
essential in uncovering this coherence and predicting the exact chronological
order. In this paper, we introduce STaCK -- a framework based on graph neural
networks and temporal commonsense knowledge to model global information and
predict the relative order of sentences. Our graph network accumulates temporal
evidence using knowledge of &#x60;past&#x27; and &#x60;future&#x27; and formulates sentence
ordering as a constrained edge classification problem. We report results on
five different datasets, and empirically show that the proposed method is
naturally suitable for order prediction. The implementation of this work is
publicly available at: https://github.com/declare-lab/sentence-ordering.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">EdgeBERT: Sentence-Level Energy Optimizations for Latency-Aware Multi-Task NLP Inference. (arXiv:2011.14203v5 [cs.AR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.14203">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Transformer-based language models such as BERT provide significant accuracy
improvement for a multitude of natural language processing (NLP) tasks.
However, their hefty computational and memory demands make them challenging to
deploy to resource-constrained edge platforms with strict latency requirements.
We present EdgeBERT, an in-depth algorithm-hardware co-design for latency-aware
energy optimization for multi-task NLP. EdgeBERT employs entropy-based early
exit predication in order to perform dynamic voltage-frequency scaling (DVFS),
at a sentence granularity, for minimal energy consumption while adhering to a
prescribed target latency. Computation and memory footprint overheads are
further alleviated by employing a calibrated combination of adaptive attention
span, selective network pruning, and floating-point quantization. Furthermore,
in order to maximize the synergistic benefits of these algorithms in always-on
and intermediate edge computing settings, we specialize a 12nm scalable
hardware accelerator system, integrating a fast-switching low-dropout voltage
regulator (LDO), an all-digital phase-locked loop (ADPLL), as well as,
high-density embedded non-volatile memories (eNVMs) wherein the sparse
floating-point bit encodings of the shared multi-task parameters are carefully
stored. Altogether, latency-aware multi-task NLP inference acceleration on the
EdgeBERT hardware system generates up to 7x, 2.5x, and 53x lower energy
compared to the conventional inference without early stopping, the
latency-unbounded early exit approach, and CUDA adaptations on an Nvidia Jetson
Tegra X2 mobile GPU, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01560">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Question Paraphrase Identification (QPI) is a critical task for large-scale
Question-Answering forums. The purpose of QPI is to determine whether a given
pair of questions are semantically identical or not. Previous approaches for
this task have yielded promising results, but have often relied on complex
recurrence mechanisms that are expensive and time-consuming in nature. In this
paper, we propose a novel architecture combining a Bidirectional Transformer
Encoder with Convolutional Neural Networks for the QPI task. We produce the
predictions from the proposed architecture using two different inference
setups: Siamese and Matched Aggregation. Experimental results demonstrate that
our model achieves state-of-the-art performance on the Quora Question Pairs
dataset. We empirically prove that the addition of convolution layers to the
model architecture improves the results in both inference setups. We also
investigate the impact of partial and complete fine-tuning and analyze the
trade-off between computational power and accuracy in the process. Based on the
obtained results, we conclude that the Matched-Aggregation setup consistently
outperforms the Siamese setup. Our work provides insights into what
architecture combinations and setups are likely to produce better results for
the QPI task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sent2Span: Span Detection for PICO Extraction in the Biomedical Text without Span Annotations. (arXiv:2109.02254v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02254">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid growth in published clinical trials makes it difficult to maintain
up-to-date systematic reviews, which requires finding all relevant trials. This
leads to policy and practice decisions based on out-of-date, incomplete, and
biased subsets of available clinical evidence. Extracting and then normalising
Population, Intervention, Comparator, and Outcome (PICO) information from
clinical trial articles may be an effective way to automatically assign trials
to systematic reviews and avoid searching and screening - the two most
time-consuming systematic review processes. We propose and test a novel
approach to PICO span detection. The major difference between our proposed
method and previous approaches comes from detecting spans without needing
annotated span data and using only crowdsourced sentence-level annotations.
Experiments on two datasets show that PICO span detection results achieve much
higher results for recall when compared to fully supervised methods with PICO
sentence detection at least as good as human annotations. By removing the
reliance on expert annotations for span detection, this work could be used in
human-machine pipeline for turning low-quality crowdsourced, and sentence-level
PICO annotations into structured information that can be used to quickly assign
trials to relevant systematic reviews.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Planning with Learned Entity Prompts for Abstractive Summarization. (arXiv:2104.07606v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07606">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce a simple but flexible mechanism to learn an intermediate plan to
ground the generation of abstractive summaries. Specifically, we prepend (or
prompt) target summaries with entity chains -- ordered sequences of entities
mentioned in the summary. Transformer-based sequence-to-sequence models are
then trained to generate the entity chain and then continue generating the
summary conditioned on the entity chain and the input. We experimented with
both pretraining and finetuning with this content planning objective. When
evaluated on CNN/DailyMail, XSum, SAMSum and BillSum, we demonstrate
empirically that the grounded generation with the planning objective improves
entity specificity and planning in summaries for all datasets, and achieves
state-of-the-art performance on XSum and SAMSum in terms of Rouge. Moreover, we
demonstrate empirically that planning with entity chains provides a mechanism
to control hallucinations in abstractive summaries. By prompting the decoder
with a modified content plan that drops hallucinated entities, we outperform
state-of-the-art approaches for faithfulness when evaluated automatically and
by humans.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v5 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work claims, our auxiliary experiments suggest that relation
prediction is contributory to named entity prediction in a non-negligible way.
The source code can be found at https://github.com/Coopercoppers/PFN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Combinatorial Optimization for Word-level Adversarial Textual Attack. (arXiv:2109.02229v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02229">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Over the past few years, various word-level textual attack approaches have
been proposed to reveal the vulnerability of deep neural networks used in
natural language processing. Typically, these approaches involve an important
optimization step to determine which substitute to be used for each word in the
original input. However, current research on this step is still rather limited,
from the perspectives of both problem-understanding and problem-solving. In
this paper, we address these issues by uncovering the theoretical properties of
the problem and proposing an efficient local search algorithm (LS) to solve it.
We establish the first provable approximation guarantee on solving the problem
in general cases. Notably, for adversarial textual attack, it is even better
than the previous bound which only holds in special case. Extensive experiments
involving five NLP tasks, six datasets and eleven NLP models show that LS can
largely reduce the number of queries usually by an order of magnitude to
achieve high attack success rates. Further experiments show that the
adversarial examples crafted by LS usually have higher quality, exhibit better
transferability, and can bring more robustness improvement to victim models by
adversarial training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text. (arXiv:2109.02289v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02289">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Numerical reasoning skills are essential for complex question answering (CQA)
over text. It requires opertaions including counting, comparison, addition and
subtraction. A successful approach to CQA on text, Neural Module Networks
(NMNs), follows the programmer-interpreter paradigm and leverages specialised
modules to perform compositional reasoning. However, the NMNs framework does
not consider the relationship between numbers and entities in both questions
and paragraphs. We propose effective techniques to improve NMNs&#x27; numerical
reasoning capabilities by making the interpreter question-aware and capturing
the relationship between entities and numbers. On the same subset of the DROP
dataset for CQA on text, experimental results show that our additions
outperform the original NMNs by 3.0 points for the overall F1 score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Feed-Forward Layers Are Key-Value Memories. (arXiv:2012.14913v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14913">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Feed-forward layers constitute two-thirds of a transformer model&#x27;s
parameters, yet their role in the network remains under-explored. We show that
feed-forward layers in transformer-based language models operate as key-value
memories, where each key correlates with textual patterns in the training
examples, and each value induces a distribution over the output vocabulary. Our
experiments show that the learned patterns are human-interpretable, and that
lower layers tend to capture shallow patterns, while upper layers learn more
semantic ones. The values complement the keys&#x27; input patterns by inducing
output distributions that concentrate probability mass on tokens likely to
appear immediately after each pattern, particularly in the upper layers.
Finally, we demonstrate that the output of a feed-forward layer is a
composition of its memories, which is subsequently refined throughout the
model&#x27;s layers via residual connections to produce the final output
distribution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02102">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper demonstrates that by fine-tuning an autoregressive language model
(GPT-Neo) on appropriately structured step-by-step demonstrations, it is
possible to teach it to execute a mathematical task that has previously proved
difficult for Transformers - longhand modulo operations - with a relatively
small number of examples. Specifically, we fine-tune GPT-Neo to solve the
numbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et
al. (arXiv:1904.01557) reported below 40% accuracy on this task with 2 million
training examples. We show that after fine-tuning on 200 appropriately
structured demonstrations of solving long division problems and reporting the
remainders, the smallest available GPT-Neo model achieves over 80% accuracy.
This is achieved by constructing an appropriate dataset for fine-tuning, with
no changes to the learning algorithm. These results suggest that fine-tuning
autoregressive language models on small sets of well-crafted demonstrations may
be a useful paradigm for enabling individuals without training in machine
learning to coax such models to perform some kinds of complex multi-step tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Citizen-generated counter speech is a promising way to fight hate speech and
promote peaceful, non-polarized discourse. However, there is a lack of
large-scale longitudinal studies of its effectiveness for reducing hate speech.
To this end, we perform an exploratory analysis of the effectiveness of counter
speech using several different macro- and micro-level measures to analyze
180,000 political conversations that took place on German Twitter over four
years. We report on the dynamic interactions of hate and counter speech over
time and provide insights into whether, as in &#x60;classic&#x27; bullying situations,
organized efforts are more effective than independent individuals in steering
online discourse. Taken together, our results build a multifaceted picture of
the dynamics of hate and counter speech online. While we make no causal claims
due to the complexity of discourse dynamics, our findings suggest that
organized hate speech is associated with changes in public discourse and that
counter speech -- especially when organized -- may help curb hateful rhetoric
in online discourse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Masked Segmental Language Model for Unsupervised Natural Language Segmentation. (arXiv:2104.07829v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07829">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmentation remains an important preprocessing step both in languages where
&quot;words&quot; or other important syntactic/semantic units (like morphemes) are not
clearly delineated by white space, as well as when dealing with continuous
speech data, where there is often no meaningful pause between words.
Near-perfect supervised methods have been developed for use in resource-rich
languages such as Chinese, but many of the world&#x27;s languages are both
morphologically complex, and have no large dataset of &quot;gold&quot; segmentations into
meaningful units. To solve this problem, we propose a new type of Segmental
Language Model (Sun and Deng, 2018; Kawakami et al., 2019; Wang et al., 2021)
for use in both unsupervised and lightly supervised segmentation tasks. We
introduce a Masked Segmental Language Model (MSLM) built on a span-masking
transformer architecture, harnessing the power of a bi-directional masked
modeling context and attention. In a series of experiments, our model
consistently outperforms Recurrent SLMs on Chinese (PKU Corpus) in segmentation
quality, and performs similarly to the Recurrent model on English (PTB). We
conclude by discussing the different challenges posed in segmenting
phonemic-type writing systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Augmentation for Cross-Domain Named Entity Recognition. (arXiv:2109.01758v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01758">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Current work in named entity recognition (NER) shows that data augmentation
techniques can produce more robust models. However, most existing techniques
focus on augmenting in-domain data in low-resource scenarios where annotated
data is quite limited. In contrast, we study cross-domain data augmentation for
the NER task. We investigate the possibility of leveraging data from
high-resource domains by projecting it into the low-resource domains.
Specifically, we propose a novel neural architecture to transform the data
representation from a high-resource to a low-resource domain by learning the
patterns (e.g. style, noise, abbreviations, etc.) in the text that
differentiate them and a shared feature space where both domains are aligned.
We experiment with diverse datasets and show that transforming the data to the
low-resource domain representation achieves significant improvements over only
using data from high-resource domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-Supervised Detection of Contextual Synonyms in a Multi-Class Setting: Phenotype Annotation Use Case. (arXiv:2109.01935v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Contextualised word embeddings is a powerful tool to detect contextual
synonyms. However, most of the current state-of-the-art (SOTA) deep learning
concept extraction methods remain supervised and underexploit the potential of
the context. In this paper, we propose a self-supervised pre-training approach
which is able to detect contextual synonyms of concepts being training on the
data created by shallow matching. We apply our methodology in the sparse
multi-class setting (over 15,000 concepts) to extract phenotype information
from electronic health records. We further investigate data augmentation
techniques to address the problem of the class sparsity. Our approach achieves
a new SOTA for the unsupervised phenotype concept annotation on clinical text
on F1 and Recall outperforming the previous SOTA with a gain of up to 4.5 and
4.0 absolute points, respectively. After fine-tuning with as little as 20\% of
the labelled data, we also outperform BioBERT and ClinicalBERT. The extrinsic
evaluation on three ICU benchmarks also shows the benefit of using the
phenotypes annotated by our model as features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pushing Paraphrase Away from Original Sentence: A Multi-Round Paraphrase Generation Approach. (arXiv:2109.01862v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01862">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In recent years, neural paraphrase generation based on Seq2Seq has achieved
superior performance, however, the generated paraphrase still has the problem
of lack of diversity. In this paper, we focus on improving the diversity
between the generated paraphrase and the original sentence, i.e., making
generated paraphrase different from the original sentence as much as possible.
We propose BTmPG (Back-Translation guided multi-round Paraphrase Generation),
which leverages multi-round paraphrase generation to improve diversity and
employs back-translation to preserve semantic information. We evaluate BTmPG on
two benchmark datasets. Both automatic and human evaluation show BTmPG can
improve the diversity of paraphrase while preserving the semantics of the
original sentence.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Attention Branch Network with Combined Loss Function for Automatic Speaker Verification Spoof Detection. (arXiv:2109.02051v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02051">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many endeavors have sought to develop countermeasure techniques as
enhancements on Automatic Speaker Verification (ASV) systems, in order to make
them more robust against spoof attacks. As evidenced by the latest ASVspoof
2019 countermeasure challenge, models currently deployed for the task of ASV
are, at their best, devoid of suitable degrees of generalization to unseen
attacks. Upon further investigation of the proposed methods, it appears that a
broader three-tiered view of the proposed systems. comprised of the classifier,
feature extraction phase, and model loss function, may to some extent lessen
the problem. Accordingly, the present study proposes the Efficient Attention
Branch Network (EABN) modular architecture with a combined loss function to
address the generalization problem...</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM), a self-supervised pretraining objective, is
widely used in natural language processing for learning text representations.
MLM trains a model to predict a random sample of input tokens that have been
replaced by a [MASK] placeholder in a multi-class setting over the entire
vocabulary. When pretraining, it is common to use alongside MLM other auxiliary
objectives on the token or sequence level to improve downstream performance
(e.g. next sentence prediction). However, no previous work so far has attempted
in examining whether other simpler linguistically intuitive or not objectives
can be used standalone as main pretraining objectives. In this paper, we
explore five simple pretraining objectives based on token-level classification
tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our
proposed methods achieve comparable or better performance to MLM using a
BERT-BASE architecture. We further validate our methods using smaller models,
showing that pretraining a model with 41% of the BERT-BASE&#x27;s parameters,
BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01754">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large-scale conversational assistants like Alexa, Siri, Cortana and Google
Assistant process every utterance using multiple models for domain, intent and
named entity recognition. Given the decoupled nature of model development and
large traffic volumes, it is extremely difficult to identify utterances
processed erroneously by such systems. We address this challenge to detect
domain classification errors using offline Transformer models. We combine
utterance encodings from a RoBERTa model with the Nbest hypothesis produced by
the production system. We then fine-tune end-to-end in a multitask setting
using a small dataset of humanannotated utterances with domain classification
errors. We tested our approach for detecting misclassifications from one domain
that accounts for &lt;0.5% of the traffic in a large-scale conversational AI
system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM
baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this
further by 2.2% to 32.2% by ensembling multiple models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Evaluation for Explainable AI. (arXiv:2109.01962v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01962">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>While recent years have witnessed the emergence of various explainable
methods in machine learning, to what degree the explanations really represent
the reasoning process behind the model prediction -- namely, the faithfulness
of explanation -- is still an open problem. One commonly used way to measure
faithfulness is \textit{erasure-based} criteria. Though conceptually simple,
erasure-based criterion could inevitably introduce biases and artifacts. We
propose a new methodology to evaluate the faithfulness of explanations from the
\textit{counterfactual reasoning} perspective: the model should produce
substantially different outputs for the original input and its corresponding
counterfactual edited on a faithful feature. Specially, we introduce two
algorithms to find the proper counterfactuals in both discrete and continuous
scenarios and then use the acquired counterfactuals to measure faithfulness.
Empirical results on several datasets show that compared with existing metrics,
our proposed counterfactual evaluation method can achieve top correlation with
the ground truth under diffe</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01942">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Pretrained multilingual models have become a de facto default approach for
zero-shot cross-lingual transfer. Previous work has shown that these models are
able to achieve cross-lingual representations when pretrained on two or more
languages with shared parameters. In this work, we provide evidence that a
model can achieve language-agnostic representations even when pretrained on a
single language. That is, we find that monolingual models pretrained and
finetuned on different languages achieve competitive performance compared to
the ones that use the same target language. Surprisingly, the models show a
similar performance on a same task regardless of the pretraining language. For
example, models pretrained on distant languages such as German and Portuguese
perform similarly on English tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As a kind of new expression elements, Internet memes are popular and
extensively used in online chatting scenarios since they manage to make
dialogues vivid, moving, and interesting. However, most current dialogue
researches focus on text-only dialogue tasks. In this paper, we propose a new
task named as \textbf{M}eme incorporated \textbf{O}pen-domain \textbf{D}ialogue
(MOD). Compared to previous dialogue tasks, MOD is much more challenging since
it requires the model to understand the multimodal elements as well as the
emotions behind them. To facilitate the MOD research, we construct a
large-scale open-domain multimodal dialogue dataset incorporating abundant
Internet memes into utterances. The dataset consists of $\sim$45K Chinese
conversations with $\sim$606K utterances. Each conversation contains about $13$
utterances with about $4$ Internet memes on average and each utterance equipped
with an Internet meme is annotated with the corresponding emotion. In addition,
we present a simple and effective method, which utilizes a unified generation
network to solve the MOD task. Experimental results demonstrate that our method
trained on the proposed corpus is able to achieve expressive communication
including texts and memes. The corpus and models have been publicly available
at https://github.com/lizekang/DSTC10-MOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Active learning has emerged as a standard paradigm in areas with scarcity of
labeled training data, such as in the medical domain. Language models have
emerged as the prevalent choice of several natural language tasks due to the
performance boost offered by these models. However, in several domains, such as
medicine, the scarcity of labeled training data is a common issue. Also, these
models may not work well in cases where class imbalance is prevalent. Active
learning may prove helpful in these cases to boost the performance with a
limited label budget. To this end, we propose a novel method using sampling
techniques based on submodular optimization and optimal transport for active
learning in language models, dubbed ALLWAS. We construct a sampling strategy
based on submodular optimization of the designed objective in the gradient
domain. Furthermore, to enable learning from few samples, we propose a novel
strategy for sampling from the Wasserstein barycenters. Our empirical
evaluations on standard benchmark datasets for text classification show that
our methods perform significantly better (&gt;20% relative increase in some cases)
than existing approaches for active learning on language models.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.CV updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13883">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The raw-RGB colors of a camera sensor vary due to the spectral sensitivity
differences across different sensor makes and models. This paper focuses on the
task of mapping between different sensor raw-RGB color spaces. Prior work
addressed this problem using a pairwise calibration to achieve accurate color
mapping. Although being accurate, this approach is less practical as it
requires: (1) capturing pair of images by both camera devices with a color
calibration object placed in each new scene; (2) accurate image alignment or
manual annotation of the color calibration object. This paper aims to tackle
color mapping in the raw space through a more practical setup. Specifically, we
present a semi-supervised raw-to-raw mapping method trained on a small set of
paired images alongside an unpaired set of images captured by each camera
device. Through extensive experiments, we show that our method achieves better
results compared to other domain adaptation alternatives in addition to the
single-calibration solution. We have generated a new dataset of raw images from
two different smartphone cameras as part of this effort. Our dataset includes
unpaired and paired sets for our semi-supervised training and evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.13920">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image style transfer aims to manipulate the appearance of a source image, or
&quot;content&quot; image, to share similar texture and colors of a target &quot;style&quot; image.
Ideally, the style transfer manipulation should also preserve the semantic
content of the source image. A commonly used approach to assist in transferring
styles is based on Gram matrix optimization. One problem of Gram matrix-based
optimization is that it does not consider the correlation between colors and
their styles. Specifically, certain textures or structures should be associated
with specific colors. This is particularly challenging when the target style
image exhibits multiple style types. In this work, we propose a color-aware
multi-style transfer method that generates aesthetically pleasing results while
preserving the style-color correlation between style and generated images. We
achieve this desired outcome by introducing a simple but efficient modification
to classic Gram matrix-based style transfer optimization. A nice feature of our
method is that it enables the users to manually select the color associations
between the target style and content image for more transfer flexibility. We
validated our method with several qualitative comparisons, including a user
study conducted with 30 participants. In comparison with prior work, our method
is simple, easy to implement, and achieves visually appealing results when
targeting images that have multiple styles. Source code is available at
https://github.com/mahmoudnafifi/color-aware-style-transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Greedy Offset-Guided Keypoint Grouping for Human Pose Estimation. (arXiv:2107.03098v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03098">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a simple yet reliable bottom-up approach with a good trade-off
between accuracy and efficiency for the problem of multi-person pose
estimation. Given an image, we employ an Hourglass Network to infer all the
keypoints from different persons indiscriminately as well as the guiding
offsets connecting the adjacent keypoints belonging to the same persons. Then,
we greedily group the candidate keypoints into multiple human poses (if any),
utilizing the predicted guiding offsets. And we refer to this process as greedy
offset-guided keypoint grouping (GOG). Moreover, we revisit the
encoding-decoding method for the multi-person keypoint coordinates and reveal
some important facts affecting accuracy. Experiments have demonstrated the
obvious performance improvements brought by the introduced components. Our
approach is comparable to the state of the art on the challenging COCO dataset
under fair conditions. The source code and our pre-trained model are publicly
available online.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.15475">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Incorrectly labeled examples, or label noise, is common in real-world
computer vision datasets. While the impact of label noise on learning in deep
neural networks has been studied in prior work, these studies have exclusively
focused on homogeneous label noise, i.e., the degree of label noise is the same
across all categories. However, in the real-world, label noise is often
heterogeneous, with some categories being affected to a greater extent than
others. Here, we address this gap in the literature. We hypothesized that
heterogeneous label noise would only affect the classes that had label noise
unless there was transfer from those classes to the classes without label
noise. To test this hypothesis, we designed a series of computer vision studies
using MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous
label noise during the training of multi-class, multi-task, and multi-label
systems. Our results provide evidence in support of our hypothesis: label noise
only affects the class affected by it unless there is transfer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Topological Semantic Mapping by Consolidation of Deep Visual Features. (arXiv:2106.12709v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12709">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Many works in the recent literature introduce semantic mapping methods that
use CNNs (Convolutional Neural Networks) to recognize semantic properties in
images. The types of properties (eg.: room size, place category, and objects)
and their classes (eg.: kitchen and bathroom, for place category) are usually
predefined and restricted to a specific task. Thus, all the visual data
acquired and processed during the construction of the maps are lost and only
the recognized semantic properties remain on the maps. In contrast, this work
introduces a topological semantic mapping method that uses deep visual features
extracted by a CNN (GoogLeNet), from 2D images captured in multiple views of
the environment as the robot operates, to create, through averages,
consolidated representations of the visual features acquired in the regions
covered by each topological node. These representations allow flexible
recognition of semantic properties of the regions and use in other visual
tasks. Experiments with a real-world indoor dataset showed that the method is
able to consolidate the visual features of regions and use them to recognize
objects and place categories as semantic properties, and to indicate the
topological location of images, with very promising results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Data augmentation is a widely used technique in classification to increase
data used in training. It improves generalization and reduces amount of
annotated human activity data needed for training which reduces labour and time
needed with the dataset. Sensor time-series data, unlike images, cannot be
augmented by computationally simple transformation algorithms. State of the art
models like Recurrent Generative Adversarial Networks (RGAN) are used to
generate realistic synthetic data. In this paper, transformer based generative
adversarial networks which have global attention on data, are compared on
PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer
approach provides improvements in time and savings in computational resources
needed for data augmentation than previous approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v6 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07770">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Video quality assessment (VQA) is now a fast-growing subject, maturing in the
full reference (FR) case, yet challenging in the exploding no reference (NR)
case. We investigate variants of the popular VMAF video quality assessment
algorithm for the FR case, using both support vector regression and feedforward
neural networks. We extend it to the NR case, using some different features but
similar learning, to develop a partially unified framework for VQA. When fully
trained, FR algorithms such as VMAF perform well on test datasets, with 90%+
match in PCC and SRCC; but for predicting performance in the wild, we
train/test from scratch for each database. With an 80/20 train/test split, we
still achieve 90%+ performance on average in both PCC and SRCC, with 8-9% gains
over VMAF. Moreover, we even get decent performance (~75%) if we ignore the
reference, treating FR as NR, partly justifying our attempts at unification. In
the true NR case, we reduce complexity vs. leading recent algorithms VIDEVAL,
RAPIQUE, yet achieve a stunning 90% in SRCC (~12% gain), while roughly matching
in PCC (78% vs. 79.6%). At lower complexities, we can still achieve 87% in
SRCC, 70% in PCC. In short, we find encouraging improvements in trainability in
both FR and NR, while also constraining computational complexity against
leading methods</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v12 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10762">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Random field and random cluster theory are used to describe certain
mathematical results concerning the probability distribution of image pixel
intensities characterized as generic $2D$ integer arrays. The size of the
smallest bounded region within an image is estimated for segmenting an image,
from which, the equilibrium distribution of intensities can be recovered. From
the estimated bounded regions, properties of the sub-optimal and equilibrium
distributions of intensities are derived, which leads to an image compression
methodology whereby only slightly more than half of all pixels are required for
a worst-case reconstruction of the original image. A custom deep belief network
and heuristic allows for the unsupervised segmentation, detection and
localization of objects in an image. An example illustrates the mathematical
results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Comprehensive Survey of Image-Based Food Recognition and Volume Estimation Methods for Dietary Assessment. (arXiv:2106.11776v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11776">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dietary studies showed that dietary-related problem such as obesity is
associated with other chronic diseases like hypertension, irregular blood sugar
levels, and increased risk of heart attacks. The primary cause of these
problems is poor lifestyle choices and unhealthy dietary habits, which are
manageable using interactive mHealth apps. However, traditional dietary
monitoring systems using manual food logging suffer from imprecision,
underreporting, time consumption, and low adherence. Recent dietary monitoring
systems tackle these challenges by automatic assessment of dietary intake
through machine learning methods. This survey discusses the most performing
methodologies that have been developed so far for automatic food recognition
and volume estimation. First, we will present the rationale of visual-based
methods for food recognition. The core of the paper is the presentation,
discussion and evaluation of these methods on popular food image databases.
Following that, we discussed the mobile applications that are implementing
these methods. The survey ends with a discussion of research gaps and open
issues in this area.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11272">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We introduce Neural Marching Cubes (NMC), a data-driven approach for
extracting a triangle mesh from a discretized implicit field. Classical MC is
defined by coarse tessellation templates isolated to individual cubes. While
more refined tessellations have been proposed, they all make heuristic
assumptions, such as trilinearity, when determining the vertex positions and
local mesh topologies in each cube. In principle, none of these approaches can
reconstruct geometric features that reveal coherence or dependencies between
nearby cubes (e.g., a sharp edge), as such information is unaccounted for,
resulting in poor estimates of the true underlying implicit field. To tackle
these challenges, we re-cast MC from a deep learning perspective, by designing
tessellation templates more apt at preserving geometric features, and learning
the vertex positions and mesh topologies from training meshes, to account for
contextual information from nearby cubes. We develop a compact per-cube
parameterization to represent the output triangle mesh, while being compatible
with neural processing, so that a simple 3D convolutional network can be
employed for the training. We show that all topological cases in each cube that
are applicable to our design can be easily derived using our representation,
and the resulting tessellations can also be obtained naturally and efficiently
by following a few design guidelines. In addition, our network learns local
features with limited receptive fields, hence it generalizes well to new shapes
and new datasets. We evaluate our neural MC approach by quantitative and
qualitative comparisons to all well-known MC variants. In particular, we
demonstrate the ability of our network to recover sharp features such as edges
and corners, a long-standing issue of MC and its variants. Our network also
reconstructs local mesh topologies more accurately than previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we tackle two important problems in low-rank learning, which
are partial singular value decomposition and numerical rank estimation of huge
matrices. By using the concepts of Krylov subspaces such as Golub-Kahan
bidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose
two methods for solving these problems in a fast and accurate way. Our
experiments show the advantages of the proposed methods compared to the
traditional and randomized singular value decomposition methods. The proposed
methods are appropriate for applications involving huge matrices where the
accuracy of the desired singular values and also all of their corresponding
singular vectors are essential. As a real application, we evaluate the
performance of our methods on the problem of Riemannian similarity learning
between two various image datasets of MNIST and USPS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents.
Inferring occupancy from agent behaviors is an inherently multimodal problem; a
driver may behave in the same manner for different occupancy patterns ahead of
them (e.g., a driver may move at constant speed in traffic or on an open road).
Past work, however, does not account for this multimodality, thus neglecting to
model this source of aleatoric uncertainty in the relationship between driver
behaviors and their environment. We propose an occlusion inference method that
characterizes observed behaviors of human agents as sensor measurements, and
fuses them with those from a standard sensor suite. To capture the aleatoric
uncertainty, we train a conditional variational autoencoder with a discrete
latent space to learn a multimodal mapping from observed driver trajectories to
an occupancy grid representation of the view ahead of the driver. Our method
handles multi-agent scenarios, combining measurements from multiple observed
drivers using evidential theory to solve the sensor fusion problem. Our
approach is validated on a real-world dataset, outperforming baselines and
demonstrating real-time capable performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">To be Critical: Self-Calibrated Weakly Supervised Learning for Salient Object Detection. (arXiv:2109.01770v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01770">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Weakly-supervised salient object detection (WSOD) aims to develop saliency
models using image-level annotations. Despite of the success of previous works,
explorations on an effective training strategy for the saliency network and
accurate matches between image-level annotations and salient objects are still
inadequate. In this work, 1) we propose a self-calibrated training strategy by
explicitly establishing a mutual calibration loop between pseudo labels and
network predictions, liberating the saliency network from error-prone
propagation caused by pseudo labels. 2) we prove that even a much smaller
dataset (merely 1.8% of ImageNet) with well-matched annotations can facilitate
models to achieve better performance as well as generalizability. This sheds
new light on the development of WSOD and encourages more contributions to the
community. Comprehensive experiments demonstrate that our method outperforms
all the existing WSOD methods by adopting the self-calibrated strategy only.
Steady improvements are further achieved by training on the proposed dataset.
Additionally, our method achieves 94.7% of the performance of fully-supervised
methods on average. And what is more, the fully supervised models adopting our
predicted results as &quot;ground truths&quot; achieve successful results (95.6% for
BASNet and 97.3% for ITSD on F-measure), while costing only 0.32% of labeling
time for pixel-level annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10623">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Predicting isocitrate dehydrogenase mutationstatus in glioma using structural brain networksand graph neural networks. (arXiv:2109.01854v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01854">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Glioma is a common malignant brain tumor that shows distinct survival among
patients. The isocitrate dehydrogenase (IDH) gene mutation status provides
critical diagnostic and prognostic value for glioma and is now accepted as the
standard of care. A non-invasive prediction of IDH mutation based on the
pre-treatment MRI has crucial clinical significance. Machine learning and deep
learning models show reasonable performance in predicting IDH mutation status.
However, most models neglect the systematic brain alterations caused by tumor
invasion, where the infiltration along white matter tracts throughout the brain
is identified as a hallmark of glioma. Structural brain network provides an
effective tool to characterise brain organisation, which could be captured by
the graph neural networks (GNN) for a more accurate prediction of IDH mutation
status.

Here we propose a method to predict the IDH mutation using GNN, based on the
structural brain network of patients. Specifically, we firstly construct a
network template of healthy subjects, which consists of atlases of edges (white
matter tracts) and nodes (cortical and subcortical brain regions) to provide
regions of interest (ROI). Next, we employ autoencoders to extract the latent
multi-modal MRI features from the ROIs of the edge and node in patients. These
features of edge and node of brain networks are used to train a GNN
architecture in predicting IDH mutation status. The results show that the
proposed method outperforms the baseline models using 3D-CNN and 3D-DenseNet.
In addition, the model interpretation suggests its ability to identify the
tracts infiltrated by tumor and corresponds to clinical prior knowledge. In
conclusion, integrating brain networks with GNN offers a new avenue to study
brain lesions using computational neuroscience and computer vision approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04563">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In order to extract knowledge from the large data collected by edge devices,
traditional cloud based approach that requires data upload may not be feasible
due to communication bandwidth limitation as well as privacy and security
concerns of end users. To address these challenges, a novel privacy preserving
edge computing framework is proposed in this paper for image classification.
Specifically, autoencoder will be trained unsupervised at each edge device
individually, then the obtained latent vectors will be transmitted to the edge
server for the training of a classifier. This framework would reduce the
communications overhead and protect the data of the end users. Comparing to
federated learning, the training of the classifier in the proposed framework
does not subject to the constraints of the edge devices, and the autoencoder
can be trained independently at each edge device without any server
involvement. Furthermore, the privacy of the end users&#x27; data is protected by
transmitting latent vectors without additional cost of encryption. Experimental
results provide insights on the image classification performance vs. various
design parameters such as the data compression ratio of the autoencoder and the
model complexity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Handwritten Character Recognition of South Indian Scripts: A Review. (arXiv:1106.0107v1 [cs.CV] CROSS LISTED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1106.0107">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Handwritten character recognition is always a frontier area of research in
the field of pattern recognition and image processing and there is a large
demand for OCR on hand written documents. Even though, sufficient studies have
performed in foreign scripts like Chinese, Japanese and Arabic characters, only
a very few work can be traced for handwritten character recognition of Indian
scripts especially for the South Indian scripts. This paper provides an
overview of offline handwritten character recognition in South Indian Scripts,
namely Malayalam, Tamil, Kannada and Telungu.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RiWNet: A moving object instance segmentation Network being Robust in adverse Weather conditions. (arXiv:2109.01820v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01820">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmenting each moving object instance in a scene is essential for many
applications. But like many other computer vision tasks, this task performs
well in optimal weather, but then adverse weather tends to fail. To be robust
in weather conditions, the usual way is to train network in data of given
weather pattern or to fuse multiple sensors. We focus on a new possibility,
that is, to improve its resilience to weather interference through the
network&#x27;s structural design. First, we propose a novel FPN structure called
RiWFPN with a progressive top-down interaction and attention refinement module.
RiWFPN can directly replace other FPN structures to improve the robustness of
the network in non-optimal weather conditions. Then we extend SOLOV2 to capture
temporal information in video to learn motion information, and propose a moving
object instance segmentation network with RiWFPN called RiWNet. Finally, in
order to verify the effect of moving instance segmentation in different weather
disturbances, we propose a VKTTI-moving dataset which is a moving instance
segmentation dataset based on the VKTTI dataset, taking into account different
weather scenes such as rain, fog, sunset, morning as well as overcast. The
experiment proves how RiWFPN improves the network&#x27;s resilience to adverse
weather effects compared to other FPN structures. We compare RiWNet to several
other state-of-the-art methods in some challenging datasets, and RiWNet shows
better performance especially under adverse weather conditions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset. (arXiv:2108.05080v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05080">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>While significant advancements have been made in the generation of deepfakes
using deep learning technologies, its misuse is a well-known issue now.
Deepfakes can cause severe security and privacy issues as they can be used to
impersonate a person&#x27;s identity in a video by replacing his/her face with
another person&#x27;s face. Recently, a new problem of generating synthesized human
voice of a person is emerging, where AI-based deep learning models can
synthesize any person&#x27;s voice requiring just a few seconds of audio. With the
emerging threat of impersonation attacks using deepfake audios and videos, a
new generation of deepfake detectors is needed to focus on both video and audio
collectively. A large amount of good quality datasets is typically required to
capture the real-world scenarios to develop a competent deepfake detector.
Existing deepfake datasets either contain deepfake videos or audios, which are
racially biased as well. Hence, there is a crucial need for creating a good
video as well as an audio deepfake dataset, which can be used to detect audio
and video deepfake simultaneously. To fill this gap, we propose a novel
Audio-Video Deepfake dataset (FakeAVCeleb) that contains not only deepfake
videos but also respective synthesized lip-synced fake audios. We generate this
dataset using the current most popular deepfake generation methods. We selected
real YouTube videos of celebrities with four racial backgrounds (Caucasian,
Black, East Asian, and South Asian) to develop a more realistic multimodal
dataset that addresses racial bias and further help develop multimodal deepfake
detectors. We performed several experiments using state-of-the-art detection
methods to evaluate our deepfake dataset and demonstrate the challenges and
usefulness of our multimodal Audio-Video deepfake dataset.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery. (arXiv:2106.00880v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Detection of objects is extremely important in various aerial vision-based
applications. Over the last few years, the methods based on convolution neural
networks have made substantial progress. However, because of the large variety
of object scales, densities, and arbitrary orientations, the current detectors
struggle with the extraction of semantically strong features for small-scale
objects by a predefined convolution kernel. To address this problem, we propose
the rotation equivariant feature image pyramid network (REFIPN), an image
pyramid network based on rotation equivariance convolution. The proposed model
adopts single-shot detector in parallel with a lightweight image pyramid module
to extract representative features and generate regions of interest in an
optimization approach. The proposed network extracts feature in a wide range of
scales and orientations by using novel convolution filters. These features are
used to generate vector fields and determine the weight and angle of the
highest-scoring orientation for all spatial locations on an image. By this
approach, the performance for small-sized object detection is enhanced without
sacrificing the performance for large-sized object detection. The performance
of the proposed model is validated on two commonly used aerial benchmarks and
the results show our proposed model can achieve state-of-the-art performance
with satisfactory efficiency.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CTRL-C: Camera calibration TRansformer with Line-Classification. (arXiv:2109.02259v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02259">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Single image camera calibration is the task of estimating the camera
parameters from a single input image, such as the vanishing points, focal
length, and horizon line. In this work, we propose Camera calibration
TRansformer with Line-Classification (CTRL-C), an end-to-end neural
network-based approach to single image camera calibration, which directly
estimates the camera parameters from an image and a set of line segments. Our
network adopts the transformer architecture to capture the global structure of
an image with multi-modal inputs in an end-to-end manner. We also propose an
auxiliary task of line classification to train the network to extract the
global geometric information from lines effectively. Our experiments
demonstrate that CTRL-C outperforms the previous state-of-the-art methods on
the Google Street View and SUN360 benchmark datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Segmentation of the Optic Nerve Head Region in Optical Coherence Tomography: A Methodological Review. (arXiv:2109.02322v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02322">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The optic nerve head represents the intraocular section of the optic nerve
(ONH), which is prone to damage by intraocular pressure. The advent of optical
coherence tomography (OCT) has enabled the evaluation of novel optic nerve head
parameters, namely the depth and curvature of the lamina cribrosa (LC).
Together with the Bruch&#x27;s membrane opening minimum-rim-width, these seem to be
promising optic nerve head parameters for diagnosis and monitoring of retinal
diseases such as glaucoma. Nonetheless, these optical coherence tomography
derived biomarkers are mostly extracted through manual segmentation, which is
time-consuming and prone to bias, thus limiting their usability in clinical
practice. The automatic segmentation of optic nerve head in OCT scans could
further improve the current clinical management of glaucoma and other diseases.

This review summarizes the current state-of-the-art in automatic segmentation
of the ONH in OCT. PubMed and Scopus were used to perform a systematic review.
Additional works from other databases (IEEE, Google Scholar and ARVO IOVS) were
also included, resulting in a total of 27 reviewed studies.

For each algorithm, the methods, the size and type of dataset used for
validation, and the respective results were carefully analyzed. The results
show that deep learning-based algorithms provide the highest accuracy,
sensitivity and specificity for segmenting the different structures of the ONH
including the LC. However, a lack of consensus regarding the definition of
segmented regions, extracted parameters and validation approaches has been
observed, highlighting the importance and need of standardized methodologies
for ONH segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Right Ventricular Segmentation from Short- and Long-Axis MRIs via Information Transition. (arXiv:2109.02171v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02171">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Right ventricular (RV) segmentation from magnetic resonance imaging (MRI) is
a crucial step for cardiac morphology and function analysis. However, automatic
RV segmentation from MRI is still challenging, mainly due to the heterogeneous
intensity, the complex variable shapes, and the unclear RV boundary. Moreover,
current methods for the RV segmentation tend to suffer from performance
degradation at the basal and apical slices of MRI. In this work, we propose an
automatic RV segmentation framework, where the information from long-axis (LA)
views is utilized to assist the segmentation of short-axis (SA) views via
information transition. Specifically, we employed the transformed segmentation
from LA views as a prior information, to extract the ROI from SA views for
better segmentation. The information transition aims to remove the surrounding
ambiguous regions in the SA views. %, such as the tricuspid valve regions. We
tested our model on a public dataset with 360 multi-center, multi-vendor and
multi-disease subjects that consist of both LA and SA MRIs. Our experimental
results show that including LA views can be effective to improve the accuracy
of the SA segmentation. Our model is publicly available at
https://github.com/NanYoMy/MMs-2.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.05248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural network pruning and quantization techniques have demonstrated it
is possible to achieve high levels of compression with surprisingly little
degradation to test set accuracy. However, this measure of performance conceals
significant differences in how different classes and images are impacted by
model compression techniques. We find that models with radically different
numbers of weights have comparable top-line performance metrics but diverge
considerably in behavior on a narrow subset of the dataset. This small subset
of data points, which we term Pruning Identified Exemplars (PIEs) are
systematically more impacted by the introduction of sparsity. Compression
disproportionately impacts model performance on the underrepresented long-tail
of the data distribution. PIEs over-index on atypical or noisy images that are
far more challenging for both humans and algorithms to classify. Our work
provides intuition into the role of capacity in deep neural networks and the
trade-offs incurred by compression. An understanding of this disparate impact
is critical given the widespread deployment of compressed models in the wild.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">OCTAVA: an open-source toolbox for quantitative analysis of optical coherence tomography angiography images. (arXiv:2109.01835v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01835">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Optical coherence tomography angiography (OCTA) performs non-invasive
visualization and characterization of microvasculature in research and clinical
applications mainly in ophthalmology and dermatology. A wide variety of
instruments, imaging protocols, processing methods and metrics have been used
to describe the microvasculature, such that comparing different study outcomes
is currently not feasible. With the goal of contributing to standardization of
OCTA data analysis, we report a user-friendly, open-source toolbox, OCTAVA
(OCTA Vascular Analyzer), to automate the pre-processing, segmentation, and
quantitative analysis of en face OCTA maximum intensity projection images in a
standardized workflow. We present each analysis step, including optimization of
filtering and choice of segmentation algorithm, and definition of metrics. We
perform quantitative analysis of OCTA images from different commercial and
non-commercial instruments and samples and show OCTAVA can accurately and
reproducibly determine metrics for characterization of microvasculature. Wide
adoption could enable studies and aggregation of data on a scale sufficient to
develop reliable microvascular biomarkers for early detection, and to guide
treatment, of microvascular disease.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A New Semi-Automated Algorithm for Volumetric Segmentation of the Left Ventricle in Temporal 3D Echocardiography Sequences. (arXiv:2109.01132v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01132">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Purpose: Echocardiography is commonly used as a non-invasive imaging tool in
clinical practice for the assessment of cardiac function. However, delineation
of the left ventricle is challenging due to the inherent properties of
ultrasound imaging, such as the presence of speckle noise and the low
signal-to-noise ratio. Methods: We propose a semi-automated segmentation
algorithm for the delineation of the left ventricle in temporal 3D
echocardiography sequences. The method requires minimal user interaction and
relies on a diffeomorphic registration approach. Advantages of the method
include no dependence on prior geometrical information, training data, or
registration from an atlas. Results: The method was evaluated using
three-dimensional ultrasound scan sequences from 18 patients from the
Mazankowski Alberta Heart Institute, Edmonton, Canada, and compared to manual
delineations provided by an expert cardiologist and four other registration
algorithms. The segmentation approach yielded the following results over the
cardiac cycle: a mean absolute difference of 1.01 (0.21) mm, a Hausdorff
distance of 4.41 (1.43) mm, and a Dice overlap score of 0.93 (0.02).
Conclusions: The method performed well compared to the four other registration
algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Out-of-Distribution Detection for Automotive Perception. (arXiv:2011.01413v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.01413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks (NNs) are widely used for object classification in autonomous
driving. However, NNs can fail on input data not well represented by the
training dataset, known as out-of-distribution (OOD) data. A mechanism to
detect OOD samples is important for safety-critical applications, such as
automotive perception, to trigger a safe fallback mode. NNs often rely on
softmax normalization for confidence estimation, which can lead to high
confidences being assigned to OOD samples, thus hindering the detection of
failures. This paper presents a method for determining whether inputs are OOD,
which does not require OOD data during training and does not increase the
computational cost of inference. The latter property is especially important in
automotive applications with limited computational resources and real-time
constraints. Our proposed approach outperforms state-of-the-art methods on
real-world automotive datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual-Camera Super-Resolution with Aligned Attention Modules. (arXiv:2109.01349v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01349">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present a novel approach to reference-based super-resolution (RefSR) with
the focus on dual-camera super-resolution (DCSR), which utilizes reference
images for high-quality and high-fidelity results. Our proposed method
generalizes the standard patch-based feature matching with spatial alignment
operations. We further explore the dual-camera super-resolution that is one
promising application of RefSR, and build a dataset that consists of 146 image
pairs from the main and telephoto cameras in a smartphone. To bridge the domain
gaps between real-world images and the training images, we propose a
self-supervised domain adaptation strategy for real-world images. Extensive
experiments on our dataset and a public benchmark demonstrate clear improvement
achieved by our method over state of the art in both quantitative evaluation
and visual comparisons.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose CX-ToM, short for counterfactual explanations with theory-of mind,
a new explainable AI (XAI) framework for explaining decisions made by a deep
convolutional neural network (CNN). In contrast to the current methods in XAI
that generate explanations as a single shot response, we pose explanation as an
iterative communication process, i.e. dialog, between the machine and human
user. More concretely, our CX-ToM framework generates sequence of explanations
in a dialog by mediating the differences between the minds of machine and human
user. To do this, we use Theory of Mind (ToM) which helps us in explicitly
modeling human&#x27;s intention, machine&#x27;s mind as inferred by the human as well as
human&#x27;s mind as inferred by the machine. Moreover, most state-of-the-art XAI
frameworks provide attention (or heat map) based explanations. In our work, we
show that these attention based explanations are not sufficient for increasing
human trust in the underlying CNN model. In CX-ToM, we instead use
counterfactual explanations called fault-lines which we define as follows:
given an input image I for which a CNN classification model M predicts class
c_pred, a fault-line identifies the minimal semantic-level features (e.g.,
stripes on zebra, pointed ears of dog), referred to as explainable concepts,
that need to be added to or deleted from I in order to alter the classification
category of I by M to another specified class c_alt. We argue that, due to the
iterative, conceptual and counterfactual nature of CX-ToM explanations, our
framework is practical and more natural for both expert and non-expert users to
understand the internal workings of complex deep learning models. Extensive
quantitative and qualitative experiments verify our hypotheses, demonstrating
that our CX-ToM significantly outperforms the state-of-the-art explainable AI
models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08212">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01181">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Targets are essential in problems such as object tracking in cluttered or
textureless environments, camera (and multi-sensor) calibration tasks, and
simultaneous localization and mapping (SLAM). Target shapes for these tasks
typically are symmetric (square, rectangular, or circular) and work well for
structured, dense sensor data such as pixel arrays (i.e., image). However,
symmetric shapes lead to pose ambiguity when using sparse sensor data such as
LiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.
This paper introduces the concept of optimizing target shape to remove pose
ambiguity for LiDAR point clouds. A target is designed to induce large
gradients at edge points under rotation and translation relative to the LiDAR
to ameliorate the quantization uncertainty associated with point cloud
sparseness. Moreover, given a target shape, we present a means that leverages
the target&#x27;s geometry to estimate the target&#x27;s vertices while globally
estimating the pose. Both the simulation and the experimental results (verified
by a motion capture system) confirm that by using the optimal shape and the
global solver, we achieve centimeter error in translation and a few degrees in
rotation even when a partially illuminated target is placed 30 meters away. All
the implementations and datasets are available at
https://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data. (arXiv:2106.00799v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.00799">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This work proposes a multi-task fully convolutional architecture for tree
species mapping in dense forests from sparse and scarce polygon-level
annotations using hyperspectral UAV-borne data. Our model implements a partial
loss function that enables dense tree semantic labeling outcomes from non-dense
training samples, and a distance regression complementary task that enforces
tree crown boundary constraints and substantially improves the model
performance. Our multi-task architecture uses a shared backbone network that
learns common representations for both tasks and two task-specific decoders,
one for the semantic segmentation output and one for the distance map
regression. We report that introducing the complementary task boosts the
semantic segmentation performance compared to the single-task counterpart in up
to 11% reaching an average user&#x27;s accuracy of 88.63% and an average producer&#x27;s
accuracy of 88.59%, achieving state-of-art performance for tree species
classification in tropical forests.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance. (arXiv:1909.10837v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.10837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Spiking neural network (SNN) is interesting both theoretically and
practically because of its strong bio-inspiration nature and potentially
outstanding energy efficiency. Unfortunately, its development has fallen far
behind the conventional deep neural network (DNN), mainly because of difficult
training and lack of widely accepted hardware experiment platforms. In this
paper, we show that a deep temporal-coded SNN can be trained easily and
directly over the benchmark datasets CIFAR10 and ImageNet, with testing
accuracy within 1% of the DNN of equivalent size and architecture. Training
becomes similar to DNN thanks to the closed-form solution to the spiking
waveform dynamics. Considering that SNNs should be implemented in practical
neuromorphic hardwares, we train the deep SNN with weights quantized to 8, 4, 2
bits and with weights perturbed by random noise to demonstrate its robustness
in practical applications. In addition, we develop a phase-domain signal
processing circuit schematic to implement our spiking neuron with 90% gain of
energy efficiency over existing work. This paper demonstrates that the
temporal-coded deep SNN is feasible for applications with high performance and
high energy efficient.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00636">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We learn an interactive vision-based driving policy from pre-recorded driving
logs via a model-based approach. A forward model of the world supervises a
driving policy that predicts the outcome of any potential driving trajectory.
To support learning from pre-recorded logs, we assume that the world is on
rails, meaning neither the agent nor its actions influence the environment.
This assumption greatly simplifies the learning problem, factorizing the
dynamics into a nonreactive world model and a low-dimensional and compact
forward model of the ego-vehicle. Our approach computes action-values for each
training trajectory using a tabular dynamic-programming evaluation of the
Bellman equations; these action-values in turn supervise the final vision-based
driving policy. Despite the world-on-rails assumption, the final driving policy
acts well in a dynamic and reactive world. At the time of writing, our method
ranks first on the CARLA leaderboard, attaining a 25% higher driving score
while using 40 times less data. Our method is also an order of magnitude more
sample-efficient than state-of-the-art model-free reinforcement learning
techniques on navigational tasks in the ProcGen benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Action Recognition Using Confidence Distillation. (arXiv:2109.02137v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02137">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Modern neural networks are powerful predictive models. However, when it comes
to recognizing that they may be wrong about their predictions, they perform
poorly. For example, for one of the most common activation functions, the ReLU
and its variants, even a well-calibrated model can produce incorrect but high
confidence predictions. In the related task of action recognition, most current
classification methods are based on clip-level classifiers that densely sample
a given video for non-overlapping, same-sized clips and aggregate the results
using an aggregation function - typically averaging - to achieve video level
predictions. While this approach has shown to be effective, it is sub-optimal
in recognition accuracy and has a high computational overhead. To mitigate both
these issues, we propose the confidence distillation framework to teach a
representation of uncertainty of the teacher to the student sampler and divide
the task of full video prediction between the student and the teacher models.
We conduct extensive experiments on three action recognition datasets and
demonstrate that our framework achieves significant improvements in action
recognition accuracy (up to 20%) and computational efficiency (more than 40%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Instance-level Image Retrieval using Reranking Transformers. (arXiv:2103.12236v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12236">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Instance-level image retrieval is the task of searching in a large database
for images that match an object in a query image. To address this task, systems
usually rely on a retrieval step that uses global image descriptors, and a
subsequent step that performs domain-specific refinements or reranking by
leveraging operations such as geometric verification based on local features.
In this work, we propose Reranking Transformers (RRTs) as a general model to
incorporate both local and global features to rerank the matching images in a
supervised fashion and thus replace the relatively expensive process of
geometric verification. RRTs are lightweight and can be easily parallelized so
that reranking a set of top matching results can be performed in a single
forward-pass. We perform extensive experiments on the Revisited Oxford and
Paris datasets, and the Google Landmarks v2 dataset, showing that RRTs
outperform previous reranking approaches while using much fewer local
descriptors. Moreover, we demonstrate that, unlike existing approaches, RRTs
can be optimized jointly with the feature extractor, which can lead to feature
representations tailored to downstream tasks and further accuracy improvements.
The code and trained models are publicly available at
https://github.com/uvavision/RerankingTransformer.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Intelligent Monitoring of Stress Induced by Water Deficiency in Plants using Deep Learning. (arXiv:2104.07911v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07911">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In the recent decade, high-throughput plant phenotyping techniques, which
combine non-invasive image analysis and machine learning, have been
successfully applied to identify and quantify plant health and diseases.
However, these techniques usually do not consider the progressive nature of
plant stress and often require images showing severe signs of stress to ensure
high confidence detection, thereby reducing the feasibility for early detection
and recovery of plants under stress. To overcome the problem mentioned above,
we propose a deep learning pipeline for the temporal analysis of the visual
changes induced in the plant due to stress and apply it to the specific water
stress identification case in Chickpea plant shoot images. For this, we have
considered an image dataset of two chickpea varieties JG-62 and Pusa-372, under
three water stress conditions; control, young seedling, and before flowering,
captured over five months. We have employed a variant of Convolutional Neural
Network - Long Short Term Memory (CNN-LSTM) network to learn spatio-temporal
patterns from the chickpea plant dataset and use them for water stress
classification. Our model has achieved ceiling level classification performance
of 98.52% on JG-62 and 97.78% on Pusa-372 chickpea plant data and has
outperformed the best reported time-invariant technique by at least 14% for
both JG-62 and Pusa-372 species, to the best of our knowledge. Furthermore, our
CNN-LSTM model has demonstrated robustness to noisy input, with a less than
2.5% dip in average model accuracy and a small standard deviation about the
mean for both species. Lastly, we have performed an ablation study to analyze
the performance of the CNN-LSTM model by decreasing the number of temporal
session data used for training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Self-supervised Product Quantization for Deep Unsupervised Image Retrieval. (arXiv:2109.02244v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02244">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Supervised deep learning-based hash and vector quantization are enabling fast
and large-scale image retrieval systems. By fully exploiting label annotations,
they are achieving outstanding retrieval performances compared to the
conventional methods. However, it is painstaking to assign labels precisely for
a vast amount of training data, and also, the annotation process is
error-prone. To tackle these issues, we propose the first deep unsupervised
image retrieval method dubbed Self-supervised Product Quantization (SPQ)
network, which is label-free and trained in a self-supervised manner. We design
a Cross Quantized Contrastive learning strategy that jointly learns codewords
and deep visual descriptors by comparing individually transformed images
(views). Our method analyzes the image contents to extract descriptive
features, allowing us to understand image representations for accurate
retrieval. By conducting extensive experiments on benchmarks, we demonstrate
that the proposed method yields state-of-the-art results even without
supervised pretraining.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast Image-Anomaly Mitigation for Autonomous Mobile Robots. (arXiv:2109.01889v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01889">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Camera anomalies like rain or dust can severelydegrade image quality and its
related tasks, such as localizationand segmentation. In this work we address
this importantissue by implementing a pre-processing step that can
effectivelymitigate such artifacts in a real-time fashion, thus supportingthe
deployment of autonomous systems with limited computecapabilities. We propose a
shallow generator with aggregation,trained in an adversarial setting to solve
the ill-posed problemof reconstructing the occluded regions. We add an enhancer
tofurther preserve high-frequency details and image colorization.We also
produce one of the largest publicly available datasets1to train our
architecture and use realistic synthetic raindrops toobtain an improved
initialization of the model. We benchmarkour framework on existing datasets and
on our own imagesobtaining state-of-the-art results while enabling real-time
per-formance, with up to 40x faster inference time than existingapproaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Surprising Efficiency of Committee-based Models. (arXiv:2012.01988v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01988">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Committee-based models, i.e., model ensembles or cascades, are underexplored
in recent work on developing efficient models. While committee-based models
themselves are not new, there lacks a systematic understanding of their
efficiency in comparison with single models. To fill this gap, we conduct a
comprehensive analysis of the efficiency of committee-based models. We find
that committee-based models provide a complementary paradigm to achieve
superior efficiency without tuning the architecture: even the most simplistic
method for building ensembles or cascades from existing pre-trained networks
can attain a significant speedup and higher accuracy over state-of-the-art
single models, and also outperforms sophisticated neural architecture search
methods (e.g., BigNAS). The superior efficiency of committee-based models holds
true for several tasks, including image classification, video classification,
and semantic segmentation, and various architecture families, such as
EfficientNet, ResNet, MobileNetV2, and X3D.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02100">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged for their deployments to resource-limited
devices. Although recent studies have successfully discretized a full-precision
network, they still incur large quantization errors after training, thus giving
rise to a significant performance gap between a full-precision network and its
quantized counterpart. In this work, we propose a novel quantization method for
neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal
quantization grids while naturally encouraging the underlying full-precision
weights to gather around those quantization grids cohesively during training.
This property of CPQ is thanks to our two main ingredients that enable
differentiable quantization: i) the use of the categorical distribution
designed by a specific probabilistic parametrization in the forward pass and
ii) our proposed multi-class straight-through estimator (STE) in the backward
pass. Since our second component, multi-class STE, is intrinsically biased, we
additionally propose a new bit-drop technique, DropBits, that revises the
standard dropout regularization to randomly drop bits instead of neurons. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer by
imposing an additional regularization on DropBits. We experimentally validate
our method on various benchmark datasets and network architectures, and also
support a new hypothesis for quantization: learning heterogeneous quantization
levels outperforms the case using the same but fixed quantization levels from
scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Utilizing Adversarial Targeted Attacks to Boost Adversarial Robustness. (arXiv:2109.01945v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01945">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks have been shown to be highly effective at degrading the
performance of deep neural networks (DNNs). The most prominent defense is
adversarial training, a method for learning a robust model. Nevertheless,
adversarial training does not make DNNs immune to adversarial perturbations. We
propose a novel solution by adopting the recently suggested Predictive
Normalized Maximum Likelihood. Specifically, our defense performs adversarial
targeted attacks according to different hypotheses, where each hypothesis
assumes a specific label for the test sample. Then, by comparing the hypothesis
probabilities, we predict the label. Our refinement process corresponds to
recent findings of the adversarial subspace properties. We extensively evaluate
our approach on 16 adversarial attack benchmarks using ResNet-50,
WideResNet-28, and a2-layer ConvNet trained with ImageNet, CIFAR10, and MNIST,
showing a significant improvement of up to 5.7%, 3.7%, and 0.6% respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Data Hiding Using Inverse Gradient Attention. (arXiv:2011.10850v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.10850">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Data hiding is the procedure of encoding desired information into the cover
image to resist potential noises while ensuring the embedded image has few
perceptual perturbations from the original one. Recently, with the tremendous
successes gained by deep neural networks in various fields, the researches of
data hiding with deep learning models have attracted an increasing number of
attentions. In the data hiding task, each pixel of cover images should be
treated differently since they have divergent tolerabilities. The neglect of
considering the sensitivity of each pixel will inevitably affect the model
robustness for information hiding. Targeting this problem, we propose a novel
deep data hiding scheme with Inverse Gradient Attention (IGA), combing the
ideas of adversarial learning and attention mechanism to endow different
sensitivities for different pixels. With the proposed component, the model can
spotlight pixels with more robustness for data hiding. Empirically, extensive
experiments show that the proposed model outperforms the state-of-the-art
methods on two prevalent datasets under multiple evaluations. Besides, we
further identify and discuss the connections between the proposed inverse
gradient attention and high-frequency regions within images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Moving Object Detection for Event-based Vision using k-means Clustering. (arXiv:2109.01879v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01879">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Moving object detection is a crucial task in computer vision. Event-based
cameras are bio-inspired cameras that work by mimicking the working of the
human eye. These cameras have multiple advantages over conventional frame-based
cameras, like reduced latency, HDR, reduced motion blur during high motion, low
power consumption, etc. However, these advantages come at a high cost, as
event-based cameras are noise sensitive and have low resolution. Moreover, the
task of moving object detection in these cameras is difficult, as event-based
sensors capture only the binary changes in brightness of a scene, lacking
useful visual features like texture and color. In this paper, we investigate
the application of the k-means clustering technique in detecting moving objects
in event-based data. Experimental results in publicly available datasets using
k-means show significant improvement in performance over the state-of-the-art
methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00436">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mainly in object classification tasks. In this paper, we
make the first large-scale study concerning the task of Long-Tail Visual
Relationship Recognition (LTVRR). LTVRR aims at improving the learning of
structured visual relationships that come from the long-tail (e.g., &quot;rabbit
grazing on grass&quot;). In this setup, the subject, relation, and object classes
each follow a long-tail distribution. To begin our study and make a future
benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed
VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of several state-of-the-art
long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic
hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR
setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top
of existing models and despite being simple, our results show that they can
remarkably improve the performance, especially on tail classes. Benchmarks,
code, and models have been made available at:
https://github.com/Vision-CAIR/LTVRR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">TearingNet: Point Cloud Autoencoder to Learn Topology-Friendly Representations. (arXiv:2006.10187v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.10187">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Topology matters. Despite the recent success of point cloud processing with
geometric deep learning, it remains arduous to capture the complex topologies
of point cloud data with a learning model. Given a point cloud dataset
containing objects with various genera, or scenes with multiple objects, we
propose an autoencoder, TearingNet, which tackles the challenging task of
representing the point clouds using a fixed-length descriptor. Unlike existing
works directly deforming predefined primitives of genus zero (e.g., a 2D square
patch) to an object-level point cloud, our TearingNet is characterized by a
proposed Tearing network module and a Folding network module interacting with
each other iteratively. Particularly, the Tearing network module learns the
point cloud topology explicitly. By breaking the edges of a primitive graph, it
tears the graph into patches or with holes to emulate the topology of a target
point cloud, leading to faithful reconstructions. Experimentation shows the
superiority of our proposal in terms of reconstructing point clouds as well as
generating more topology-friendly representations than benchmarks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on Spark AR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on the enhanced data sets and showing that they outperform
equivalent systems trained on the original data sets (containing faces without
masks), when the test benchmark contains masked faces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Generate Scene Graph from Natural Language Supervision. (arXiv:2109.02227v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02227">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning from image-text data has demonstrated recent success for many
recognition tasks, yet is currently limited to visual features or individual
visual concepts such as objects. In this paper, we propose one of the first
methods that learn from image-sentence pairs to extract a graphical
representation of localized objects and their relationships within an image,
known as scene graph. To bridge the gap between images and texts, we leverage
an off-the-shelf object detector to identify and localize object instances,
match labels of detected regions to concepts parsed from captions, and thus
create &quot;pseudo&quot; labels for learning scene graph. Further, we design a
Transformer-based model to predict these &quot;pseudo&quot; labels via a masked token
prediction task. Learning from only image-sentence pairs, our model achieves
30% relative gain over a latest method trained with human-annotated unlocalized
scene graphs. Our model also shows strong results for weakly and fully
supervised scene graph generation. In addition, we explore an open-vocabulary
setting for detecting scene graphs, and present the first result for open-set
scene graph generation. Our code is available at
https://github.com/YiwuZhong/SGG_from_NLS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training (AT) has been demonstrated as one of the most promising
defense methods against various adversarial attacks. To our knowledge, existing
AT-based methods usually train with the locally most adversarial perturbed
points and treat all the perturbed points equally, which may lead to
considerably weaker adversarial robust generalization on test data. In this
work, we introduce a new adversarial training framework that considers the
diversity as well as characteristics of the perturbed points in the vicinity of
benign samples. To realize the framework, we propose a Regional Adversarial
Training (RAT) defense method that first utilizes the attack path generated by
the typical iterative attack method of projected gradient descent (PGD), and
constructs an adversarial region based on the attack path. Then, RAT samples
diverse perturbed training points efficiently inside this region, and utilizes
a distance-aware label smoothing mechanism to capture our intuition that
perturbed points at different locations should have different impact on the
model performance. Extensive experiments on several benchmark datasets show
that RAT consistently makes significant improvement on standard adversarial
training (SAT), and exhibits better robust generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatiotemporal Inconsistency Learning for DeepFake Video Detection. (arXiv:2109.01860v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01860">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid development of facial manipulation techniques has aroused public
concerns in recent years. Following the success of deep learning, existing
methods always formulate DeepFake video detection as a binary classification
problem and develop frame-based and video-based solutions. However, little
attention has been paid to capturing the spatial-temporal inconsistency in
forged videos. To address this issue, we term this task as a Spatial-Temporal
Inconsistency Learning (STIL) process and instantiate it into a novel STIL
block, which consists of a Spatial Inconsistency Module (SIM), a Temporal
Inconsistency Module (TIM), and an Information Supplement Module (ISM).
Specifically, we present a novel temporal modeling paradigm in TIM by
exploiting the temporal difference over adjacent frames along with both
horizontal and vertical directions. And the ISM simultaneously utilizes the
spatial information from SIM and temporal information from TIM to establish a
more comprehensive spatial-temporal representation. Moreover, our STIL block is
flexible and could be plugged into existing 2D CNNs. Extensive experiments and
visualizations are presented to demonstrate the effectiveness of our method
against the state-of-the-art competitors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Graph Signal Processing for Geometric Data and Beyond: Theory and Applications. (arXiv:2008.01918v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Geometric data acquired from real-world scenes, e.g., 2D depth images, 3D
point clouds, and 4D dynamic point clouds, have found a wide range of
applications including immersive telepresence, autonomous driving,
surveillance, etc. Due to irregular sampling patterns of most geometric data,
traditional image/video processing methodologies are limited, while Graph
Signal Processing (GSP) -- a fast-developing field in the signal processing
community -- enables processing signals that reside on irregular domains and
plays a critical role in numerous applications of geometric data from low-level
processing to high-level analysis. To further advance the research in this
field, we provide the first timely and comprehensive overview of GSP
methodologies for geometric data in a unified manner by bridging the
connections between geometric data and graphs, among the various geometric data
modalities, and with spectral/nodal graph filtering techniques. We also discuss
the recently developed Graph Neural Networks (GNNs) and interpret the operation
of these networks from the perspective of GSP. We conclude with a brief
discussion of open problems and challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark. (arXiv:2109.01839v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01839">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As a kind of new expression elements, Internet memes are popular and
extensively used in online chatting scenarios since they manage to make
dialogues vivid, moving, and interesting. However, most current dialogue
researches focus on text-only dialogue tasks. In this paper, we propose a new
task named as \textbf{M}eme incorporated \textbf{O}pen-domain \textbf{D}ialogue
(MOD). Compared to previous dialogue tasks, MOD is much more challenging since
it requires the model to understand the multimodal elements as well as the
emotions behind them. To facilitate the MOD research, we construct a
large-scale open-domain multimodal dialogue dataset incorporating abundant
Internet memes into utterances. The dataset consists of $\sim$45K Chinese
conversations with $\sim$606K utterances. Each conversation contains about $13$
utterances with about $4$ Internet memes on average and each utterance equipped
with an Internet meme is annotated with the corresponding emotion. In addition,
we present a simple and effective method, which utilizes a unified generation
network to solve the MOD task. Experimental results demonstrate that our method
trained on the proposed corpus is able to achieve expressive communication
including texts and memes. The corpus and models have been publicly available
at https://github.com/lizekang/DSTC10-MOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Underwater 3D Reconstruction Using Light Fields. (arXiv:2109.02116v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02116">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Underwater 3D reconstruction is challenging due to the refraction of light at
the water-air interface (most electronic devices cannot be directly submerged
in water). In this paper, we present an underwater 3D reconstruction solution
using light field cameras. We first develop a light field camera calibration
algorithm that simultaneously estimates the camera parameters and the geometry
of the water-air interface. We then design a novel depth estimation algorithm
for 3D reconstruction. Specifically, we match correspondences on curved
epipolar lines caused by water refraction. We also observe that the
view-dependent specular reflection is very weak in the underwater environment,
resulting the angularly sampled rays in light field has uniform intensity. We
therefore propose an angular uniformity constraint for depth optimization. We
also develop a fast algorithm for locating the angular patches in presence of
non-linear light paths. Extensive synthetic and real experiments demonstrate
that our method can perform underwater 3D reconstruction with high accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Learning for Embodied Vision Navigation: A Survey. (arXiv:2108.04097v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04097">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Navigation is one of the fundamental features of a autonomous robot. And the
ability of long-term navigation with semantic instruction is a &#x60;holy grail&#x60;
goals of intelligent robots. The development of 3D simulation technology
provide a large scale of data to simulate the real-world environment. The deep
learning proves its ability to robustly learn various embodied navigation
tasks. However, deep learning on embodied navigation is still in its infancy
due to the unique challenges faced by the navigation exploration and learning
from partial observed visual input. Recently, deep learning in embodied
navigation has become even thriving, with numerous methods have been proposed
to tackle different challenges in this area. To give a promising direction for
future research, in this paper, we present a comprehensive review of embodied
navigation tasks and the recent progress in deep learning based methods. It
includes two major tasks: target-oriented navigation and the
instruction-oriented navigation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v4 [cs.PF] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.03459">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">DRIVE: Deep Reinforced Accident Anticipation with Visual Explanation. (arXiv:2107.10189v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10189">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Traffic accident anticipation aims to accurately and promptly predict the
occurrence of a future accident from dashcam videos, which is vital for a
safety-guaranteed self-driving system. To encourage an early and accurate
decision, existing approaches typically focus on capturing the cues of spatial
and temporal context before a future accident occurs. However, their
decision-making lacks visual explanation and ignores the dynamic interaction
with the environment. In this paper, we propose Deep ReInforced accident
anticipation with Visual Explanation, named DRIVE. The method simulates both
the bottom-up and top-down visual attention mechanism in a dashcam observation
environment so that the decision from the proposed stochastic multi-task agent
can be visually explained by attentive regions. Moreover, the proposed dense
anticipation reward and sparse fixation reward are effective in training the
DRIVE model with our improved reinforcement learning algorithm. Experimental
results show that the DRIVE model achieves state-of-the-art performance on
multiple real-world traffic accident datasets. Code and pre-trained model are
available at \url{https://www.rit.edu/actionlab/drive}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02344">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-view representation learning captures comprehensive information from
multiple views of a shared context. Recent works intuitively apply contrastive
learning (CL) to learn representations, regarded as a pairwise manner, which is
still scalable: view-specific noise is not filtered in learning view-shared
representations; the fake negative pairs, where the negative terms are actually
within the same class as the positive, and the real negative pairs are
coequally treated; and evenly measuring the similarities between terms might
interfere with optimization. Importantly, few works research the theoretical
framework of generalized self-supervised multi-view learning, especially for
more than two views. To this end, we rethink the existing multi-view learning
paradigm from the information theoretical perspective and then propose a novel
information theoretical framework for generalized multi-view learning. Guided
by it, we build a multi-view coding method with a three-tier progressive
architecture, namely Information theory-guided heuristic Progressive Multi-view
Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between
views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted
pools for contrasting, which utilizes a view filter to adaptively modify the
pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn
discriminative representations and reduce the gradient interference.
Theoretically and empirically, we demonstrate the superiority of IPMC over
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Audio-Visual Transformer Based Crowd Counting. (arXiv:2109.01926v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01926">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Crowd estimation is a very challenging problem. The most recent study tries
to exploit auditory information to aid the visual models, however, the
performance is limited due to the lack of an effective approach for feature
extraction and integration. The paper proposes a new audiovisual multi-task
network to address the critical challenges in crowd counting by effectively
utilizing both visual and audio inputs for better modalities association and
productive feature extraction. The proposed network introduces the notion of
auxiliary and explicit image patch-importance ranking (PIR) and patch-wise
crowd estimate (PCE) information to produce a third (run-time) modality. These
modalities (audio, visual, run-time) undergo a transformer-inspired
cross-modality co-attention mechanism to finally output the crowd estimate. To
acquire rich visual features, we propose a multi-branch structure with
transformer-style fusion in-between. Extensive experimental evaluations show
that the proposed scheme outperforms the state-of-the-art networks under all
evaluation settings with up to 33.8% improvement. We also analyze and compare
the vision-only variant of our network and empirically demonstrate its
superiority over previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatial Domain Feature Extraction Methods for Unconstrained Handwritten Malayalam Character Recognition. (arXiv:2109.02153v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02153">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Handwritten character recognition is an active research challenge,especially
for Indian scripts. This paper deals with handwritten Malayalam, with a
complete set of basic characters, vowel and consonant signs and compound
characters that may be present in the script. Spatial domain features suitable
for recognition are chosen in this work. For classification, k-NN, SVM and ELM
are employed</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-view Image-based Hand Geometry Refinement using Differentiable Monte Carlo Ray Tracing. (arXiv:2107.05509v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05509">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The amount and quality of datasets and tools available in the research field
of hand pose and shape estimation act as evidence to the significant progress
that has been made.However, even the datasets of the highest quality, reported
to date, have shortcomings in annotation. We propose a refinement approach,
based on differentiable ray tracing,and demonstrate how a high-quality publicly
available, multi-camera dataset of hands(InterHand2.6M) can become an even
better dataset, with respect to annotation quality. Differentiable ray tracing
has not been employed so far to relevant problems and is hereby shown to be
superior to the approximative alternatives that have been employed in the past.
To tackle the lack of reliable ground truth, as far as quantitative evaluation
is concerned, we resort to realistic synthetic data, to show that the
improvement we induce is indeed significant. The same becomes evident in real
data through visual evaluation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Representing human-made objects as a collection of base primitives has a long
history in computer vision and reverse engineering. In the case of
high-resolution point cloud scans, the challenge is to be able to detect both
large primitives as well as those explaining the detailed parts. While the
classical RANSAC approach requires case-specific parameter tuning,
state-of-the-art networks are limited by memory consumption of their backbone
modules such as PointNet++, and hence fail to detect the fine-scale primitives.
We present Cascaded Primitive Fitting Networks (CPFN) that relies on an
adaptive patch sampling network to assemble detection results of global and
local primitive detection networks. As a key enabler, we present a merging
formulation that dynamically aggregates the primitives across global and local
scales. Our evaluation demonstrates that CPFN improves the state-of-the-art
SPFN performance by 13-14% on high-resolution point cloud datasets and
specifically improves the detection of fine-scale primitives by 20-22%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image Compression with Recurrent Neural Network and Generalized Divisive Normalization. (arXiv:2109.01999v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01999">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Image compression is a method to remove spatial redundancy between adjacent
pixels and reconstruct a high-quality image. In the past few years, deep
learning has gained huge attention from the research community and produced
promising image reconstruction results. Therefore, recent methods focused on
developing deeper and more complex networks, which significantly increased
network complexity. In this paper, two effective novel blocks are developed:
analysis and synthesis block that employs the convolution layer and Generalized
Divisive Normalization (GDN) in the variable-rate encoder and decoder side. Our
network utilizes a pixel RNN approach for quantization. Furthermore, to improve
the whole network, we encode a residual image using LSTM cells to reduce
unnecessary information. Experimental results demonstrated that the proposed
variable-rate framework with novel blocks outperforms existing methods and
standard image codecs, such as George&#x27;s ~\cite{002} and JPEG in terms of image
similarity. The project page along with code and models are available at
https://khawar512.github.io/cvpr/</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Triplet-Watershed for Hyperspectral Image Classification. (arXiv:2103.09384v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.09384">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hyperspectral images (HSI) consist of rich spatial and spectral information,
which can potentially be used for several applications. However, noise, band
correlations and high dimensionality restrict the applicability of such data.
This is recently addressed using creative deep learning network architectures
such as ResNet, SSRN, and A2S2K. However, the last layer, i.e the
classification layer, remains unchanged and is taken to be the softmax
classifier. In this article, we propose to use a watershed classifier.
Watershed classifier extends the watershed operator from Mathematical
Morphology for classification. In its vanilla form, the watershed classifier
does not have any trainable parameters. In this article, we propose a novel
approach to train deep learning networks to obtain representations suitable for
the watershed classifier. The watershed classifier exploits the connectivity
patterns, a characteristic of HSI datasets, for better inference. We show that
exploiting such characteristics allows the Triplet-Watershed to achieve
state-of-art results in supervised and semi-supervised contexts. These results
are validated on Indianpines (IP), University of Pavia (UP), Kennedy Space
Center (KSC) and University of Houston (UH) datasets, relying on simple convnet
architecture using a quarter of parameters compared to previous
state-of-the-art networks. The source code for reproducing the experiments and
supplementary material (high resolution images) is available at
https://github.com/ac20/TripletWatershed Code.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Spectral Image Synthesis for Crop/Weed Segmentation in Precision Farming. (arXiv:2009.05750v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.05750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>An effective perception system is a fundamental component for farming robots,
as it enables them to properly perceive the surrounding environment and to
carry out targeted operations. The most recent methods make use of
state-of-the-art machine learning techniques to learn a valid model for the
target task. However, those techniques need a large amount of labeled data for
training. A recent approach to deal with this issue is data augmentation
through Generative Adversarial Networks (GANs), where entire synthetic scenes
are added to the training data, thus enlarging and diversifying their
informative content. In this work, we propose an alternative solution with
respect to the common data augmentation methods, applying it to the fundamental
problem of crop/weed segmentation in precision farming. Starting from real
images, we create semi-artificial samples by replacing the most relevant object
classes (i.e., crop and weeds) with their synthesized counterparts. To do that,
we employ a conditional GAN (cGAN), where the generative model is trained by
conditioning the shape of the generated object. Moreover, in addition to RGB
data, we take into account also near-infrared (NIR) information, generating
four channel multi-spectral synthetic images. Quantitative experiments, carried
out on three publicly available datasets, show that (i) our model is capable of
generating realistic multi-spectral images of plants and (ii) the usage of such
synthetic images in the training process improves the segmentation performance
of state-of-the-art semantic segmentation convolutional networks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Tightness of Semidefinite Relaxations for Rotation Estimation. (arXiv:2101.02099v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02099">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Why is it that semidefinite relaxations have been so successful in numerous
applications in computer vision and robotics for solving non-convex
optimization problems involving rotations? In studying the empirical
performance we note that there are few failure cases reported in the
literature, in particular for estimation problems with a single rotation,
motivating us to gain further theoretical understanding.

A general framework based on tools from algebraic geometry is introduced for
analyzing the power of semidefinite relaxations of problems with quadratic
objective functions and rotational constraints. Applications include
registration, hand-eye calibration and rotation averaging. We characterize the
extreme points, and show that there exist failure cases for which the
relaxation is not tight, even in the case of a single rotation. We also show
that some problem classes are always tight given an appropriate
parametrization. Our theoretical findings are accompanied with numerical
simulations, providing further evidence and understanding of the results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02123">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural Radiance Fields (NeRF) has become a popular framework for learning
implicit 3D representations and addressing different tasks such as novel-view
synthesis or depth-map estimation. However, in downstream applications where
decisions need to be made based on automatic predictions, it is critical to
leverage the confidence associated with the model estimations. Whereas
uncertainty quantification is a long-standing problem in Machine Learning, it
has been largely overlooked in the recent NeRF literature. In this context, we
propose Stochastic Neural Radiance Fields (S-NeRF), a generalization of
standard NeRF that learns a probability distribution over all the possible
radiance fields modeling the scene. This distribution allows to quantify the
uncertainty associated with the scene information provided by the model. S-NeRF
optimization is posed as a Bayesian learning problem which is efficiently
addressed using the Variational Inference framework. Exhaustive experiments
over benchmark datasets demonstrate that S-NeRF is able to provide more
reliable predictions and confidence values than generic approaches previously
proposed for uncertainty estimation in other domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. (arXiv:2109.02226v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02226">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this manuscript, we introduce a semi-automatic scene graph annotation tool
for images, the GeneAnnotator. This software allows human annotators to
describe the existing relationships between participators in the visual scene
in the form of directed graphs, hence enabling the learning and reasoning on
visual relationships, e.g., image captioning, VQA and scene graph generation,
etc. The annotations for certain image datasets could either be merged in a
single VG150 data-format file to support most existing models for scene graph
learning or transformed into a separated annotation file for each single image
to build customized datasets. Moreover, GeneAnnotator provides a rule-based
relationship recommending algorithm to reduce the heavy annotation workload.
With GeneAnnotator, we propose Traffic Genome, a comprehensive scene graph
dataset with 1000 diverse traffic images, which in return validates the
effectiveness of the proposed software for scene graph annotation. The project
source code, with usage examples and sample data is available at
https://github.com/Milomilo0320/A-Semi-automatic-Annotation-Software-for-Scene-Graph,
under the Apache open-source license.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing. (arXiv:2109.02281v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02281">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Compared with image scene parsing, video scene parsing introduces temporal
information, which can effectively improve the consistency and accuracy of
prediction. In this paper, we propose a Spatial-Temporal Semantic Consistency
method to capture class-exclusive context information. Specifically, we design
a spatial-temporal consistency loss to constrain the semantic consistency in
spatial and temporal dimensions. In addition, we adopt an pseudo-labeling
strategy to enrich the training dataset. We obtain the scores of 59.84% and
58.85% mIoU on development (test part 1) and testing set of VSPW, respectively.
And our method wins the 1st place on VSPW challenge at ICCV2021.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10423">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as &quot;review&quot; trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the &quot;review&quot; trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Segmentation plays a crucial role in diagnosis. Studying the retinal
vasculatures from fundus images help identify early signs of many crucial
illnesses such as diabetic retinopathy. Due to the varying shape, size, and
patterns of retinal vessels, along with artefacts and noises in fundus images,
no one-stage method can accurately segment retinal vessels. In this work, we
propose a multi-scale, multi-level attention embedded CNN architecture
((M)SLAe-Net) to address the issue of multi-stage processing for robust and
precise segmentation of retinal vessels. We do this by extracting features at
multiple scales and multiple levels of the network, enabling our model to
holistically extracts the local and global features. Multi-scale features are
extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We
also aggregate the features from all the network levels. These effectively
resolved the issues of varying shapes and artefacts and hence the need for
multiple stages. To assist in better pixel-level classification, we use the
Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and
Excitation(SE) module for segmentation tasks in our network to facilitate
pixel-group attention. Our unique network design and novel D-DPP module with
efficient task-specific loss function for thin vessels enabled our model for
better cross data performance. Exhaustive experimental results on DRIVE, STARE,
HRF, and CHASE-DB1 show the superiority of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The use of mobiles phones when driving have been a major factor when it comes
to road traffic incidents and the process of capturing such violations can be a
laborious task. Advancements in both modern object detection frameworks and
high-performance hardware has paved the way for a more automated approach when
it comes to video surveillance. In this work, we propose a custom-trained
state-of-the-art object detector to work with roadside cameras to capture
driver phone usage without the need for human intervention. The proposed
approach also addresses the issues caused by windscreen glare and introduces
the steps required to remedy this. Twelve pre-trained models are fine-tuned
with our custom dataset using four popular object detection methods: YOLO, SSD,
Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO
yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to
~30 FPS. DeepSort object tracking algorithm is also integrated into the
best-performing model to collect records of only the unique violations, and
enable the proposed approach to count the number of vehicles. The proposed
automated system will collect the output images of the identified violations,
timestamps of each violation, and total vehicle count. Data can be accessed via
a purpose-built user interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reasoning Graph Networks for Kinship Verification: from Star-shaped to Hierarchical. (arXiv:2109.02219v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02219">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we investigate the problem of facial kinship verification by
learning hierarchical reasoning graph networks. Conventional methods usually
focus on learning discriminative features for each facial image of a paired
sample and neglect how to fuse the obtained two facial image features and
reason about the relations between them. To address this, we propose a
Star-shaped Reasoning Graph Network (S-RGN). Our S-RGN first constructs a
star-shaped graph where each surrounding node encodes the information of
comparisons in a feature dimension and the central node is employed as the
bridge for the interaction of surrounding nodes. Then we perform relational
reasoning on this star graph with iterative message passing. The proposed S-RGN
uses only one central node to analyze and process information from all
surrounding nodes, which limits its reasoning capacity. We further develop a
Hierarchical Reasoning Graph Network (H-RGN) to exploit more powerful and
flexible capacity. More specifically, our H-RGN introduces a set of latent
reasoning nodes and constructs a hierarchical graph with them. Then bottom-up
comparative information abstraction and top-down comprehensive signal
propagation are iteratively performed on the hierarchical graph to update the
node features. Extensive experimental results on four widely used kinship
databases show that the proposed methods achieve very competitive results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider the problem of achieving fair classification in Federated
Learning (FL) under data heterogeneity. Most of the approaches proposed for
fair classification require diverse data that represent the different
demographic groups involved. In contrast, it is common for each client to own
data that represents only a single demographic group. Hence the existing
approaches cannot be adopted for fair classification models at the client
level. To resolve this challenge, we propose several aggregation techniques. We
empirically validate these techniques by comparing the resulting fairness
metrics and accuracy on CelebA, UTK, and FairFace datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hierarchical Object-to-Zone Graph for Object Navigation. (arXiv:2109.02066v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02066">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The goal of object navigation is to reach the expected objects according to
visual information in the unseen environments. Previous works usually implement
deep models to train an agent to predict actions in real-time. However, in the
unseen environment, when the target object is not in egocentric view, the agent
may not be able to make wise decisions due to the lack of guidance. In this
paper, we propose a hierarchical object-to-zone (HOZ) graph to guide the agent
in a coarse-to-fine manner, and an online-learning mechanism is also proposed
to update HOZ according to the real-time observation in new environments. In
particular, the HOZ graph is composed of scene nodes, zone nodes and object
nodes. With the pre-learned HOZ graph, the real-time observation and the target
goal, the agent can constantly plan an optimal path from zone to zone. In the
estimated path, the next potential zone is regarded as sub-goal, which is also
fed into the deep reinforcement learning model for action prediction. Our
methods are evaluated on the AI2-Thor simulator. In addition to widely used
evaluation metrics SR and SPL, we also propose a new evaluation metric of SAE
that focuses on the effective action rate. Experimental results demonstrate the
effectiveness and efficiency of our proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02283">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automated Cardiac Resting Phase Detection Targeted on the Right Coronary Artery. (arXiv:2109.02342v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02342">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Purpose: Static cardiac imaging such as late gadolinium enhancement, mapping,
or 3-D coronary angiography require prior information, e.g., the phase during a
cardiac cycle with least motion, called resting phase (RP). The purpose of this
work is to propose a fully automated framework that allows the detection of the
right coronary artery (RCA) RP within CINE series. Methods: The proposed
prototype system consists of three main steps. First, the localization of the
regions of interest (ROI) is performed. Second, as CINE series are
time-resolved, the cropped ROI series over all time points are taken for
tracking motions quantitatively. Third, the output motion values are used to
classify RPs. In this work, we focused on the detection of the area with the
outer edge of the cross-section of the RCA as our target. The proposed
framework was evaluated on 102 clinically acquired dataset at 1.5T and 3T. The
automatically classified RPs were compared with the ground truth RPs annotated
manually by a medical expert for testing the robustness and feasibility of the
framework. Results: The predicted RCA RPs showed high agreement with the
experts annotated RPs with 92.7% accuracy, 90.5% sensitivity and 95.0%
specificity for the unseen study dataset. The mean absolute difference of the
start and end RP was 13.6 ${\pm}$ 18.6 ms for the validation study dataset
(n&#x3D;102). Conclusion: In this work, automated RP detection has been introduced
by the proposed framework and demonstrated feasibility, robustness, and
applicability for diverse static imaging acquisitions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GDP: Stabilized Neural Network Pruning via Gates with Differentiable Polarization. (arXiv:2109.02220v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02220">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Model compression techniques are recently gaining explosive attention for
obtaining efficient AI models for various real-time applications. Channel
pruning is one important compression strategy and is widely used in slimming
various DNNs. Previous gate-based or importance-based pruning methods aim to
remove channels whose importance is smallest. However, it remains unclear what
criteria the channel importance should be measured on, leading to various
channel selection heuristics. Some other sampling-based pruning methods deploy
sampling strategies to train sub-nets, which often causes the training
instability and the compressed model&#x27;s degraded performance. In view of the
research gaps, we present a new module named Gates with Differentiable
Polarization (GDP), inspired by principled optimization ideas. GDP can be
plugged before convolutional layers without bells and whistles, to control the
on-and-off of each channel or whole layer block. During the training process,
the polarization effect will drive a subset of gates to smoothly decrease to
exact zero, while other gates gradually stay away from zero by a large margin.
When training terminates, those zero-gated channels can be painlessly removed,
while other non-zero gates can be absorbed into the succeeding convolution
kernel, causing completely no interruption to training nor damage to the
trained model. Experiments conducted over CIFAR-10 and ImageNet datasets show
that the proposed GDP algorithm achieves the state-of-the-art performance on
various benchmark DNNs at a broad range of pruning ratios. We also apply GDP to
DeepLabV3Plus-ResNet50 on the challenging Pascal VOC segmentation task, whose
test performance sees no drop (even slightly improved) with over 60% FLOPs
saving.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis. (arXiv:2109.02081v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Deep person generation has attracted extensive research attention due to its
wide applications in virtual agents, video conferencing, online shopping and
art/movie production. With the advancement of deep learning, visual appearances
(face, pose, cloth) of a person image can be easily generated or manipulated on
demand. In this survey, we first summarize the scope of person generation, and
then systematically review recent progress and technical trends in deep person
generation, covering three major tasks: talking-head generation (face),
pose-guided person generation (pose) and garment-oriented person generation
(cloth). More than two hundred papers are covered for a thorough overview, and
the milestone works are highlighted to witness the major technical
breakthrough. Based on these fundamental tasks, a number of applications are
investigated, e.g., virtual fitting, digital human, generative data
augmentation. We hope this survey could shed some light on the future prospects
of deep person generation, and provide a helpful foundation for full
applications towards digital human.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Investigating Customization Strategies and Convergence Behaviors of Task-specific ADMM. (arXiv:1909.10819v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1909.10819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Alternating Direction Method of Multiplier (ADMM) has been a popular
algorithmic framework for separable optimization problems with linear
constraints. For numerical ADMM fail to exploit the particular structure of the
problem at hand nor the input data information, leveraging task-specific
modules (e.g., neural networks and other data-driven architectures) to extend
ADMM is a significant but challenging task. This work focuses on designing a
flexible algorithmic framework to incorporate various task-specific modules
(with no additional constraints) to improve the performance of ADMM in
real-world applications. Specifically, we propose Guidance from Optimality
(GO), a new customization strategy, to embed task-specific modules into ADMM
(GO-ADMM). By introducing an optimality-based criterion to guide the
propagation, GO-ADMM establishes an updating scheme agnostic to the choice of
additional modules. The existing task-specific methods just plug their
task-specific modules into the numerical iterations in a straightforward
manner. Even with some restrictive constraints on the plug-in modules, they can
only obtain some relatively weaker convergence properties for the resulted ADMM
iterations. Fortunately, without any restrictions on the embedded modules, we
prove the convergence of GO-ADMM regarding objective values and constraint
violations, and derive the worst-case convergence rate measured by iteration
complexity. Extensive experiments are conducted to verify the theoretical
results and demonstrate the efficiency of GO-ADMM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
spatial-temporal representation of a dynamic scene from a set of RGB images.
Key to our approach is the use of a neural implicit representation that learns
to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
consistency across different modalities, our representation enables multi-view
rendering in diverse dynamic scenes, including water pouring, robotic
interaction, and real images, outperforming state-of-the-art methods for
spatial-temporal view synthesis. Our approach works even when inputs images are
captured with only one camera. We further demonstrate that the learned
representation can serve as an implicit scene prior, enabling video processing
tasks such as image super-resolution and de-noising without any additional
supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Fine-Grained Motion Embedding for Landscape Animation. (arXiv:2109.02216v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02216">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we focus on landscape animation, which aims to generate
time-lapse videos from a single landscape image. Motion is crucial for
landscape animation as it determines how objects move in videos. Existing
methods are able to generate appealing videos by learning motion from real
time-lapse videos. However, current methods suffer from inaccurate motion
generation, which leads to unrealistic video results. To tackle this problem,
we propose a model named FGLA to generate high-quality and realistic videos by
learning Fine-Grained motion embedding for Landscape Animation. Our model
consists of two parts: (1) a motion encoder which embeds time-lapse motion in a
fine-grained way. (2) a motion generator which generates realistic motion to
animate input images. To train and evaluate on diverse time-lapse videos, we
build the largest high-resolution Time-lapse video dataset with Diverse scenes,
namely Time-lapse-D, which includes 16,874 video clips with over 10 million
frames. Quantitative and qualitative experimental results demonstrate the
superiority of our method. In particular, our method achieves relative
improvements by 19% on LIPIS and 5.6% on FVD compared with state-of-the-art
methods on our dataset. A user study carried out with 700 human subjects shows
that our approach visually outperforms existing methods by a large margin.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dissecting Image Crops. (arXiv:2011.11831v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.11831">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The elementary operation of cropping underpins nearly every computer vision
system, ranging from data augmentation and translation invariance to
computational photography and representation learning. This paper investigates
the subtle traces introduced by this operation. For example, despite
refinements to camera optics, lenses will leave behind certain clues, notably
chromatic aberration and vignetting. Photographers also leave behind other
clues relating to image aesthetics and scene composition. We study how to
detect these traces, and investigate the impact that cropping has on the image
distribution. While our aim is to dissect the fundamental impact of spatial
crops, there are also a number of practical implications to our work, such as
revealing faulty photojournalism and equipping neural network researchers with
a better understanding of shortcut learning. Code is available at
https://github.com/basilevh/dissecting-image-crops.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Mixture of Experts (MoE) with sparse conditional computation has been proved
an effective architecture for scaling attention-based models to more parameters
with comparable computation cost. In this paper, we propose Sparse-MLP, scaling
the recent MLP-Mixer model with sparse MoE layers, to achieve a more
computation-efficient architecture. We replace a subset of dense MLP blocks in
the MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two
stages of MoE layers: one with MLP experts mixing information within channels
along image patch dimension, one with MLP experts mixing information within
patches along the channel dimension. Besides, to reduce computational cost in
routing and improve experts capacity, we design Re-represent layers in each
Sparse block. These layers are to re-scale image representations by two simple
but effective linear transformations. By pre-training on ImageNet-1k with MoCo
v3 algorithm, our models can outperform dense MLP models with comparable
parameters and less computational cost on several downstream image
classification tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1710.03113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multidiversified ensemble clustering approach. In particular, we create a
large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Square Root Marginalization for Sliding-Window Bundle Adjustment. (arXiv:2109.02182v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02182">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper we propose a novel square root sliding-window bundle adjustment
suitable for real-time odometry applications. The square root formulation
pervades three major aspects of our optimization-based sliding-window
estimator: for bundle adjustment we eliminate landmark variables with nullspace
projection; to store the marginalization prior we employ a matrix square root
of the Hessian; and when marginalizing old poses we avoid forming normal
equations and update the square root prior directly with a specialized QR
decomposition. We show that the proposed square root marginalization is
algebraically equivalent to the conventional use of Schur complement (SC) on
the Hessian. Moreover, it elegantly deals with rank-deficient Jacobians
producing a prior equivalent to SC with Moore-Penrose inverse. Our evaluation
of visual and visual-inertial odometry on real-world datasets demonstrates that
the proposed estimator is 36% faster than the baseline. It furthermore shows
that in single precision, conventional Hessian-based marginalization leads to
numeric failures and reduced accuracy. We analyse numeric properties of the
marginalization prior to explain why our square root form does not suffer from
the same effect and therefore entails superior performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigational Path-Planning For All-Terrain Autonomous Agricultural Robot. (arXiv:2109.02015v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02015">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The shortage of workforce and increasing cost of maintenance has forced many
farm industrialists to shift towards automated and mechanized approaches. The
key component for autonomous systems is the path planning techniques used.
Coverage path planning (CPP) algorithm is used for navigating over farmlands to
perform various agricultural operations such as seeding, ploughing, or spraying
pesticides and fertilizers. This report paper compares novel algorithms for
autonomous navigation of farmlands. For reduction of navigational constraints,
a high-resolution grid map representation is taken into consideration specific
to Indian environments. The free space is covered by distinguishing the grid
cells as covered, unexplored, partially explored and presence of an obstacle.
The performance of the compared algorithms is evaluated with metrics such as
time efficiency, space efficiency, accuracy, and robustness to changes in the
environment. Robotic Operating System (ROS), Dassault Systemes Experience
Platform (3DS Experience), MATLAB along Python were used for the simulation of
the compared algorithms. The results proved the applicability of the algorithms
for autonomous field navigation and feasibility with robotic path planning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stimuli-Aware Visual Emotion Analysis. (arXiv:2109.01812v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01812">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Visual emotion analysis (VEA) has attracted great attention recently, due to
the increasing tendency of expressing and understanding emotions through images
on social networks. Different from traditional vision tasks, VEA is inherently
more challenging since it involves a much higher level of complexity and
ambiguity in human cognitive process. Most of the existing methods adopt deep
learning techniques to extract general features from the whole image,
disregarding the specific features evoked by various emotional stimuli.
Inspired by the \textit{Stimuli-Organism-Response (S-O-R)} emotion model in
psychological theory, we proposed a stimuli-aware VEA method consisting of
three stages, namely stimuli selection (S), feature extraction (O) and emotion
prediction (R). First, specific emotional stimuli (i.e., color, object, face)
are selected from images by employing the off-the-shelf tools. To the best of
our knowledge, it is the first time to introduce stimuli selection process into
VEA in an end-to-end network. Then, we design three specific networks, i.e.,
Global-Net, Semantic-Net and Expression-Net, to extract distinct emotional
features from different stimuli simultaneously. Finally, benefiting from the
inherent structure of Mikel&#x27;s wheel, we design a novel hierarchical
cross-entropy loss to distinguish hard false examples from easy ones in an
emotion-specific manner. Experiments demonstrate that the proposed method
consistently outperforms the state-of-the-art approaches on four public visual
emotion datasets. Ablation study and visualizations further prove the validity
and interpretability of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01668">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The recent achievements of Deep Learning rely on the test data being similar
in distribution to the training data. In an ideal case, Deep Learning models
would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make
predictions on out-of-distribution data. Yet in practice, models usually fail
to generalize well when facing a shift in distribution. Several methods were
thereby designed to improve the robustness of the features learned by a model
through Regularization- or Domain-Prediction-based schemes. Segmenting medical
images such as MRIs of the hippocampus is essential for the diagnosis and
treatment of neuropsychiatric disorders. But these brain images often suffer
from distribution shift due to the patient&#x27;s age and various pathologies
affecting the shape of the organ. In this work, we evaluate OoD Generalization
solutions for the problem of hippocampus segmentation in MR data using both
fully- and semi-supervised training. We find that no method performs reliably
in all experiments. Only the V-REx loss stands out as it remains easy to tune,
while it outperforms a standard U-Net in most cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recognition of COVID-19 Disease Utilizing X-Ray Imaging of the Chest Using CNN. (arXiv:2109.02103v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02103">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Since this COVID-19 pandemic thrives, the utilization of X-Ray images of the
Chest (CXR) as a complementary screening technique to RT-PCR testing grows to
its clinical use for respiratory complaints. Many new deep learning approaches
have developed as a consequence. The goal of this research is to assess the
convolutional neural networks (CNNs) to diagnosis COVID-19 utisizing X-ray
images of chest. The performance of CNN with one, three, and four convolution
layers has been evaluated in this research. A dataset of 13,808 CXR photographs
are used in this research. When evaluated on X-ray images with three splits of
the dataset, our preliminary experimental results show that the CNN model with
three convolution layers can reliably detect with 96 percent accuracy
(precision being 96 percent). This fact indicates the commitment of our
suggested model for reliable screening of COVID-19.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">F3S: Free Flow Fever Screening. (arXiv:2109.01733v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identification of people with elevated body temperature can reduce or
dramatically slow down the spread of infectious diseases like COVID-19. We
present a novel fever-screening system, F3S, that uses edge machine learning
techniques to accurately measure core body temperatures of multiple individuals
in a free-flow setting. F3S performs real-time sensor fusion of visual camera
with thermal camera data streams to detect elevated body temperature, and it
has several unique features: (a) visual and thermal streams represent very
different modalities, and we dynamically associate semantically-equivalent
regions across visual and thermal frames by using a new, dynamic alignment
technique that analyzes content and context in real-time, (b) we track people
through occlusions, identify the eye (inner canthus), forehead, face and head
regions where possible, and provide an accurate temperature reading by using a
prioritized refinement algorithm, and (c) we robustly detect elevated body
temperature even in the presence of personal protective equipment like masks,
or sunglasses or hats, all of which can be affected by hot weather and lead to
spurious temperature readings. F3S has been deployed at over a dozen large
commercial establishments, providing contact-less, free-flow, real-time fever
screening for thousands of employees and customers in indoors and outdoor
settings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Dual Transfer Learning for Event-based End-task Prediction via Pluggable Event to Image Translation. (arXiv:2109.01801v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01801">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Event cameras are novel sensors that perceive the per-pixel intensity changes
and output asynchronous event streams with high dynamic range and less motion
blur. It has been shown that events alone can be used for end-task learning,
\eg, semantic segmentation, based on encoder-decoder-like networks. However, as
events are sparse and mostly reflect edge information, it is difficult to
recover original details merely relying on the decoder. Moreover, most methods
resort to pixel-wise loss alone for supervision, which might be insufficient to
fully exploit the visual details from sparse events, thus leading to less
optimal performance. In this paper, we propose a simple yet flexible two-stream
framework named Dual Transfer Learning (DTL) to effectively enhance the
performance on the end-tasks without adding extra inference cost. The proposed
approach consists of three parts: event to end-task learning (EEL) branch,
event to image translation (EIT) branch, and transfer learning (TL) module that
simultaneously explores the feature-level affinity information and pixel-level
knowledge from the EIT branch to improve the EEL branch. This simple yet novel
method leads to strong representation learning from events and is evidenced by
the significant performance boost on the end-tasks such as semantic
segmentation and depth estimation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sleep stage classification is essential for sleep assessment and disease
diagnosis. Although previous attempts to classify sleep stages have achieved
high classification performance, several challenges remain open: 1) How to
effectively utilize time-varying spatial and temporal features from
multi-channel brain signals remains challenging. Prior works have not been able
to fully utilize the spatial topological information among brain regions. 2)
Due to the many differences found in individual biological signals, how to
overcome the differences of subjects and improve the generalization of deep
neural networks is important. 3) Most deep learning methods ignore the
interpretability of the model to the brain. To address the above challenges, we
propose a multi-view spatial-temporal graph convolutional networks (MSTGCN)
with domain generalization for sleep stage classification. Specifically, we
construct two brain view graphs for MSTGCN based on the functional connectivity
and physical distance proximity of the brain regions. The MSTGCN consists of
graph convolutions for extracting spatial features and temporal convolutions
for capturing the transition rules among sleep stages. In addition, attention
mechanism is employed for capturing the most relevant spatial-temporal
information for sleep stage classification. Finally, domain generalization and
MSTGCN are integrated into a unified framework to extract subject-invariant
sleep features. Experiments on two public datasets demonstrate that the
proposed model outperforms the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Accurate Alignment in Real-time 3D Hand-Mesh Reconstruction. (arXiv:2109.01723v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D hand-mesh reconstruction from RGB images facilitates many applications,
including augmented reality (AR). However, this requires not only real-time
speed and accurate hand pose and shape but also plausible mesh-image alignment.
While existing works already achieve promising results, meeting all three
requirements is very challenging. This paper presents a novel pipeline by
decoupling the hand-mesh reconstruction task into three stages: a joint stage
to predict hand joints and segmentation; a mesh stage to predict a rough hand
mesh; and a refine stage to fine-tune it with an offset mesh for mesh-image
alignment. With careful design in the network structure and in the loss
functions, we can promote high-quality finger-level mesh-image alignment and
drive the models together to deliver real-time predictions. Extensive
quantitative and qualitative results on benchmark datasets demonstrate that the
quality of our results outperforms the state-of-the-art methods on
hand-mesh/pose precision and hand-image alignment. In the end, we also showcase
several real-time AR scenarios.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spatio-temporal-spectral-angular observation model that integrates observations from UAV and mobile mapping vehicle for better urban mapping. (arXiv:2109.00900v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In a complex urban scene, observation from a single sensor unavoidably leads
to voids in observations, failing to describe urban objects in a comprehensive
manner. In this paper, we propose a spatio-temporal-spectral-angular
observation model to integrate observations from UAV and mobile mapping vehicle
platform, realizing a joint, coordinated observation operation from both air
and ground. We develop a multi-source remote sensing data acquisition system to
effectively acquire multi-angle data of complex urban scenes. Multi-source data
fusion solves the missing data problem caused by occlusion and achieves
accurate, rapid, and complete collection of holographic spatial and temporal
information in complex urban scenes. We carried out an experiment on Baisha
Town, Chongqing, China and obtained multi-sensor, multi-angle data from UAV and
mobile mapping vehicle. We first extracted the point cloud from UAV and then
integrated the UAV and mobile mapping vehicle point cloud. The integrated
results combined both the characteristic of UAV and mobile mapping vehicle
point cloud, confirming the practicability of the proposed joint data
acquisition platform and the effectiveness of spatio-temporal-spectral-angular
observation model. Compared with the observation from UAV or mobile mapping
vehicle alone, the integrated system provides an effective data acquisition
solution towards comprehensive urban monitoring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Seam Carving Detection and Localization using Two-Stage Deep Neural Networks. (arXiv:2109.01764v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01764">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Seam carving is a method to resize an image in a content aware fashion.
However, this method can also be used to carve out objects from images. In this
paper, we propose a two-step method to detect and localize seam carved images.
First, we build a detector to detect small patches in an image that has been
seam carved. Next, we compute a heatmap on an image based on the patch
detector&#x27;s output. Using these heatmaps, we build another detector to detect if
a whole image is seam carved or not. Our experimental results show that our
approach is effective in detecting and localizing seam carved images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01844">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Catastrophic forgetting of previously learned knowledge while learning new
tasks is a widely observed limitation of contemporary neural networks. Although
many continual learning methods are proposed to mitigate this drawback, the
main question remains unanswered: what is the root cause of catastrophic
forgetting? In this work, we aim at answering this question by posing and
validating a set of research hypotheses related to the specificity of
representations built internally by neural models. More specifically, we design
a set of empirical evaluations that compare the robustness of representations
in discriminative and generative models against catastrophic forgetting. We
observe that representations learned by discriminative models are more prone to
catastrophic forgetting than their generative counterparts, which sheds new
light on the advantages of developing generative models for continual learning.
Finally, our work opens new research pathways and possibilities to adopt
generative models in continual learning beyond mere replay mechanisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">PR-Net: Preference Reasoning for Personalized Video Highlight Detection. (arXiv:2109.01799v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01799">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Personalized video highlight detection aims to shorten a long video to
interesting moments according to a user&#x27;s preference, which has recently raised
the community&#x27;s attention. Current methods regard the user&#x27;s history as
holistic information to predict the user&#x27;s preference but negating the inherent
diversity of the user&#x27;s interests, resulting in vague preference
representation. In this paper, we propose a simple yet efficient preference
reasoning framework (PR-Net) to explicitly take the diverse interests into
account for frame-level highlight prediction. Specifically, distinct
user-specific preferences for each input query frame are produced, presented as
the similarity weighted sum of history highlights to the corresponding query
frame. Next, distinct comprehensive preferences are formed by the user-specific
preferences and a learnable generic preference for more overall highlight
measurement. Lastly, the degree of highlight and non-highlight for each query
frame is calculated as semantic similarity to its comprehensive and
non-highlight preferences, respectively. Besides, to alleviate the ambiguity
due to the incomplete annotation, a new bi-directional contrastive loss is
proposed to ensure a compact and differentiable metric space. In this way, our
method significantly outperforms state-of-the-art methods with a relative
improvement of 12% in mean accuracy precision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards disease-aware image editing of chest X-rays. (arXiv:2109.01071v2 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01071">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Disease-aware image editing by means of generative adversarial networks
(GANs) constitutes a promising avenue for advancing the use of AI in the
healthcare sector. Here, we present a proof of concept of this idea. While
GAN-based techniques have been successful in generating and manipulating
natural images, their application to the medical domain, however, is still in
its infancy. Working with the CheXpert data set, we show that StyleGAN can be
trained to generate realistic chest X-rays. Inspired by the Cyclic Reverse
Generator (CRG) framework, we train an encoder that allows for faithfully
inverting the generator on synthetic X-rays and provides organ-level
reconstructions of real ones. Employing a guided manipulation of latent codes,
we confer the medical condition of cardiomegaly (increased heart size) onto
real X-rays from healthy patients. This work was presented in the Medical
Imaging meets Neurips Workshop 2020, which was held as part of the 34th
Conference on Neural Information Processing Systems (NeurIPS 2020) in
Vancouver, Canada</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Personalized Image Semantic Segmentation. (arXiv:2107.13978v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation models trained on public datasets have achieved great
success in recent years. However, these models didn&#x27;t consider the
personalization issue of segmentation though it is important in practice. In
this paper, we address the problem of personalized image segmentation. The
objective is to generate more accurate segmentation results on unlabeled
personalized images by investigating the data&#x27;s personalized traits. To open up
future research in this area, we collect a large dataset containing various
users&#x27; personalized images called PIS (Personalized Image Semantic
Segmentation). We also survey some recent researches related to this problem
and report their performance on our dataset. Furthermore, by observing the
correlation among a user&#x27;s personalized images, we propose a baseline method
that incorporates the inter-image context when segmenting certain images.
Extensive experiments show that our method outperforms the existing methods on
the proposed dataset. The code and the PIS dataset will be made publicly
available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02288">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging
task. The recordings are mostly sparse and contain only small objects. In this
work, we propose a simple tiling method that improves the detection capability
in the remote sensing case without modifying the model itself. By reducing the
background bias and enabling the usage of higher image resolutions during
training, our method can improve the performance of models substantially. The
procedure was validated on three different data sets and outperformed similar
approaches in performance and speed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02093">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We unveil a long-standing problem in the prevailing co-saliency detection
systems: there is indeed inconsistency between training and testing.
Constructing a high-quality co-saliency detection dataset involves
time-consuming and labor-intensive pixel-level labeling, which has forced most
recent works to rely instead on semantic segmentation or saliency detection
datasets for training. However, the lack of proper co-saliency and the absence
of multiple foreground objects in these datasets can lead to spurious
variations and inherent biases learned by models. To tackle this, we introduce
the idea of counterfactual training through context adjustment, and propose a
&quot;cost-free&quot; group-cut-paste (GCP) procedure to leverage images from
off-the-shelf saliency detection datasets and synthesize new samples. Following
GCP, we collect a novel dataset called Context Adjustment Training (CAT). CAT
consists of 33,500 images, making it four times larger than the current
co-saliency detection datasets. All images are automatically annotated with
high-quality mask annotations, object categories, and edge maps. Extensive
experiments with state-of-the-art models are conducted to demonstrate the
superiority of our dataset. We hope that the scale, diversity, and quality of
our dataset can benefit researchers in this area and beyond. The dataset and
benchmark toolkit will be publicly accessible through our project page.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Generative Models Improve Radiomics Performance in Different Tasks and Different Datasets: An Experimental Study. (arXiv:2109.02252v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02252">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Radiomics is an active area of research focusing on high throughput feature
extraction from medical images with a wide array of applications in clinical
practice, such as clinical decision support in oncology. However, noise in low
dose computed tomography (CT) scans can impair the accurate extraction of
radiomic features. In this article, we investigate the possibility of using
deep learning generative models to improve the performance of radiomics from
low dose CTs. We used two datasets of low dose CT scans -NSCLC Radiogenomics
and LIDC-IDRI - as test datasets for two tasks - pre-treatment survival
prediction and lung cancer diagnosis. We used encoder-decoder networks and
conditional generative adversarial networks (CGANs) trained in a previous study
as generative models to transform low dose CT images into full dose CT images.
Radiomic features extracted from the original and improved CT scans were used
to build two classifiers - a support vector machine (SVM) and a deep attention
based multiple instance learning model - for survival prediction and lung
cancer diagnosis respectively. Finally, we compared the performance of the
models derived from the original and improved CT scans. Encoder-decoder
networks and CGANs improved the area under the curve (AUC) of survival
prediction from 0.52 to 0.57 (p-value&lt;0.01). On the other hand, Encoder-decoder
network and CGAN can improve the AUC of lung cancer diagnosis from 0.84 to 0.88
and 0.89 respectively (p-value&lt;0.01). Moreover, there are no statistically
significant differences in improving AUC by using encoder-decoder network and
CGAN (p-value&#x3D;0.34) when networks trained at 75 and 100 epochs. Generative
models can improve the performance of low dose CT-based radiomics in different
tasks. Hence, denoising using generative models seems to be a necessary
pre-processing step for calculating radiomic features from low dose CTs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging
method providing multiplex molecular and functional information from the rodent
brain. It can be greatly augmented by magnetic resonance imaging (MRI) that
offers excellent soft-tissue contrast and high-resolution brain anatomy.
Nevertheless, registration of multi-modal images remains challenging, chiefly
due to the entirely different image contrast rendered by these modalities.
Previously reported registration algorithms mostly relied on manual
user-dependent brain segmentation, which compromised data interpretation and
accurate quantification. Here we propose a fully automated registration method
for MSOT-MRI multimodal imaging empowered by deep learning. The automated
workflow includes neural network-based image segmentation to generate suitable
masks, which are subsequently registered using an additional neural network.
Performance of the algorithm is showcased with datasets acquired by
cross-sectional MSOT and high-field MRI preclinical scanners. The automated
registration method is further validated with manual and half-automated
registration, demonstrating its robustness and accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v3 [eess.IV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.02711">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Dynamic imaging is a beneficial tool for interventions to assess
physiological changes. Nonetheless during dynamic MRI, while achieving a high
temporal resolution, the spatial resolution is compromised. To overcome this
spatio-temporal trade-off, this research presents a super-resolution (SR) MRI
reconstruction with prior knowledge based fine-tuning to maximise spatial
information while reducing the required scan-time for dynamic MRIs. An U-Net
based network with perceptual loss is trained on a benchmark dataset and
fine-tuned using one subject-specific static high resolution MRI as prior
knowledge to obtain high resolution dynamic images during the inference stage.
3D dynamic data for three subjects were acquired with different parameters to
test the generalisation capabilities of the network. The method was tested for
different levels of in-plane undersampling for dynamic MRI. The reconstructed
dynamic SR results after fine-tuning showed higher similarity with the high
resolution ground-truth, while quantitatively achieving statistically
significant improvement. The average SSIM of the lowest resolution experimented
during this research (6.25~\% of the k-space) before and after fine-tuning were
0.939 $\pm$ 0.008 and 0.957 $\pm$ 0.006 respectively. This could theoretically
result in an acceleration factor of 16, which can potentially be acquired in
less than half a second. The proposed approach shows that the super-resolution
MRI reconstruction with prior-information can alleviate the spatio-temporal
trade-off in dynamic MRI, even for high acceleration factors.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation is a classic computer vision task with multiple
applications, which includes medical and remote sensing image analysis. Despite
recent advances with deep-based approaches, labeling samples (pixels) for
training models is laborious and, in some cases, unfeasible. In this paper, we
present two novel meta learning methods, named WeaSeL and ProtoSeg, for the
few-shot semantic segmentation task with sparse annotations. We conducted
extensive evaluation of the proposed methods in different applications (12
datasets) in medical imaging and agricultural remote sensing, which are very
distinct fields of knowledge and usually subject to data scarcity. The results
demonstrated the potential of our method, achieving suitable results for
segmenting both coffee/orange crops and anatomical parts of the human body in
comparison with full dense annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Ranking Models in Unlabeled New Environments. (arXiv:2108.10310v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10310">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Consider a scenario where we are supplied with a number of ready-to-use
models trained on a certain source domain and hope to directly apply the most
appropriate ones to different target domains based on the models&#x27; relative
performance. Ideally we should annotate a validation set for model performance
assessment on each new target environment, but such annotations are often very
expensive. Under this circumstance, we introduce the problem of ranking models
in unlabeled new environments. For this problem, we propose to adopt a proxy
dataset that 1) is fully labeled and 2) well reflects the true model rankings
in a given target environment, and use the performance rankings on the proxy
sets as surrogates. We first select labeled datasets as the proxy.
Specifically, datasets that are more similar to the unlabeled target domain are
found to better preserve the relative performance rankings. Motivated by this,
we further propose to search the proxy set by sampling images from various
datasets that have similar distributions as the target. We analyze the problem
and its solutions on the person re-identification (re-ID) task, for which
sufficient datasets are publicly available, and show that a carefully
constructed proxy set effectively captures relative performance ranking in new
environments. Code is available at \url{https://github.com/sxzrt/Proxy-Set}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ISyNet: Convolutional Neural Networks design for AI accelerator. (arXiv:2109.01932v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01932">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years Deep Learning reached significant results in many practical
problems, such as computer vision, natural language processing, speech
recognition and many others. For many years the main goal of the research was
to improve the quality of models, even if the complexity was impractically
high. However, for the production solutions, which often require real-time
work, the latency of the model plays a very important role. Current
state-of-the-art architectures are found with neural architecture search (NAS)
taking model complexity into account. However, designing of the search space
suitable for specific hardware is still a challenging task. To address this
problem we propose a measure of hardware efficiency of neural architecture
search space - matrix efficiency measure (MEM); a search space comprising of
hardware-efficient operations; a latency-aware scaling method; and ISyNet - a
set of architectures designed to be fast on the specialized neural processing
unit (NPU) hardware and accurate at the same time. We show the advantage of the
designed architectures for the NPU devices on ImageNet and the generalization
ability for the downstream classification and detection tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sensor Data Augmentation with Resampling for Contrastive Learning in Human Activity Recognition. (arXiv:2109.02054v1 [cs.HC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02054">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Human activity recognition plays an increasingly important role not only in
our daily lives, but also in the medical and rehabilitation fields. The
development of deep learning has also contributed to the advancement of human
activity recognition, but the large amount of data annotation work required to
train deep learning models is a major obstacle to the development of human
activity recognition. Contrastive learning has started to be used in the field
of sensor-based human activity recognition due to its ability to avoid the cost
of labeling large datasets and its ability to better distinguish between sample
representations of different instances. Among them, data augmentation, an
important part of contrast learning, has a significant impact on model
effectiveness, but current data augmentation methods do not perform too
successfully in contrast learning frameworks for wearable sensor-based activity
recognition. To optimize the effect of contrast learning models, in this paper,
we investigate the sampling frequency of sensors and propose a resampling data
augmentation method. In addition, we also propose a contrast learning framework
based on human activity recognition and apply the resampling augmentation
method to the data augmentation phase of contrast learning. The experimental
results show that the resampling augmentation method outperforms supervised
learning by 9.88% on UCI HAR and 7.69% on Motion Sensor in the fine-tuning
evaluation of contrast learning with a small amount of labeled data, and also
reveal that not all data augmentation methods will have positive effects in the
contrast learning framework. Finally, we explored the influence of the
combination of different augmentation methods on contrastive learning, and the
experimental results showed that the effect of most combination augmentation
methods was better than that of single augmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.06911">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper considers online object-level mapping using partial point-cloud
observations obtained online in an unknown environment. We develop and approach
for fully Convolutional Object Retrieval and Symmetry-AIded Registration
(CORSAIR). Our model extends the Fully Convolutional Geometric Features model
to learn a global object-shape embedding in addition to local point-wise
features from the point-cloud observations. The global feature is used to
retrieve a similar object from a category database, and the local features are
used for robust pose registration between the observed and the retrieved
object. Our formulation also leverages symmetries, present in the object
shapes, to obtain promising local-feature pairs from different symmetry classes
for matching. We present results from synthetic and real-world datasets with
different object categories to verify the robustness of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Attentive Deep Neural Network for Exposing GAN-generated Faces. (arXiv:2109.02167v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02167">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>GAN-based techniques that generate and synthesize realistic faces have caused
severe social concerns and security problems. Existing methods for detecting
GAN-generated faces can perform well on limited public datasets. However,
images from existing public datasets do not represent real-world scenarios well
enough in terms of view variations and data distributions (where real faces
largely outnumber synthetic faces). The state-of-the-art methods do not
generalize well in real-world problems and lack the interpretability of
detection results. Performance of existing GAN-face detection models degrades
significantly when facing imbalanced data distributions. To address these
shortcomings, we propose a robust, attentive, end-to-end network that can spot
GAN-generated faces by analyzing their eye inconsistencies. Specifically, our
model learns to identify inconsistent eye components by localizing and
comparing the iris artifacts between the two eyes automatically. Our deep
network addresses the imbalance learning issues by considering the AUC loss and
the traditional cross-entropy loss jointly. Comprehensive evaluations of the
FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the
superiority of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SHAD3S: A model to Sketch, Shade and Shadow. (arXiv:2011.06822v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hatching is a common method used by artists to accentuate the third dimension
of a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete
with a human at hatching generic three-dimensional (3D) shapes, and also tries
to assist her in a form exploration exercise. The novelty of our approach lies
in the fact that we make no assumptions about the input other than that it
represents a 3D shape, and yet, given a contextual information of illumination
and texture, we synthesise an accurate hatch pattern over the sketch, without
access to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet
effective method to synthesise a sufficiently large high fidelity dataset,
pertinent to task; b) creating a pipeline with conditional generative
adversarial network (CGAN); and c) creating an interactive utility with GIMP,
that is a tool for artists to engage with automated hatching or a
form-exploration exercise. User evaluation of the tool suggests that the model
performance does generalise satisfactorily over diverse input, both in terms of
style as well as shape. A simple comparison of inception scores suggest that
the generated distribution is as diverse as the ground truth.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Clustering Networks for Self-supervised Learning from Unlabeled Videos. (arXiv:2104.12671v3 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12671">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multimodal self-supervised learning is getting more and more attention as it
allows not only to train large networks without human supervision but also to
search and retrieve data across various modalities. In this context, this paper
proposes a self-supervised training framework that learns a common multimodal
embedding space that, in addition to sharing representations across different
modalities, enforces a grouping of semantically similar instances. To this end,
we extend the concept of instance-level contrastive learning with a multimodal
clustering step in the training pipeline to capture semantic similarities
across modalities. The resulting embedding space enables retrieval of samples
across all modalities, even from unseen datasets and different domains. To
evaluate our approach, we train our model on the HowTo100M dataset and evaluate
its zero-shot retrieval capabilities in two challenging domains, namely
text-to-video retrieval, and temporal action localization, showing
state-of-the-art results on four different datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Privacy-Preserving Image Retrieval Scheme Using A Codebook Generated From Independent Plain-Image Dataset. (arXiv:2109.01841v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01841">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a privacy-preserving image-retrieval scheme using a
codebook generated by using a plain-image dataset. Encryption-then-compression
(EtC) images, which were proposed for EtC systems, have been used in
conventional privacy-preserving image-retrieval schemes, in which a codebook is
generated from EtC images uploaded by image owners, and extended SIMPLE
descriptors are then calculated as image descriptors by using the codebook. In
contrast, in the proposed scheme, a codebook is generated from a dataset
independent of uploaded images. The use of an independent dataset enables us
not only to use a codebook that does not require recalculation but also to
constantly provide a high retrieval accuracy. In an experiment, the proposed
scheme is demonstrated to maintain a high retrieval performance, even if
codebooks are generated from a plain image dataset independent of image owners&#x27;
encrypted images.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Image recognition via Vietoris-Rips complex. (arXiv:2109.02231v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02231">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Extracting informative features from images has been of capital importance in
computer vision. In this paper, we propose a way to extract such features from
images by a method based on algebraic topology. To that end, we construct a
weighted graph from an image, which extracts local information of an image. By
considering this weighted graph as a pseudo-metric space, we construct a
Vietoris-Rips complex with a parameter $\varepsilon$ by a well-known process of
algebraic topology. We can extract information of complexity of the image and
can detect a sub-image with a relatively high concentration of information from
this Vietoris-Rips complex. The parameter $\varepsilon$ of the Vietoris-Rips
complex produces robustness to noise. We empirically show that the extracted
feature captures well images&#x27; characteristics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Realistic Single-View 3D Object Reconstructionwith Unsupervised Learning from Multiple Images. (arXiv:2109.02288v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02288">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recovering the 3D structure of an object from a single image is a challenging
task due to its ill-posed nature. One approach is to utilize the plentiful
photos of the same object category to learn a strong 3D shape prior for the
object. This approach has successfully been demonstrated by a recent work of Wu
et al. (2020), which obtained impressive 3D reconstruction networks with
unsupervised learning. However, their algorithm is only applicable to symmetric
objects. In this paper, we eliminate the symmetry requirement with a novel
unsupervised algorithm that can learn a 3D reconstruction network from a
multi-image dataset. Our algorithm is more general and covers the
symmetry-required scenario as a special case. Besides, we employ a novel albedo
loss that improves the reconstructed details and realisticity. Our method
surpasses the previous work in both quality and robustness, as shown in
experiments on datasets of various structures, including single-view,
multi-view, image-collection, and video sets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse Spatial Attention Network for Semantic Segmentation. (arXiv:2109.01915v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01915">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The spatial attention mechanism captures long-range dependencies by
aggregating global contextual information to each query location, which is
beneficial for semantic segmentation. In this paper, we present a sparse
spatial attention network (SSANet) to improve the efficiency of the spatial
attention mechanism without sacrificing the performance. Specifically, a sparse
non-local (SNL) block is proposed to sample a subset of key and value elements
for each query element to capture long-range relations adaptively and generate
a sparse affinity matrix to aggregate contextual information efficiently.
Experimental results show that the proposed approach outperforms other context
aggregation methods and achieves state-of-the-art performance on the
Cityscapes, PASCAL Context and ADE20K datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parsing Table Structures in the Wild. (arXiv:2109.02199v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02199">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper tackles the problem of table structure parsing (TSP) from images
in the wild. In contrast to existing studies that mainly focus on parsing
well-aligned tabular images with simple layouts from scanned PDF documents, we
aim to establish a practical table structure parsing system for real-world
scenarios where tabular input images are taken or scanned with severe
deformation, bending or occlusions. For designing such a system, we propose an
approach named Cycle-CenterNet on the top of CenterNet with a novel
cycle-pairing module to simultaneously detect and group tabular cells into
structured tables. In the cycle-pairing module, a new pairing loss function is
proposed for the network training. Alongside with our Cycle-CenterNet, we also
present a large-scale dataset, named Wired Table in the Wild (WTW), which
includes well-annotated structure parsing of multiple style tables in several
scenes like the photo, scanning files, web pages, \emph{etc.}. In experiments,
we demonstrate that our Cycle-CenterNet consistently achieves the best accuracy
of table structure parsing on the new WTW dataset by 24.6\% absolute
improvement evaluated by the TEDS metric. A more comprehensive experimental
analysis also validates the advantages of our proposed methods for the TSP
task.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02357">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In practice, and more especially when training deep neural networks, visual
recognition rules are often learned based on various sources of information. On
the other hand, the recent deployment of facial recognition systems with uneven
predictive performances on different population segments highlights the
representativeness issues possibly induced by a naive aggregation of image
datasets. Indeed, sampling bias does not vanish simply by considering larger
datasets, and ignoring its impact may completely jeopardize the generalization
capacity of the learned prediction rules. In this paper, we show how biasing
models, originally introduced for nonparametric estimation in (Gill et al.,
1988), and recently revisited from the perspective of statistical learning
theory in (Laforgue and Cl\&#x27;emen\c{c}on, 2019), can be applied to remedy these
problems in the context of visual recognition. Based on the (approximate)
knowledge of the biasing mechanisms at work, our approach consists in
reweighting the observations, so as to form a nearly debiased estimator of the
target distribution. One key condition for our method to be theoretically valid
is that the supports of the distributions generating the biased datasets at
disposal must overlap, and cover the support of the target distribution. In
order to meet this requirement in practice, we propose to use a low dimensional
image representation, shared across the image databases. Finally, we provide
numerical experiments highlighting the relevance of our approach whenever the
biasing functions are appropriately chosen.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Inpainting Transformer for Anomaly Detection. (arXiv:2104.13897v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.13897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Anomaly detection in computer vision is the task of identifying images which
deviate from a set of normal images. A common approach is to train deep
convolutional autoencoders to inpaint covered parts of an image and compare the
output with the original image. By training on anomaly-free samples only, the
model is assumed to not being able to reconstruct anomalous regions properly.
For anomaly detection by inpainting we suggest it to be beneficial to
incorporate information from potentially distant regions. In particular we pose
anomaly detection as a patch-inpainting problem and propose to solve it with a
purely self-attention based approach discarding convolutions. The proposed
Inpainting Transformer (InTra) is trained to inpaint covered patches in a large
sequence of image patches, thereby integrating information across large regions
of the input image. When training from scratch, in comparison to other methods
not using extra training data, InTra achieves results on par with the current
state-of-the-art on the MVTec AD dataset for detection and surpassing them on
segmentation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This research project investigates the application of deep learning to timbre
transfer, where the timbre of a source audio can be converted to the timbre of
a target audio with minimal loss in quality. The adopted approach combines
Variational Autoencoders with Generative Adversarial Networks to construct
meaningful representations of the source audio and produce realistic
generations of the target audio and is applied to the Flickr 8k Audio dataset
for transferring the vocal timbre between speakers and the URMP dataset for
transferring the musical timbre between instruments. Furthermore, variations of
the adopted approach are trained, and generalised performance is compared using
the metrics SSIM (Structural Similarity Index) and FAD (Frech\&#x27;et Audio
Distance). It was found that a many-to-many approach supersedes a one-to-one
approach in terms of reconstructive capabilities, and that the adoption of a
basic over a bottleneck residual block design is more suitable for enriching
content information about a latent space. It was also found that the decision
on whether cyclic loss takes on a variational autoencoder or vanilla
autoencoder approach does not have a significant impact on reconstructive and
adversarial translation aspects of the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution. (arXiv:2109.02079v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02079">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Hyperspectral image has become increasingly crucial due to its abundant
spectral information. However, It has poor spatial resolution with the
limitation of the current imaging mechanism. Nowadays, many convolutional
neural networks have been proposed for the hyperspectral image super-resolution
problem. However, convolutional neural network (CNN) based methods only
consider the local information instead of the global one with the limited
kernel size of receptive field in the convolution operation. In this paper, we
design a network based on the transformer for fusing the low-resolution
hyperspectral images and high-resolution multispectral images to obtain the
high-resolution hyperspectral images. Thanks to the representing ability of the
transformer, our approach is able to explore the intrinsic relationships of
features globally. Furthermore, considering the LR-HSIs hold the main spectral
structure, the network focuses on the spatial detail estimation releasing from
the burden of reconstructing the whole data. It reduces the mapping space of
the proposed network, which enhances the final performance. Various experiments
and quality indexes show our approach&#x27;s superiority compared with other
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose Estimation. (arXiv:2109.02303v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02303">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>3D human shape and pose estimation is the essential task for human motion
analysis, which is widely used in many 3D applications. However, existing
methods cannot simultaneously capture the relations at multiple levels,
including spatial-temporal level and human joint level. Therefore they fail to
make accurate predictions in some hard scenarios when there is cluttered
background, occlusion, or extreme pose. To this end, we propose Multi-level
Attention Encoder-Decoder Network (MAED), including a Spatial-Temporal Encoder
(STE) and a Kinematic Topology Decoder (KTD) to model multi-level attentions in
a unified framework. STE consists of a series of cascaded blocks based on
Multi-Head Self-Attention, and each block uses two parallel branches to learn
spatial and temporal attention respectively. Meanwhile, KTD aims at modeling
the joint level attention. It regards pose estimation as a top-down
hierarchical process similar to SMPL kinematic tree. With the training set of
3DPW, MAED outperforms previous state-of-the-art methods by 6.2, 7.2, and 2.4
mm of PA-MPJPE on the three widely used benchmarks 3DPW, MPI-INF-3DHP, and
Human3.6M respectively. Our code is available at
https://github.com/ziniuwan/maed.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Multi-Object Detector by Estimating Bounding Box Distribution for Input Image. (arXiv:1911.12721v4 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.12721">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In multi-object detection using neural networks, the fundamental problem is,
&quot;How should the network learn a variable number of bounding boxes in different
input images?&quot;. Previous methods train a multi-object detection network through
a procedure that directly assigns the ground truth bounding boxes to the
specific locations of the network&#x27;s output. However, this procedure makes the
training of a multi-object detection network too heuristic and complicated. In
this paper, we reformulate the multi-object detection task as a problem of
density estimation of bounding boxes. Instead of assigning each ground truth to
specific locations of network&#x27;s output, we train a network by estimating the
probability density of bounding boxes in an input image using a mixture model.
For this purpose, we propose a novel network for object detection called
Mixture Density Object Detector (MDOD), and the corresponding objective
function for the density-estimation-based training. We applied MDOD to MS COCO
dataset. Our proposed method not only deals with multi-object detection
problems in a new approach, but also improves detection performances through
MDOD. The code is available: https://github.com/yoojy31/MDOD.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Large pre-trained models such as CLIP offer consistent accuracy across a
range of data distributions when performing zero-shot inference (i.e., without
fine-tuning on a specific dataset). Although existing fine-tuning approaches
substantially improve accuracy in-distribution, they also reduce
out-of-distribution robustness. We address this tension by introducing a simple
and effective method for improving robustness: ensembling the weights of the
zero-shot and fine-tuned models. Compared to standard fine-tuning, the
resulting weight-space ensembles provide large accuracy improvements
out-of-distribution, while matching or improving in-distribution accuracy. On
ImageNet and five derived distribution shifts, weight-space ensembles improve
out-of-distribution accuracy by 2 to 10 percentage points while increasing
in-distribution accuracy by nearly 1 percentage point relative to standard
fine-tuning. These improvements come at no additional computational cost during
fine-tuning or inference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Comprehensive Approach for UAV Small Object Detection with Simulation-based Transfer Learning and Adaptive Fusion. (arXiv:2109.01800v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01800">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Precisely detection of Unmanned Aerial Vehicles(UAVs) plays a critical role
in UAV defense systems. Deep learning is widely adopted for UAV object
detection whereas researches on this topic are limited by the amount of dataset
and small scale of UAV. To tackle these problems, a novel comprehensive
approach that combines transfer learning based on simulation data and adaptive
fusion is proposed. Firstly, the open-source plugin AirSim proposed by
Microsoft is used to generate mass realistic simulation data. Secondly,
transfer learning is applied to obtain a pre-trained YOLOv5 model on the
simulated dataset and fine-tuned model on the real-world dataset. Finally, an
adaptive fusion mechanism is proposed to further improve small object detection
performance. Experiment results demonstrate the effectiveness of
simulation-based transfer learning which leads to a 2.7% performance increase
on UAV object detection. Furthermore, with transfer learning and adaptive
fusion mechanism, 7.1% improvement is achieved compared to the original YOLO v5
model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01980">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Using only a model that was trained to predict where people look at images,
and no additional training data, we can produce a range of powerful editing
effects for reducing distraction in images. Given an image and a mask
specifying the region to edit, we backpropagate through a state-of-the-art
saliency model to parameterize a differentiable editing operator, such that the
saliency within the masked region is reduced. We demonstrate several operators,
including: a recoloring operator, which learns to apply a color transform that
camouflages and blends distractors into their surroundings; a warping operator,
which warps less salient image regions to cover distractors, gradually
collapsing objects into themselves and effectively removing them (an effect
akin to inpainting); a GAN operator, which uses a semantic prior to fully
replace image regions with plausible, less salient alternatives. The resulting
effects are consistent with cognitive research on the human visual system
(e.g., since color mismatch is salient, the recoloring operator learns to
harmonize objects&#x27; colors with their surrounding to reduce their saliency),
and, importantly, are all achieved solely through the guidance of the
pretrained saliency model, with no additional supervision. We present results
on a variety of natural images and conduct a perceptual study to evaluate and
validate the changes in viewers&#x27; eye-gaze between the original images and our
edited results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Mitosis Detection Using a Cascade Mask-RCNN Approach With Domain-Specific Residual Cycle-GAN Data Augmentation. (arXiv:2109.01878v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01878">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For the MIDOG mitosis detection challenge, we created a cascade algorithm
consisting of a Mask-RCNN detector, followed by a classification ensemble
consisting of ResNet50 and DenseNet201 to refine detected mitotic candidates.
The MIDOG training data consists of 200 frames originating from four scanners,
three of which are annotated for mitotic instances with centroid annotations.
Our main algorithmic choices are as follows: first, to enhance the
generalizability of our detector and classification networks, we use a
state-of-the-art residual Cycle-GAN to transform each scanner domain to every
other scanner domain. During training, we then randomly load, for each image,
one of the four domains. In this way, our networks can learn from the fourth
non-annotated scanner domain even if we don&#x27;t have annotations for it. Second,
for training the detector network, rather than using centroid-based fixed-size
bounding boxes, we create mitosis-specific bounding boxes. We do this by
manually annotating a small selection of mitoses, training a Mask-RCNN on this
small dataset, and applying it to the rest of the data to obtain full
annotations. We trained the follow-up classification ensemble using only the
challenge-provided positive and hard-negative examples. On the preliminary test
set, the algorithm scores an F1 score of 0.7578, putting us as the second-place
team on the leaderboard.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.
correlation clustering) problem, a classical graph clustering problem widely
used in machine learning and computer vision. Our algorithm consists of three
steps executed recursively: (1) Finding conflicted cycles that correspond to
violated inequalities of the underlying multicut relaxation, (2) Performing
message passing between the edges and cycles to optimize the Lagrange
relaxation coming from the found violated cycles producing reduced costs and
(3) Contracting edges with high reduced costs through matrix-matrix
multiplications.

Our algorithm produces primal solutions and dual lower bounds that estimate
the distance to optimum. We implement our algorithm on GPUs and show resulting
one to two order-of-magnitudes improvements in execution speed without
sacrificing solution quality compared to traditional serial algorithms that run
on CPUs. We can solve very large scale benchmark problems with up to
$\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We
make our code available at https://github.com/pawelswoboda/RAMA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01732">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents a computational method of analysis that draws from
machine learning, library science, and literary studies to map the visual
layouts of multi-ethnic newspapers from the late 19th and early 20th century
United States. This work departs from prior approaches to newspapers that focus
on individual pieces of textual and visual content. Our method combines
Chronicling America&#x27;s MARC data and the Newspaper Navigator machine learning
dataset to identify the visual patterns of newspaper page layouts. By analyzing
high-dimensional visual similarity, we aim to better understand how editors
spoke and protested through the layout of their papers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Separable Attention for Multi-Contrast MR Image Super-Resolution. (arXiv:2109.01664v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01664">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Super-resolving the Magnetic Resonance (MR) image of a target contrast under
the guidance of the corresponding auxiliary contrast, which provides additional
anatomical information, is a new and effective solution for fast MR imaging.
However, current multi-contrast super-resolution (SR) methods tend to
concatenate different contrasts directly, ignoring their relationships in
different clues, \eg, in the foreground and background. In this paper, we
propose a separable attention network (comprising a foreground priority
attention and background separation attention), named SANet. Our method can
explore the foreground and background areas in the forward and reverse
directions with the help of the auxiliary contrast, enabling it to learn
clearer anatomical structures and edge information for the SR of a
target-contrast MR image. SANet provides three appealing benefits: (1) It is
the first model to explore a separable attention mechanism that uses the
auxiliary contrast to predict the foreground and background regions, diverting
more attention to refining any uncertain details between these regions and
correcting the fine areas in the reconstructed results. (2) A multi-stage
integration module is proposed to learn the response of multi-contrast fusion
at different stages, obtain the dependency between the fused features, and
improve their representation ability. (3) Extensive experiments with various
state-of-the-art multi-contrast SR methods on fastMRI and clinical \textit{in
vivo} datasets demonstrate the superiority of our model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01696">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A recent work from Bello shows that training and scaling strategies may be
more significant than model architectures for visual recognition. This short
note studies effective training and scaling strategies for video recognition
models. We propose a simple scaling strategy for 3D ResNets, in combination
with improved training strategies and minor architectural changes. The
resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0
on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained
on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on
Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated
in a self-supervised setup using contrastive learning, demonstrating improved
performance. Code is available at:
https://github.com/tensorflow/models/tree/master/official.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robotic Waste Sorter with Agile Manipulation and Quickly Trainable Detector. (arXiv:2104.01260v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01260">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Owing to human labor shortages, the automation of labor-intensive manual
waste-sorting is needed. The goal of automating waste-sorting is to replace the
human role of robust detection and agile manipulation of waste items with
robots. To achieve this, we propose three methods. First, we provide a combined
manipulation method using graspless push-and-drop and pick-and-release
manipulation. Second, we provide a robotic system that can automatically
collect object images to quickly train a deep neural-network model. Third, we
provide a method to mitigate the differences in the appearance of target
objects from two scenes: one for dataset collection and the other for waste
sorting in a recycling factory. If differences exist, the performance of a
trained waste detector may decrease. We address differences in illumination and
background by applying object scaling, histogram matching with histogram
equalization, and background synthesis to the source target-object images. Via
experiments in an indoor experimental workplace for waste-sorting, we confirm
that the proposed methods enable quick collection of the training image sets
for three classes of waste items (i.e., aluminum can, glass bottle, and plastic
bottle) and detection with higher performance than the methods that do not
consider the differences. We also confirm that the proposed method enables the
robot quickly manipulate the objects.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Virtual Temporal Samples for Recurrent Neural Networks: applied to semantic segmentation in agriculture. (arXiv:2106.10118v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.10118">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper explores the potential for performing temporal semantic
segmentation in the context of agricultural robotics without temporally
labelled data. We achieve this by proposing to generate virtual temporal
samples from labelled still images. By exploiting the relatively static scene
and assuming that the robot (camera) moves we are able to generate virtually
labelled temporal sequences with no extra annotation effort. Normally, to train
a recurrent neural network (RNN), labelled samples from a video (temporal)
sequence are required which is laborious and has stymied work in this
direction. By generating virtual temporal samples, we demonstrate that it is
possible to train a lightweight RNN to perform semantic segmentation on two
challenging agricultural datasets. Our results show that by training a temporal
semantic segmenter using virtual samples we can increase the performance by an
absolute amount of $4.6$ and $4.9$ on sweet pepper and sugar beet datasets,
respectively. This indicates that our virtual data augmentation technique is
able to accurately classify agricultural images temporally without the use of
complicated synthetic data generation techniques nor with the overhead of
labelling large amounts of temporal sequences.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering. (arXiv:2109.01847v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01847">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Implicit neural rendering techniques have shown promising results for novel
view synthesis. However, existing methods usually encode the entire scene as a
whole, which is generally not aware of the object identity and limits the
ability to the high-level editing tasks such as moving or adding furniture. In
this paper, we present a novel neural scene rendering system, which learns an
object-compositional neural radiance field and produces realistic rendering
with editing capability for a clustered and real-world scene. Specifically, we
design a novel two-pathway architecture, in which the scene branch encodes the
scene geometry and appearance, and the object branch encodes each standalone
object conditioned on learnable object activation codes. To survive the
training in heavily cluttered scenes, we propose a scene-guided training
strategy to solve the 3D space ambiguity in the occluded regions and learn
sharp boundaries for each object. Extensive experiments demonstrate that our
system not only achieves competitive performance for static scene novel-view
synthesis, but also produces realistic rendering for object-level editing.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To understand the black-box characteristics of deep networks, counterfactual
explanation that deduces not only the important features of an input space but
also how those features should be modified to classify input as a target class
has gained an increasing interest. The patterns that deep networks have learned
from a training dataset can be grasped by observing the feature variation among
various classes. However, current approaches perform the feature modification
to increase the classification probability for the target class irrespective of
the internal characteristics of deep networks. This often leads to unclear
explanations that deviate from real-world data distributions. To address this
problem, we propose a counterfactual explanation method that exploits the
statistics learned from a training dataset. Especially, we gradually construct
an explanation by iterating over masking and composition steps. The masking
step aims to select an important feature from the input data to be classified
as a target class. Meanwhile, the composition step aims to optimize the
previously selected feature by ensuring that its output score is close to the
logit space of the training data that are classified as the target class.
Experimental results show that our method produces human-friendly
interpretations on various classification datasets and verify that such
interpretations can be achieved with fewer feature modification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01111">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Scene graph is a structured representation of a scene that can clearly
express the objects, attributes, and relationships between objects in the
scene. As computer vision technology continues to develop, people are no longer
satisfied with simply detecting and recognizing objects in images; instead,
people look forward to a higher level of understanding and reasoning about
visual scenes. For example, given an image, we want to not only detect and
recognize objects in the image, but also know the relationship between objects
(visual relationship detection), and generate a text description (image
captioning) based on the image content. Alternatively, we might want the
machine to tell us what the little girl in the image is doing (Visual Question
Answering (VQA)), or even remove the dog from the image and find similar images
(image editing and retrieval), etc. These tasks require a higher level of
understanding and reasoning for image vision tasks. The scene graph is just
such a powerful tool for scene understanding. Therefore, scene graphs have
attracted the attention of a large number of researchers, and related research
is often cross-modal, complex, and rapidly developing. However, no relatively
systematic survey of scene graphs exists at present. To this end, this survey
conducts a comprehensive investigation of the current scene graph research.
More specifically, we first summarized the general definition of the scene
graph, then conducted a comprehensive and systematic discussion on the
generation method of the scene graph (SGG) and the SGG with the aid of prior
knowledge. We then investigated the main applications of scene graphs and
summarized the most commonly used datasets. Finally, we provide some insights
into the future development of scene graphs. We believe this will be a very
helpful foundation for future research on scene graphs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we introduce pixel wise tensor normalization, which is inserted
after rectifier linear units and, together with batch normalization, provides a
significant improvement in the accuracy of modern deep neural networks. In
addition, this work deals with the robustness of networks. We show that the
factorized superposition of images from the training set and the reformulation
of the multi class problem into a multi-label problem yields significantly more
robust networks. The reformulation and the adjustment of the multi class log
loss also improves the results compared to the overlay with only one class as
label.
https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p&#x3D;%2FTNandFDT&amp;mode&#x3D;list</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">GOHOME: Graph-Oriented Heatmap Output forfuture Motion Estimation. (arXiv:2109.01827v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01827">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose GOHOME, a method leveraging graph representations
of the High Definition Map and sparse projections to generate a heatmap output
representing the future position probability distribution for a given agent in
a traffic scene. This heatmap output yields an unconstrained 2D grid
representation of agent future possible locations, allowing inherent
multimodality and a measure of the uncertainty of the prediction. Our
graph-oriented model avoids the high computation burden of representing the
surrounding context as squared images and processing it with classical CNNs,
but focuses instead only on the most probable lanes where the agent could end
up in the immediate future. GOHOME reaches 3$rd$ on Argoverse Motion
Forecasting Benchmark on the MissRate$_6$ metric while achieving significant
speed-up and memory burden diminution compared to 1$^{st}$ place method HOME.
We also highlight that heatmap output enables multimodal ensembling and improve
1$^{st}$ place MissRate$_6$ by more than 15$\%$ with our best ensemble.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We compare the machine readability of pictograms found on Austrian and German
traffic signs. To that end, we train classification models on synthetic data
sets and evaluate their classification accuracy in a controlled setting. In
particular, we focus on differences between currently deployed pictograms in
the two countries, and a set of new pictograms designed to increase human
readability. Besides other results, we find that machine-learning models
generalize poorly to data sets with pictogram designs they have not been
trained on. We conclude that manufacturers of advanced driver-assistance
systems (ADAS) must take special care to properly address small visual
differences between current and newly designed traffic sign pictograms, as well
as between pictograms from different countries.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.IR updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Retrieval-based Conversational Recommendation. (arXiv:2109.02311v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02311">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Conversational recommender systems have attracted immense attention recently.
The most recent approaches rely on neural models trained on recorded dialogs
between humans, implementing an end-to-end learning process. These systems are
commonly designed to generate responses given the user&#x27;s utterances in natural
language. One main challenge is that these generated responses both have to be
appropriate for the given dialog context and must be grammatically and
semantically correct. An alternative to such generation-based approaches is to
retrieve responses from pre-recorded dialog data and to adapt them if needed.
Such retrieval-based approaches were successfully explored in the context of
general conversational systems, but have received limited attention in recent
years for CRS. In this work, we re-assess the potential of such approaches and
design and evaluate a novel technique for response retrieval and ranking. A
user study (N&#x3D;90) revealed that the responses by our system were on average of
higher quality than those of two recent generation-based systems. We
furthermore found that the quality ranking of the two generation-based
approaches is not aligned with the results from the literature, which points to
open methodological questions. Overall, our research underlines that
retrieval-based approaches should be considered an alternative or complement to
language generation approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Patent-KG: A Patent Knowledge Graph for Engineering Design. (arXiv:2108.11899v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11899">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To facilitate the knowledge reuse in engineering design, several dataset
approaches have been proposed and applied by designers. This paper builds a
patent-based knowledge graph, patent-KG, to represent the knowledge facts in
patents for engineering design. The arising patent-KG approach proposes a new
unsupervised mechanism to extract the knowledge facts in patent, by searching
the attention graph in language models. This method avoids using expensive
labelled data in supervised learning or listing complex syntactic rules in
rule-based extraction. The extracted entities are compared with other
benchmarks and shows a higher coverage of engineering words. The extracted
relationships are also compared with other benchmarks, and the result shows
meaningful advantages.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models. We use a multi-objective approach to introduce
temporal context into traditionally time-unaware recommender systems and
confirm its advantage via the proposed evaluation protocol. Finally, we
validate that the Pareto Fronts obtained with the added objective dominate
those produced by state-of-the-art models that are only optimized for accuracy
on three real-world publicly available datasets. The results show that
including our temporal objective can improve recall@20 by up to 20%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Temporal set prediction is becoming increasingly important as many companies
employ recommender systems in their online businesses, e.g., personalized
purchase prediction of shopping baskets. While most previous techniques have
focused on leveraging a user&#x27;s history, the study of combining it with others&#x27;
histories remains untapped potential. This paper proposes Global-Local Item
Embedding (GLOIE) that learns to utilize the temporal properties of sets across
whole users as well as within a user by coining the names as global and local
information to distinguish the two temporal patterns. GLOIE uses Variational
Autoencoder (VAE) and dynamic graph-based model to capture global and local
information and then applies attention to integrate resulting item embeddings.
Additionally, we propose to use Tweedie output for the decoder of VAE as it can
easily model zero-inflated and long-tailed distribution, which is more suitable
for several real-world data distributions than Gaussian or multinomial
counterparts. When evaluated on three public benchmarks, our algorithm
consistently outperforms previous state-of-the-art methods in most ranking
metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Adherence and Constancy in LIME-RS Explanations for Recommendation. (arXiv:2109.00818v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Explainable Recommendation has attracted a lot of attention due to a renewed
interest in explainable artificial intelligence. In particular, post-hoc
approaches have proved to be the most easily applicable ones to increasingly
complex recommendation models, which are then treated as black-boxes. The most
recent literature has shown that for post-hoc explanations based on local
surrogate models, there are problems related to the robustness of the approach
itself. This consideration becomes even more relevant in human-related tasks
like recommendation. The explanation also has the arduous task of enhancing
increasingly relevant aspects of user experience such as transparency or
trustworthiness. This paper aims to show how the characteristics of a classical
post-hoc model based on surrogates is strongly model-dependent and does not
prove to be accountable for the explanations generated.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07858">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.
Recent studies propose supervised PQ, where the embedding and quantization
models can be jointly trained with supervised learning. However, there is a
lack of appropriate formulation of the joint training objective; thus, the
improvements over previous non-supervised baselines are limited in reality. In
this work, we propose the Matching-oriented Product Quantization (MoPQ), where
a novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the
minimization of MCL, we are able to maximize the matching probability of query
and ground-truth key, which contributes to the optimal retrieval accuracy.
Given that the exact computation of MCL is intractable due to the demand of
vast contrastive samples, we further propose the Differentiable Cross-device
Sampling (DCS), which significantly augments the contrastive samples for
precise approximation of MCL. We conduct extensive experimental studies on four
real-world datasets, whose results verify the effectiveness of MoPQ.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.

In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FedMatch: Federated Learning Over Heterogeneous Question Answering Data. (arXiv:2108.05069v2 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05069">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Question Answering (QA), a popular and promising technique for intelligent
information access, faces a dilemma about data as most other AI techniques. On
one hand, modern QA methods rely on deep learning models which are typically
data-hungry. Therefore, it is expected to collect and fuse all the available QA
datasets together in a common site for developing a powerful QA model. On the
other hand, real-world QA datasets are typically distributed in the form of
isolated islands belonging to different parties. Due to the increasing
awareness of privacy security, it is almost impossible to integrate the data
scattered around, or the cost is prohibited. A possible solution to this
dilemma is a new approach known as federated learning, which is a
privacy-preserving machine learning technique over distributed datasets. In
this work, we propose to adopt federated learning for QA with the special
concern on the statistical heterogeneity of the QA data. Here the heterogeneity
refers to the fact that annotated QA data are typically with non-identical and
independent distribution (non-IID) and unbalanced sizes in practice.
Traditional federated learning methods may sacrifice the accuracy of individual
models under the heterogeneous situation. To tackle this problem, we propose a
novel Federated Matching framework for QA, named FedMatch, with a
backbone-patch architecture. The shared backbone is to distill the common
knowledge of all the participants while the private patch is a compact and
efficient module to retain the domain information for each participant. To
facilitate the evaluation, we build a benchmark collection based on several QA
datasets from different domains to simulate the heterogeneous situation in
practice. Empirical studies demonstrate that our model can achieve significant
improvements against the baselines over all the datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attentive Knowledge-aware Graph Convolutional Networks with Collaborative Guidance for Recommendation. (arXiv:2109.02046v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02046">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>To alleviate data sparsity and cold-start problems of traditional recommender
systems (RSs), incorporating knowledge graphs (KGs) to supplement auxiliary
information has attracted considerable attention recently. However, simply
integrating KGs in current KG-based RS models is not necessarily a guarantee to
improve the recommendation performance, which may even weaken the holistic
model capability. This is because the construction of these KGs is independent
of the collection of historical user-item interactions; hence, information in
these KGs may not always be helpful for recommendation to all users.

In this paper, we propose attentive Knowledge-aware Graph convolutional
networks with Collaborative Guidance for personalized Recommendation (CG-KGR).
CG-KGR is a novel knowledge-aware recommendation model that enables ample and
coherent learning of KGs and user-item interactions, via our proposed
Collaborative Guidance Mechanism. Specifically, CG-KGR first encapsulates
historical interactions to interactive information summarization. Then CG-KGR
utilizes it as guidance to extract information out of KGs, which eventually
provides more precise personalized recommendation. We conduct extensive
experiments on four real-world datasets over two recommendation tasks, i.e.,
Top-K recommendation and Click-Through rate (CTR) prediction. The experimental
results show that the CG-KGR model significantly outperforms recent
state-of-the-art models by 4.0-53.2% and 0.4-3.2%, in terms of Recall metric on
Top-K recommendation and AUC on CTR prediction, respectively.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Recommending Researchers in Machine Learning based on Author-Topic Model. (arXiv:2109.02022v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02022">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The aim of this paper is to uncover the researchers in machine learning using
the author-topic model (ATM). We collect 16,855 scientific papers from six top
journals in the field of machine learning published from 1997 to 2016 and
analyze them using ATM. The dataset is broken down into 4 intervals to identify
the top researchers and find similar researchers using their similarity score.
The similarity score is calculated using Hellinger distance. The researchers
are plotted using t-SNE, which reduces the dimensionality of the data while
keeping the same distance between the points. The analysis of our study helps
the upcoming researchers to find the top researchers in their area of interest.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Navigating the Mise-en-Page: Interpretive Machine Learning Approaches to the Visual Layouts of Multi-Ethnic Periodicals. (arXiv:2109.01732v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01732">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper presents a computational method of analysis that draws from
machine learning, library science, and literary studies to map the visual
layouts of multi-ethnic newspapers from the late 19th and early 20th century
United States. This work departs from prior approaches to newspapers that focus
on individual pieces of textual and visual content. Our method combines
Chronicling America&#x27;s MARC data and the Newspaper Navigator machine learning
dataset to identify the visual patterns of newspaper page layouts. By analyzing
high-dimensional visual similarity, we aim to better understand how editors
spoke and protested through the layout of their papers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this article, we propose a systematic approach for fire station location
planning. We develop a machine learning model, based on Random Forest, for
demand prediction and utilize the model further to define a generalized index
to measure quality of fire service in urban settings. Our model is built upon
spatial data collected from multiple different sources. Efficacy of proper
facility planning depends on choice of candidates where fire stations can be
located along with existing stations, if any. Also, the travel time from these
candidates to demand locations need to be taken care of to maintain fire safety
standard. Here, we propose a travel time based clustering technique to identify
suitable candidates. Finally, we develop an optimization problem to select best
locations to install new fire stations. Our optimization problem is built upon
maximum coverage problem, based on integer programming. We present a detailed
experimental study of our proposed approach in collaboration with city of
Victoria Fire Department, MN, USA. Our demand prediction model achieves true
positive rate of 70% and false positive rate of 22% approximately. We aid
Victoria Fire Department to select a location for a new fire station using our
approach. We present detailed results on improvement statistics by locating a
new facility, as suggested by our methodology, in the city of Victoria.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
      <li class="source">
        <section>
          <h3 class="source-name"><a class="source-name__link" href="http://arxiv.org/">cs.LG updates on arXiv.org</a></h3>
          <section class="articles-per-source">
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning. (arXiv:2107.08888v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08888">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Maintaining the long-term exploration capability of the agent remains one of
the critical challenges in deep reinforcement learning. A representative
solution is to leverage reward shaping to provide intrinsic rewards for the
agent to encourage exploration. However, most existing methods suffer from
vanishing intrinsic rewards, which cannot provide sustainable exploration
incentives. Moreover, they rely heavily on complex models and additional memory
to record learning procedures, resulting in high computational complexity and
low robustness. To tackle this problem, entropy-based methods are proposed to
evaluate the global exploration performance, encouraging the agent to visit the
state space more equitably. However, the sample complexity of estimating the
state visitation entropy is prohibitive when handling environments with
high-dimensional observations. In this paper, we introduce a novel metric
entitled Jain&#x27;s fairness index (JFI) to replace the entropy regularizer, which
solves the exploration problem from a brand new perspective. In sharp contrast
to the entropy regularizer, JFI is more computable and robust and can be easily
applied generalized into arbitrary tasks. Furthermore, we leverage a
variational auto-encoder (VAE) model to capture the life-long novelty of
states, which is combined with the global JFI score to form multimodal
intrinsic rewards. Finally, extensive simulation results demonstrate that our
multimodal reward shaping (MMRS) method can achieve higher performance than
other benchmark schemes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Marching Cubes. (arXiv:2106.11272v3 [cs.GR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.11272">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We introduce Neural Marching Cubes (NMC), a data-driven approach for
extracting a triangle mesh from a discretized implicit field. Classical MC is
defined by coarse tessellation templates isolated to individual cubes. While
more refined tessellations have been proposed, they all make heuristic
assumptions, such as trilinearity, when determining the vertex positions and
local mesh topologies in each cube. In principle, none of these approaches can
reconstruct geometric features that reveal coherence or dependencies between
nearby cubes (e.g., a sharp edge), as such information is unaccounted for,
resulting in poor estimates of the true underlying implicit field. To tackle
these challenges, we re-cast MC from a deep learning perspective, by designing
tessellation templates more apt at preserving geometric features, and learning
the vertex positions and mesh topologies from training meshes, to account for
contextual information from nearby cubes. We develop a compact per-cube
parameterization to represent the output triangle mesh, while being compatible
with neural processing, so that a simple 3D convolutional network can be
employed for the training. We show that all topological cases in each cube that
are applicable to our design can be easily derived using our representation,
and the resulting tessellations can also be obtained naturally and efficiently
by following a few design guidelines. In addition, our network learns local
features with limited receptive fields, hence it generalizes well to new shapes
and new datasets. We evaluate our neural MC approach by quantitative and
qualitative comparisons to all well-known MC variants. In particular, we
demonstrate the ability of our network to recover sharp features such as edges
and corners, a long-standing issue of MC and its variants. Our network also
reconstructs local mesh topologies more accurately than previous approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Reference Alignment for sparse signals, Uniform Uncertainty Principles and the Beltway Problem. (arXiv:2106.12996v2 [math.ST] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.12996">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by cutting-edge applications like cryo-electron microscopy
(cryo-EM), the Multi-Reference Alignment (MRA) model entails the learning of an
unknown signal from repeated measurements of its images under the latent action
of a group of isometries and additive noise of magnitude $\sigma$. Despite
significant interest, a clear picture for understanding rates of estimation in
this model has emerged only recently, particularly in the high-noise regime
$\sigma \gg 1$ that is highly relevant in applications. Recent investigations
have revealed a remarkable asymptotic sample complexity of order $\sigma^6$ for
certain signals whose Fourier transforms have full support, in stark contrast
to the traditional $\sigma^2$ that arise in regular models. Often prohibitively
large in practice, these results have prompted the investigation of variations
around the MRA model where better sample complexity may be achieved. In this
paper, we show that \emph{sparse} signals exhibit an intermediate $\sigma^4$
sample complexity even in the classical MRA model. Our results explore and
exploit connections of the MRA estimation problem with two classical topics in
applied mathematics: the \textit{beltway problem} from combinatorial
optimization, and \textit{uniform uncertainty principles} from harmonic
analysis.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A method for estimating the entropy of time series using artificial neural network. (arXiv:2107.08399v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.08399">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Measuring the predictability and complexity of time series is an essential
tool in designing and controlling the nonlinear system. Different entropy
measures exist in the literature to analyze the predictability and complexity
of time series. However, the existed methods have some drawbacks related to a
strong dependence of entropy on the parameters of the methods, as well as on
the length and amplitude of the time series. To overcome these difficulties,
this study proposes a new method for estimating the entropy of a time series
using the LogNNet neural network model. The LogNNet reservoir matrix is filled
with the time series elements according to our algorithm. The network is
trained on MNIST-10 dataset and the classification accuracy is calculated. The
accuracy is considered as the entropy measure and denoted by NNetEn. The
novelty of entropy calculation is that the time series is involved in mixing
the input information in the reservoir. The greater complexity of the time
series leads to the better ability of the neural network to learn, and to the
higher classification accuracy and NNetEn values. The epochs number in the
training process of LogNNet is considered as the control parameter. We
introduce a new time series characteristic, called time series learning
inertia, that determines the learning rate of the neural network. The
robustness and efficiency of the method is verified on chaotic, periodic,
random, binary and constant time series. The comparison of NNetEn with other
methods of entropy estimation demonstrates that our method is more robust and
accurate and can be widely used in practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Explainable AI (XAI) for PHM of Industrial Asset: A State-of-The-Art, PRISMA-Compliant Systematic Review. (arXiv:2107.03869v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03869">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A state-of-the-art systematic review on XAI applied to Prognostic and Health
Management (PHM) of industrial asset is presented. This work provides an
overview of the general trend of XAI in PHM, answers the question of accuracy
versus explainability, the extent of human involvement, the explanation
assessment and uncertainty quantification in PHM-XAI domain. Research articles
associated with the subject, from 2015 to 2021 were selected from five known
databases following PRISMA guidelines. Data was then extracted from the
selected articles and examined. Several findings were synthesized. Firstly,
while the discipline is still young, the analysis indicated the growing
acceptance of XAI in PHM domain. Secondly, XAI functions as a double edge
sword, where it is assimilated as a tool to execute PHM tasks as well as a mean
of explanation, particularly in diagnostic and anomaly detection activities,
implying a real need for XAI in PHM. Thirdly, the review showed that PHM-XAI
papers produce either good or excellent result in general, suggesting that PHM
performance is unaffected by XAI. Fourthly, human role, evaluation metrics and
uncertainty management are areas requiring further attention by the PHM
community. Adequate assessment metrics to cater for PHM need are urgently
needed.Finally, most case study featured on the accepted articles are based on
real, industrial data, indicating that the available PHM-XAI blends are fit to
solve complex,real-world challenges, increasing the confidence in AI adoption
in the industry.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Constrained Restless Bandits for Dynamic Scheduling in Cyber-Physical Systems. (arXiv:1904.08962v5 [cs.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1904.08962">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>This paper studies a class of constrained restless multi-armed bandits
(CRMAB). The constraints are in the form of time varying set of actions (set of
available arms). This variation can be either stochastic or semi-deterministic.
Given a set of arms, a fixed number of them can be chosen to be played in each
decision interval. The play of each arm yields a state dependent reward. The
current states of arms are partially observable through binary feedback signals
from arms that are played. The current availability of arms is fully
observable. The objective is to maximize long term cumulative reward. The
uncertainty about future availability of arms along with partial state
information makes this objective challenging. Applications for CRMAB can be
found in resource allocation in cyber-physical systems involving components
with time varying availability.

First, this optimization problem is analyzed using Whittle&#x27;s index policy. To
this end, a constrained restless single-armed bandit is studied. It is shown to
admit a threshold-type optimal policy and is also indexable. An algorithm to
compute Whittle&#x27;s index is presented. An alternate solution method with lower
complexity is also presented in the form of an online rollout policy. A
detailed discussion on the complexity of both these schemes is also presented,
which suggests that online rollout policy with short look ahead is simpler to
implement than Whittle&#x27;s index computation. Further, upper bounds on the value
function are derived in order to estimate the degree of sub-optimality of
various solutions. The simulation study compares the performance of Whittle&#x27;s
index, online rollout, myopic and modified Whittle&#x27;s index policies.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach. (arXiv:2102.05406v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05406">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We propose a black-box reduction that turns a certain reinforcement learning
algorithm with optimal regret in a (near-)stationary environment into another
algorithm with optimal dynamic regret in a non-stationary environment,
importantly without any prior knowledge on the degree of non-stationarity. By
plugging different algorithms into our black-box, we provide a list of examples
showing that our approach not only recovers recent results for (contextual)
multi-armed bandits achieved by very specialized algorithms, but also
significantly improves the state of the art for (generalized) linear bandits,
episodic MDPs, and infinite-horizon MDPs in various ways. Specifically, in most
cases our algorithm achieves the optimal dynamic regret
$\widetilde{\mathcal{O}}(\min\{\sqrt{LT}, \Delta^{1/3}T^{2/3}\})$ where $T$ is
the number of rounds and $L$ and $\Delta$ are the number and amount of changes
of the world respectively, while previous works only obtain suboptimal bounds
and/or require the knowledge of $L$ and $\Delta$.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v4 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01034">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The Intergovernmental Panel on Climate Change proposes different mitigation
strategies to achieve the net emissions reductions that would be required to
follow a pathway that limits global warming to 1.5{\deg}C with no or limited
overshoot. The transition towards a carbon-free society goes through an
inevitable increase in the share of renewable generation in the energy mix and
a drastic decrease in the total consumption of fossil fuels. Therefore, this
thesis studies the integration of renewables in power systems by investigating
forecasting and decision-making tools. Indeed, in contrast to conventional
power plants, renewable energy is subject to uncertainty. Most of the
generation technologies based on renewable sources are non-dispatchable, and
their production is stochastic and complex to predict in advance. A high share
of renewables is challenging for power systems that have been designed and
sized for dispatchable units. In this context, probabilistic forecasts, which
aim at modeling the distribution of all possible future realizations, have
become a vital tool to equip decision-makers, hopefully leading to better
decisions in energy applications. This thesis focuses on two main research
questions: (1) How to produce reliable probabilistic renewable generation
forecasts, consumption, and electricity prices? (2) How to make decisions with
uncertainty using probabilistic forecasts? The thesis perimeter is the energy
management of &quot;small&quot; systems such as microgrids at a residential scale on a
day-ahead basis. It is divided into two main parts to propose directions to
address both research questions (1) a forecasting part; (2) a planning and
control part.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Critical Connectivity Radius for Segmenting Randomly-Generated, High Dimensional Data Points. (arXiv:1602.03822v8 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1602.03822">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivated by a $2$-dimensional (unsupervised) image segmentation task whereby
local regions of pixels are clustered via edge detection methods, a more
general probabilistic mathematical framework is devised. Critical thresholds
are calculated that indicate strong correlation between randomly-generated,
high dimensional data points that have been projected into structures in a
partition of a bounded, $2$-dimensional area, of which, an image is a special
case. A neighbor concept for structures in the partition is defined and a
critical radius is uncovered. Measured from a central structure in localized
regions of the partition, the radius indicates strong, long and short range
correlation in the count of occupied structures. The size of a short interval
of radii is estimated upon which the transition from short-to-long range
correlation is virtually assured, which defines a demarcation of when an image
ceases to be &quot;interesting&quot;.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Accurate and fast matrix factorization for low-rank learning. (arXiv:2104.10785v4 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.10785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this paper, we tackle two important problems in low-rank learning, which
are partial singular value decomposition and numerical rank estimation of huge
matrices. By using the concepts of Krylov subspaces such as Golub-Kahan
bidiagonalization (GK-bidiagonalization) as well as Ritz vectors, we propose
two methods for solving these problems in a fast and accurate way. Our
experiments show the advantages of the proposed methods compared to the
traditional and randomized singular value decomposition methods. The proposed
methods are appropriate for applications involving huge matrices where the
accuracy of the desired singular values and also all of their corresponding
singular vectors are essential. As a real application, we evaluate the
performance of our methods on the problem of Riemannian similarity learning
between two various image datasets of MNIST and USPS.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using BART to Perform Pareto Optimization and Quantify its Uncertainties. (arXiv:2101.02558v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.02558">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Techniques to reduce the energy burden of an industrial ecosystem often
require solving a multiobjective optimization problem. However, collecting
experimental data can often be either expensive or time-consuming. In such
cases, statistical methods can be helpful. This article proposes Pareto Front
(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression
Trees (BART), which is a non-parametric model whose assumptions are typically
less restrictive than popular alternatives, such as Gaussian Processes (GPs).
These less restrictive assumptions allow BART to handle scenarios (e.g.
high-dimensional input spaces, nonsmooth responses, large datasets) that GPs
find difficult. The performance of our BART-based method is compared to a
GP-based method using analytic test functions, demonstrating convincing
advantages. Finally, our BART-based methodology is applied to a motivating
engineering problem. Supplementary materials, which include a theorem proof,
algorithms, and R code, for this article are available online.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning with Holographic Reduced Representations. (arXiv:2109.02157v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02157">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Holographic Reduced Representations (HRR) are a method for performing
symbolic AI on top of real-valued vectors \cite{Plate1995} by associating each
vector with an abstract concept, and providing mathematical operations to
manipulate vectors as if they were classic symbolic objects. This method has
seen little use outside of older symbolic AI work and cognitive science. Our
goal is to revisit this approach to understand if it is viable for enabling a
hybrid neural-symbolic approach to learning as a differentiable component of a
deep learning architecture. HRRs today are not effective in a differentiable
solution due to numerical instability, a problem we solve by introducing a
projection step that forces the vectors to exist in a well behaved point in
space. In doing so we improve the concept retrieval efficacy of HRRs by over
$100\times$. Using multi-label classification we demonstrate how to leverage
the symbolic HRR properties to develop an output layer and loss function that
is able to learn effectively, and allows us to investigate some of the pros and
cons of an HRR neuro-symbolic learning approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RDFFrames: Knowledge Graph Access for Machine Learning Tools. (arXiv:2002.03614v4 [cs.DB] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03614">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>Knowledge graphs represented as RDF datasets are integral to many machine
learning applications. RDF is supported by a rich ecosystem of data management
systems and tools, most notably RDF database systems that provide a SPARQL
query interface. Surprisingly, machine learning tools for knowledge graphs do
not use SPARQL, despite the obvious advantages of using a database system. This
is due to the mismatch between SPARQL and machine learning tools in terms of
data model and programming style. Machine learning tools work on data in
tabular format and process it using an imperative programming style, while
SPARQL is declarative and has as its basic operation matching graph patterns to
RDF triples. We posit that a good interface to knowledge graphs from a machine
learning software stack should use an imperative, navigational programming
paradigm based on graph traversal rather than the SPARQL query paradigm based
on graph patterns. In this paper, we present RDFFrames, a framework that
provides such an interface. RDFFrames provides an imperative Python API that
gets internally translated to SPARQL, and it is integrated with the PyData
machine learning software stack. RDFFrames enables the user to make a sequence
of Python calls to define the data to be extracted from a knowledge graph
stored in an RDF database system, and it translates these calls into a compact
SPQARL query, executes it on the database system, and returns the results in a
standard tabular format. Thus, RDFFrames is a useful tool for data preparation
that combines the usability of PyData with the flexibility and performance of
RDF database systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence in Dry Eye Disease. (arXiv:2109.01658v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01658">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Dry eye disease (DED) has a prevalence of between 5 and 50\%, depending on
the diagnostic criteria used and population under study. However, it remains
one of the most underdiagnosed and undertreated conditions in ophthalmology.
Many tests used in the diagnosis of DED rely on an experienced observer for
image interpretation, which may be considered subjective and result in
variation in diagnosis. Since artificial intelligence (AI) systems are capable
of advanced problem solving, use of such techniques could lead to more
objective diagnosis. Although the term &#x60;AI&#x27; is commonly used, recent success in
its applications to medicine is mainly due to advancements in the sub-field of
machine learning, which has been used to automatically classify images and
predict medical outcomes. Powerful machine learning techniques have been
harnessed to understand nuances in patient data and medical images, aiming for
consistent diagnosis and stratification of disease severity. This is the first
literature review on the use of AI in DED. We provide a brief introduction to
AI, report its current use in DED research and its potential for application in
the clinic. Our review found that AI has been employed in a wide range of DED
clinical tests and research applications, primarily for interpretation of
interferometry, slit-lamp and meibography images. While initial results are
promising, much work is still needed on model development, clinical testing and
standardisation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Frustratingly Simple Pretraining Alternatives to Masked Language Modeling. (arXiv:2109.01819v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01819">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Masked language modeling (MLM), a self-supervised pretraining objective, is
widely used in natural language processing for learning text representations.
MLM trains a model to predict a random sample of input tokens that have been
replaced by a [MASK] placeholder in a multi-class setting over the entire
vocabulary. When pretraining, it is common to use alongside MLM other auxiliary
objectives on the token or sequence level to improve downstream performance
(e.g. next sentence prediction). However, no previous work so far has attempted
in examining whether other simpler linguistically intuitive or not objectives
can be used standalone as main pretraining objectives. In this paper, we
explore five simple pretraining objectives based on token-level classification
tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our
proposed methods achieve comparable or better performance to MLM using a
BERT-BASE architecture. We further validate our methods using smaller models,
showing that pretraining a model with 41% of the BERT-BASE&#x27;s parameters,
BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Stochastic-Sign SGD for Federated Learning with Theoretical Guarantees. (arXiv:2002.10940v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.10940">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Federated learning (FL) has emerged as a prominent distributed learning
paradigm. FL entails some pressing needs for developing novel parameter
estimation approaches with theoretical guarantees of convergence, which are
also communication efficient, differentially private and Byzantine resilient in
the heterogeneous data distribution settings. Quantization-based SGD solvers
have been widely adopted in FL and the recently proposed SIGNSGD with majority
vote shows a promising direction. However, no existing methods enjoy all the
aforementioned properties. In this paper, we propose an intuitively-simple yet
theoretically-sound method based on SIGNSGD to bridge the gap. We present
Stochastic-Sign SGD which utilizes novel stochastic-sign based gradient
compressors enabling the aforementioned properties in a unified framework. We
also present an error-feedback variant of the proposed Stochastic-Sign SGD
which further improves the learning performance in FL. We test the proposed
method with extensive experiments using deep neural networks on the MNIST
dataset and the CIFAR-10 dataset. The experimental results corroborate the
effectiveness of the proposed method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-label Classification via Adaptive Resonance Theory-based Clustering. (arXiv:2103.01511v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01511">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper proposes a multi-label classification algorithm capable of
continual learning by applying an Adaptive Resonance Theory (ART)-based
clustering algorithm and the Bayesian approach for label probability
computation. The ART-based clustering algorithm adaptively and continually
generates prototype nodes corresponding to given data, and the generated nodes
are used as classifiers. The label probability computation independently counts
the number of label appearances for each class and calculates the Bayesian
probabilities. Thus, the label probability computation can cope with an
increase in the number of labels. Experimental results with synthetic and
real-world multi-label datasets show that the proposed algorithm has
competitive classification performance to other well-known algorithms while
realizing continual learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">QUEACO: Borrowing Treasures from Weakly-labeled Behavior Data for Query Attribute Value Extraction. (arXiv:2108.08468v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08468">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(3 min)</span>
                    <span>We study the problem of query attribute value extraction, which aims to
identify named entities from user queries as diverse surface form attribute
values and afterward transform them into formally canonical forms. Such a
problem consists of two phases: {named entity recognition (NER)} and {attribute
value normalization (AVN)}. However, existing works only focus on the NER phase
but neglect equally important AVN. To bridge this gap, this paper proposes a
unified query attribute value extraction system in e-commerce search named
QUEACO, which involves both two phases. Moreover, by leveraging large-scale
weakly-labeled behavior data, we further improve the extraction performance
with less supervision cost. Specifically, for the NER phase, QUEACO adopts a
novel teacher-student network, where a teacher network that is trained on the
strongly-labeled data generates pseudo-labels to refine the weakly-labeled data
for training a student network. Meanwhile, the teacher network can be
dynamically adapted by the feedback of the student&#x27;s performance on
strongly-labeled data to maximally denoise the noisy supervisions from the weak
labels. For the AVN phase, we also leverage the weakly-labeled
query-to-attribute behavior data to normalize surface form attribute values
from queries into canonical forms from products. Extensive experiments on a
real-world large-scale E-commerce dataset demonstrate the effectiveness of
QUEACO.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An empirical evaluation of attention-based multi-head models for improved turbofan engine remaining useful life prediction. (arXiv:2109.01761v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01761">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>A single unit (head) is the conventional input feature extractor in deep
learning architectures trained on multivariate time series signals. The
importance of the fixed-dimensional vector representation generated by the
single-head network has been demonstrated for industrial machinery condition
monitoring and predictive maintenance. However, processing heterogeneous sensor
signals with a single head may result in a model that cannot explicitly account
for the diversity in time-varying multivariate inputs. This work extends the
conventional single-head deep learning models to a more robust form by
developing context-specific heads to independently capture the inherent pattern
of each sensor reading in multivariate time series signals. Using the turbofan
aircraft engine benchmark dataset (CMAPSS), an extensive experiment is
performed to verify the effectiveness and benefits of multi-head fully
connected neurons, recurrent networks, convolution network, the
transformer-style stand-alone attention network, and their variants for
remaining useful life estimation. Moreover, the effect of different attention
mechanisms on the multi-head models is also evaluated. In addition, each
architecture&#x27;s relative advantage and computational overhead are analyzed.
Results show that utilizing the attention layer is task-sensitive and
model-dependent, as it does not provide consistent improvement across the
models investigated. The result is further compared with five state-of-the-art
models, and the comparison shows that a relatively simple multi-head
architecture performs better than the state-of-the-art models. The results
presented in this study demonstrate the importance of multi-head models and
attention mechanisms to improved understanding of the remaining useful life of
industrial assets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-View Spatial-Temporal Graph Convolutional Networks with Domain Generalization for Sleep Stage Classification. (arXiv:2109.01824v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01824">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Sleep stage classification is essential for sleep assessment and disease
diagnosis. Although previous attempts to classify sleep stages have achieved
high classification performance, several challenges remain open: 1) How to
effectively utilize time-varying spatial and temporal features from
multi-channel brain signals remains challenging. Prior works have not been able
to fully utilize the spatial topological information among brain regions. 2)
Due to the many differences found in individual biological signals, how to
overcome the differences of subjects and improve the generalization of deep
neural networks is important. 3) Most deep learning methods ignore the
interpretability of the model to the brain. To address the above challenges, we
propose a multi-view spatial-temporal graph convolutional networks (MSTGCN)
with domain generalization for sleep stage classification. Specifically, we
construct two brain view graphs for MSTGCN based on the functional connectivity
and physical distance proximity of the brain regions. The MSTGCN consists of
graph convolutions for extracting spatial features and temporal convolutions
for capturing the transition rules among sleep stages. In addition, attention
mechanism is employed for capturing the most relevant spatial-temporal
information for sleep stage classification. Finally, domain generalization and
MSTGCN are integrated into a unified framework to extract subject-invariant
sleep features. Experiments on two public datasets demonstrate that the
proposed model outperforms the state-of-the-art baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning-based decentralized offloading decision making in an adversarial environment. (arXiv:2104.12827v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12827">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vehicular fog computing (VFC) pushes the cloud computing capability to the
distributed fog nodes at the edge of the Internet, enabling compute-intensive
and latency-sensitive computing services for vehicles through task offloading.
However, a heterogeneous mobility environment introduces uncertainties in terms
of resource supply and demand, which are inevitable bottlenecks for the optimal
offloading decision. Also, these uncertainties bring extra challenges to task
offloading under the oblivious adversary attack and data privacy risks. In this
article, we develop a new adversarial online learning algorithm with bandit
feedback based on the adversarial multi-armed bandit theory, to enable scalable
and low-complexity offloading decision making. Specifically, we focus on
optimizing fog node selection with the aim of minimizing the offloading service
costs in terms of delay and energy. The key is to implicitly tune the
exploration bonus in the selection process and the assessment rules of the
designed algorithm, taking into account volatile resource supply and demand. We
theoretically prove that the input-size dependent selection rule allows to
choose a suitable fog node without exploring the sub-optimal actions, and also
an appropriate score patching rule allows to quickly adapt to evolving
circumstances, which reduce variance and bias simultaneously, thereby achieving
a better exploitation-exploration balance. Simulation results verify the
effectiveness and robustness of the proposed algorithm.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">NAS-OoD: Neural Architecture Search for Out-of-Distribution Generalization. (arXiv:2109.02038v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02038">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recent advances on Out-of-Distribution (OoD) generalization reveal the
robustness of deep learning models against distribution shifts. However,
existing works focus on OoD algorithms, such as invariant risk minimization,
domain generalization, or stable learning, without considering the influence of
deep model architectures on OoD generalization, which may lead to sub-optimal
performance. Neural Architecture Search (NAS) methods search for architecture
based on its performance on the training data, which may result in poor
generalization for OoD tasks. In this work, we propose robust Neural
Architecture Search for OoD generalization (NAS-OoD), which optimizes the
architecture with respect to its performance on generated OoD data by gradient
descent. Specifically, a data generator is learned to synthesize OoD data by
maximizing losses computed by different neural architectures, while the goal
for architecture search is to find the optimal architecture parameters that
minimize the synthetic OoD data losses. The data generator and the neural
architecture are jointly optimized in an end-to-end manner, and the minimax
training process effectively discovers robust architectures that generalize
well for different distribution shifts. Extensive experimental results show
that NAS-OoD achieves superior performance on various OoD generalization
benchmarks with deep models having a much fewer number of parameters. In
addition, on a real industry dataset, the proposed NAS-OoD method reduces the
error rate by more than 70% compared with the state-of-the-art method,
demonstrating the proposed method&#x27;s practicality for real applications.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient Privacy Preserving Edge Computing Framework for Image Classification. (arXiv:2005.04563v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.04563">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In order to extract knowledge from the large data collected by edge devices,
traditional cloud based approach that requires data upload may not be feasible
due to communication bandwidth limitation as well as privacy and security
concerns of end users. To address these challenges, a novel privacy preserving
edge computing framework is proposed in this paper for image classification.
Specifically, autoencoder will be trained unsupervised at each edge device
individually, then the obtained latent vectors will be transmitted to the edge
server for the training of a classifier. This framework would reduce the
communications overhead and protect the data of the end users. Comparing to
federated learning, the training of the classifier in the proposed framework
does not subject to the constraints of the edge devices, and the autoencoder
can be trained independently at each edge device without any server
involvement. Furthermore, the privacy of the end users&#x27; data is protected by
transmitting latent vectors without additional cost of encryption. Experimental
results provide insights on the image classification performance vs. various
design parameters such as the data compression ratio of the autoencoder and the
model complexity.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Federated Learning using Smart Contracts on Blockchains, based on Reward Driven Approach. (arXiv:2107.10243v2 [cs.CR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10243">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Over the recent years, Federated machine learning continues to gain interest
and momentum where there is a need to draw insights from data while preserving
the data provider&#x27;s privacy. However, one among other existing challenges in
the adoption of federated learning has been the lack of fair, transparent and
universally agreed incentivization schemes for rewarding the federated learning
contributors. Smart contracts on a blockchain network provide transparent,
immutable and independently verifiable proofs by all participants of the
network. We leverage this open and transparent nature of smart contracts on a
blockchain to define incentivization rules for the contributors, which is based
on a novel scalar quantity - federated contribution. Such a smart contract
based reward-driven model has the potential to revolutionize the federated
learning adoption in enterprises. Our contribution is two-fold: first is to
show how smart contract based blockchain can be a very natural communication
channel for federated learning. Second, leveraging this infrastructure, we can
show how an intuitive measure of each agents&#x27; contribution can be built and
integrated with the life cycle of the training and reward process.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02344">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-view representation learning captures comprehensive information from
multiple views of a shared context. Recent works intuitively apply contrastive
learning (CL) to learn representations, regarded as a pairwise manner, which is
still scalable: view-specific noise is not filtered in learning view-shared
representations; the fake negative pairs, where the negative terms are actually
within the same class as the positive, and the real negative pairs are
coequally treated; and evenly measuring the similarities between terms might
interfere with optimization. Importantly, few works research the theoretical
framework of generalized self-supervised multi-view learning, especially for
more than two views. To this end, we rethink the existing multi-view learning
paradigm from the information theoretical perspective and then propose a novel
information theoretical framework for generalized multi-view learning. Guided
by it, we build a multi-view coding method with a three-tier progressive
architecture, namely Information theory-guided heuristic Progressive Multi-view
Coding (IPMC). In the distribution-tier, IPMC aligns the distribution between
views to reduce view-specific noise. In the set-tier, IPMC builds self-adjusted
pools for contrasting, which utilizes a view filter to adaptively modify the
pools. Lastly, in the instance-tier, we adopt a designed unified loss to learn
discriminative representations and reduce the gradient interference.
Theoretically and empirically, we demonstrate the superiority of IPMC over
state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep learning facilitates fully automated brain image registration of optoacoustic tomography and magnetic resonance imaging. (arXiv:2109.01880v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01880">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Multi-spectral optoacoustic tomography (MSOT) is an emerging optical imaging
method providing multiplex molecular and functional information from the rodent
brain. It can be greatly augmented by magnetic resonance imaging (MRI) that
offers excellent soft-tissue contrast and high-resolution brain anatomy.
Nevertheless, registration of multi-modal images remains challenging, chiefly
due to the entirely different image contrast rendered by these modalities.
Previously reported registration algorithms mostly relied on manual
user-dependent brain segmentation, which compromised data interpretation and
accurate quantification. Here we propose a fully automated registration method
for MSOT-MRI multimodal imaging empowered by deep learning. The automated
workflow includes neural network-based image segmentation to generate suitable
masks, which are subsequently registered using an additional neural network.
Performance of the algorithm is showcased with datasets acquired by
cross-sectional MSOT and high-field MRI preclinical scanners. The automated
registration method is further validated with manual and half-automated
registration, demonstrating its robustness and accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating the probabilities of causation via deep monotonic twin networks. (arXiv:2109.01904v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01904">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>There has been much recent work using machine learning to answer causal
queries. Most focus on interventional queries, such as the conditional average
treatment effect. However, as noted by Pearl, interventional queries only form
part of a larger hierarchy of causal queries, with counterfactuals sitting at
the top. Despite this, our community has not fully succeeded in adapting
machine learning tools to answer counterfactual queries. This work addresses
this challenge by showing how to implement twin network counterfactual
inference -- an alternative to abduction, action, &amp; prediction counterfactual
inference -- with deep learning to estimate counterfactual queries. We show how
the graphical nature of twin networks makes them particularly amenable to deep
learning, yielding simple neural network architectures that, when trained, are
capable of counterfactual inference. Importantly, we show how to enforce known
identifiability constraints during training, ensuring the answer to each
counterfactual query is uniquely determined. We demonstrate our approach by
using it to accurately estimate the probabilities of causation -- important
counterfactual queries that quantify the degree to which one event was a
necessary or sufficient cause of another -- on both synthetic and real data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Differentiable Programming for Flexible Statistical Modeling. (arXiv:2012.05722v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.05722">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Differentiable programming has recently received much interest as a paradigm
that facilitates taking gradients of computer programs. While the corresponding
flexible gradient-based optimization approaches so far have been used
predominantly for deep learning or enriching the latter with modeling
components, we want to demonstrate that they can also be useful for statistical
modeling per se, e.g., for quick prototyping when classical maximum likelihood
approaches are challenging or not feasible. In an application from a COVID-19
setting, we utilize differentiable programming to quickly build and optimize a
flexible prediction model adapted to the data quality challenges at hand.
Specifically, we develop a regression model, inspired by delay differential
equations, that can bridge temporal gaps of observations in the central German
registry of COVID-19 intensive care cases for predicting future demand. With
this exemplary modeling challenge, we illustrate how differentiable programming
can enable simple gradient-based optimization of the model by automatic
differentiation. This allowed us to quickly prototype a model under time
pressure that outperforms simpler benchmark models. We thus exemplify the
potential of differentiable programming also outside deep learning
applications, to provide more options for flexible applied statistical
modeling.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Bacteriophage classification for assembled contigs using Graph Convolutional Network. (arXiv:2102.03746v2 [q-bio.GN] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.03746">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Motivation: Bacteriophages (aka phages), which mainly infect bacteria, play
key roles in the biology of microbes. As the most abundant biological entities
on the planet, the number of discovered phages is only the tip of the iceberg.
Recently, many new phages have been revealed using high throughput sequencing,
particularly metagenomic sequencing. Compared to the fast accumulation of
phage-like sequences, there is a serious lag in taxonomic classification of
phages. High diversity, abundance, and limited known phages pose great
challenges for taxonomic analysis. In particular, alignment-based tools have
difficulty in classifying fast accumulating contigs assembled from metagenomic
data. Results: In this work, we present a novel semi-supervised learning model,
named PhaGCN, to conduct taxonomic classification for phage contigs. In this
learning model, we construct a knowledge graph by combining the DNA sequence
features learned by convolutional neural network (CNN) and protein sequence
similarity gained from gene-sharing network. Then we apply graph convolutional
network (GCN) to utilize both the labeled and unlabeled samples in training to
enhance the learning ability. We tested PhaGCN on both simulated and real
sequencing data. The results clearly show that our method competes favorably
against available phage classification tools.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Trainable Discrete Feature Embeddings for Variational Quantum Classifier. (arXiv:2106.09415v2 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09415">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Quantum classifiers provide sophisticated embeddings of input data in Hilbert
space promising quantum advantage. The advantage stems from quantum feature
maps encoding the inputs into quantum states with variational quantum circuits.
A recent work shows how to map discrete features with fewer quantum bits using
Quantum Random Access Coding (QRAC), an important primitive to encode binary
strings into quantum states. We propose a new method to embed discrete features
with trainable quantum circuits by combining QRAC and a recently proposed
strategy for training quantum feature map called quantum metric learning. We
show that the proposed trainable embedding requires not only as few qubits as
QRAC but also overcomes the limitations of QRAC to classify inputs whose
classes are based on hard Boolean functions. We numerically demonstrate its use
in variational quantum classifiers to achieve better performances in
classifying real-world datasets, and thus its possibility to leverage near-term
quantum computers for quantum machine learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model retraining and information sharing in a supply chain with long-term fluctuating demands. (arXiv:2109.01784v1 [physics.soc-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01784">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Demand forecasting based on empirical data is a viable approach for
optimizing a supply chain. However, in this approach, a model constructed from
past data occasionally becomes outdated due to long-term changes in the
environment, in which case the model should be updated (i.e., retrained) using
the latest data. In this study, we examine the effects of updating models in a
supply chain using a minimal setting. We demonstrate that when each party in
the supply chain has its own forecasting model, uncoordinated model retraining
causes the bullwhip effect even if a very simple replenishment policy is
applied. Our results also indicate that sharing the forecasting model among the
parties involved significantly reduces the bullwhip effect.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Customer 360-degree Insights in Predicting Chronic Diabetes. (arXiv:2109.01863v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01863">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Chronic diseases such as diabetes are quite prevalent in the world and are
responsible for a significant number of deaths per year. In addition,
treatments for such chronic diseases account for a high healthcare cost.
However, research has shown that diabetes can be proactively managed and
prevented while lowering these healthcare costs. We have mined a sample of ten
million customers&#x27; 360-degree data representing the state of Texas, USA, with
attributes current as of late 2018. The sample received from a market research
data vendor has over 1000 customer attributes consisting of demography,
lifestyle, and in some cases self-reported chronic conditions. In this study,
we have developed a classification model to predict chronic diabetes with an
accuracy of 80%. We demonstrate a use case where a large volume of 360-degree
customer data can be useful to predict and hence proactively prevent chronic
diseases such as diabetes.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust fine-tuning of zero-shot models. (arXiv:2109.01903v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01903">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Large pre-trained models such as CLIP offer consistent accuracy across a
range of data distributions when performing zero-shot inference (i.e., without
fine-tuning on a specific dataset). Although existing fine-tuning approaches
substantially improve accuracy in-distribution, they also reduce
out-of-distribution robustness. We address this tension by introducing a simple
and effective method for improving robustness: ensembling the weights of the
zero-shot and fine-tuned models. Compared to standard fine-tuning, the
resulting weight-space ensembles provide large accuracy improvements
out-of-distribution, while matching or improving in-distribution accuracy. On
ImageNet and five derived distribution shifts, weight-space ensembles improve
out-of-distribution accuracy by 2 to 10 percentage points while increasing
in-distribution accuracy by nearly 1 percentage point relative to standard
fine-tuning. These improvements come at no additional computational cost during
fine-tuning or inference.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Efficient On-Chip Learning for Optical Neural Networks Through Power-Aware Sparse Zeroth-Order Optimization. (arXiv:2012.11148v3 [cs.ET] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.11148">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Optical neural networks (ONNs) have demonstrated record-breaking potential in
high-performance neuromorphic computing due to their ultra-high execution speed
and low energy consumption. However, current learning protocols fail to provide
scalable and efficient solutions to photonic circuit optimization in practical
applications. In this work, we propose a novel on-chip learning framework to
release the full potential of ONNs for power-efficient in situ training.
Instead of deploying implementation-costly back-propagation, we directly
optimize the device configurations with computation budgets and power
constraints. We are the first to model the ONN on-chip learning as a
resource-constrained stochastic noisy zeroth-order optimization problem, and
propose a novel mixed-training strategy with two-level sparsity and power-aware
dynamic pruning to offer a scalable on-chip training solution in practical ONN
deployment. Compared with previous methods, we are the first to optimize over
2,500 optical components on chip. We can achieve much better optimization
stability, 3.7x-7.6x higher efficiency, and save &gt;90% power under practical
device variations and thermal crosstalk.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Critical Review of the state-of-the-art on Deep Neural Networks for Blood Glucose Prediction in Patients with Diabetes. (arXiv:2109.02178v1 [q-bio.QM])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02178">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This article compares ten recently proposed neural networks and proposes two
ensemble neural network-based models for blood glucose prediction. All of them
are tested under the same dataset, preprocessing workflow, and tools using the
OhioT1DM Dataset at three different prediction horizons: 30, 60, and 120
minutes. We compare their performance using the most common metrics in blood
glucose prediction and rank the best-performing ones using three methods
devised for the statistical comparison of the performance of multiple
algorithms: scmamp, model confidence set, and superior predictive ability. Our
analysis highlights those models with the highest probability of being the best
predictors, estimates the increase in error of the models that perform more
poorly with respect to the best ones, and provides a guide for their use in
clinical practice.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Bayesian Networks Under Sparsity Constraints: A Parameterized Complexity Analysis. (arXiv:2004.14724v3 [cs.DS] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14724">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study the problem of learning the structure of an optimal Bayesian network
when additional constraints are posed on the network or on its moralized graph.
More precisely, we consider the constraint that the network or its moralized
graph are close, in terms of vertex or edge deletions, to a sparse graph class
$\Pi$. For example, we show that learning an optimal network whose moralized
graph has vertex deletion distance at most $k$ from a graph with maximum degree
1 can be computed in polynomial time when $k$ is constant. This extends
previous work that gave an algorithm with such a running time for the vertex
deletion distance to edgeless graphs [Korhonen &amp; Parviainen, NIPS 2015]. We
then show that further extensions or improvements are presumably impossible.
For example, we show that learning optimal networks where the network or its
moralized graph have maximum degree $2$ or connected components of size at most
$c$, $c\ge 3$, is NP-hard. Finally, we show that learning an optimal network
with at most $k$ edges in the moralized graph presumably has no $f(k)\cdot
|I|^{O(1)}$-time algorithm and that, in contrast, an optimal network with at
most $k$ arcs can be computed in $2^{O(k)}\cdot |I|^{O(1)}$ time where $|I|$ is
the total input size.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Toward Multidiversified Ensemble Clustering of High-Dimensional Data: From Subspaces to Metrics and Beyond. (arXiv:1710.03113v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1710.03113">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid emergence of high-dimensional data in various areas has brought new
challenges to current ensemble clustering research. To deal with the curse of
dimensionality, recently considerable efforts in ensemble clustering have been
made by means of different subspace-based techniques. However, besides the
emphasis on subspaces, rather limited attention has been paid to the potential
diversity in similarity/dissimilarity metrics. It remains a surprisingly open
problem in ensemble clustering how to create and aggregate a large population
of diversified metrics, and furthermore, how to jointly investigate the
multi-level diversity in the large populations of metrics, subspaces, and
clusters in a unified framework. To tackle this problem, this paper proposes a
novel multidiversified ensemble clustering approach. In particular, we create a
large number of diversified metrics by randomizing a scaled exponential
similarity kernel, which are then coupled with random subspaces to form a large
set of metric-subspace pairs. Based on the similarity matrices derived from
these metric-subspace pairs, an ensemble of diversified base clusterings can
thereby be constructed. Further, an entropy-based criterion is utilized to
explore the cluster-wise diversity in ensembles, based on which three specific
ensemble clustering algorithms are presented by incorporating three types of
consensus functions. Extensive experiments are conducted on 30 high-dimensional
datasets, including 18 cancer gene expression datasets and 12 image/speech
datasets, which demonstrate the superiority of our algorithms over the
state-of-the-art. The source code is available at
https://github.com/huangdonghere/MDEC.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">An Open-set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments. (arXiv:2002.11561v7 [cs.SD] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11561">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The problem of training with a small set of positive samples is known as
few-shot learning (FSL). It is widely known that traditional deep learning (DL)
algorithms usually show very good performance when trained with large datasets.
However, in many applications, it is not possible to obtain such a high number
of samples. In the image domain, typical FSL applications include those related
to face recognition. In the audio domain, music fraud or speaker recognition
can be clearly benefited from FSL methods. This paper deals with the
application of FSL to the detection of specific and intentional acoustic events
given by different types of sound alarms, such as door bells or fire alarms,
using a limited number of samples. These sounds typically occur in domestic
environments where many events corresponding to a wide variety of sound classes
take place. Therefore, the detection of such alarms in a practical scenario can
be considered an open-set recognition (OSR) problem. To address the lack of a
dedicated public dataset for audio FSL, researchers usually make modifications
on other available datasets. This paper is aimed at poviding the audio
recognition community with a carefully annotated dataset
(https://zenodo.org/record/3689288) for FSL in an OSR context comprised of 1360
clips from 34 classes divided into pattern sounds} and unwanted sounds. To
facilitate and promote research on this area, results with state-of-the-art
baseline systems based on transfer learning are also presented.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Transformer-based Model to Detect Phishing URLs. (arXiv:2109.02138v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02138">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Phishing attacks are among emerging security issues that recently draws
significant attention in the cyber security community. There are numerous
existing approaches for phishing URL detection. However, malicious URL
detection is still a research hotspot because attackers can bypass newly
introduced detection mechanisms by changing their tactics. This paper will
introduce a transformer-based malicious URL detection model, which has
significant accuracy and outperforms current detection methods. We conduct
experiments and compare them with six existing classical detection models.
Experiments demonstrate that our transformer-based model is the best performing
model from all perspectives among the seven models and achieves 97.3 % of
detection accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards high-accuracy deep learning inference of compressible turbulent flows over aerofoils. (arXiv:2109.02183v1 [physics.flu-dyn])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02183">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The present study investigates the accurate inference of Reynolds-averaged
Navier-Stokes solutions for the compressible flow over aerofoils in two
dimensions with a deep neural network. Our approach yields networks that learn
to generate precise flow fields for varying body-fitted, structured grids by
providing them with an encoding of the corresponding mapping to a canonical
space for the solutions. We apply the deep neural network model to a benchmark
case of incompressible flow at randomly given angles of attack and Reynolds
numbers and achieve an improvement of more than an order of magnitude compared
to previous work. Further, for transonic flow cases, the deep neural network
model accurately predicts complex flow behaviour at high Reynolds numbers, such
as shock wave/boundary layer interaction, and quantitative distributions like
pressure coefficient, skin friction coefficient as well as wake total pressure
profiles downstream of aerofoils. The proposed deep learning method
significantly speeds up the predictions of flow fields and shows promise for
enabling fast aerodynamic designs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Event-Based Communication in Multi-Agent Distributed Q-Learning. (arXiv:2109.01417v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01417">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present in this work an approach to reduce the communication of
information needed on a multi-agent learning system inspired by Event Triggered
Control (ETC) techniques. We consider a baseline scenario of a distributed
Q-learning problem on a Markov Decision Process (MDP). Following an event-based
approach, N agents explore the MDP and communicate experiences to a central
learner only when necessary, which performs updates of the actor Q functions.
We analyse the convergence guarantees retained with respect to a regular
Q-learning algorithm, and present experimental results showing that event-based
communication results in a substantial reduction of data transmission rates in
such distributed systems. Additionally, we discuss what effects (desired and
undesired) these event-based approaches have on the learning processes studied,
and how they can be applied to more complex multi-agent learning systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Optimal transport weights for causal inference. (arXiv:2109.01991v1 [stat.ME])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01991">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Weighting methods are a common tool to de-bias estimates of causal effects.
And though there are an increasing number of seemingly disparate methods, many
of them can be folded into one unifying regime: causal optimal transport. This
new method directly targets distributional balance by minimizing optimal
transport distances between treatment and control groups or, more generally,
between a source and target population. Our approach is model-free but can also
incorporate moments or any other important functions of covariates that the
researcher desires to balance. We find that the causal optimal transport
outperforms competitor methods when both the propensity score and outcome
models are misspecified, indicating it is a robust alternative to common
weighting methods. Finally, we demonstrate the utility of our method in an
external control study examining the effect of misoprostol versus oxytocin for
treatment of post-partum hemorrhage.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Variational Physics Informed Neural Networks: the role of quadratures and test functions. (arXiv:2109.02035v1 [math.NA])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02035">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work we analyze how Gaussian or Newton-Cotes quadrature rules of
different precisions and piecewise polynomial test functions of different
degrees affect the convergence rate of Variational Physics Informed Neural
Networks (VPINN) with respect to mesh refinement, while solving elliptic
boundary-value problems. Using a Petrov-Galerkin framework relying on an
inf-sup condition, we derive an a priori error estimate in the energy norm
between the exact solution and a suitable high-order piecewise interpolant of a
computed neural network. Numerical experiments confirm the theoretical
predictions, and also indicate that the error decay follows the same behavior
when the neural network is not interpolated. Our results suggest, somehow
counterintuitively, that for smooth solutions the best strategy to achieve a
high decay rate of the error consists in choosing test functions of the lowest
polynomial degree, while using quadrature formulas of suitably high precision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fair Federated Learning for Heterogeneous Face Data. (arXiv:2109.02351v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the problem of achieving fair classification in Federated
Learning (FL) under data heterogeneity. Most of the approaches proposed for
fair classification require diverse data that represent the different
demographic groups involved. In contrast, it is common for each client to own
data that represents only a single demographic group. Hence the existing
approaches cannot be adopted for fair classification models at the client
level. To resolve this challenge, we propose several aggregation techniques. We
empirically validate these techniques by comparing the resulting fairness
metrics and accuracy on CelebA, UTK, and FairFace datasets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Anomaly Detection on IT Operation Series via Online Matrix Profile. (arXiv:2108.12093v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12093">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Anomaly detection on time series is a fundamental task in monitoring the Key
Performance Indicators (KPIs) of IT systems. Many of the existing approaches in
the literature show good performance while requiring a lot of training
resources. In this paper, the online matrix profile, which requires no
training, is proposed to address this issue. The anomalies are detected by
referring to the past subsequence that is the closest to the current one. The
distance significance is introduced based on the online matrix profile, which
demonstrates a prominent pattern when an anomaly occurs. Another training-free
approach spectral residual is integrated into our approach to further enhance
the detection accuracy. Moreover, the proposed approach is sped up by at least
four times for long time series by the introduced cache strategy. In comparison
to the existing approaches, the online matrix profile makes a good trade-off
between accuracy and efficiency. More importantly, it is generic to various
types of time series in the sense that it works without the constraint from any
trained model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Transformer Networks for Data Augmentation of Human Physical Activity Recognition. (arXiv:2109.01081v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01081">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Data augmentation is a widely used technique in classification to increase
data used in training. It improves generalization and reduces amount of
annotated human activity data needed for training which reduces labour and time
needed with the dataset. Sensor time-series data, unlike images, cannot be
augmented by computationally simple transformation algorithms. State of the art
models like Recurrent Generative Adversarial Networks (RGAN) are used to
generate realistic synthetic data. In this paper, transformer based generative
adversarial networks which have global attention on data, are compared on
PAMAP2 and Real World Human Activity Recognition data sets with RGAN. The newer
approach provides improvements in time and savings in computational resources
needed for data augmentation than previous approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hindsight Reward Tweaking via Conditional Deep Reinforcement Learning. (arXiv:2109.02332v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02332">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Designing optimal reward functions has been desired but extremely difficult
in reinforcement learning (RL). When it comes to modern complex tasks,
sophisticated reward functions are widely used to simplify policy learning yet
even a tiny adjustment on them is expensive to evaluate due to the drastically
increasing cost of training. To this end, we propose a hindsight reward
tweaking approach by designing a novel paradigm for deep reinforcement learning
to model the influences of reward functions within a near-optimal space. We
simply extend the input observation with a condition vector linearly correlated
with the effective environment reward parameters and train the model in a
conventional manner except for randomizing reward configurations, obtaining a
hyper-policy whose characteristics are sensitively regulated over the condition
space. We demonstrate the feasibility of this approach and study one of its
potential application in policy performance boosting with multiple MuJoCo
tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v3 [cs.IR] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08978">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models. We use a multi-objective approach to introduce
temporal context into traditionally time-unaware recommender systems and
confirm its advantage via the proposed evaluation protocol. Finally, we
validate that the Pareto Fronts obtained with the added objective dominate
those produced by state-of-the-art models that are only optimized for accuracy
on three real-world publicly available datasets. The results show that
including our temporal objective can improve recall@20 by up to 20%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Inferring feature importance with uncertainties in high-dimensional data. (arXiv:2109.00855v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00855">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Estimating feature importance is a significant aspect of explaining
data-based models. Besides explaining the model itself, an equally relevant
question is which features are important in the underlying data generating
process. We present a Shapley value based framework for inferring the
importance of individual features, including uncertainty in the estimator. We
build upon the recently published feature importance measure of SAGE (Shapley
additive global importance) and introduce sub-SAGE which can be estimated
without resampling for tree-based models. We argue that the uncertainties can
be estimated from bootstrapping and demonstrate the approach for tree ensemble
methods. The framework is exemplified on synthetic data as well as
high-dimensional genomics data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">(M)SLAe-Net: Multi-Scale Multi-Level Attention embedded Network for Retinal Vessel Segmentation. (arXiv:2109.02084v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02084">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Segmentation plays a crucial role in diagnosis. Studying the retinal
vasculatures from fundus images help identify early signs of many crucial
illnesses such as diabetic retinopathy. Due to the varying shape, size, and
patterns of retinal vessels, along with artefacts and noises in fundus images,
no one-stage method can accurately segment retinal vessels. In this work, we
propose a multi-scale, multi-level attention embedded CNN architecture
((M)SLAe-Net) to address the issue of multi-stage processing for robust and
precise segmentation of retinal vessels. We do this by extracting features at
multiple scales and multiple levels of the network, enabling our model to
holistically extracts the local and global features. Multi-scale features are
extracted using our novel dynamic dilated pyramid pooling (D-DPP) module. We
also aggregate the features from all the network levels. These effectively
resolved the issues of varying shapes and artefacts and hence the need for
multiple stages. To assist in better pixel-level classification, we use the
Squeeze and Attention(SA) module, a smartly adapted version of the Squeeze and
Excitation(SE) module for segmentation tasks in our network to facilitate
pixel-group attention. Our unique network design and novel D-DPP module with
efficient task-specific loss function for thin vessels enabled our model for
better cross data performance. Exhaustive experimental results on DRIVE, STARE,
HRF, and CHASE-DB1 show the superiority of our method.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fairness via AI: Bias Reduction in Medical Information. (arXiv:2109.02202v1 [cs.AI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02202">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.

In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Impact and dynamics of hate and counter speech online. (arXiv:2009.08392v3 [cs.SI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.08392">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Citizen-generated counter speech is a promising way to fight hate speech and
promote peaceful, non-polarized discourse. However, there is a lack of
large-scale longitudinal studies of its effectiveness for reducing hate speech.
To this end, we perform an exploratory analysis of the effectiveness of counter
speech using several different macro- and micro-level measures to analyze
180,000 political conversations that took place on German Twitter over four
years. We report on the dynamic interactions of hate and counter speech over
time and provide insights into whether, as in &#x60;classic&#x27; bullying situations,
organized efforts are more effective than independent individuals in steering
online discourse. Taken together, our results build a multifaceted picture of
the dynamics of hate and counter speech online. While we make no causal claims
due to the complexity of discourse dynamics, our findings suggest that
organized hate speech is associated with changes in public discourse and that
counter speech -- especially when organized -- may help curb hateful rhetoric
in online discourse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">High-Dimensional Sparse Linear Bandits. (arXiv:2011.04020v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.04020">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Stochastic linear bandits with high-dimensional sparse features are a
practical model for a variety of domains, including personalized medicine and
online advertising. We derive a novel $\Omega(n^{2/3})$ dimension-free minimax
regret lower bound for sparse linear bandits in the data-poor regime where the
horizon is smaller than the ambient dimension and where the feature vectors
admit a well-conditioned exploration distribution. This is complemented by a
nearly matching upper bound for an explore-then-commit algorithm showing that
that $\Theta(n^{2/3})$ is the optimal rate in the data-poor regime. The results
complement existing bounds for the data-rich regime and provide another example
where carefully balancing the trade-off between information and regret is
necessary. Finally, we prove a dimension-free $O(\sqrt{n})$ regret upper bound
under an additional assumption on the magnitude of the signal for relevant
features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">BEAUTY Powered BEAST. (arXiv:2103.00674v3 [stat.ME] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00674">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We study nonparametric dependence detection with the proposed binary
expansion approximation of uniformity (BEAUTY) approach, which generalizes the
celebrated Euler&#x27;s formula, and approximates the characteristic function of any
copula with a linear combination of expectations of binary interactions from
marginal binary expansions. This novel theory enables a unification of many
important tests through approximations from some quadratic forms of symmetry
statistics, where the deterministic weight matrix characterizes the power
properties of each test. To achieve a robust power, we study test statistics
with data-adaptive weights, referred to as the binary expansion adaptive
symmetry test (BEAST). By utilizing the properties of the binary expansion
filtration, we show that the Neyman-Pearson test of uniformity can be
approximated by an oracle weighted sum of symmetry statistics. The BEAST with
this oracle provides a benchmark of feasible power against any alternative by
leading all existing tests with a substantial margin. To approach this oracle
power, we develop the BEAST through a regularized resampling approximation of
the oracle test. The BEAST improves the empirical power of many existing tests
against a wide spectrum of common alternatives while providing clear
interpretation of the form of dependency upon rejection.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online mirror descent and dual averaging: keeping pace in the dynamic case. (arXiv:2006.02585v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.02585">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online mirror descent (OMD) and dual averaging (DA) -- two fundamental
algorithms for online convex optimization -- are known to have very similar
(and sometimes identical) performance guarantees when used with a fixed
learning rate. Under dynamic learning rates, however, OMD is provably inferior
to DA and suffers a linear regret, even in common settings such as prediction
with expert advice. We modify the OMD algorithm through a simple technique that
we call stabilization. We give essentially the same abstract regret bound for
OMD with stabilization and for DA by modifying the classical OMD convergence
analysis in a careful and modular way that allows for straightforward and
flexible proofs. Simple corollaries of these bounds show that OMD with
stabilization and DA enjoy the same performance guarantees in many applications
-- even under dynamic learning rates. We also shed light on the similarities
between OMD and DA and show simple conditions under which stabilized-OMD and DA
generate the same iterates.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">From Static to Dynamic Prediction: Wildfire Risk Assessment Based on Multiple Environmental Factors. (arXiv:2103.10901v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.10901">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Wildfire is one of the biggest disasters that frequently occurs on the west
coast of the United States. Many efforts have been made to understand the
causes of the increases in wildfire intensity and frequency in recent years. In
this work, we propose static and dynamic prediction models to analyze and
assess the areas with high wildfire risks in California by utilizing a
multitude of environmental data including population density, Normalized
Difference Vegetation Index (NDVI), Palmer Drought Severity Index (PDSI), tree
mortality area, tree mortality number, and altitude. Moreover, we focus on a
better understanding of the impacts of different factors so as to inform
preventive actions. To validate our models and findings, we divide the land of
California into 4,242 grids of 0.1 degrees $\times$ 0.1 degrees in latitude and
longitude, and compute the risk of each grid based on spatial and temporal
conditions. To verify the generalizability of our models, we further expand the
scope of wildfire risk assessment from California to Washington without any
fine tuning. By performing counterfactual analysis, we uncover the effects of
several possible methods on reducing the number of high risk wildfires. Taken
together, our study has the potential to estimate, monitor, and reduce the
risks of wildfires across diverse areas provided that such environment data is
available.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Visual Recognition with Deep Learning from Biased Image Datasets. (arXiv:2109.02357v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02357">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In practice, and more especially when training deep neural networks, visual
recognition rules are often learned based on various sources of information. On
the other hand, the recent deployment of facial recognition systems with uneven
predictive performances on different population segments highlights the
representativeness issues possibly induced by a naive aggregation of image
datasets. Indeed, sampling bias does not vanish simply by considering larger
datasets, and ignoring its impact may completely jeopardize the generalization
capacity of the learned prediction rules. In this paper, we show how biasing
models, originally introduced for nonparametric estimation in (Gill et al.,
1988), and recently revisited from the perspective of statistical learning
theory in (Laforgue and Cl\&#x27;emen\c{c}on, 2019), can be applied to remedy these
problems in the context of visual recognition. Based on the (approximate)
knowledge of the biasing mechanisms at work, our approach consists in
reweighting the observations, so as to form a nearly debiased estimator of the
target distribution. One key condition for our method to be theoretically valid
is that the supports of the distributions generating the biased datasets at
disposal must overlap, and cover the support of the target distribution. In
order to meet this requirement in practice, we propose to use a low dimensional
image representation, shared across the image databases. Finally, we provide
numerical experiments highlighting the relevance of our approach whenever the
biasing functions are appropriately chosen.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Model Compression. (arXiv:2105.10059v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.10059">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With time, machine learning models have increased in their scope,
functionality and size. Consequently, the increased functionality and size of
such models requires high-end hardware to both train and provide inference
after the fact. This paper aims to explore the possibilities within the domain
of model compression, discuss the efficiency of combining various levels of
pruning and quantization, while proposing a quality measurement metric to
objectively decide which combination is best in terms of minimizing the
accuracy delta and maximizing the size reduction factor.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Robust Importance Sampling for Error Estimation in the Context of Optimal Bayesian Transfer Learning. (arXiv:2109.02150v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02150">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Classification has been a major task for building intelligent systems as it
enables decision-making under uncertainty. Classifier design aims at building
models from training data for representing feature-label distributions--either
explicitly or implicitly. In many scientific or clinical settings, training
data are typically limited, which makes designing accurate classifiers and
evaluating their classification error extremely challenging. While transfer
learning (TL) can alleviate this issue by incorporating data from relevant
source domains to improve learning in a different target domain, it has
received little attention for performance assessment, notably in error
estimation. In this paper, we fill this gap by investigating knowledge
transferability in the context of classification error estimation within a
Bayesian paradigm. We introduce a novel class of Bayesian minimum mean-square
error (MMSE) estimators for optimal Bayesian transfer learning (OBTL), which
enables rigorous evaluation of classification error under uncertainty in a
small-sample setting. Using Monte Carlo importance sampling, we employ the
proposed estimator to evaluate the classification accuracy of a broad family of
classifiers that span diverse learning capabilities. Experimental results based
on both synthetic data as well as real-world RNA sequencing (RNA-seq) data show
that our proposed OBTL error estimation scheme clearly outperforms standard
error estimators, especially in a small-sample setting, by tapping into the
data from other relevant domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards Memory-Efficient Neural Networks via Multi-Level in situ Generation. (arXiv:2108.11430v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11430">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural networks (DNN) have shown superior performance in a variety of
tasks. As they rapidly evolve, their escalating computation and memory demands
make it challenging to deploy them on resource-constrained edge devices. Though
extensive efficient accelerator designs, from traditional electronics to
emerging photonics, have been successfully demonstrated, they are still
bottlenecked by expensive memory accesses due to tremendous gaps between the
bandwidth/power/latency of electrical memory and computing cores. Previous
solutions fail to fully-leverage the ultra-fast computational speed of emerging
DNN accelerators to break through the critical memory bound. In this work, we
propose a general and unified framework to trade expensive memory transactions
with ultra-fast on-chip computations, directly translating to performance
improvement. We are the first to jointly explore the intrinsic correlations and
bit-level redundancy within DNN kernels and propose a multi-level in situ
generation mechanism with mixed-precision bases to achieve on-the-fly recovery
of high-resolution parameters with minimum hardware overhead. Extensive
experiments demonstrate that our proposed joint method can boost the memory
efficiency by 10-20x with comparable accuracy over four state-of-the-art
designs, when benchmarked on ResNet-18/DenseNet-121/MobileNetV2/V3 with various
tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.09381">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data science and Machine learning in the Clouds: A Perspective for the Future. (arXiv:2109.01661v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01661">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>As we are fast approaching the beginning of a paradigm shift in the field of
science, Data driven science (the so called fourth science paradigm) is going
to be the driving force in research and innovation. From medicine to
biodiversity and astronomy to geology, all these terms are somehow going to be
affected by this paradigm shift. The huge amount of data to be processed under
this new paradigm will be a major concern in the future and one will strongly
require cloud based services in all the aspects of these computations (from
storage to compute and other services). Another aspect will be energy
consumption and performance of prediction jobs and tasks within such a
scientific paradigm which will change the way one sees computation. Data
science has heavily impacted or rather triggered the emergence of Machine
Learning, Signal/Image/Video processing related algorithms, Artificial
intelligence, Robotics, health informatics, geoinformatics, and many more such
areas of interest. Hence, we envisage an era where Data science can deliver its
promises with the help of the existing cloud based platforms and services with
the addition of new services. In this article, we discuss about data driven
science and Machine learning and how they are going to be linked through cloud
based services in the future. It also discusses the rise of paradigms like
approximate computing, quantum computing and many more in recent times and
their applicability in big data processing, data science, analytics, prediction
and machine learning in the cloud environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Meta-Surrogate Model for Transferable Adversarial Attack. (arXiv:2109.01983v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01983">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>We consider adversarial attacks to a black-box model when no queries are
allowed. In this setting, many methods directly attack surrogate models and
transfer the obtained adversarial examples to fool the target model. Plenty of
previous works investigated what kind of attacks to the surrogate model can
generate more transferable adversarial examples, but their performances are
still limited due to the mismatches between surrogate models and the target
model. In this paper, we tackle this problem from a novel angle -- instead of
using the original surrogate models, can we obtain a Meta-Surrogate Model (MSM)
such that attacks to this model can be easier transferred to other models? We
show that this goal can be mathematically formulated as a well-posed
(bi-level-like) optimization problem and design a differentiable attacker to
make training feasible. Given one or a set of surrogate models, our method can
thus obtain an MSM such that adversarial examples generated on MSM enjoy
eximious transferability. Comprehensive experiments on Cifar-10 and ImageNet
demonstrate that by attacking the MSM, we can obtain stronger transferable
adversarial examples to fool black-box models including adversarially trained
ones, with much higher success rates than existing methods. The proposed method
reveals significant security challenges of deep models and is promising to be
served as a state-of-the-art benchmark for evaluating the robustness of deep
models in the black-box setting.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">How Reliable Are Out-of-Distribution Generalization Methods for Medical Image Segmentation?. (arXiv:2109.01668v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01668">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The recent achievements of Deep Learning rely on the test data being similar
in distribution to the training data. In an ideal case, Deep Learning models
would achieve Out-of-Distribution (OoD) Generalization, i.e. reliably make
predictions on out-of-distribution data. Yet in practice, models usually fail
to generalize well when facing a shift in distribution. Several methods were
thereby designed to improve the robustness of the features learned by a model
through Regularization- or Domain-Prediction-based schemes. Segmenting medical
images such as MRIs of the hippocampus is essential for the diagnosis and
treatment of neuropsychiatric disorders. But these brain images often suffer
from distribution shift due to the patient&#x27;s age and various pathologies
affecting the shape of the organ. In this work, we evaluate OoD Generalization
solutions for the problem of hippocampus segmentation in MR data using both
fully- and semi-supervised training. We find that no method performs reliably
in all experiments. Only the V-REx loss stands out as it remains easy to tune,
while it outperforms a standard U-Net in most cases.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pointspectrum: Equivariance Meets Laplacian Filtering for Graph Representation Learning. (arXiv:2109.02358v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02358">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph Representation Learning (GRL) has become essential for modern graph
data mining and learning tasks. GRL aims to capture the graph&#x27;s structural
information and exploit it in combination with node and edge attributes to
compute low-dimensional representations. While Graph Neural Networks (GNNs)
have been used in state-of-the-art GRL architectures, they have been shown to
suffer from over smoothing when many GNN layers need to be stacked. In a
different GRL approach, spectral methods based on graph filtering have emerged
addressing over smoothing; however, up to now, they employ traditional neural
networks that cannot efficiently exploit the structure of graph data. Motivated
by this, we propose PointSpectrum, a spectral method that incorporates a set
equivariant network to account for a graph&#x27;s structure. PointSpectrum enhances
the efficiency and expressiveness of spectral methods, while it outperforms or
competes with state-of-the-art GRL methods. Overall, PointSpectrum addresses
over smoothing by employing a graph filter and captures a graph&#x27;s structure
through set equivariance, lying on the intersection of GNNs and spectral
methods. Our findings are promising for the benefits and applicability of this
architectural shift for spectral methods and GRL.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">RAMA: A Rapid Multicut Algorithm on GPU. (arXiv:2109.01838v1 [cs.DC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01838">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.
correlation clustering) problem, a classical graph clustering problem widely
used in machine learning and computer vision. Our algorithm consists of three
steps executed recursively: (1) Finding conflicted cycles that correspond to
violated inequalities of the underlying multicut relaxation, (2) Performing
message passing between the edges and cycles to optimize the Lagrange
relaxation coming from the found violated cycles producing reduced costs and
(3) Contracting edges with high reduced costs through matrix-matrix
multiplications.

Our algorithm produces primal solutions and dual lower bounds that estimate
the distance to optimum. We implement our algorithm on GPUs and show resulting
one to two order-of-magnitudes improvements in execution speed without
sacrificing solution quality compared to traditional serial algorithms that run
on CPUs. We can solve very large scale benchmark problems with up to
$\mathcal{O}(10^8)$ variables in a few seconds with small primal-dual gaps. We
make our code available at https://github.com/pawelswoboda/RAMA.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tolerating Adversarial Attacks and Byzantine Faults in Distributed Machine Learning. (arXiv:2109.02018v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02018">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks attempt to disrupt the training, retraining and utilizing
of artificial intelligence and machine learning models in large-scale
distributed machine learning systems. This causes security risks on its
prediction outcome. For example, attackers attempt to poison the model by
either presenting inaccurate misrepresentative data or altering the models&#x27;
parameters. In addition, Byzantine faults including software, hardware, network
issues occur in distributed systems which also lead to a negative impact on the
prediction outcome. In this paper, we propose a novel distributed training
algorithm, partial synchronous stochastic gradient descent (ParSGD), which
defends adversarial attacks and/or tolerates Byzantine faults. We demonstrate
the effectiveness of our algorithm under three common adversarial attacks again
the ML models and a Byzantine fault during the training phase. Our results show
that using ParSGD, ML models can still produce accurate predictions as if it is
not being attacked nor having failures at all when almost half of the nodes are
being compromised or failed. We will report the experimental evaluations of
ParSGD in comparison with other algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01745">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic raises the problem of adapting face recognition systems
to the new reality, where people may wear surgical masks to cover their noses
and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for
training these systems were released before the pandemic, so they now seem
unsuited due to the lack of examples of people wearing masks. We propose a
method for enhancing data sets containing faces without masks by creating
synthetic masks and overlaying them on faces in the original images. Our method
relies on Spark AR Studio, a developer program made by Facebook that is used to
create Instagram face filters. In our approach, we use 9 masks of different
colors, shapes and fabrics. We employ our method to generate a number of
445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254
(96.8%) masks for the CelebA data set, releasing the mask images at
https://github.com/securifai/masked_faces. We show that our method produces
significantly more realistic training examples of masks overlaid on faces by
asking volunteers to qualitatively compare it to other methods or data sets
designed for the same task. We also demonstrate the usefulness of our method by
evaluating state-of-the-art face recognition systems (FaceNet, VGG-face,
ArcFace) trained on the enhanced data sets and showing that they outperform
equivalent systems trained on the original data sets (containing faces without
masks), when the test benchmark contains masked faces.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Automatic Online Multi-Source Domain Adaptation. (arXiv:2109.01996v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01996">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Knowledge transfer across several streaming processes remain challenging
problem not only because of different distributions of each stream but also
because of rapidly changing and never-ending environments of data streams.
Albeit growing research achievements in this area, most of existing works are
developed for a single source domain which limits its resilience to exploit
multi-source domains being beneficial to recover from concept drifts quickly
and to avoid the negative transfer problem. An online domain adaptation
technique under multisource streaming processes, namely automatic online
multi-source domain adaptation (AOMSDA), is proposed in this paper. The online
domain adaptation strategy of AOMSDA is formulated under a coupled generative
and discriminative approach of denoising autoencoder (DAE) where the central
moment discrepancy (CMD)-based regularizer is integrated to handle the
existence of multi-source domains thereby taking advantage of complementary
information sources. The asynchronous concept drifts taking place at different
time periods are addressed by a self-organizing structure and a node
re-weighting strategy. Our numerical study demonstrates that AOMSDA is capable
of outperforming its counterparts in 5 of 8 study cases while the ablation
study depicts the advantage of each learning component. In addition, AOMSDA is
general for any number of source streams. The source code of AOMSDA is shared
publicly in https://github.com/Renchunzi-Xie/AOMSDA.git.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multimodal Detection of COVID-19 Symptoms using Deep Learning &amp; Probability-based Weighting of Modes. (arXiv:2109.01669v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01669">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic is one of the most challenging healthcare crises during
the 21st century. As the virus continues to spread on a global scale, the
majority of efforts have been on the development of vaccines and the mass
immunization of the public. While the daily case numbers were following a
decreasing trend, the emergent of new virus mutations and variants still pose a
significant threat. As economies start recovering and societies start opening
up with people going back into office buildings, schools, and malls, we still
need to have the ability to detect and minimize the spread of COVID-19.
Individuals with COVID-19 may show multiple symptoms such as cough, fever, and
shortness of breath. Many of the existing detection techniques focus on
symptoms having the same equal importance. However, it has been shown that some
symptoms are more prevalent than others. In this paper, we present a multimodal
method to predict COVID-19 by incorporating existing deep learning classifiers
using convolutional neural networks and our novel probability-based weighting
function that considers the prevalence of each symptom. The experiments were
performed on an existing dataset with respect to the three considered modes of
coughs, fever, and shortness of breath. The results show considerable
improvements in the detection of COVID-19 using our weighting function when
compared to an equal weighting function.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">VARGAN: Variance Enforcing Network Enhanced GAN. (arXiv:2109.02117v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02117">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative adversarial networks (GANs) are one of the most widely used
generative models. GANs can learn complex multi-modal distributions, and
generate real-like samples. Despite the major success of GANs in generating
synthetic data, they might suffer from unstable training process, and mode
collapse. In this paper, we introduce a new GAN architecture called variance
enforcing GAN (VARGAN), which incorporates a third network to introduce
diversity in the generated samples. The third network measures the diversity of
the generated samples, which is used to penalize the generator&#x27;s loss for low
diversity samples. The network is trained on the available training data and
undesired distributions with limited modality. On a set of synthetic and
real-world image data, VARGAN generates a more diverse set of samples compared
to the recent state-of-the-art models. High diversity and low computational
complexity, as well as fast convergence, make VARGAN a promising model to
alleviate mode collapse.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">What Do Compressed Deep Neural Networks Forget?. (arXiv:1911.05248v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.05248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Deep neural network pruning and quantization techniques have demonstrated it
is possible to achieve high levels of compression with surprisingly little
degradation to test set accuracy. However, this measure of performance conceals
significant differences in how different classes and images are impacted by
model compression techniques. We find that models with radically different
numbers of weights have comparable top-line performance metrics but diverge
considerably in behavior on a narrow subset of the dataset. This small subset
of data points, which we term Pruning Identified Exemplars (PIEs) are
systematically more impacted by the introduction of sparsity. Compression
disproportionately impacts model performance on the underrepresented long-tail
of the data distribution. PIEs over-index on atypical or noisy images that are
far more challenging for both humans and algorithms to classify. Our work
provides intuition into the role of capacity in deep neural networks and the
trade-offs incurred by compression. An understanding of this disparate impact
is critical given the widespread deployment of compressed models in the wild.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Neural Networks with ReLU-Sine-Exponential Activations Break Curse of Dimensionality on H\&quot;older Class. (arXiv:2103.00542v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00542">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we construct neural networks with ReLU, sine and $2^x$ as
activation functions. For general continuous $f$ defined on $[0,1]^d$ with
continuity modulus $\omega_f(\cdot)$, we construct ReLU-sine-$2^x$ networks
that enjoy an approximation rate
$\mathcal{O}(\omega_f(\sqrt{d})\cdot2^{-M}+\omega_{f}\left(\frac{\sqrt{d}}{N}\right))$,
where $M,N\in \mathbb{N}^{+}$ denote the hyperparameters related to widths of
the networks. As a consequence, we can construct ReLU-sine-$2^x$ network with
the depth $5$ and width
$\max\left\{\left\lceil2d^{3/2}\left(\frac{3\mu}{\epsilon}\right)^{1/{\alpha}}\right\rceil,2\left\lceil\log_2\frac{3\mu
d^{\alpha/2}}{2\epsilon}\right\rceil+2\right\}$ that approximates $f\in
\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$ within a given tolerance $\epsilon &gt;0$
measured in $L^p$ norm $p\in[1,\infty)$, where
$\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$ denotes the H\&quot;older continuous function
class defined on $[0,1]^d$ with order $\alpha \in (0,1]$ and constant $\mu &gt;
0$. Therefore, the ReLU-sine-$2^x$ networks overcome the curse of
dimensionality on $\mathcal{H}_{\mu}^{\alpha}([0,1]^d)$. In addition to its
supper expressive power, functions implemented by ReLU-sine-$2^x$ networks are
(generalized) differentiable, enabling us to apply SGD to train.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cohort Characteristics and Factors Associated with Cannabis Use among Adolescents in Canada Using Pattern Discovery and Disentanglement Method. (arXiv:2109.01739v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01739">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>COMPASS is a longitudinal, prospective cohort study collecting data annually
from students attending high school in jurisdictions across Canada. We aimed to
discover significant frequent/rare associations of behavioral factors among
Canadian adolescents related to cannabis use. We use a subset of COMPASS
dataset which contains 18,761 records of students in grades 9 to 12 with 31
selected features (attributes) involving various characteristics, from living
habits to academic performance. We then used the Pattern Discovery and
Disentanglement (PDD) algorithm that we have developed to detect strong and
rare (yet statistically significant) associations from the dataset. PDD used
the criteria derived from disentangled statistical spaces (known as
Re-projected Adjusted-Standardized Residual Vector Spaces, notated as RARV). It
outperformed methods using other criteria (i.e. support and confidence) popular
as reported in the literature. Association results showed that PDD can
discover: i) a smaller set of succinct significant associations in clusters;
ii) frequent and rare, yet significant, patterns supported by population health
relevant study; iii) patterns from a dataset with extremely imbalanced groups
(majority class: minority class &#x3D; 88.3%: 11.7%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Soft Hierarchical Graph Recurrent Networks for Many-Agent Partially Observable Environments. (arXiv:2109.02032v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02032">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The recent progress in multi-agent deep reinforcement learning(MADRL) makes
it more practical in real-world tasks, but its relatively poor scalability and
the partially observable constraints raise challenges to its performance and
deployment. Based on our intuitive observation that the human society could be
regarded as a large-scale partially observable environment, where each
individual has the function of communicating with neighbors and remembering its
own experience, we propose a novel network structure called hierarchical graph
recurrent network(HGRN) for multi-agent cooperation under partial
observability. Specifically, we construct the multi-agent system as a graph,
use the hierarchical graph attention network(HGAT) to achieve communication
between neighboring agents, and exploit GRU to enable agents to record
historical information. To encourage exploration and improve robustness, we
design a maximum-entropy learning method to learn stochastic policies of a
configurable target action entropy. Based on the above technologies, we
proposed a value-based MADRL algorithm called Soft-HGRN and its actor-critic
variant named SAC-HRGN. Experimental results based on three homogeneous tasks
and one heterogeneous environment not only show that our approach achieves
clear improvements compared with four baselines, but also demonstrates the
interpretability, scalability, and transferability of the proposed model.
Ablation studies prove the function and necessity of each component.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP). (arXiv:2010.16413v3 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.16413">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The COVID-19 pandemic has had a significant impact on society, both because
of the serious health effects of COVID-19 and because of public health measures
implemented to slow its spread. Many of these difficulties are fundamentally
information needs; attempts to address these needs have caused an information
overload for both researchers and the public. Natural language processing
(NLP), the branch of artificial intelligence that interprets human language,
can be applied to address many of the information needs made urgent by the
COVID-19 pandemic. This review surveys approximately 150 NLP studies and more
than 50 systems and datasets addressing the COVID-19 pandemic. We detail work
on four core NLP tasks: information retrieval, named entity recognition,
literature-based discovery, and question answering. We also describe work that
directly addresses aspects of the pandemic through four additional tasks: topic
modeling, sentiment and emotion analysis, caseload forecasting, and
misinformation detection. We conclude by discussing observable trends and
remaining challenges.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">ALLWAS: Active Learning on Language models in WASserstein space. (arXiv:2109.01691v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01691">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Active learning has emerged as a standard paradigm in areas with scarcity of
labeled training data, such as in the medical domain. Language models have
emerged as the prevalent choice of several natural language tasks due to the
performance boost offered by these models. However, in several domains, such as
medicine, the scarcity of labeled training data is a common issue. Also, these
models may not work well in cases where class imbalance is prevalent. Active
learning may prove helpful in these cases to boost the performance with a
limited label budget. To this end, we propose a novel method using sampling
techniques based on submodular optimization and optimal transport for active
learning in language models, dubbed ALLWAS. We construct a sampling strategy
based on submodular optimization of the designed objective in the gradient
domain. Furthermore, to enable learning from few samples, we propose a novel
strategy for sampling from the Wasserstein barycenters. Our empirical
evaluations on standard benchmark datasets for text classification show that
our methods perform significantly better (&gt;20% relative increase in some cases)
than existing approaches for active learning on language models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Communication Efficient Tensor Factorization for Decentralized Healthcare Networks. (arXiv:2109.01718v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01718">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Tensor factorization has been proved as an efficient unsupervised learning
approach for health data analysis, especially for computational phenotyping,
where the high-dimensional Electronic Health Records (EHRs) with patients
history of medical procedures, medications, diagnosis, lab tests, etc., are
converted to meaningful and interpretable medical concepts. Federated tensor
factorization distributes the tensor computation to multiple workers under the
coordination of a central server, which enables jointly learning the phenotypes
across multiple hospitals while preserving the privacy of the patient
information. However, existing federated tensor factorization algorithms
encounter the single-point-failure issue with the involvement of the central
server, which is not only easily exposed to external attacks, but also limits
the number of clients sharing information with the server under restricted
uplink bandwidth. In this paper, we propose CiderTF, a communication-efficient
decentralized generalized tensor factorization, which reduces the uplink
communication cost by leveraging a four-level communication reduction strategy
designed for a generalized tensor factorization, which has the flexibility of
modeling different tensor distribution with multiple kinds of loss functions.
Experiments on two real-world EHR datasets demonstrate that CiderTF achieves
comparable convergence with the communication reduction up to 99.99%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Node Feature Kernels Increase Graph Convolutional Network Robustness. (arXiv:2109.01785v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01785">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The robustness of the much-used Graph Convolutional Networks (GCNs) to
perturbations of their input is becoming a topic of increasing importance. In
this paper, the random GCN is introduced for which a random matrix theory
analysis is possible. This analysis suggests that if the graph is sufficiently
perturbed, or in the extreme case random, then the GCN fails to benefit from
the node features. It is furthermore observed that enhancing the message
passing step in GCNs by adding the node feature kernel to the adjacency matrix
of the graph structure solves this problem. An empirical study of a GCN
utilised for node classification on six real datasets further confirms the
theoretical findings and demonstrates that perturbations of the graph structure
can result in GCNs performing significantly worse than Multi-Layer Perceptrons
run on the node features alone. In practice, adding a node feature kernel to
the message passing of perturbed graphs results in a significant improvement of
the GCN&#x27;s performance, thereby rendering it more robust to graph perturbations.
Our code is publicly available at:https://github.com/ChangminWu/RobustGCN.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment. (arXiv:2109.01949v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01949">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Self-supervised learning provides an opportunity to explore unlabeled chest
X-rays and their associated free-text reports accumulated in clinical routine
without manual supervision. This paper proposes a Joint Image Text
Representation Learning Network (JoImTeRNet) for pre-training on chest X-ray
images and their radiology reports. The model was pre-trained on both the
global image-sentence level and the local image region-word level for
visual-textual matching. Both are bidirectionally constrained on Cross-Entropy
based and ranking-based Triplet Matching Losses. The region-word matching is
calculated using the attention mechanism without direct supervision about their
mapping. The pre-trained multi-modal representation learning paves the way for
downstream tasks concerning image and/or text encoding. We demonstrate the
representation learning quality by cross-modality retrievals and multi-label
classifications on two datasets: OpenI-IU and MIMIC-CXR</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Acceleration Method for Learning Fine-Layered Optical Neural Networks. (arXiv:2109.01731v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01731">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>An optical neural network (ONN) is a promising system due to its high-speed
and low-power operation. Its linear unit performs a multiplication of an input
vector and a weight matrix in optical analog circuits. Among them, a circuit
with a multiple-layered structure of programmable Mach-Zehnder interferometers
(MZIs) can realize a specific class of unitary matrices with a limited number
of MZIs as its weight matrix. The circuit is effective for balancing the number
of programmable MZIs and ONN performance. However, it takes a lot of time to
learn MZI parameters of the circuit with a conventional automatic
differentiation (AD), which machine learning platforms are equipped with. To
solve the time-consuming problem, we propose an acceleration method for
learning MZI parameters. We create customized complex-valued derivatives for an
MZI, exploiting Wirtinger derivatives and a chain rule. They are incorporated
into our newly developed function module implemented in C++ to collectively
calculate their values in a multi-layered structure. Our method is simple,
fast, and versatile as well as compatible with the conventional AD. We
demonstrate that our method works 20 times faster than the conventional AD when
a pixel-by-pixel MNIST task is performed in a complex-valued recurrent neural
network with an MZI-based hidden unit.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reconfigurable Intelligent Surface Empowered Over-the-Air Federated Edge Learning. (arXiv:2109.02353v1 [cs.IT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02353">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Federated edge learning (FEEL) has emerged as a revolutionary paradigm to
develop AI services at the edge of 6G wireless networks as it supports
collaborative model training at a massive number of mobile devices. However,
model communication over wireless channels, especially in uplink model
uploading of FEEL, has been widely recognized as a bottleneck that critically
limits the efficiency of FEEL. Although over-the-air computation can alleviate
the excessive cost of radio resources in FEEL model uploading, practical
implementations of over-the-air FEEL still suffer from several challenges,
including strong straggler issues, large communication overheads, and potential
privacy leakage. In this article, we study these challenges in over-the-air
FEEL and leverage reconfigurable intelligent surface (RIS), a key enabler of
future wireless systems, to address these challenges. We study the
state-of-the-art solutions on RIS-empowered FEEL and explore the promising
research opportunities for adopting RIS to enhance FEEL performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS. (arXiv:2109.01818v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01818">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In recent years, convolutional neural networks (CNNs) have experienced an
increasing interest for their ability to perform fast approximation of
effective hydrodynamic parameters in porous media research and applications.
This paper presents a novel methodology for permeability prediction from
micro-CT scans of geological rock samples. The training data set for CNNs
dedicated to permeability prediction consists of permeability labels that are
typically generated by classical lattice Boltzmann methods (LBM) that simulate
the flow through the pore space of the segmented image data. We instead perform
direct numerical simulation (DNS) by solving the stationary Stokes equation in
an efficient and distributed-parallel manner. As such, we circumvent the
convergence issues of LBM that frequently are observed on complex pore
geometries, and therefore, improve on the generality and accuracy of our
training data set. Using the DNS-computed permeabilities, a physics-informed
CNN PhyCNN) is trained by additionally providing a tailored characteristic
quantity of the pore space. More precisely, by exploiting the connection to
flow problems on a graph representation of the pore space, additional
information about confined structures is provided to the network in terms of
the maximum flow value, which is the key innovative component of our workflow.
As a result, unprecedented prediction accuracy and robustness are observed for
a variety of sandstone samples from archetypal rock formations.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Kernel Correntropy for Robust Learning. (arXiv:1905.10115v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.10115">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>As a novel similarity measure that is defined as the expectation of a kernel
function between two random variables, correntropy has been successfully
applied in robust machine learning and signal processing to combat large
outliers. The kernel function in correntropy is usually a zero-mean Gaussian
kernel. In a recent work, the concept of mixture correntropy (MC) was proposed
to improve the learning performance, where the kernel function is a mixture
Gaussian kernel, namely a linear combination of several zero-mean Gaussian
kernels with different widths. In both correntropy and mixture correntropy, the
center of the kernel function is, however, always located at zero. In the
present work, to further improve the learning performance, we propose the
concept of multi-kernel correntropy (MKC), in which each component of the
mixture Gaussian kernel can be centered at a different location. The properties
of the MKC are investigated and an efficient approach is proposed to determine
the free parameters in MKC. Experimental results show that the learning
algorithms under the maximum multi-kernel correntropy criterion (MMKCC) can
outperform those under the original maximum correntropy criterion (MCC) and the
maximum mixture correntropy criterion (MMCC).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.02549">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Hierarchical 3D Feature Learning for Pancreas Segmentation. (arXiv:2109.01667v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01667">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a novel 3D fully convolutional deep network for automated pancreas
segmentation from both MRI and CT scans. More specifically, the proposed model
consists of a 3D encoder that learns to extract volume features at different
scales; features taken at different points of the encoder hierarchy are then
sent to multiple 3D decoders that individually predict intermediate
segmentation maps. Finally, all segmentation maps are combined to obtain a
unique detailed segmentation mask. We test our model on both CT and MRI imaging
data: the publicly available NIH Pancreas-CT dataset (consisting of 82
contrast-enhanced CTs) and a private MRI dataset (consisting of 40 MRI scans).
Experimental results show that our model outperforms existing methods on CT
pancreas segmentation, obtaining an average Dice score of about 88%, and yields
promising segmentation performance on a very challenging MRI data set (average
Dice score is about 77%). Additional control experiments demonstrate that the
achieved performance is due to the combination of our 3D fully-convolutional
deep network and the hierarchical representation decoding, thus substantiating
our architectural design.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nonasymptotic one-and two-sample tests in high dimension with unknown covariance structure. (arXiv:2109.01730v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01730">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Let $\mathbf{X} &#x3D; (X_i)_{1\leq i \leq n}$ be an i.i.d. sample of
square-integrable variables in $\mathbb{R}^d$, with common expectation $\mu$
and covariance matrix $\Sigma$, both unknown. We consider the problem of
testing if $\mu$ is $\eta$-close to zero, i.e. $\|\mu\| \leq \eta $ against
$\|\mu\| \geq (\eta + \delta)$; we also tackle the more general two-sample mean
closeness testing problem. The aim of this paper is to obtain nonasymptotic
upper and lower bounds on the minimal separation distance $\delta$ such that we
can control both the Type I and Type II errors at a given level. The main
technical tools are concentration inequalities, first for a suitable estimator
of $\|\mu\|^2$ used a test statistic, and secondly for estimating the operator
and Frobenius norms of $\Sigma$ coming into the quantiles of said test
statistic. These properties are obtained for Gaussian and bounded
distributions. A particular attention is given to the dependence in the
pseudo-dimension $d_*$ of the distribution, defined as $d_* :&#x3D;
\|\Sigma\|_2^2/\|\Sigma\|_\infty^2$. In particular, for $\eta&#x3D;0$, the minimum
separation distance is ${\Theta}(d_*^{\frac{1}{4}}\sqrt{\|\Sigma\|_\infty/n})$,
in contrast with the minimax estimation distance for $\mu$, which is
${\Theta}(d_e^{\frac{1}{2}}\sqrt{\|\Sigma\|_\infty/n})$ (where
$d_e:&#x3D;\|\Sigma\|_1/\|\Sigma\|_\infty$). This generalizes a phenomenon spelled
out in particular by Baraud (2002).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning Temporal Quantum Tomography. (arXiv:2103.13973v3 [quant-ph] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.13973">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantifying and verifying the control level in preparing a quantum state are
central challenges in building quantum devices. The quantum state is
characterized from experimental measurements, using a procedure known as
tomography, which requires a vast number of resources. Furthermore, the
tomography for a quantum device with temporal processing, which is
fundamentally different from the standard tomography, has not been formulated.
We develop a practical and approximate tomography method using a recurrent
machine learning framework for this intriguing situation. The method is based
on repeated quantum interactions between a system called quantum reservoir with
a stream of quantum states. Measurement data from the reservoir are connected
to a linear readout to train a recurrent relation between quantum channels
applied to the input stream. We demonstrate our algorithms for quantum learning
tasks followed by the proposal of a quantum short-term memory capacity to
evaluate the temporal processing ability of near-term quantum devices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Attentive Neural Controlled Differential Equations for Time-series Classification and Forecasting. (arXiv:2109.01876v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01876">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Neural networks inspired by differential equations have proliferated for the
past several years. Neural ordinary differential equations (NODEs) and neural
controlled differential equations (NCDEs) are two representative examples of
them. In theory, NCDEs provide better representation learning capability for
time-series data than NODEs. In particular, it is known that NCDEs are suitable
for processing irregular time-series data. Whereas NODEs have been successfully
extended after adopting attention, however, it had not been studied yet how to
integrate attention into NCDEs. To this end, we present the method of Attentive
Neural Controlled Differential Equations (ANCDEs) for time-series
classification and forecasting, where dual NCDEs are used: one for generating
attention values, and the other for evolving hidden vectors for a downstream
machine learning task. We conduct experiments with three real-world time-series
datasets and 10 baselines. After dropping some values, we also conduct
irregular time-series experiments. Our method consistently shows the best
accuracy in all cases by non-trivial margins. Our visualizations also show that
the presented attention mechanism works as intended by focusing on crucial
information.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02644">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cross-Task Generalization via Natural Language Crowdsourcing Instructions. (arXiv:2104.08773v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Humans (e.g., crowdworkers) have a remarkable ability in solving different
tasks, by simply reading textual instructions that define them and looking at a
few examples. NLP models built with the conventional paradigm, however, often
struggle with generalization across tasks (e.g., a question-answering system
cannot solve classification tasks). A long-standing challenge in AI is to build
a model that is equipped with the understanding of human-readable instructions
that define the tasks, and can generalize to new tasks. To study this, we
introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their
human-authored instructions and 193k task instances. The instructions are
obtained from crowdsourcing instructions used to collect existing NLP datasets
and mapped to a unified schema. We adopt generative pre-trained language models
to encode task-specific instructions along with input and generate task output.
Our results indicate that models can benefit from instructions to generalize
across tasks. These models, however, are far behind supervised task-specific
models, indicating significant room for more progress in this direction.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sparse-MLP: A Fully-MLP Architecture with Conditional Computation. (arXiv:2109.02008v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02008">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Mixture of Experts (MoE) with sparse conditional computation has been proved
an effective architecture for scaling attention-based models to more parameters
with comparable computation cost. In this paper, we propose Sparse-MLP, scaling
the recent MLP-Mixer model with sparse MoE layers, to achieve a more
computation-efficient architecture. We replace a subset of dense MLP blocks in
the MLP-Mixer model with Sparse blocks. In each Sparse block, we apply two
stages of MoE layers: one with MLP experts mixing information within channels
along image patch dimension, one with MLP experts mixing information within
patches along the channel dimension. Besides, to reduce computational cost in
routing and improve experts capacity, we design Re-represent layers in each
Sparse block. These layers are to re-scale image representations by two simple
but effective linear transformations. By pre-training on ImageNet-1k with MoCo
v3 algorithm, our models can outperform dense MLP models with comparable
parameters and less computational cost on several downstream image
classification tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mobility Functional Areas and COVID-19 Spread. (arXiv:2103.16894v2 [stat.AP] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.16894">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This work introduces a new concept of functional areas called Mobility
Functional Areas (MFAs), i.e., the geographic zones highly interconnected
according to the analysis of mobile positioning data. The MFAs do not coincide
necessarily with administrative borders as they are built observing natural
human mobility and, therefore, they can be used to inform, in a bottom-up
approach, local transportation, spatial planning, health and economic policies.
After presenting the methodology behind the MFAs, this study focuses on the
link between the COVID-19 pandemic and the MFAs in Austria. It emerges that the
MFAs registered an average number of infections statistically larger than the
areas in the rest of the country, suggesting the usefulness of the MFAs in the
context of targeted re-escalation policy responses to this health crisis. The
MFAs dataset is openly available to other scholars for further analyses.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Relative Spatial Reasoning for Visual Question Answering. (arXiv:2109.01934v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01934">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Vision-and-language (V\&amp;L) reasoning necessitates perception of visual
concepts such as objects and actions, understanding semantics and language
grounding, and reasoning about the interplay between the two modalities. One
crucial aspect of visual reasoning is spatial understanding, which involves
understanding relative locations of objects, i.e.\ implicitly learning the
geometry of the scene. In this work, we evaluate the faithfulness of V\&amp;L
models to such geometric understanding, by formulating the prediction of
pair-wise relative locations of objects as a classification as well as a
regression task. Our findings suggest that state-of-the-art transformer-based
V\&amp;L models lack sufficient abilities to excel at this task. Motivated by this,
we design two objectives as proxies for 3D spatial reasoning (SR) -- object
centroid estimation, and relative position estimation, and train V\&amp;L with weak
supervision from off-the-shelf depth estimators. This leads to considerable
improvements in accuracy for the &quot;GQA&quot; visual question answering challenge (in
fully supervised, few-shot, and O.O.D settings) as well as improvements in
relative spatial reasoning. Code and data will be released
\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Perceptually-Validated Metric for Crowd Trajectory Quality Evaluation. (arXiv:2108.12346v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12346">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Simulating crowds requires controlling a very large number of trajectories
and is usually performed using crowd motion algorithms for which appropriate
parameter values need to be found. The study of the relation between parametric
values for simulation techniques and the quality of the resulting trajectories
has been studied either through perceptual experiments or by comparison with
real crowd trajectories. In this paper, we integrate both strategies. A quality
metric, QF, is proposed to abstract from reference data while capturing the
most salient features that affect the perception of trajectory realism. QF
weights and combines cost functions that are based on several individual, local
and global properties of trajectories. These trajectory features are selected
from the literature and from interviews with experts. To validate the capacity
of QF to capture perceived trajectory quality, we conduct an online experiment
that demonstrates the high agreement between the automatic quality score and
non-expert users. To further demonstrate the usefulness of QF, we use it in a
data-free parameter tuning application able to tune any parametric microscopic
crowd simulation model that outputs independent trajectories for characters.
The learnt parameters for the tuned crowd motion model maintain the influence
of the reference data which was used to weight the terms of QF.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers. (arXiv:2108.12284v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12284">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Recently, many datasets have been proposed to test the systematic
generalization ability of neural networks. The companion baseline Transformers,
typically trained with default hyper-parameters from standard tasks, are shown
to fail dramatically. Here we demonstrate that by revisiting model
configurations as basic as scaling of embeddings, early stopping, relative
positional embedding, and Universal Transformer variants, we can drastically
improve the performance of Transformers on systematic generalization. We report
improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics
dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity
split, and from 35% to 81% on COGS. On SCAN, relative positional embedding
largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100%
accuracy on the length split with a cutoff at 26. Importantly, performance
differences between these models are typically invisible on the IID data split.
This calls for proper generalization validation sets for developing neural
networks that generalize systematically. We publicly release the code to
reproduce our results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment. (arXiv:2108.10828v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10828">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Considerable research has been devoted to deep learning-based predictive
models for system prognostics and health management in the reliability and
safety community. However, there is limited study on the utilization of deep
learning for system reliability assessment. This paper aims to bridge this gap
and explore this new interface between deep learning and system reliability
assessment by exploiting the recent advances of physics-informed deep learning.
Particularly, we present an approach to frame system reliability assessment in
the context of physics-informed deep learning and discuss the potential value
of physics-informed generative adversarial networks for the uncertainty
quantification and measurement data incorporation in system reliability
assessment. The proposed approach is demonstrated by three numerical examples
involving a dual-processor computing system. The results indicate the potential
value of physics-informed deep learning to alleviate computational challenges
and combine measurement data and mathematical models for system reliability
assessment.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data-Driven Learning of 3-Point Correlation Functions as Microstructure Representations. (arXiv:2109.02255v1 [cond-mat.mtrl-sci])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02255">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>This paper considers the open challenge of identifying complete, concise, and
explainable quantitative microstructure representations for disordered
heterogeneous material systems. Completeness and conciseness have been achieved
through existing data-driven methods, e.g., deep generative models, which,
however, do not provide mathematically explainable latent representations. This
study investigates representations composed of three-point correlation
functions, which are a special type of spatial convolutions. We show that a
variety of microstructures can be characterized by a concise subset of
three-point correlations, and the identification of such subsets can be
achieved by Bayesian optimization. Lastly, we show that the proposed
representation can directly be used to compute material properties based on the
effective medium theory.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Variational Approach to Privacy and Fairness. (arXiv:2006.06332v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.06332">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>In this article, we propose a new variational approach to learn private
and/or fair representations. This approach is based on the Lagrangians of a new
formulation of the privacy and fairness optimization problems that we propose.
In this formulation, we aim to generate representations of the data that keep a
prescribed level of the relevant information that is not shared by the private
or sensitive data, while minimizing the remaining information they keep. The
proposed approach (i) exhibits the similarities of the privacy and fairness
problems, (ii) allows us to control the trade-off between utility and privacy
or fairness through the Lagrange multiplier parameter, and (iii) can be
comfortably incorporated to common representation learning algorithms such as
the VAE, the $\beta$-VAE, the VIB, or the nonlinear IB.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">The Phonexia VoxCeleb Speaker Recognition Challenge 2021 System Description. (arXiv:2109.02052v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02052">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We describe the Phonexia submission for the VoxCeleb Speaker Recognition
Challenge 2021 (VoxSRC-21) in the unsupervised speaker verification track. Our
solution was very similar to IDLab&#x27;s winning submission for VoxSRC-20. An
embedding extractor was bootstrapped using momentum contrastive learning, with
input augmentations as the only source of supervision. This was followed by
several iterations of clustering to assign pseudo-speaker labels that were then
used for supervised embedding extractor training. Finally, a score fusion was
done, by averaging the zt-normalized cosine scores of five different embedding
extractors. We briefly also describe unsuccessful solutions involving i-vectors
instead of DNN embeddings and PLDA instead of cosine scoring.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Approximate information state for approximate planning and reinforcement learning in partially observed systems. (arXiv:2010.08843v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.08843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose a theoretical framework for approximate planning and learning in
partially observed systems. Our framework is based on the fundamental notion of
information state. We provide two equivalent definitions of information state
-- i) a function of history which is sufficient to compute the expected reward
and predict its next value; ii) equivalently, a function of the history which
can be recursively updated and is sufficient to compute the expected reward and
predict the next observation. An information state always leads to a dynamic
programming decomposition. Our key result is to show that if a function of the
history (called approximate information state (AIS)) approximately satisfies
the properties of the information state, then there is a corresponding
approximate dynamic program. We show that the policy computed using this is
approximately optimal with bounded loss of optimality. We show that several
approximations in state, observation and action spaces in literature can be
viewed as instances of AIS. In some of these cases, we obtain tighter bounds. A
salient feature of AIS is that it can be learnt from data. We present AIS based
multi-time scale policy gradient algorithms. and detailed numerical experiments
with low, moderate and high dimensional environments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the Complexity of Computing Markov Perfect Equilibrium in General-Sum Stochastic Games. (arXiv:2109.01795v1 [cs.GT])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01795">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Similar to the role of Markov decision processes in reinforcement learning,
Stochastic Games (SGs) lay the foundation for the study of multi-agent
reinforcement learning (MARL) and sequential agent interactions. In this paper,
we derive that computing an approximate Markov Perfect Equilibrium (MPE) in a
finite-state discounted Stochastic Game within the exponential precision is
\textbf{PPAD}-complete. We adopt a function with a polynomially bounded
description in the strategy space to convert the MPE computation to a
fixed-point problem, even though the stochastic game may demand an exponential
number of pure strategies, in the number of states, for each agent. The
completeness result follows the reduction of the fixed-point problem to {\sc
End of the Line}. Our results indicate that finding an MPE in SGs is highly
unlikely to be \textbf{NP}-hard unless \textbf{NP}&#x3D;\textbf{co-NP}. Our work
offers confidence for MARL research to study MPE computation on general-sum SGs
and to develop fruitful algorithms as currently on zero-sum SGs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Structural Optimization Makes Graph Classification Simpler and Better. (arXiv:2109.02027v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02027">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In deep neural networks, better results can often be obtained by increasing
the complexity of previously developed basic models. However, it is unclear
whether there is a way to boost performance by decreasing the complexity of
such models. Here, based on an optimization method, we investigate the
feasibility of improving graph classification performance while simplifying the
model learning process. Inspired by progress in structural information
assessment, we optimize the given data sample from graphs to encoding trees. In
particular, we minimize the structural entropy of the transformed encoding tree
to decode the key structure underlying a graph. This transformation is denoted
as structural optimization. Furthermore, we propose a novel feature combination
scheme, termed hierarchical reporting, for encoding trees. In this scheme,
features are transferred from leaf nodes to root nodes by following the
hierarchical structures of encoding trees. We then present an implementation of
the scheme in a tree kernel and a convolutional network to perform graph
classification. The tree kernel follows label propagation in the
Weisfeiler-Lehman (WL) subtree kernel, but it has a lower runtime complexity
$O(n)$. The convolutional network is a special implementation of our tree
kernel in the deep learning field and is called Encoding Tree Learning (ETL).
We empirically validate our tree kernel and convolutional network with several
graph classification benchmarks and demonstrate that our methods achieve better
performance and lower computational consumption than competing approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly supervised semantic segmentation of tomographic images in the diagnosis of stroke. (arXiv:2109.01887v1 [eess.IV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01887">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper presents an automatic algorithm for the segmentation of areas
affected by an acute stroke on the non-contrast computed tomography brain
images. The proposed algorithm is designed for learning in a weakly supervised
scenario when some images are labeled accurately, and some images are labeled
inaccurately. Wrong labels appear as a result of inaccuracy made by a
radiologist in the process of manual annotation of computed tomography images.
We propose methods for solving the segmentation problem in the case of
inaccurately labeled training data. We use the U-Net neural network
architecture with several modifications. Experiments on real computed
tomography scans show that the proposed methods increase the segmentation
accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Weakly Supervised Few-Shot Segmentation Via Meta-Learning. (arXiv:2109.01693v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01693">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Semantic segmentation is a classic computer vision task with multiple
applications, which includes medical and remote sensing image analysis. Despite
recent advances with deep-based approaches, labeling samples (pixels) for
training models is laborious and, in some cases, unfeasible. In this paper, we
present two novel meta learning methods, named WeaSeL and ProtoSeg, for the
few-shot semantic segmentation task with sparse annotations. We conducted
extensive evaluation of the proposed methods in different applications (12
datasets) in medical imaging and agricultural remote sensing, which are very
distinct fields of knowledge and usually subject to data scarcity. The results
demonstrated the potential of our method, achieving suitable results for
segmenting both coffee/orange crops and anatomical parts of the human body in
comparison with full dense annotation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep Saliency Prior for Reducing Visual Distraction. (arXiv:2109.01980v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01980">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Using only a model that was trained to predict where people look at images,
and no additional training data, we can produce a range of powerful editing
effects for reducing distraction in images. Given an image and a mask
specifying the region to edit, we backpropagate through a state-of-the-art
saliency model to parameterize a differentiable editing operator, such that the
saliency within the masked region is reduced. We demonstrate several operators,
including: a recoloring operator, which learns to apply a color transform that
camouflages and blends distractors into their surroundings; a warping operator,
which warps less salient image regions to cover distractors, gradually
collapsing objects into themselves and effectively removing them (an effect
akin to inpainting); a GAN operator, which uses a semantic prior to fully
replace image regions with plausible, less salient alternatives. The resulting
effects are consistent with cognitive research on the human visual system
(e.g., since color mismatch is salient, the recoloring operator learns to
harmonize objects&#x27; colors with their surrounding to reduce their saliency),
and, importantly, are all achieved solely through the guidance of the
pretrained saliency model, with no additional supervision. We present results
on a variety of natural images and conduct a perceptual study to evaluate and
validate the changes in viewers&#x27; eye-gaze between the original images and our
edited results.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Assessing the Knowledge State of Online Students -- New Data, New Approaches, Improved Accuracy. (arXiv:2109.01753v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01753">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We consider the problem of assessing the changing knowledge state of
individual students as they go through online courses. This student performance
(SP) modeling problem, also known as knowledge tracing, is a critical step for
building adaptive online teaching systems. Specifically, we conduct a study of
how to utilize various types and large amounts of students log data to train
accurate machine learning models that predict the knowledge state of future
students. This study is the first to use four very large datasets made
available recently from four distinct intelligent tutoring systems. Our results
include a new machine learning approach that defines a new state of the art for
SP modeling, improving over earlier methods in several ways: First, we achieve
improved accuracy by introducing new features that can be easily computed from
conventional question-response logs (e.g., the pattern in the student&#x27;s most
recent answers). Second, we take advantage of features of the student history
that go beyond question-response pairs (e.g., which video segments the student
watched, or skipped) as well as information about prerequisite structure in the
curriculum. Third, we train multiple specialized modeling models for different
aspects of the curriculum (e.g., specializing in early versus later segments of
the student history), then combine these specialized models to create a group
prediction of student knowledge. Taken together, these innovations yield an
average AUC score across these four datasets of 0.807 compared to the previous
best logistic regression approach score of 0.766, and also outperforming
state-of-the-art deep neural net approaches. Importantly, we observe consistent
improvements from each of our three methodological innovations, in each
dataset, suggesting that our methods are of general utility and likely to
produce improvements for other online tutoring systems as well.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Providing an Approach to Predicting Customer Quality in E-Commerce Social Networks Based on Big Data and Unsupervised Learning Method. (arXiv:2109.02080v1 [cs.SI])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02080">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>One of the goals of every business enterprise is to increase customer
loyalty. The degree of customer loyalty is called customer quality which its
forecasting will affect strategic marketing practices. The purpose of this
study is to predict the quality of customers of large e-commerce social
networks by big data algorithms and unsupervised learning. For this purpose, a
graph-based social network analysis framework was used for community detection
in the Stanford Network Analysis Platform (SNAP). Then in the found
communities, the quality of customers was predicted. The results showed that
various visits with an impact of 37.13% can have the greatest impact on
customer quality and the order of impact of other parameters were from highest
to lowest: number of frequent customer visits (28.56%), role in social networks
(28.37%), Indirect transactions (26.74%), activity days (25.62%) and customer
social network size (25.06%).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. (arXiv:2004.00436v6 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.00436">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several approaches have been proposed in recent literature to alleviate the
long-tail problem, mainly in object classification tasks. In this paper, we
make the first large-scale study concerning the task of Long-Tail Visual
Relationship Recognition (LTVRR). LTVRR aims at improving the learning of
structured visual relationships that come from the long-tail (e.g., &quot;rabbit
grazing on grass&quot;). In this setup, the subject, relation, and object classes
each follow a long-tail distribution. To begin our study and make a future
benchmark for the community, we introduce two LTVRR-related benchmarks, dubbed
VG8K-LT and GQA-LT, built upon the widely used Visual Genome and GQA datasets.
We use these benchmarks to study the performance of several state-of-the-art
long-tail models on the LTVRR setup. Lastly, we propose a visiolinguistic
hubless (VilHub) loss and a Mixup augmentation technique adapted to LTVRR
setup, dubbed as RelMix. Both VilHub and RelMix can be easily integrated on top
of existing models and despite being simple, our results show that they can
remarkably improve the performance, especially on tail classes. Benchmarks,
code, and models have been made available at:
https://github.com/Vision-CAIR/LTVRR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">User-Oriented Smart General AI System under Causal Inference. (arXiv:2103.14561v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14561">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>General AI system solves a wide range of tasks with high performance in an
automated fashion. The best general AI algorithm designed by one individual is
different from that devised by another. The best performance records achieved
by different users are also different. An inevitable component of general AI is
tacit knowledge that depends upon user-specific comprehension of task
information and individual model design preferences that are related to user
technical experiences. Tacit knowledge affects model performance but cannot be
automatically optimized in general AI algorithms. In this paper, we propose
User-Oriented Smart General AI System under Causal Inference, abbreviated as
UOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means
under the framework of causal inference. User characteristics that have a
significant influence upon tacit knowledge can be extracted from observed model
training experiences of many users in external memory modules. Under the
framework of causal inference, we manage to identify the optimal value of user
characteristics that are connected with the best model performance designed by
users. We make suggestions to users about how different user characteristics
can improve the best model performance achieved by users. By recommending
updating user characteristics associated with individualized tacit knowledge
comprehension and technical preferences, UOGAS helps users design models with
better performance.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Closed Loop Gradient Descent Algorithm applied to Rosenbrock&#x27;s function. (arXiv:2108.12883v3 [math.OC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12883">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We introduce a novel adaptive damping technique for an inertial gradient
system which finds application as a gradient descent algorithm for
unconstrained optimisation. In an example using the non-convex Rosenbrock&#x27;s
function, we show an improvement on existing momentum-based gradient
optimisation methods. Also using Lyapunov stability analysis, we demonstrate
the performance of the continuous-time version of the algorithm. Using
numerical simulations, we consider the performance of its discrete-time
counterpart obtained by using the symplectic Euler method of discretisation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Agents using Upside-Down Reinforcement Learning. (arXiv:1912.02877v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1912.02877">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We develop Upside-Down Reinforcement Learning (UDRL), a method for learning
to act using only supervised learning techniques. Unlike traditional
algorithms, UDRL does not use reward prediction or search for an optimal
policy. Instead, it trains agents to follow commands such as &quot;obtain so much
total reward in so much time.&quot; Many of its general principles are outlined in a
companion report; the goal of this paper is to develop a practical learning
algorithm and show that this conceptually simple perspective on agent training
can produce a range of rewarding behaviors for multiple episodic environments.
Experiments show that on some tasks UDRL&#x27;s performance can be surprisingly
competitive with, and even exceed that of some traditional baseline algorithms
developed over decades of research. Based on these results, we suggest that
alternative approaches to expected reward maximization have an important role
to play in training useful autonomous agents.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. (arXiv:2009.10623v5 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.10623">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>From CNNs to attention mechanisms, encoding inductive biases into neural
networks has been a fruitful source of improvement in machine learning. Adding
auxiliary losses to the main objective function is a general way of encoding
biases that can help networks learn better representations. However, since
auxiliary losses are minimized only on training data, they suffer from the same
generalization gap as regular task losses. Moreover, by adding a term to the
loss function, the model optimizes a different objective than the one we care
about. In this work we address both problems: first, we take inspiration from
\textit{transductive learning} and note that after receiving an input but
before making a prediction, we can fine-tune our networks on any unsupervised
loss. We call this process {\em tailoring}, because we customize the model to
each input to ensure our prediction satisfies the inductive bias. Second, we
formulate {\em meta-tailoring}, a nested optimization similar to that in
meta-learning, and train our models to perform well on the task objective after
adapting them using an unsupervised loss. The advantages of tailoring and
meta-tailoring are discussed theoretically and demonstrated empirically on a
diverse set of examples.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to drive from a world on rails. (arXiv:2105.00636v2 [cs.RO] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00636">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We learn an interactive vision-based driving policy from pre-recorded driving
logs via a model-based approach. A forward model of the world supervises a
driving policy that predicts the outcome of any potential driving trajectory.
To support learning from pre-recorded logs, we assume that the world is on
rails, meaning neither the agent nor its actions influence the environment.
This assumption greatly simplifies the learning problem, factorizing the
dynamics into a nonreactive world model and a low-dimensional and compact
forward model of the ego-vehicle. Our approach computes action-values for each
training trajectory using a tabular dynamic-programming evaluation of the
Bellman equations; these action-values in turn supervise the final vision-based
driving policy. Despite the world-on-rails assumption, the final driving policy
acts well in a dynamic and reactive world. At the time of writing, our method
ranks first on the CARLA leaderboard, attaining a 25% higher driving score
while using 40 times less data. Our method is also an order of magnitude more
sample-efficient than state-of-the-art model-free reinforcement learning
techniques on navigational tasks in the ProcGen benchmark.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Novel Multi-Centroid Template Matching Algorithm and Its Application to Cough Detection. (arXiv:2109.00630v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00630">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Cough is a major symptom of respiratory-related diseases. There exists a
tremendous amount of work in detecting coughs from audio but there has been no
effort to identify coughs from solely inertial measurement unit (IMU). Coughing
causes motion across the whole body and especially on the neck and head.
Therefore, head motion data during coughing captured by a head-worn IMU sensor
could be leveraged to detect coughs using a template matching algorithm. In
time series template matching problems, K-Nearest Neighbors (KNN) combined with
elastic distance measurement (esp. Dynamic Time Warping (DTW)) achieves
outstanding performance. However, it is often regarded as prohibitively
time-consuming. Nearest Centroid Classifier is thereafter proposed. But the
accuracy is comprised of only one centroid obtained for each class.
Centroid-based Classifier performs clustering and averaging for each cluster,
but requires manually setting the number of clusters. We propose a novel
self-tuning multi-centroid template-matching algorithm, which can automatically
adjust the number of clusters to balance accuracy and inference time. Through
experiments conducted on synthetic datasets and a real-world earbud-based cough
dataset, we demonstrate the superiority of our proposed algorithm and present
the result of cough detection with a single accelerometer sensor on the earbuds
platform.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Cluster-Promoting Quantization with Bit-Drop for Minimizing Network Quantization Loss. (arXiv:2109.02100v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02100">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Network quantization, which aims to reduce the bit-lengths of the network
weights and activations, has emerged for their deployments to resource-limited
devices. Although recent studies have successfully discretized a full-precision
network, they still incur large quantization errors after training, thus giving
rise to a significant performance gap between a full-precision network and its
quantized counterpart. In this work, we propose a novel quantization method for
neural networks, Cluster-Promoting Quantization (CPQ) that finds the optimal
quantization grids while naturally encouraging the underlying full-precision
weights to gather around those quantization grids cohesively during training.
This property of CPQ is thanks to our two main ingredients that enable
differentiable quantization: i) the use of the categorical distribution
designed by a specific probabilistic parametrization in the forward pass and
ii) our proposed multi-class straight-through estimator (STE) in the backward
pass. Since our second component, multi-class STE, is intrinsically biased, we
additionally propose a new bit-drop technique, DropBits, that revises the
standard dropout regularization to randomly drop bits instead of neurons. As a
natural extension of DropBits, we further introduce the way of learning
heterogeneous quantization levels to find proper bit-length for each layer by
imposing an additional regularization on DropBits. We experimentally validate
our method on various benchmark datasets and network architectures, and also
support a new hypothesis for quantization: learning heterogeneous quantization
levels outperforms the case using the same but fixed quantization levels from
scratch.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Siamese Basis Function Networks for Data-efficient Defect Classification in Technical Domains. (arXiv:2012.01338v7 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.01338">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Training deep learning models in technical domains is often accompanied by
the challenge that although the task is clear, insufficient data for training
is available. In this work, we propose a novel approach based on the
combination of Siamese networks and radial basis function networks to perform
data-efficient classification without pretraining by measuring the distance
between images in semantic space in a data-efficient manner. We develop the
models using three technical datasets, the NEU dataset, the BSD dataset, and
the TEX dataset. In addition to the technical domain, we show the general
applicability to classical datasets (cifar10 and MNIST) as well. The approach
is tested against state-of-the-art models (Resnet50 and Resnet101) by stepwise
reduction of the number of samples available for training. The authors show
that the proposed approach outperforms the state-of-the-art models in the low
data regime.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Minimal Variance Sampling with Provable Guarantees for Fast Training of Graph Neural Networks. (arXiv:2006.13866v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2006.13866">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Sampling methods (e.g., node-wise, layer-wise, or subgraph) has become an
indispensable strategy to speed up training large-scale Graph Neural Networks
(GNNs). However, existing sampling methods are mostly based on the graph
structural information and ignore the dynamicity of optimization, which leads
to high variance in estimating the stochastic gradients. The high variance
issue can be very pronounced in extremely large graphs, where it results in
slow convergence and poor generalization. In this paper, we theoretically
analyze the variance of sampling methods and show that, due to the composite
structure of empirical risk, the variance of any sampling method can be
decomposed into \textit{embedding approximation variance} in the forward stage
and \textit{stochastic gradient variance} in the backward stage that
necessities mitigating both types of variance to obtain faster convergence
rate. We propose a decoupled variance reduction strategy that employs
(approximate) gradient information to adaptively sample nodes with minimal
variance, and explicitly reduces the variance introduced by embedding
approximation. We show theoretically and empirically that the proposed method,
even with smaller mini-batch sizes, enjoys a faster convergence rate and
entails a better generalization compared to the existing methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Counterfactual Explanation Based on Gradual Construction for Deep Networks. (arXiv:2008.01897v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.01897">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>To understand the black-box characteristics of deep networks, counterfactual
explanation that deduces not only the important features of an input space but
also how those features should be modified to classify input as a target class
has gained an increasing interest. The patterns that deep networks have learned
from a training dataset can be grasped by observing the feature variation among
various classes. However, current approaches perform the feature modification
to increase the classification probability for the target class irrespective of
the internal characteristics of deep networks. This often leads to unclear
explanations that deviate from real-world data distributions. To address this
problem, we propose a counterfactual explanation method that exploits the
statistics learned from a training dataset. Especially, we gradually construct
an explanation by iterating over masking and composition steps. The masking
step aims to select an important feature from the input data to be classified
as a target class. Meanwhile, the composition step aims to optimize the
previously selected feature by ensuring that its output score is close to the
logit space of the training data that are classified as the target class.
Experimental results show that our method produces human-friendly
interpretations on various classification datasets and verify that such
interpretations can be achieved with fewer feature modification.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Scalable Feature Selection for (Multitask) Gradient Boosted Trees. (arXiv:2109.01965v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01965">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Gradient Boosted Decision Trees (GBDTs) are widely used for building ranking
and relevance models in search and recommendation. Considerations such as
latency and interpretability dictate the use of as few features as possible to
train these models. Feature selection in GBDT models typically involves
heuristically ranking the features by importance and selecting the top few, or
by performing a full backward feature elimination routine. On-the-fly feature
selection methods proposed previously scale suboptimally with the number of
features, which can be daunting in high dimensional settings. We develop a
scalable forward feature selection variant for GBDT, via a novel group testing
procedure that works well in high dimensions, and enjoys favorable theoretical
performance and computational guarantees. We show via extensive experiments on
both public and proprietary datasets that the proposed method offers
significant speedups in training time, while being as competitive as existing
GBDT methods in terms of model performance metrics. We also extend the method
to the multitask setting, allowing the practitioner to select common features
across tasks, as well as selecting task-specific features.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mitigating harm in language models with conditional-likelihood filtration. (arXiv:2108.07790v2 [cs.CL] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.07790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Language models trained on large-scale unfiltered datasets curated from the
open web acquire systemic biases, prejudices, and harmful views from their
training data. We present a methodology for programmatically identifying and
removing harmful text from web-scale datasets. A pretrained language model is
used to calculate the log-likelihood of researcher-written trigger phrases
conditioned on a specific document, which is used to identify and filter
documents from the dataset. We demonstrate that models trained on this filtered
dataset exhibit lower propensity to generate harmful text, with a marginal
decrease in performance on standard language modeling benchmarks compared to
unfiltered baselines. We provide a partial explanation for this performance gap
by surfacing examples of hate speech and other undesirable content from
standard language modeling benchmarks. Finally, we discuss the generalization
of this method and how trigger phrases which reflect specific values can be
used by researchers to build language models which are more closely aligned
with their values.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Gradient Normalization for Generative Adversarial Networks. (arXiv:2109.02235v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02235">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a novel normalization method called gradient
normalization (GN) to tackle the training instability of Generative Adversarial
Networks (GANs) caused by the sharp gradient space. Unlike existing work such
as gradient penalty and spectral normalization, the proposed GN only imposes a
hard 1-Lipschitz constraint on the discriminator function, which increases the
capacity of the discriminator. Moreover, the proposed gradient normalization
can be applied to different GAN architectures with little modification.
Extensive experiments on four datasets show that GANs trained with gradient
normalization outperform existing methods in terms of both Frechet Inception
Distance and Inception Score.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Pandemic Drugs at Pandemic Speed: Infrastructure for Accelerating COVID-19 Drug Discovery with Hybrid Machine Learning- and Physics-based Simulations on High Performance Computers. (arXiv:2103.02843v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.02843">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The race to meet the challenges of the global pandemic has served as a
reminder that the existing drug discovery process is expensive, inefficient and
slow. There is a major bottleneck screening the vast number of potential small
molecules to shortlist lead compounds for antiviral drug development. New
opportunities to accelerate drug discovery lie at the interface between machine
learning methods, in this case developed for linear accelerators, and
physics-based methods. The two in silico methods, each have their own
advantages and limitations which, interestingly, complement each other. Here,
we present an innovative infrastructural development that combines both
approaches to accelerate drug discovery. The scale of the potential resulting
workflow is such that it is dependent on supercomputing to achieve extremely
high throughput. We have demonstrated the viability of this workflow for the
study of inhibitors for four COVID-19 target proteins and our ability to
perform the required large-scale calculations to identify lead antiviral
compounds through repurposing on a variety of supercomputers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Regional Adversarial Training for Better Robust Generalization. (arXiv:2109.00678v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00678">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial training (AT) has been demonstrated as one of the most promising
defense methods against various adversarial attacks. To our knowledge, existing
AT-based methods usually train with the locally most adversarial perturbed
points and treat all the perturbed points equally, which may lead to
considerably weaker adversarial robust generalization on test data. In this
work, we introduce a new adversarial training framework that considers the
diversity as well as characteristics of the perturbed points in the vicinity of
benign samples. To realize the framework, we propose a Regional Adversarial
Training (RAT) defense method that first utilizes the attack path generated by
the typical iterative attack method of projected gradient descent (PGD), and
constructs an adversarial region based on the attack path. Then, RAT samples
diverse perturbed training points efficiently inside this region, and utilizes
a distance-aware label smoothing mechanism to capture our intuition that
perturbed points at different locations should have different impact on the
model performance. Extensive experiments on several benchmark datasets show
that RAT consistently makes significant improvement on standard adversarial
training (SAT), and exhibits better robust generalization.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Effective and scalable clustering of SARS-CoV-2 sequences. (arXiv:2108.08143v3 [q-bio.PE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08143">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>SARS-CoV-2, like any other virus, continues to mutate as it spreads,
according to an evolutionary process. Unlike any other virus, the number of
currently available sequences of SARS-CoV-2 in public databases such as GISAID
is already several million. This amount of data has the potential to uncover
the evolutionary dynamics of a virus like never before. However, a million is
already several orders of magnitude beyond what can be processed by the
traditional methods designed to reconstruct a virus&#x27;s evolutionary history,
such as those that build a phylogenetic tree. Hence, new and scalable methods
will need to be devised in order to make use of the ever increasing number of
viral sequences being collected.

Since identifying variants is an important part of understanding the
evolution of a virus, in this paper, we propose an approach based on clustering
sequences to identify the current major SARS-CoV-2 variants. Using a $k$-mer
based feature vector generation and efficient feature selection methods, our
approach is effective in identifying variants, as well as being efficient and
scalable to millions of sequences. Such a clustering method allows us to show
the relative proportion of each variant over time, giving the rate of spread of
each variant in different locations -- something which is important for vaccine
development and distribution. We also compute the importance of each amino acid
position of the spike protein in identifying a given variant in terms of
information gain. Positions of high variant-specific importance tend to agree
with those reported by the USA&#x27;s Centers for Disease Control and Prevention
(CDC), further demonstrating our approach.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Deep State-Space Gaussian Processes. (arXiv:2008.04733v3 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2008.04733">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper is concerned with a state-space approach to deep Gaussian process
(DGP) regression. We construct the DGP by hierarchically putting transformed
Gaussian process (GP) priors on the length scales and magnitudes of the next
level of Gaussian processes in the hierarchy. The idea of the state-space
approach is to represent the DGP as a non-linear hierarchical system of linear
stochastic differential equations (SDEs), where each SDE corresponds to a
conditional GP. The DGP regression problem then becomes a state estimation
problem, and we can estimate the state efficiently with sequential methods by
using the Markov property of the state-space DGP. The computational complexity
scales linearly with respect to the number of measurements. Based on this, we
formulate state-space MAP as well as Bayesian filtering and smoothing solutions
to the DGP regression problem. We demonstrate the performance of the proposed
models and methods on synthetic non-stationary signals and apply the
state-space DGP to detection of the gravitational waves from LIGO measurements.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Urban Fire Station Location Planning: A Systematic Approach using Predicted Demand and Service Quality Index. (arXiv:2109.02160v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02160">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this article, we propose a systematic approach for fire station location
planning. We develop a machine learning model, based on Random Forest, for
demand prediction and utilize the model further to define a generalized index
to measure quality of fire service in urban settings. Our model is built upon
spatial data collected from multiple different sources. Efficacy of proper
facility planning depends on choice of candidates where fire stations can be
located along with existing stations, if any. Also, the travel time from these
candidates to demand locations need to be taken care of to maintain fire safety
standard. Here, we propose a travel time based clustering technique to identify
suitable candidates. Finally, we develop an optimization problem to select best
locations to install new fire stations. Our optimization problem is built upon
maximum coverage problem, based on integer programming. We present a detailed
experimental study of our proposed approach in collaboration with city of
Victoria Fire Department, MN, USA. Our demand prediction model achieves true
positive rate of 70% and false positive rate of 22% approximately. We aid
Victoria Fire Department to select a location for a new fire station using our
approach. We present detailed results on improvement statistics by locating a
new facility, as suggested by our methodology, in the city of Victoria.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Quantifying the Reproducibility of Graph Neural Networks using Multigraph Brain Data. (arXiv:2109.02248v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02248">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Graph neural networks (GNNs) have witnessed an unprecedented proliferation in
tackling several problems in computer vision, computer-aided diagnosis, and
related fields. While prior studies have focused on boosting the model
accuracy, quantifying the reproducibility of the most discriminative features
identified by GNNs is still an intact problem that yields concerns about their
reliability in clinical applications in particular. Specifically, the
reproducibility of biological markers across clinical datasets and distribution
shifts across classes (e.g., healthy and disordered brains) is of paramount
importance in revealing the underpinning mechanisms of diseases as well as
propelling the development of personalized treatment. Motivated by these
issues, we propose, for the first time, reproducibility-based GNN selection
(RG-Select), a framework for GNN reproducibility assessment via the
quantification of the most discriminative features (i.e., biomarkers) shared
between different models. To ascertain the soundness of our framework, the
reproducibility assessment embraces variations of different factors such as
training strategies and data perturbations. Despite these challenges, our
framework successfully yielded replicable conclusions across different training
strategies and various clinical datasets. Our findings could thus pave the way
for the development of biomarker trustworthiness and reliability assessment
methods for computer-aided diagnosis and prognosis tasks. RG-Select code is
available on GitHub at https://github.com/basiralab/RG-Select.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Representation Learning for Efficient and Effective Similarity Search and Recommendation. (arXiv:2109.01815v1 [cs.IR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01815">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>How data is represented and operationalized is critical for building
computational solutions that are both effective and efficient. A common
approach is to represent data objects as binary vectors, denoted \textit{hash
codes}, which require little storage and enable efficient similarity search
through direct indexing into a hash table or through similarity computations in
an appropriate space. Due to the limited expressibility of hash codes, compared
to real-valued representations, a core open challenge is how to generate hash
codes that well capture semantic content or latent properties using a small
number of bits, while ensuring that the hash codes are distributed in a way
that does not reduce their search efficiency. State of the art methods use
representation learning for generating such hash codes, focusing on neural
autoencoder architectures where semantics are encoded into the hash codes by
learning to reconstruct the original inputs of the hash codes. This thesis
addresses the above challenge and makes a number of contributions to
representation learning that (i) improve effectiveness of hash codes through
more expressive representations and a more effective similarity measure than
the current state of the art, namely the Hamming distance, and (ii) improve
efficiency of hash codes by learning representations that are especially suited
to the choice of search method. The contributions are empirically validated on
several tasks related to similarity search and recommendation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Efficient Masked Language Modeling for Vision and Language. (arXiv:2109.02040v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02040">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Masked language modeling (MLM) is one of the key sub-tasks in vision-language
pretraining. In the cross-modal setting, tokens in the sentence are masked at
random, and the model predicts the masked tokens given the image and the text.
In this paper, we observe several key disadvantages of MLM in this setting.
First, as captions tend to be short, in a third of the sentences no token is
sampled. Second, the majority of masked tokens are stop-words and punctuation,
leading to under-utilization of the image. We investigate a range of
alternative masking strategies specific to the cross-modal setting that address
these shortcomings, aiming for better fusion of text and image in the learned
representation. When pre-training the LXMERT model, our alternative masking
strategies consistently improve over the original masking strategy on three
downstream tasks, especially in low resource settings. Further, our
pre-training approach substantially outperforms the baseline model on a
prompt-based probing task designed to elicit image objects. These results and
our analysis indicate that our method allows for better utilization of the
training data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Radiance Flow for 4D View Synthesis and Video Processing. (arXiv:2012.09790v2 [cs.CV] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.09790">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We present a method, Neural Radiance Flow (NeRFlow),to learn a 4D
spatial-temporal representation of a dynamic scene from a set of RGB images.
Key to our approach is the use of a neural implicit representation that learns
to capture the 3D occupancy, radiance, and dynamics of the scene. By enforcing
consistency across different modalities, our representation enables multi-view
rendering in diverse dynamic scenes, including water pouring, robotic
interaction, and real images, outperforming state-of-the-art methods for
spatial-temporal view synthesis. Our approach works even when inputs images are
captured with only one camera. We further demonstrate that the learned
representation can serve as an implicit scene prior, enabling video processing
tasks such as image super-resolution and de-noising without any additional
supervision.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Estimating Leaf Water Content using Remotely Sensed Hyperspectral Data. (arXiv:2109.02250v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02250">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Plant water stress may occur due to the limited availability of water to the
roots/soil or due to increased transpiration. These factors adversely affect
plant physiology and photosynthetic ability to the extent that it has been
shown to have inhibitory effects in both growth and yield [18]. Early
identification of plant water stress status enables suitable corrective
measures to be applied to obtain the expected crop yield. Further, improving
crop yield through precision agriculture methods is a key component of climate
policy and the UN sustainable development goals [1]. Leaf water content (LWC)
is a measure that can be used to estimate water content and identify stressed
plants. LWC during the early crop growth stages is an important indicator of
plant productivity and yield. The effect of water stress can be instantaneous
[15], affecting gaseous exchange or long-term, significantly reducing [9, 18,
22]. It is thus necessary to identify potential plant water stress during the
early stages of growth [15] to introduce corrective irrigation and alleviate
stress. LWC is also useful for identifying plant genotypes that are tolerant to
water stress and salinity by measuring the stability of LWC even under
artificially induced water stress [18, 25]. Such experiments generally employ
destructive procedures to obtain the LWC, which is time-consuming and labor
intensive. Accordingly, this research has developed a non-destructive method to
estimate LWC from UAV-based hyperspectral data.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Communication Efficient Federated Learning with Energy Awareness over Wireless Networks. (arXiv:2004.07351v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.07351">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In federated learning (FL), reducing the communication overhead is one of the
most critical challenges since the parameter server and the mobile devices
share the training parameters over wireless links. With such consideration, we
adopt the idea of SignSGD in which only the signs of the gradients are
exchanged. Moreover, most of the existing works assume Channel State
Information (CSI) available at both the mobile devices and the parameter
server, and thus the mobile devices can adopt fixed transmission rates dictated
by the channel capacity. In this work, only the parameter server side CSI is
assumed, and channel capacity with outage is considered. In this case, an
essential problem for the mobile devices is to select appropriate local
processing and communication parameters (including the transmission rates) to
achieve a desired balance between the overall learning performance and their
energy consumption. Two optimization problems are formulated and solved, which
optimize the learning performance given the energy consumption requirement, and
vice versa. Furthermore, considering that the data may be distributed across
the mobile devices in a highly uneven fashion in FL, a stochastic sign-based
algorithm is proposed. Extensive simulations are performed to demonstrate the
effectiveness of the proposed methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Questioning the AI: Informing Design Practices for Explainable AI User Experiences. (arXiv:2001.02478v3 [cs.HC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.02478">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A surge of interest in explainable AI (XAI) has led to a vast collection of
algorithmic work on the topic. While many recognize the necessity to
incorporate explainability features in AI systems, how to address real-world
user needs for understanding AI remains an open question. By interviewing 20 UX
and design practitioners working on various AI products, we seek to identify
gaps between the current XAI algorithmic work and practices to create
explainable AI products. To do so, we develop an algorithm-informed XAI
question bank in which user needs for explainability are represented as
prototypical questions users might ask about the AI, and use it as a study
probe. Our work contributes insights into the design space of XAI, informs
efforts to support design practices in this space, and identifies opportunities
for future XAI work. We also provide an extended XAI question bank and discuss
how it can be used for creating user-centered XAI.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Learning to Select Cuts for Efficient Mixed-Integer Programming. (arXiv:2105.13645v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.13645">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Cutting plane methods play a significant role in modern solvers for tackling
mixed-integer programming (MIP) problems. Proper selection of cuts would remove
infeasible solutions in the early stage, thus largely reducing the
computational burden without hurting the solution accuracy. However, the major
cut selection approaches heavily rely on heuristics, which strongly depend on
the specific problem at hand and thus limit their generalization capability. In
this paper, we propose a data-driven and generalizable cut selection approach,
named Cut Ranking, in the settings of multiple instance learning. To measure
the quality of the candidate cuts, a scoring function, which takes the
instance-specific cut features as inputs, is trained and applied in cut ranking
and selection. In order to evaluate our method, we conduct extensive
experiments on both synthetic datasets and real-world datasets. Compared with
commonly used heuristics for cut selection, the learning-based policy has shown
to be more effective, and is capable of generalizing over multiple problems
with different properties. Cut Ranking has been deployed in an industrial
solver for large-scale MIPs. In the online A/B testing of the product planning
problems with more than $10^7$ variables and constraints daily, Cut Ranking has
achieved the average speedup ratio of 12.42% over the production solver without
any accuracy loss of solution.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-agent online learning in time-varying games. (arXiv:1809.03066v3 [cs.GT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1809.03066">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We examine the long-run behavior of multi-agent online learning in games that
evolve over time. Specifically, we focus on a wide class of policies based on
mirror descent, and we show that the induced sequence of play (a) converges to
Nash equilibrium in time-varying games that stabilize in the long run to a
strictly monotone limit; and (b) it stays asymptotically close to the evolving
equilibrium of the sequence of stage games (assuming they are strongly
monotone). Our results apply to both gradient-based and payoff-based feedback -
i.e., the &quot;bandit feedback&quot; case where players only get to observe the payoffs
of their chosen actions.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Non-Euclidean Analysis of Joint Variations in Multi-Object Shapes. (arXiv:2109.02230v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02230">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper considers joint analysis of multiple functionally related
structures in classification tasks. In particular, our method developed is
driven by how functionally correlated brain structures vary together between
autism and control groups. To do so, we devised a method based on a novel
combination of (1) non-Euclidean statistics that can faithfully represent
non-Euclidean data in Euclidean spaces and (2) a non-parametric integrative
analysis method that can decompose multi-block Euclidean data into joint,
individual, and residual structures. We find that the resulting joint structure
is effective, robust, and interpretable in recognizing the underlying patterns
of the joint variation of multi-block non-Euclidean data. We verified the
method in classifying the structural shape data collected from cases that
developed and did not develop into Autistic Spectrum Disorder (ASD).</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Identification of Driver Phone Usage Violations via State-of-the-Art Object Detection with Tracking. (arXiv:2109.02119v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02119">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The use of mobiles phones when driving have been a major factor when it comes
to road traffic incidents and the process of capturing such violations can be a
laborious task. Advancements in both modern object detection frameworks and
high-performance hardware has paved the way for a more automated approach when
it comes to video surveillance. In this work, we propose a custom-trained
state-of-the-art object detector to work with roadside cameras to capture
driver phone usage without the need for human intervention. The proposed
approach also addresses the issues caused by windscreen glare and introduces
the steps required to remedy this. Twelve pre-trained models are fine-tuned
with our custom dataset using four popular object detection methods: YOLO, SSD,
Faster R-CNN, and CenterNet. Out of all the object detectors tested, the YOLO
yields the highest accuracy levels of up to 96% (AP10) and frame rates of up to
~30 FPS. DeepSort object tracking algorithm is also integrated into the
best-performing model to collect records of only the unique violations, and
enable the proposed approach to count the number of vehicles. The proposed
automated system will collect the output images of the identified violations,
timestamps of each violation, and total vehicle count. Data can be accessed via
a purpose-built user interface.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Revisiting 3D ResNets for Video Recognition. (arXiv:2109.01696v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01696">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A recent work from Bello shows that training and scaling strategies may be
more significant than model architectures for visual recognition. This short
note studies effective training and scaling strategies for video recognition
models. We propose a simple scaling strategy for 3D ResNets, in combination
with improved training strategies and minor architectural changes. The
resulting models, termed 3D ResNet-RS, attain competitive performance of 81.0
on Kinetics-400 and 83.8 on Kinetics-600 without pre-training. When pre-trained
on a large Web Video Text dataset, our best model achieves 83.5 and 84.3 on
Kinetics-400 and Kinetics-600. The proposed scaling rule is further evaluated
in a self-supervised setup using contrastive learning, demonstrating improved
performance. Code is available at:
https://github.com/tensorflow/models/tree/master/official.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models. (arXiv:2109.01754v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01754">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Large-scale conversational assistants like Alexa, Siri, Cortana and Google
Assistant process every utterance using multiple models for domain, intent and
named entity recognition. Given the decoupled nature of model development and
large traffic volumes, it is extremely difficult to identify utterances
processed erroneously by such systems. We address this challenge to detect
domain classification errors using offline Transformer models. We combine
utterance encodings from a RoBERTa model with the Nbest hypothesis produced by
the production system. We then fine-tune end-to-end in a multitask setting
using a small dataset of humanannotated utterances with domain classification
errors. We tested our approach for detecting misclassifications from one domain
that accounts for &lt;0.5% of the traffic in a large-scale conversational AI
system. Our approach achieves an F1 score of 30% outperforming a bi- LSTM
baseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this
further by 2.2% to 32.2% by ensembling multiple models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Training Graph Neural Networks by Graphon Estimation. (arXiv:2109.01918v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01918">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we propose to train a graph neural network via resampling from
a graphon estimate obtained from the underlying network data. More
specifically, the graphon or the link probability matrix of the underlying
network is first obtained from which a new network will be resampled and used
during the training process at each layer. Due to the uncertainty induced from
the resampling, it helps mitigate the well-known issue of over-smoothing in a
graph neural network (GNN) model. Our framework is general, computationally
efficient, and conceptually simple. Another appealing feature of our method is
that it requires minimal additional tuning during the training process.
Extensive numerical results show that our approach is competitive with and in
many cases outperform the other over-smoothing reducing GNN training methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Multi-Agent Variational Occlusion Inference Using People as Sensors. (arXiv:2109.02173v1 [cs.RO])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02173">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Autonomous vehicles must reason about spatial occlusions in urban
environments to ensure safety without being overly cautious. Prior work
explored occlusion inference from observed social behaviors of road agents.
Inferring occupancy from agent behaviors is an inherently multimodal problem; a
driver may behave in the same manner for different occupancy patterns ahead of
them (e.g., a driver may move at constant speed in traffic or on an open road).
Past work, however, does not account for this multimodality, thus neglecting to
model this source of aleatoric uncertainty in the relationship between driver
behaviors and their environment. We propose an occlusion inference method that
characterizes observed behaviors of human agents as sensor measurements, and
fuses them with those from a standard sensor suite. To capture the aleatoric
uncertainty, we train a conditional variational autoencoder with a discrete
latent space to learn a multimodal mapping from observed driver trajectories to
an occupancy grid representation of the view ahead of the driver. Our method
handles multi-agent scenarios, combining measurements from multiple observed
drivers using evidential theory to solve the sensor fusion problem. Our
approach is validated on a real-world dataset, outperforming baselines and
demonstrating real-time capable performance. Our code is available at
https://github.com/sisl/MultiAgentVariationalOcclusionInference .</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models. (arXiv:2109.01401v2 [cs.AI] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01401">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose CX-ToM, short for counterfactual explanations with theory-of mind,
a new explainable AI (XAI) framework for explaining decisions made by a deep
convolutional neural network (CNN). In contrast to the current methods in XAI
that generate explanations as a single shot response, we pose explanation as an
iterative communication process, i.e. dialog, between the machine and human
user. More concretely, our CX-ToM framework generates sequence of explanations
in a dialog by mediating the differences between the minds of machine and human
user. To do this, we use Theory of Mind (ToM) which helps us in explicitly
modeling human&#x27;s intention, machine&#x27;s mind as inferred by the human as well as
human&#x27;s mind as inferred by the machine. Moreover, most state-of-the-art XAI
frameworks provide attention (or heat map) based explanations. In our work, we
show that these attention based explanations are not sufficient for increasing
human trust in the underlying CNN model. In CX-ToM, we instead use
counterfactual explanations called fault-lines which we define as follows:
given an input image I for which a CNN classification model M predicts class
c_pred, a fault-line identifies the minimal semantic-level features (e.g.,
stripes on zebra, pointed ears of dog), referred to as explainable concepts,
that need to be added to or deleted from I in order to alter the classification
category of I by M to another specified class c_alt. We argue that, due to the
iterative, conceptual and counterfactual nature of CX-ToM explanations, our
framework is practical and more natural for both expert and non-expert users to
understand the internal workings of complex deep learning models. Extensive
quantitative and qualitative experiments verify our hypotheses, demonstrating
that our CX-ToM significantly outperforms the state-of-the-art explainable AI
models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Temporal Aware Deep Reinforcement Learning. (arXiv:2109.02145v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02145">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The function approximators employed by traditional image based Deep
Reinforcement Learning (DRL) algorithms usually lack a temporal learning
component and instead focus on learning the spatial component. We propose a
technique wherein both temporal as well as spatial components are jointly
learned. Our tested was tested with a generic DQN and it outperformed it in
terms of maximum rewards as well as sample complexity. This algorithm has
implications in the robotics as well as sequential decision making domains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Growing Cosine Unit: A Novel Oscillatory Activation Function That Can Speedup Training and Reduce Parameters in Convolutional Neural Networks. (arXiv:2108.12943v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12943">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Convolution neural networks have been successful in solving many socially
important and economically significant problems. Their ability to learn complex
high-dimensional functions hierarchically can be attributed to the use of
nonlinear activation functions. A key discovery that made training deep
networks feasible was the adoption of the Rectified Linear Unit (ReLU)
activation function to alleviate the vanishing gradient problem caused by using
saturating activation functions. Since then many improved variants of the ReLU
activation have been proposed. However a majority of activation functions used
today are non-oscillatory and monotonically increasing due to their biological
plausibility. This paper demonstrates that oscillatory activation functions can
improve gradient flow and reduce network size. It is shown that oscillatory
activation functions allow neurons to switch classification (sign of output)
within the interior of neuronal hyperplane positive and negative half-spaces
allowing complex decisions with fewer neurons. A new oscillatory activation
function C(z) &#x3D; z cos z that outperforms Sigmoids, Swish, Mish and ReLU on a
variety of architectures and benchmarks is presented. This new activation
function allows even single neurons to exhibit nonlinear decision boundaries.
This paper presents a single neuron solution to the famous XOR problem.
Experimental results indicate that replacing the activation function in the
convolutional layers with C(z) significantly improves performance on CIFAR-10,
CIFAR-100 and Imagenette.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Reinforcement Learning for Battery Energy Storage Dispatch augmented with Model-based Optimizer. (arXiv:2109.01659v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01659">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Reinforcement learning has been found useful in solving optimal power flow
(OPF) problems in electric power distribution systems. However, the use of
largely model-free reinforcement learning algorithms that completely ignore the
physics-based modeling of the power grid compromises the optimizer performance
and poses scalability challenges. This paper proposes a novel approach to
synergistically combine the physics-based models with learning-based algorithms
using imitation learning to solve distribution-level OPF problems.
Specifically, we propose imitation learning based improvements in deep
reinforcement learning (DRL) methods to solve the OPF problem for a specific
case of battery storage dispatch in the power distribution systems. The
proposed imitation learning algorithm uses the approximate optimal solutions
obtained from a linearized model-based OPF solver to provide a good initial
policy for the DRL algorithms while improving the training efficiency. The
effectiveness of the proposed approach is demonstrated using IEEE 34-bus and
123-bus distribution feeders with numerous distribution-level battery storage
systems.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Data Quality Toolkit: Automatic assessment of data quality and remediation for machine learning datasets. (arXiv:2108.05935v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05935">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The quality of training data has a huge impact on the efficiency, accuracy
and complexity of machine learning tasks. Various tools and techniques are
available that assess data quality with respect to general cleaning and
profiling checks. However these techniques are not applicable to detect data
issues in the context of machine learning tasks, like noisy labels, existence
of overlapping classes etc. We attempt to re-look at the data quality issues in
the context of building a machine learning pipeline and build a tool that can
detect, explain and remediate issues in the data, and systematically and
automatically capture all the changes applied to the data. We introduce the
Data Quality Toolkit for machine learning as a library of some key quality
metrics and relevant remediation techniques to analyze and enhance the
readiness of structured training datasets for machine learning projects. The
toolkit can reduce the turn-around times of data preparation pipelines and
streamline the data quality assessment process. Our toolkit is publicly
available via IBM API Hub [1] platform, any developer can assess the data
quality using the IBM&#x27;s Data Quality for AI apis [2]. Detailed tutorials are
also available on IBM Learning Path [3].</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Distributed Learning with Dependent Samples. (arXiv:2002.03757v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.03757">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This paper focuses on learning rate analysis of distributed kernel ridge
regression for strong mixing sequences. Using a recently developed integral
operator approach and a classical covariance inequality for Banach-valued
strong mixing sequences, we succeed in deriving optimal learning rate for
distributed kernel ridge regression. As a byproduct, we also deduce a
sufficient condition for the mixing property to guarantee the optimal learning
rates for kernel ridge regression. Our results extend the applicable range of
distributed learning from i.i.d. samples to non-i.i.d. sequences.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Multi-view Multi-task Learning Framework for Multi-variate Time Series Forecasting. (arXiv:2109.01657v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01657">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Multi-variate time series (MTS) data is a ubiquitous class of data
abstraction in the real world. Any instance of MTS is generated from a hybrid
dynamical system and their specific dynamics are usually unknown. The hybrid
nature of such a dynamical system is a result of complex external attributes,
such as geographic location and time of day, each of which can be categorized
into either spatial attributes or temporal attributes. Therefore, there are two
fundamental views which can be used to analyze MTS data, namely the spatial
view and the temporal view. Moreover, from each of these two views, we can
partition the set of data samples of MTS into disjoint forecasting tasks in
accordance with their associated attribute values. Then, samples of the same
task will manifest similar forthcoming pattern, which is less sophisticated to
be predicted in comparison with the original single-view setting. Considering
this insight, we propose a novel multi-view multi-task (MVMT) learning
framework for MTS forecasting. Instead of being explicitly presented in most
scenarios, MVMT information is deeply concealed in the MTS data, which severely
hinders the model from capturing it naturally. To this end, we develop two
kinds of basic operations, namely task-wise affine transformation and task-wise
normalization, respectively. Applying these two operations with prior knowledge
on the spatial and temporal view allows the model to adaptively extract MVMT
information while predicting. Extensive experiments on three datasets are
conducted to illustrate that canonical architectures can be greatly enhanced by
the MVMT learning framework in terms of both effectiveness and efficiency. In
addition, we design rich case studies to reveal the properties of
representations produced at different phases in the entire prediction
procedure.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations. (arXiv:2109.01924v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01924">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Linguistic entrainment is a phenomenon where people tend to mimic each other
in conversation. The core instrument to quantify entrainment is a linguistic
similarity measure between conversational partners. Most of the current
similarity measures are based on bag-of-words approaches that rely on
linguistic markers, ignoring the overall language structure and dialogue
context. To address this issue, we propose to use a neural network model to
perform the similarity measure for entrainment. Our model is context-aware, and
it further leverages a novel component to learn the shared high-level
linguistic features across dialogues. We first investigate the effectiveness of
our novel component. Then we use the model to perform similarity measure in a
corpus-based entrainment analysis. We observe promising results for both
evaluation tasks.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Confidence Adaptive Regularization for Deep Learning with Noisy Labels. (arXiv:2108.08212v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08212">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Recent studies on the memorization effects of deep neural networks on noisy
labels show that the networks first fit the correctly-labeled training samples
before memorizing the mislabeled samples. Motivated by this early-learning
phenomenon, we propose a novel method to prevent memorization of the mislabeled
samples. Unlike the existing approaches which use the model output to identify
or ignore the mislabeled samples, we introduce an indicator branch to the
original model and enable the model to produce a confidence value for each
sample. The confidence values are incorporated in our loss function which is
learned to assign large confidence values to correctly-labeled samples and
small confidence values to mislabeled samples. We also propose an auxiliary
regularization term to further improve the robustness of the model. To improve
the performance, we gradually correct the noisy labels with a well-designed
target estimation strategy. We provide the theoretical analysis and conduct the
experiments on synthetic and real-world datasets, demonstrating that our
approach achieves comparable results to the state-of-the-art methods.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy. (arXiv:2012.02972v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.02972">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Growing use of machine learning in policy and social impact settings have
raised concerns for fairness implications, especially for racial minorities.
These concerns have generated considerable interest among machine learning and
artificial intelligence researchers, who have developed new methods and
established theoretical bounds for improving fairness, focusing on the source
data, regularization and model training, or post-hoc adjustments to model
scores. However, little work has studied the practical trade-offs between
fairness and accuracy in real-world settings to understand how these bounds and
methods translate into policy choices and impact on society. Our empirical
study fills this gap by investigating the impact of mitigating disparities on
accuracy, focusing on the common context of using machine learning to inform
benefit allocation in resource-constrained programs across education, mental
health, criminal justice, and housing safety. Here we describe applied work in
which we find fairness-accuracy trade-offs to be negligible in practice. In
each setting studied, explicitly focusing on achieving equity and using our
proposed post-hoc disparity mitigation methods, fairness was substantially
improved without sacrificing accuracy. This observation was robust across
policy contexts studied, scale of resources available for intervention, time,
and relative size of the protected groups. These empirical results challenge a
commonly held assumption that reducing disparities either requires accepting an
appreciable drop in accuracy or the development of novel, complex methods,
making reducing disparities in these applications more practical.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Analysis of Discriminator in RKHS Function Space for Kullback-Leibler Divergence Estimation. (arXiv:2002.11187v4 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.11187">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Several scalable sample-based methods to compute the Kullback Leibler (KL)
divergence between two distributions have been proposed and applied in
large-scale machine learning models. While they have been found to be unstable,
the theoretical root cause of the problem is not clear. In this paper, we study
a generative adversarial network based approach that uses a neural network
discriminator to estimate KL divergence. We argue that, in such case, high
fluctuations in the estimates are a consequence of not controlling the
complexity of the discriminator function space. We provide a theoretical
underpinning and remedy for this problem by first constructing a discriminator
in the Reproducing Kernel Hilbert Space (RKHS). This enables us to leverage
sample complexity and mean embedding to theoretically relate the error
probability bound of the KL estimates to the complexity of the discriminator in
RKHS. Based on this theory, we then present a scalable way to control the
complexity of the discriminator for a reliable estimation of KL divergence. We
support both our proposed theory and method to control the complexity of the
RKHS discriminator through controlled experiments.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.10423">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as &quot;review&quot; trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the &quot;review&quot; trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Postulating Exoplanetary Habitability via a Novel Anomaly Detection Method. (arXiv:2109.02273v1 [astro-ph.EP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02273">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>A profound shift in the study of cosmology came with the discovery of
thousands of exoplanets and the possibility of the existence of billions of
them in our Galaxy. The biggest goal in these searches is whether there are
other life-harbouring planets. However, the question which of these detected
planets are habitable, potentially-habitable, or maybe even inhabited, is still
not answered. Some potentially habitable exoplanets have been hypothesized, but
since Earth is the only known habitable planet, measures of habitability are
necessarily determined with Earth as the reference. Several recent works
introduced new habitability metrics based on optimization methods.
Classification of potentially habitable exoplanets using supervised learning is
another emerging area of study. However, both modeling and supervised learning
approaches suffer from drawbacks. We propose an anomaly detection method, the
Multi-Stage Memetic Algorithm (MSMA), to detect anomalies and extend it to an
unsupervised clustering algorithm MSMVMCA to use it to detect potentially
habitable exoplanets as anomalies. The algorithm is based on the postulate that
Earth is an anomaly, with the possibility of existence of few other anomalies
among thousands of data points. We describe an MSMA-based clustering approach
with a novel distance function to detect habitable candidates as anomalies
(including Earth). The results are cross-matched with the habitable exoplanet
catalog (PHL-HEC) of the Planetary Habitability Laboratory (PHL) with both
optimistic and conservative lists of potentially habitable exoplanets.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FBCNN: A Deep Neural Network Architecture for Portable and Fast Brain-Computer Interfaces. (arXiv:2109.02165v1 [eess.SP])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02165">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Objective: To propose a novel deep neural network (DNN) architecture -- the
filter bank convolutional neural network (FBCNN) -- to improve SSVEP
classification in single-channel BCIs with small data lengths.

Methods: We propose two models: the FBCNN-2D and the FBCNN-3D. The FBCNN-2D
utilizes a filter bank to create sub-band components of the
electroencephalography (EEG) signal, which it transforms using the fast Fourier
transform (FFT) and analyzes with a 2D CNN. The FBCNN-3D utilizes the same
filter bank, but it transforms the sub-band components into spectrograms via
short-time Fourier transform (STFT), and analyzes them with a 3D CNN. We made
use of transfer learning. To train the FBCNN-3D, we proposed a new technique,
called inter-dimensional transfer learning, to transfer knowledge from a 2D DNN
to a 3D DNN. Our BCI was conceived so as not to require calibration from the
final user: therefore, the test subject data was separated from training and
validation.

Results: The mean test accuracy was 85.7% for the FBCCA-2D and 85% for the
FBCCA-3D. Mean F1-Scores were 0.858 and 0.853. Alternative classification
methods, SVM, FBCCA and a CNN, had mean accuracy of 79.2%, 80.1% and 81.4%,
respectively.

Conclusion: The FBCNNs surpassed traditional SSVEP classification methods in
our simulated BCI, by a considerable margin (about 5% higher accuracy).
Transfer learning and inter-dimensional transfer learning made training much
faster and more predictable.

Significance: We proposed a new and flexible type of DNN, which had a better
performance than standard methods in SSVEP classification for portable and fast
BCIs.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Does Melania Trump have a body double from the perspective of automatic face recognition?. (arXiv:2109.02283v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02283">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we explore whether automatic face recognition can help in
verifying widespread misinformation on social media, particularly conspiracy
theories that are based on the existence of body doubles. The conspiracy theory
addressed in this paper is the case of the Melania Trump body double. We
employed four different state-of-the-art descriptors for face recognition to
verify the integrity of the claim of the studied conspiracy theory. In
addition, we assessed the impact of different image quality metrics on the
variation of face recognition results. Two sets of image quality metrics were
considered: acquisition-related metrics and subject-related metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">SEC4SR: A Security Analysis Platform for Speaker Recognition. (arXiv:2109.01766v1 [cs.CR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01766">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Adversarial attacks have been expanded to speaker recognition (SR). However,
existing attacks are often assessed using different SR models, recognition
tasks and datasets, and only few adversarial defenses borrowed from computer
vision are considered. Yet,these defenses have not been thoroughly evaluated
against adaptive attacks. Thus, there is still a lack of quantitative
understanding about the strengths and limitations of adversarial attacks and
defenses. More effective defenses are also required for securing SR systems. To
bridge this gap, we present SEC4SR, the first platform enabling researchers to
systematically and comprehensively evaluate adversarial attacks and defenses in
SR. SEC4SR incorporates 4 white-box and 2 black-box attacks, 24 defenses
including our novel feature-level transformations. It also contains techniques
for mounting adaptive attacks. Using SEC4SR, we conduct thus far the
largest-scale empirical study on adversarial attacks and defenses in SR,
involving 23 defenses, 15 attacks and 4 attack settings. Our study provides
lots of useful findings that may advance future research: such as (1) all the
transformations slightly degrade accuracy on benign examples and their
effectiveness vary with attacks; (2) most transformations become less effective
under adaptive attacks, but some transformations become more effective; (3) few
transformations combined with adversarial training yield stronger defenses over
some but not all attacks, while our feature-level transformation combined with
adversarial training yields the strongest defense over all the attacks.
Extensive experiments demonstrate capabilities and advantages of SEC4SR which
can benefit future research in SR.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Uncovering the Limits of Text-based Emotion Detection. (arXiv:2109.01900v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01900">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Identifying emotions from text is crucial for a variety of real world tasks.
We consider the two largest now-available corpora for emotion classification:
GoEmotions, with 58k messages labelled by readers, and Vent, with 33M
writer-labelled messages. We design a benchmark and evaluate several feature
spaces and learning algorithms, including two simple yet novel models on top of
BERT that outperform previous strong baselines on GoEmotions. Through an
experiment with human participants, we also analyze the differences between how
writers express emotions and how readers perceive them. Our results suggest
that emotions expressed by writers are harder to identify than emotions that
readers perceive. We share a public web interface for researchers to explore
our models.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Comparing the Machine Readability of Traffic Sign Pictograms in Austria and Germany. (arXiv:2109.02362v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02362">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We compare the machine readability of pictograms found on Austrian and German
traffic signs. To that end, we train classification models on synthetic data
sets and evaluate their classification accuracy in a controlled setting. In
particular, we focus on differences between currently deployed pictograms in
the two countries, and a set of new pictograms designed to increase human
readability. Besides other results, we find that machine-learning models
generalize poorly to data sets with pictogram designs they have not been
trained on. We conclude that manufacturers of advanced driver-assistance
systems (ADAS) must take special care to properly address small visual
differences between current and newly designed traffic sign pictograms, as well
as between pictograms from different countries.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Supervised DKRC with Images for Offline System Identification. (arXiv:2109.02241v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02241">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Koopman spectral theory has provided a new perspective in the field of
dynamical systems in recent years. Modern dynamical systems are becoming
increasingly non-linear and complex, and there is a need for a framework to
model these systems in a compact and comprehensive representation for
prediction and control. The central problem in applying Koopman theory to a
system of interest is that the choice of finite-dimensional basis functions is
typically done apriori, using expert knowledge of the systems dynamics. Our
approach learns these basis functions using a supervised learning approach
where a combination of autoencoders and deep neural networks learn the basis
functions for any given system. We demonstrate this approach on a simple
pendulum example in which we obtain a linear representation of the non-linear
system and then predict the future state trajectories given some initial
conditions. We also explore how changing the input representation of the
dynamic systems time series data can impact the quality of learned basis
functions. This alternative representation is compared to the traditional raw
time series data approach to determine which method results in lower
reconstruction and prediction error of the true non-linear dynamics of the
system.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v1 [cs.CL])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01951">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>The task of learning from only a few examples (called a few-shot setting) is
of key importance and relevance to a real-world setting. For question answering
(QA), the current state-of-the-art pre-trained models typically need
fine-tuning on tens of thousands of examples to obtain good results. Their
performance degrades significantly in a few-shot setting (&lt; 100 examples). To
address this, we propose a simple fine-tuning framework that leverages
pre-trained text-to-text models and is directly aligned with their pre-training
framework. Specifically, we construct the input as a concatenation of the
question, a mask token representing the answer span and a context. Given this
input, the model is fine-tuned using the same objective as that of its
pre-training objective. Through experimental studies on various few-shot
configurations, we show that this formulation leads to significant gains on
multiple QA benchmarks (an absolute gain of 34.2 F1 points on average when
there are only 16 training examples). The gains extend further when used with
larger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)
and translate well to a multilingual setting . On the multilingual TydiQA
benchmark, our model outperforms the XLM-Roberta-large by an absolute margin of
upto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;&#x3D; 64
training examples). We conduct detailed ablation studies to analyze factors
contributing to these gains.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">MLCTR: A Fast Scalable Coupled Tensor Completion Based on Multi-Layer Non-Linear Matrix Factorization. (arXiv:2109.01773v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01773">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>Firms earning prediction plays a vital role in investment decisions,
dividends expectation, and share price. It often involves multiple
tensor-compatible datasets with non-linear multi-way relationships,
spatiotemporal structures, and different levels of sparsity. Current non-linear
tensor completion algorithms tend to learn noisy embedding and incur
overfitting. This paper focuses on the embedding learning aspect of the tensor
completion problem and proposes a new multi-layer neural network architecture
for tensor factorization and completion (MLCTR). The network architecture
entails multiple advantages: a series of low-rank matrix factorizations (MF)
building blocks to minimize overfitting, interleaved transfer functions in each
layer for non-linearity, and by-pass connections to reduce the gradient
diminishing problem and increase the depths of neural networks. Furthermore,
the model employs Stochastic Gradient Descent(SGD) based optimization for fast
convergence in training. Our algorithm is highly efficient for imputing missing
values in the EPS data. Experiments confirm that our strategy of incorporating
non-linearity in factor matrices demonstrates impressive performance in
embedding learning and end-to-end tensor models, and outperforms approaches
with non-linearity in the phase of reconstructing tensors from factor matrices.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Barycenteric distribution alignment and manifold-restricted invertibility for domain generalization. (arXiv:2109.01902v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01902">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(2 min)</span>
                    <span>For the Domain Generalization (DG) problem where the hypotheses are composed
of a common representation function followed by a labeling function, we point
out a shortcoming in existing approaches that fail to explicitly optimize for a
term, appearing in a well-known and widely adopted upper bound to the risk on
the unseen domain, that is dependent on the representation to be learned. To
this end, we first derive a novel upper bound to the prediction risk. We show
that imposing a mild assumption on the representation to be learned, namely
manifold restricted invertibility, is sufficient to deal with this issue.
Further, unlike existing approaches, our novel upper bound doesn&#x27;t require the
assumption of Lipschitzness of the loss function. In addition, the
distributional discrepancy in the representation space is handled via the
Wasserstein-2 barycenter cost. In this context, we creatively leverage old and
recent transport inequalities, which link various optimal transport metrics, in
particular the $L^1$ distance (also known as the total variation distance) and
the Wasserstein-2 distances, with the Kullback-Liebler divergence. These
analyses and insights motivate a new representation learning cost for DG that
additively balances three competing objectives: 1) minimizing classification
error across seen domains via cross-entropy, 2) enforcing domain-invariance in
the representation space via the Wasserstein-2 barycenter cost, and 3)
promoting non-degenerate, nearly-invertible representation via one of two
mechanisms, viz., an autoencoder-based reconstruction loss or a mutual
information loss. It is to be noted that the proposed algorithms completely
bypass the use of any adversarial training mechanism that is typical of many
current domain generalization approaches. Simulation results on several
standard datasets demonstrate superior performance compared to several
well-known DG algorithms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Characterization and Prediction of Deep Learning Workloads in Large-Scale GPU Datacenters. (arXiv:2109.01313v2 [cs.DC] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01313">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Modern GPU datacenters are critical for delivering Deep Learning (DL) models
and services in both the research community and industry. When operating a
datacenter, optimization of resource scheduling and management can bring
significant financial benefits. Achieving this goal requires a deep
understanding of the job features and user behaviors. We present a
comprehensive study about the characteristics of DL jobs and resource
management. First, we perform a large-scale analysis of real-world job traces
from SenseTime. We uncover some interesting conclusions from the perspectives
of clusters, jobs and users, which can facilitate the cluster system designs.
Second, we introduce a general-purpose framework, which manages resources based
on historical data. As case studies, we design: a Quasi-Shortest-Service-First
scheduling service, which can minimize the cluster-wide average job completion
time by up to 6.5x; and a Cluster Energy Saving service, which improves overall
cluster utilization by up to 13%.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Global-Local Item Embedding for Temporal Set Prediction. (arXiv:2109.02074v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02074">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Temporal set prediction is becoming increasingly important as many companies
employ recommender systems in their online businesses, e.g., personalized
purchase prediction of shopping baskets. While most previous techniques have
focused on leveraging a user&#x27;s history, the study of combining it with others&#x27;
histories remains untapped potential. This paper proposes Global-Local Item
Embedding (GLOIE) that learns to utilize the temporal properties of sets across
whole users as well as within a user by coining the names as global and local
information to distinguish the two temporal patterns. GLOIE uses Variational
Autoencoder (VAE) and dynamic graph-based model to capture global and local
information and then applies attention to integrate resulting item embeddings.
Additionally, we propose to use Tweedie output for the decoder of VAE as it can
easily model zero-inflated and long-tailed distribution, which is more suitable
for several real-world data distributions than Gaussian or multinomial
counterparts. When evaluated on three public benchmarks, our algorithm
consistently outperforms previous state-of-the-art methods in most ranking
metrics.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Neural Operator: Learning Maps Between Function Spaces. (arXiv:2108.08481v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08481">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The classical development of neural networks has primarily focused on
learning mappings between finite dimensional Euclidean spaces or finite sets.
We propose a generalization of neural networks tailored to learn operators
mapping between infinite dimensional function spaces. We formulate the
approximation of operators by composition of a class of linear integral
operators and nonlinear activation functions, so that the composed operator can
approximate complex nonlinear operators. We prove a universal approximation
theorem for our construction. Furthermore, we introduce four classes of
operator parameterizations: graph-based operators, low-rank operators,
multipole graph-based operators, and Fourier operators and describe efficient
algorithms for computing with each one. The proposed neural operators are
resolution-invariant: they share the same network parameters between different
discretizations of the underlying function spaces and can be used for zero-shot
super-resolutions. Numerically, the proposed models show superior performance
compared to existing machine learning based methodologies on Burgers&#x27; equation,
Darcy flow, and the Navier-Stokes equation, while being several order of
magnitude faster compared to conventional PDE solvers.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning. (arXiv:2109.02355v1 [stat.ML])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02355">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The rapid recent progress in machine learning (ML) has raised a number of
scientific questions that challenge the longstanding dogma of the field. One of
the most important riddles is the good empirical generalization of
overparameterized models. Overparameterized models are excessively complex with
respect to the size of the training dataset, which results in them perfectly
fitting (i.e., interpolating) the training data, which is usually noisy. Such
interpolation of noisy data is traditionally associated with detrimental
overfitting, and yet a wide range of interpolating models -- from simple linear
models to deep neural networks -- have recently been observed to generalize
extremely well on fresh test data. Indeed, the recently discovered double
descent phenomenon has revealed that highly overparameterized models often
improve over the best underparameterized model in test performance.

Understanding learning in this overparameterized regime requires new theory
and foundational empirical studies, even for the simplest case of the linear
model. The underpinnings of this understanding have been laid in very recent
analyses of overparameterized linear regression and related statistical
learning tasks, which resulted in precise analytic characterizations of double
descent. This paper provides a succinct overview of this emerging theory of
overparameterized ML (henceforth abbreviated as TOPML) that explains these
recent findings through a statistical signal processing perspective. We
emphasize the unique aspects that define the TOPML research area as a subfield
of modern ML theory and outline interesting open questions that remain.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning. (arXiv:1910.01723v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1910.01723">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>It is notoriously difficult to control the behavior of reinforcement learning
agents. Agents often learn to exploit the environment or reward signal and need
to be retrained multiple times. The multi-objective reinforcement learning
(MORL) framework separates a reward function into several objectives. An ideal
MORL agent learns to generalize to novel combinations of objectives allowing
for better control of an agent&#x27;s behavior without requiring retraining. Many
MORL approaches use a weight vector to parameterize the importance of each
objective. However, this approach suffers from lack of expressiveness and
interpretability. We propose using propositional logic to specify the
importance of multiple objectives. By using a logic where predicates correspond
directly to objectives, specifications are inherently more interpretable.
Additionally the set of specifications that can be expressed with formal
languages is a superset of what can be expressed by weight vectors. In this
paper, we define a formal language based on propositional logic with
quantitative semantics. We encode logical specifications using a recurrent
neural network and show that MORL agents parameterized by these encodings are
able to generalize to novel specifications over objectives and achieve
performance comparable to single objective baselines.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Thompson Sampling for Bandits with Clustered Arms. (arXiv:2109.01656v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01656">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>We propose algorithms based on a multi-level Thompson sampling scheme, for
the stochastic multi-armed bandit and its contextual variant with linear
expected rewards, in the setting where arms are clustered. We show, both
theoretically and empirically, how exploiting a given cluster structure can
significantly improve the regret and computational cost compared to using
standard Thompson sampling. In the case of the stochastic multi-armed bandit we
give upper bounds on the expected cumulative regret showing how it depends on
the quality of the clustering. Finally, we perform an empirical evaluation
showing that our algorithms perform well compared to previously proposed
algorithms for bandits with clustered arms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Sense and Learn: Self-Supervision for Omnipresent Sensors. (arXiv:2009.13233v2 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13233">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Learning general-purpose representations from multisensor data produced by
the omnipresent sensing systems (or IoT in general) has numerous applications
in diverse use cases. Existing purely supervised end-to-end deep learning
techniques depend on the availability of a massive amount of well-curated data,
acquiring which is notoriously difficult but required to achieve a sufficient
level of generalization on a task of interest. In this work, we leverage the
self-supervised learning paradigm towards realizing the vision of continual
learning from unlabeled inputs. We present a generalized framework named Sense
and Learn for representation or feature learning from raw sensory data. It
consists of several auxiliary tasks that can learn high-level and broadly
useful features entirely from unannotated data without any human involvement in
the tedious labeling process. We demonstrate the efficacy of our approach on
several publicly available datasets from different domains and in various
settings, including linear separability, semi-supervised or few shot learning,
and transfer learning. Our methodology achieves results that are competitive
with the supervised approaches and close the gap through fine-tuning a network
while learning the downstream tasks in most cases. In particular, we show that
the self-supervised network can be utilized as initialization to significantly
boost the performance in a low-data regime with as few as 5 labeled instances
per class, which is of high practical importance to real-world problems.
Likewise, the learned representations with self-supervision are found to be
highly transferable between related datasets, even when few labeled instances
are available from the target domains. The self-learning nature of our
methodology opens up exciting possibilities for on-device continual learning.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Connecting GANs, MFGs, and OT. (arXiv:2002.04112v4 [cs.GT] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2002.04112">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Generative adversarial networks (GANs) have enjoyed tremendous success in
image generation and processing, and have recently attracted growing interests
in financial modelings. This paper analyzes GANs from the perspectives of
mean-field games (MFGs) and optimal transport. More specifically, from the game
theoretical perspective, GANs are interpreted as MFGs under Pareto Optimality
criterion or mean-field controls; from the optimal transport perspective, GANs
are to minimize the optimal transport cost indexed by the generator from the
known latent distribution to the unknown true distribution of data. The MFGs
perspective of GANs leads to a GAN-based computational method (MFGANs) to solve
MFGs: one neural network for the backward Hamilton-Jacobi-Bellman equation and
one neural network for the forward Fokker-Planck equation, with the two neural
networks trained in an adversarial way. Numerical experiments demonstrate
superior performance of this proposed algorithm, especially in the higher
dimensional case, when compared with existing neural network approaches.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Exathlon: A Benchmark for Explainable Anomaly Detection over Time Series. (arXiv:2010.05073v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.05073">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Access to high-quality data repositories and benchmarks have been
instrumental in advancing the state of the art in many experimental research
domains. While advanced analytics tasks over time series data have been gaining
lots of attention, lack of such community resources severely limits scientific
progress. In this paper, we present Exathlon, the first comprehensive public
benchmark for explainable anomaly detection over high-dimensional time series
data. Exathlon has been systematically constructed based on real data traces
from repeated executions of large-scale stream processing jobs on an Apache
Spark cluster. Some of these executions were intentionally disturbed by
introducing instances of six different types of anomalous events (e.g.,
misbehaving inputs, resource contention, process failures). For each of the
anomaly instances, ground truth labels for the root cause interval as well as
those for the extended effect interval are provided, supporting the development
and evaluation of a wide range of anomaly detection (AD) and explanation
discovery (ED) tasks. We demonstrate the practical utility of Exathlon&#x27;s
dataset, evaluation methodology, and end-to-end data science pipeline design
through an experimental study with three state-of-the-art AD and ED techniques.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">High-quality Thermal Gibbs Sampling with Quantum Annealing Hardware. (arXiv:2109.01690v1 [quant-ph])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01690">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Quantum Annealing (QA) was originally intended for accelerating the solution
of combinatorial optimization tasks that have natural encodings as Ising
models. However, recent experiments on QA hardware platforms have demonstrated
that, in the operating regime corresponding to weak interactions, the QA
hardware behaves like a noisy Gibbs sampler at a hardware-specific effective
temperature. This work builds on those insights and identifies a class of small
hardware-native Ising models that are robust to noise effects and proposes a
novel procedure for executing these models on QA hardware to maximize Gibbs
sampling performance. Experimental results indicate that the proposed protocol
results in high-quality Gibbs samples from a hardware-specific effective
temperature and that the QA annealing time can be used to adjust the effective
temperature of the output distribution. The procedure proposed in this work
provides a new approach to using QA hardware for Ising model sampling
presenting potential new opportunities for applications in machine learning and
physics simulation.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">CodeNeRF: Disentangled Neural Radiance Fields for Object Categories. (arXiv:2109.01750v1 [cs.GR])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01750">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>CodeNeRF is an implicit 3D neural representation that learns the variation of
object shapes and textures across a category and can be trained, from a set of
posed images, to synthesize novel views of unseen objects. Unlike the original
NeRF, which is scene specific, CodeNeRF learns to disentangle shape and texture
by learning separate embeddings. At test time, given a single unposed image of
an unseen object, CodeNeRF jointly estimates camera viewpoint, and shape and
appearance codes via optimization. Unseen objects can be reconstructed from a
single image, and then rendered from new viewpoints or their shape and texture
edited by varying the latent codes. We conduct experiments on the SRN
benchmark, which show that CodeNeRF generalises well to unseen objects and
achieves on-par performance with methods that require known camera pose at test
time. Our results on real-world images demonstrate that CodeNeRF can bridge the
sim-to-real gap. Project page: \url{https://github.com/wayne1123/code-nerf}</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Towards a Common Testing Terminology for Software Engineering and Artificial Intelligence Experts. (arXiv:2108.13837v2 [cs.SE] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13837">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Analytical quality assurance, especially testing, is an integral part of
software-intensive system development. With the increased usage of Artificial
Intelligence (AI) and Machine Learning (ML) as part of such systems, this
becomes more difficult as well-understood software testing approaches cannot be
applied directly to the AI-enabled parts of the system. The required adaptation
of classical testing approaches and development of new concepts for AI would
benefit from a deeper understanding and exchange between AI and software
engineering experts. A major obstacle on this way, we see in the different
terminologies used in the two communities. As we consider a mutual
understanding of the testing terminology as a key, this paper contributes a
mapping between the most important concepts from classical software testing and
AI testing. In the mapping, we highlight differences in relevance and naming of
the mapped concepts.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Timbre Transfer with Variational Auto Encoding and Cycle-Consistent Adversarial Networks. (arXiv:2109.02096v1 [cs.SD])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02096">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>This research project investigates the application of deep learning to timbre
transfer, where the timbre of a source audio can be converted to the timbre of
a target audio with minimal loss in quality. The adopted approach combines
Variational Autoencoders with Generative Adversarial Networks to construct
meaningful representations of the source audio and produce realistic
generations of the target audio and is applied to the Flickr 8k Audio dataset
for transferring the vocal timbre between speakers and the URMP dataset for
transferring the musical timbre between instruments. Furthermore, variations of
the adopted approach are trained, and generalised performance is compared using
the metrics SSIM (Structural Similarity Index) and FAD (Frech\&#x27;et Audio
Distance). It was found that a many-to-many approach supersedes a one-to-one
approach in terms of reconstructive capabilities, and that the adoption of a
basic over a bottleneck residual block design is more suitable for enriching
content information about a latent space. It was also found that the decision
on whether cyclic loss takes on a variational autoencoder or vanilla
autoencoder approach does not have a significant impact on reconstructive and
adversarial translation aspects of the model.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Fast Hypergraph Regularized Nonnegative Tensor Ring Factorization Based on Low-Rank Approximation. (arXiv:2109.02314v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02314">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>For the high dimensional data representation, nonnegative tensor ring (NTR)
decomposition equipped with manifold learning has become a promising model to
exploit the multi-dimensional structure and extract the feature from tensor
data. However, the existing methods such as graph regularized tensor ring
decomposition (GNTR) only models the pair-wise similarities of objects. For
tensor data with complex manifold structure, the graph can not exactly
construct similarity relationships. In this paper, in order to effectively
utilize the higher-dimensional and complicated similarities among objects, we
introduce hypergraph to the framework of NTR to further enhance the feature
extraction, upon which a hypergraph regularized nonnegative tensor ring
decomposition (HGNTR) method is developed. To reduce the computational
complexity and suppress the noise, we apply the low-rank approximation trick to
accelerate HGNTR (called LraHGNTR). Our experimental results show that compared
with other state-of-the-art algorithms, the proposed HGNTR and LraHGNTR can
achieve higher performance in clustering tasks, in addition, LraHGNTR can
greatly reduce running time without decreasing accuracy.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On Faster Convergence of Scaled Sign Gradient Descent. (arXiv:2109.01806v1 [math.OC])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Communication has been seen as a significant bottleneck in industrial
applications over large-scale networks. To alleviate the communication burden,
sign-based optimization algorithms have gained popularity recently in both
industrial and academic communities, which is shown to be closely related to
adaptive gradient methods, such as Adam. Along this line, this paper
investigates faster convergence for a variant of sign-based gradient descent,
called scaled signGD, in three cases: 1) the objective function is strongly
convex; 2) the objective function is nonconvex but satisfies the
Polyak-Lojasiewicz (PL) inequality; 3) the gradient is stochastic, called
scaled signGD in this case. For the first two cases, it can be shown that the
scaled signGD converges at a linear rate. For case 3), the algorithm is shown
to converge linearly to a neighborhood of the optimal value when a constant
learning rate is employed, and the algorithm converges at a rate of $O(1/k)$
when using a diminishing learning rate, where $k$ is the iteration number. The
results are also extended to the distributed setting by majority vote in a
parameter-server framework. Finally, numerical experiments on logistic
regression are performed to corroborate the theoretical findings.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms. (arXiv:2109.01768v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01768">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>With AlphaGo defeats top human players, reinforcement learning(RL) algorithms
have gradually become the code-base of building stronger artificial
intelligence(AI). The RL algorithm design firstly needs to adapt to the
specific environment, so the designed environment guides the rapid and profound
development of RL algorithms. However, the existing environments, which can be
divided into real world games and customized toy environments, have obvious
shortcomings. For real world games, it is designed for human entertainment, and
too much difficult for most of RL researchers. For customized toy environments,
there is no widely accepted unified evaluation standard for all RL algorithms.
Therefore, we introduce the first virtual user-friendly environment framework
for RL. In this framework, the environment can be easily configured to realize
all kinds of RL tasks in the mainstream research. Then all the mainstream
state-of-the-art(SOTA) RL algorithms can be conveniently evaluated and
compared. Therefore, our contributions mainly includes the following aspects:
1.single configured environment for all classification of SOTA RL algorithms;
2.combined environment of more than one classification RL algorithms; 3.the
evaluation standard for all kinds of RL algorithms. With all these efforts, a
possibility for breeding an AI with capability of general competency in a
variety of tasks is provided, and maybe it will open up a new chapter for AI.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On robustness of generative representations against catastrophic forgetting. (arXiv:2109.01844v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01844">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Catastrophic forgetting of previously learned knowledge while learning new
tasks is a widely observed limitation of contemporary neural networks. Although
many continual learning methods are proposed to mitigate this drawback, the
main question remains unanswered: what is the root cause of catastrophic
forgetting? In this work, we aim at answering this question by posing and
validating a set of research hypotheses related to the specificity of
representations built internally by neural models. More specifically, we design
a set of empirical evaluations that compare the robustness of representations
in discriminative and generative models against catastrophic forgetting. We
observe that representations learned by discriminative models are more prone to
catastrophic forgetting than their generative counterparts, which sheds new
light on the advantages of developing generative models for continual learning.
Finally, our work opens new research pathways and possibilities to adopt
generative models in continual learning beyond mere replay mechanisms.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Mean-Variance Efficient Reinforcement Learning by Expected Quadratic Utility Maximization. (arXiv:2010.01404v3 [cs.LG] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01404">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Risk management is critical in decision making, and mean-variance (MV)
trade-off is one of the most common criteria. However, in reinforcement
learning (RL) for sequential decision making under uncertainty, most of the
existing methods for MV control suffer from computational difficulties caused
by the double sampling problem. In this paper, in contrast to strict MV
control, we consider learning MV efficient policies that achieve Pareto
efficiency regarding MV trade-off. To achieve this purpose, we train an agent
to maximize the expected quadratic utility function, a common objective of risk
management in finance and economics. We call our approach direct expected
quadratic utility maximization (EQUM). The EQUM does not suffer from the double
sampling issue because it does not include gradient estimation of variance. We
confirm that the maximizer of the objective in the EQUM directly corresponds to
an MV efficient policy under a certain condition. We conduct experiments with
benchmark settings to demonstrate the effectiveness of the EQUM.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Tensor Normalization and Full Distribution Training. (arXiv:2109.02345v1 [cs.CV])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02345">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this work, we introduce pixel wise tensor normalization, which is inserted
after rectifier linear units and, together with batch normalization, provides a
significant improvement in the accuracy of modern deep neural networks. In
addition, this work deals with the robustness of networks. We show that the
factorized superposition of images from the training set and the reformulation
of the multi class problem into a multi-label problem yields significantly more
robust networks. The reformulation and the adjustment of the multi class log
loss also improves the results compared to the overlay with only one class as
label.
https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p&#x3D;%2FTNandFDT&amp;mode&#x3D;list</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Spike and slab variational Bayes for high dimensional logistic regression. (arXiv:2010.11665v2 [stat.ML] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11665">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>Variational Bayes (VB) is a popular scalable alternative to Markov chain
Monte Carlo for Bayesian inference. We study a mean-field spike and slab VB
approximation of widely used Bayesian model selection priors in sparse
high-dimensional logistic regression. We provide non-asymptotic theoretical
guarantees for the VB posterior in both $\ell_2$ and prediction loss for a
sparse truth, giving optimal (minimax) convergence rates. Since the VB
algorithm does not depend on the unknown truth to achieve optimality, our
results shed light on effective prior choices. We confirm the improved
performance of our VB algorithm over common sparse VB approaches in a numerical
study.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Provably Correct Training of Neural Network Controllers Using Reachability Analysis. (arXiv:2102.10806v2 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.10806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we consider the problem of training neural network (NN)
controllers for nonlinear dynamical systems that are guaranteed to satisfy
safety and liveness (e.g., reach-avoid) properties. Our approach is to combine
model-based design methodologies for dynamical systems with data-driven
approaches to achieve this target. We confine our attention to NNs with
Rectifier Linear Unit (ReLU) nonlinearity which are known to represent
Continuous Piece-Wise Affine (CPWA) functions. Given a mathematical model of
the dynamical system, we compute a finite-state abstract model that captures
the closed-loop behavior under all possible CPWA controllers. Using this
finite-state abstract model, our framework identifies a family of CPWA
functions guaranteed to satisfy the safety requirements. We augment the
learning algorithm with a NN weight projection operator during training that
enforces the resulting NN to represent a CPWA function from the provably safe
family of CPWA functions. Moreover, the proposed framework uses the
finite-state abstract model to identify candidate CPWA functions that may
satisfy the liveness properties. Using such candidate CPWA functions, the
proposed framework biases the NN training to achieve the liveness
specification. We show the efficacy of the proposed framework both in
simulation and on an actual robotic vehicle.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">On the stability properties of Gated Recurrent Units neural networks. (arXiv:2011.06806v5 [eess.SY] UPDATED)</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.06806">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>The goal of this paper is to provide sufficient conditions for guaranteeing
the Input-to-State Stability (ISS) and the Incremental Input-to-State Stability
({\delta}ISS) of Gated Recurrent Units (GRUs) neural networks. These
conditions, devised for both single-layer and multi-layer architectures,
consist of nonlinear inequalities on network&#x27;s weights. They can be employed to
check the stability of trained networks, or can be enforced as constraints
during the training procedure of a GRU. The resulting training procedure is
tested on a Quadruple Tank nonlinear benchmark system, showing satisfactory
modeling performances.</span>
                  </div>
                </a>
              </details>
            </article>
            <article>
              <details class="article-expander">
                <summary class="article-expander__title">Nonparametric Extrema Analysis in Time Series for Envelope Extraction, Peak Detection and Clustering. (arXiv:2109.02082v1 [cs.LG])</summary>
                <a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02082">
                  <div class="article-summary-box-inner">
                    <span class="article-reading-time">(0 min)</span>
                    <span>In this paper, we propose a nonparametric approach that can be used in
envelope extraction, peak-burst detection and clustering in time series. Our
problem formalization results in a naturally defined splitting/forking of the
time series. With a possibly hierarchical implementation, it can be used for
various applications in machine learning, signal processing and mathematical
finance. From an incoming input signal, our iterative procedure sequentially
creates two signals (one upper bounding and one lower bounding signal) by
minimizing the cumulative $L_1$ drift. We show that a solution can be
efficiently calculated by use of a Viterbi-like path tracking algorithm
together with an optimal elimination rule. We consider many interesting
settings, where our algorithm has near-linear time complexities.</span>
                  </div>
                </a>
              </details>
            </article>
        </section>
      </li>
    </ul>
  </section>

  <footer>
    <time id="build-timestamp" datetime="2021-09-14T20:21:51.203Z">2021-09-14T20:21:51.203Z</time>
    <span><a class="footer-link" href="https://github.com/osmoscraft/osmosfeed">osmosfeed 1.11.0</a></span>
  </footer>
  <script src="index.js"></script>
  <!-- %before-body-end.html% -->
</body>

</html>